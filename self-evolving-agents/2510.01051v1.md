<a id='d3fbf975-2d38-492f-b7a6-bf23c2c4ae7b'></a>

GEM: A Gym for Agentic LLMs

<a id='aab7c8e8-d2ab-42d8-98ca-c3564e841fb5'></a>

GEM: A GYM FOR AGENTIC LLMS

Zichen Liu†¹², Anya Sims†¹³*, Keyu Duan†¹²*, Changyu Chen†¹´*, Simon Yu⁶⁹, Xiangxin Zhou¹*
Haotian Xu⁷, Shaopan Xiong⁸, Bo Liu², Chenmien Tan⁹, Chuen Yang Beh², Weixun Wang⁸
Hao Zhu⁵, Weiyan Shi⁶, Diyi Yang⁵, Michael Shieh², Yee Whye Teh³, Wee Sun Lee², Min Lin¹
¹Sea AI Lab ²NUS ³Oxford ⁴SMU ⁵Stanford ⁶Northeastern ⁷OpenRLHF ⁸ROLL ⁹RL2

<a id='fa1514c8-162c-4ac7-876c-a25362284ec1'></a>

# ABSTRACT
The training paradigm for large language models (LLMs) is moving from static datasets to experience-based learning, where agents acquire skills via interacting with complex environments. To facilitate this transition we introduce GEM (General Experience Maker), an open-source environment simulator designed for the age of LLMs. Analogous to OpenAI-Gym for traditional reinforcement learning (RL), GEM provides a standardized framework for the environment-agent interface, including asynchronous vectorized execution for high throughput, and flexible wrappers for easy extensibility. GEM also features a diverse suite of environments, robust integrated tools, and single-file example scripts demonstrating using GEM with five popular RL training frameworks. Along with this, we also provide a set of baselines across 24 environments using REINFORCE with _Return Batch Normalization (ReBN)_, which—unlike GRPO—is compatible with the full RL setting of dense per-turn rewards and offers better credit assignment. We further conduct apple-to-apple benchmarking of PPO, GRPO and REINFORCE in both single- and multi-turn settings using GEM to shed light on the algorithmic designs. Lastly, GEM also functions as a convenient evaluation toolkit besides a training environment. We hope this framework can help accelerate future agentic LLM research¹.

<a id='9ca7fde6-05ee-42bf-a192-490fd0805dd9'></a>

<::chart: A grid of 24 line charts (4 rows, 6 columns), each titled with a specific environment and model size (e.g., "game:GuessTheNumber (1.7B)", "rg:simple_equations (1.7B)", "code:CodeContest (4B)", "math:Orz57K+Tool (4B)", "qa:HotpotQA+Tool (4B)"). The y-axis for all charts is labeled "Episode return" and the x-axis is labeled "Policy iteration steps". Each chart displays two learning curves: a solid purple line labeled "w. ReBN" and a dotted purple line labeled "w.o. ReBN", both surrounded by a lighter shaded area indicating variance. In most subplots, the "w. ReBN" curve shows higher episode returns or faster convergence compared to the "w.o. ReBN" curve.
Figure 1: Learning curves of Qwen3-based agents across diverse environments of 5 categories: game (language games); rg (ReasoningGym); code (coding tasks); math (python-integrated math questions); qa (search-integrated general questions). All agents are learned via a simple yet general multi-turn algorithm based on REINFORCE (Algorithm 1). The comparison between two curves in each subplot illustrate the effectiveness of Return Batch Normalization (ReBN).::>

<a id='9f923c43-9a2f-425d-9285-5e4f52448fec'></a>

# 1 INTRODUCTION
Reinforcement learning (RL) (Sutton and Barto, 2018) has emerged as a powerful paradigm for improving the reasoning capabilities of large language models (LLMs) (OpenAI, 2024; Guo et al., 2025). By collecting experience in interactive environments, RL allows agents to learn complex,

<a id='06a4791d-9f37-4470-9b55-d638faba9acd'></a>

†Equal contribution with random order. *Work done during their associate membership at Sea AI Lab.
¹Code is available at: https://github.com/axon-rl/gem.

<a id='3aa975ee-fe1c-446e-9f00-954fe2a1a7b7'></a>

1

<a id='ddd30743-4acf-4f1e-b26b-dcb5cb10d076'></a>

arXiv:2510.01051v1 [cs.LG] 1 Oct 2025

<!-- PAGE BREAK -->

<a id='cf402e13-4709-467d-9de0-381fb6a3c6c6'></a>

GEM: A Gym for Agentic LLMs

<a id='ad883e0e-d155-40f5-973d-1f07f83add17'></a>

open-ended tasks without direct supervision (Silver and Sutton, 2025). This approach promises to create powerful agents for a variety of domains. For instance, an agent could develop entire software modules by writing, testing, and debugging code, while also adapting to integration failures or evolving requirements. Similarly, in scientific discovery, an agent could be trained to develop hypotheses, design relevant experiments, and adjust its long-term strategy based on the results.

<a id='d7268e4c-f8fd-498b-802e-bdf75e778f5a'></a>

However, current research on RL for LLMs has largely focused on single-turn tasks, such as answering math questions or retrieving specific data (Lambert et al., 2024; Guo et al., 2025). While these tasks are a valuable starting point, they significantly oversimplify multi-turn interactions (Liu et al., 2025a). This oversimplification means that algorithms which excel in the single-turn setting (e.g., GRPO (Shao et al., 2024)) are fundamentally inapplicable to full multi-turn problems. If the goal is to train agentic LLMs capable of long-horizon planning, trial-and-error, iterative refinement etc, it is crucial to transition to testbeds that support these more complex multi-turn interactions.

<a id='4581a715-92e0-424a-8fab-8be7c3649105'></a>

To facilitate this next step, we introduce GEM (General Experience Maker), an open-source environment framework for diverse, multi-turn, long-horizon tasks. Motivated by OpenAI-Gym (Brockman et al., 2016) which catalyzed research in traditional RL by providing a unified interface and standardized environments, GEM aims to provide analogous foundational infrastructure for LLM agents. GEM offers a diverse suite of environments spanning single- and multi-turn (over 100 turns) tasks (including tool integrated responses, reasoning games etc), flexible observation and action wrappers, asynchronous parallel execution, and a rich set of tools (python, search, and external MCP compatible tools). Additionally, GEM includes validated baselines and single-file training scripts showcasing seamless integration with five popular RL training frameworks (Oat, Verl, OpenRLHF, ROLL, and RL2—see Section 4.5).

<a id='c573fdfb-fdde-4538-89f3-f9365e1b5d2f'></a>

Besides introducing the GEM framework, this paper also presents and discusses a simple yet effective algorithmic variant of REINFORCE (Williams, 1992) which incorporates Return Batch Normalization (ReBN), a useful technique similar to advantage normalization (Andrychowicz et al., 2021; Liu et al., 2025b) that brings consistent improvements (Figure 1). Unlike GRPO and its variants, REINFORCE with ReBN is fully compatible with the multi-turn RL setting, including turn-level dense rewards and arbitrary discount factors. We further compare REINFORCE-based algorithms with multi-turn PPO (Schulman et al., 2017) and GRPO, showing its theoretical connections and empirical tradeoffs. We also provide case studies on the impact of the discount factor γ on multi-turn learning, extensive results of tool-integrated RL, and performance benchmarks on terminal and MCP usage of strong LLMs using GEM as a unified evaluation toolkit. We hope this framework will accelerate RL research on agentic LLMs and advance progress toward more capable and autonomous AI systems.

<a id='ee99db90-6666-49bf-9a93-a9cf8803a359'></a>

## 2 GEM ENVIRONMENTS

This section introduces GEM's core functionality, covering its main interface (Section 2.1), the environment design (Section 2.2), and advanced features such as asynchronous vectorization and modular wrappers (Sections 2.3 and 2.4).

<a id='0c1ce0d5-4dc8-4d1f-b347-f19664d5f419'></a>

## 2.1 INTERFACE
GEM employs a standardized environment interface closely following the well-established OpenAI Gym API with the main functions being `reset()` and `step()`. A basic agent-environment interaction loop is as follows (multi-agent interface shown in Section D.3):

<a id='3d70f667-8016-42f0-902b-181776be2c24'></a>

```python
1 import gem
2 # gem.print_envs() # to list all available environments
3 env = gem.make("game:GuessTheNumber-v0")
4 observation, info = env.reset()
5 
6 while True:
7     # (1) Agent acting:
8     action = env.sample_random_action()
9     # action = agent.act(observation) # real acting by LLM sampling
10 
11     # (2) Environment execution:
12     next_obs, reward, terminated, truncated, info = env.step(action)
13 
14     # (3) Agent learning:
15     # agent.learn(observation, action, reward)
16 
17     observation = next_obs
18     if terminated or truncated: break
```

<a id='32650fce-7629-4796-b017-2e838f0f98e6'></a>

2

<!-- PAGE BREAK -->

<a id='81743c28-a4d9-41f0-af4f-d574e3a290d1'></a>

GEM: A Gym for Agentic LLMs

<a id='b21e1613-eee2-4072-871c-6eaac052d7c2'></a>

## 2.2 TASKS AND TOOLS
GEM's core environment components are **tasks** and **tools**. Each combination of a task and an optional set of tools constitutes an environment that tests complex capabilities such as reasoning, multi-step planning, and tool use. These environments can therefore be used to benchmark LLMs and to test and develop new algorithms. GEM currently features seven main categories of tasks:

<a id='7c033721-77b5-491e-9663-68bb63b97252'></a>

**Math**: Solve math problems with chain-of-thought reasoning.
**Math with image**: Solve geometry math problems with images using chain-of-thought reasoning.
**Code**: Generate code to solve competitive programming problems.
**Game**: Multi-turn text-based games adapted from TextArena (Guertler et al., 2025).
**QA**: General, potentially knowledge-intensive questions (useful for testing search tool capability).
**ReasoningGym**: A unified interface of ReasoningGym (Stojanovski et al., 2025) which provides 100+ single-turn verifiable tasks.
**Terminal**: Perform complex tasks through a containerized terminal environment.

<a id='0cc1c5f9-65dd-4f60-ba79-c6f3fa17fc07'></a>

GEM's modular design simplifies task integration. Math (with images), code, and QA tasks can be integrated by simply providing a new dataset. Terminal tasks require a new Docker file, instructions, and test cases. New games and other custom tasks can be added by inheriting from GEM's environment base class and defining their state transition and reward logic. In addition, tasks can be augmented with any combination of tools. GEM currently supports:

<a id='7ef3eb7b-d4b9-46fe-9193-f755cb089589'></a>

**Python**: Parses and executes code blocks, returning the stdout or execution error.
**Search**: Parses a query, executes a search against an external engine, and returns the results.
**MCP**: General tool calling to any external servers that conform to the model context protocol.

<a id='c138160a-a8f4-4c6d-9f7e-252ba7f2cacb'></a>

The use of tools converts single-turn tasks, like Math or ReasoningGym, into multi-turn tasks in which an agent can learn to call tools and adapt based on their output.

<a id='55ddad1c-4c07-4650-ab38-d5926bdb765c'></a>

## 2.3 ASYNCHRONOUS VECTORIZATION AND AUTORESET
To facilitate efficient agent RL training, we support parallel execution of vectorized environments via asynchronous tool calls to collect episodes in batches. In addition to the latency reduction, the use of vectorized environments with autoreset streamlines the experience collection logic. Users can run a single .reset() at the initialization stage and simply continue with .step() in the following agent-environment loop for continuous data generation. In addition, the user code can use the returned terminated flag to prevent value bootstrapping across episode boundaries, ensuring the correctness of critic learning. An illustration of the autoreset mechanism can be found in Figure 2.

<a id='34a58b0e-35cd-44e7-af53-c42f585b36c2'></a>

<::diagram: Autoreset in Vectorized Environments::>Figure 2: Illustration of autoreset in vectorized environments. The diagram shows three parallel environments, each executing a sequence of steps, denoted by ".step()" at the top. Each environment is represented by a horizontal track of rectangular boxes connected by arrows. When an environment reaches a "done" state (gray box labeled "done"), it triggers an autoreset (indicated by a curved arrow returning to a new colored box). The subsequent steps in that environment are then shown in a new color, signifying a new episode. For example, in the top environment: after the first "done", it resets to blue steps; after the second "done", it resets to green steps; and after the third "done", it resets to orange steps. In the middle environment: after a "done" state, it resets to blue steps. The bottom environment shows continuous gray steps without a "done" state depicted. A legend on the right lists "Episode 1" (gray), "Episode 2" (blue), "Episode 3" (green), and "Episode 3" (orange), followed by "...". A circular arrow symbol is labeled "autoreset". Autoresetting resets the environment automatically after termination, allowing users to collect batches of episodes by simply running .step() without needing more complicated logic such as keeping track of whether individual episodes have terminated.

<a id='8ae8d171-445e-415d-8b57-fa70ed0880a5'></a>

## 2.4 WRAPPERS
Like in OpenAI-Gym, GEM uses wrappers for easy extensibility. Observation wrappers, for example, control how the episode is converted into an observation. Options include observing just the most recent environment output, a concatenation of all previous environment outputs, a concatenation of all previous environment outputs and actions, or some parsed/summarized version of this. The

<a id='84748534-3334-458f-8926-e92174cf5d2d'></a>

3

<!-- PAGE BREAK -->

<a id='3be08e35-82a5-44af-a395-4a64b1d0ea5e'></a>

GEM: A Gym for Agentic LLMs
---

<a id='2240301f-f69f-4052-9773-d717a36cb53f'></a>

<::The image presents a diagram illustrating different views of agentic Reinforcement Learning (RL), labeled as Figure 3. Green nodes denote tokens responsible for loss. The diagram consists of three sub-diagrams, (a), (b), and (c), arranged in two rows.::>

<::flowchart::>
(a) Single Token as Action:
- A blue square node labeled `o_1[:]` points with an arrow to a dotted rectangular box.
- Inside the dotted box, there's a sequence of green circular nodes, with ellipses on both ends, representing tokens `... a_1^{t-1}, a_1^t, a_1^{t+1} ...`.
- An arrow points from the dotted box to a blue square node labeled `r`.

(c) Whole Interaction as Action:
- A blue square node labeled `o_1[:]` points with an arrow to a dotted rectangular box.
- Inside the dotted box, there's a sequence of circular nodes, with ellipses on both ends, representing tokens `... a_1^{t-1}` (green), `a_1^t` (green), `o_2^1` (blue), `o_2^2` (blue), `a_2^1` (green) `...`.
- An arrow points from the dotted box to a blue square node labeled `r`.

(b) Response as Action:
- A blue square node labeled `o_1[:]` points with an arrow to a dotted rectangular box.
- Inside the first dotted box, there's a sequence of green circular nodes, with ellipses on both ends, representing tokens `... a_1^t, a_1^{t+1} ...`.
- An arrow points from the first dotted box to a blue square node labeled `r_1`.
- An arrow points from `r_1` to a second dotted rectangular box.
- Inside the second dotted box, there's a sequence of blue circular nodes, with ellipses on both ends, representing tokens `... o_2^t, o_2^{t+1} ...`.
- An arrow points from the second dotted box to a blue square node labeled `r_2`.
- An arrow points from `r_2` through an ellipsis to a blue square node labeled `r_N`.
<::>


<a id='4b667aef-64ed-45d4-91dc-390e337a9146'></a>

Python interpreter or database/web search tools are also formulated as wrappers which can be added on top of any specified task environment.

<a id='9b89f70f-5222-48d3-b6d8-0048d792dde5'></a>

# 3 REINFORCEMENT LEARNING WITH GEM

In this section, we begin by describing the main RL formulations for LLMs, including their respective flexibilities and limitations (Section 3.1). Motivated by this, we then present our baseline algorithm which is applicable to the more flexible RL formulation (Section 3.2).

<a id='67269f1a-dbbc-40cf-91ea-c80b807470d5'></a>

## 3.1 PRELIMINARY: LLMS AS AGENTS

There are three main ways of treating LLM-environment interactions in RL algorithms which each have different limitations and strengths:

<a id='a2149110-9b63-4553-8b78-dddba9bfcd2f'></a>

Action = Single token (Figure 3(a)): The first approach is to treat each token generated by the LLM as an individual action (Ziegler et al., 2019). This, however, means that episodes are typically very long (thousands of tokens), and it also requires specifying the reward for the addition of every token, which is difficult to evaluate. Successful applications of RL in this formulation tend to use sparse outcome reward with discount factor ̒ = 1 (Guo et al., 2025).

<a id='edc67f67-1b85-4a58-a2a4-b0e21459e9be'></a>

**Action = Response (Figure 3(b))**: To avoid these complications the second approach is to treat a whole response (a sequence of tokens until an EOS) as a single action² (Ahmadian et al., 2024; Liu et al., 2025a). In answering math problems for example---currently the most common testbed for RL for LLMs---each episode contains a question and response. With this view all episodes therefore have length 1 and the RL problem essentially degenerates to contextual bandits (Abe et al., 2003). This is convenient as it means sample-based advantage estimation methods such as GRPO (Shao et al., 2024) can be applied efficiently, and these have been demonstrated to be highly effective. Extending to multi-turn episodes (e.g. for games or tool use), however, results in an issue: Multi-turn interactions have episode lengths > 1, meaning sample-based advantage estimation methods (e.g., Kazemnejad et al. (2025)) become infeasible (since they require collecting multiple episode completions from each turn (state) in the episode, leading to exponential complexity).

<a id='44b9d584-6b15-4ee4-a803-d29de0628a88'></a>

**Action = Whole interaction (Figure 3(c))**: One approach to make GRPO applicable to multi-turn interactions is to treat the whole interaction as a single action while masking the loss on tool outputs. This view again degenerates the full RL problem back to one-step RL or contextual bandits, meaning GRPO etc. can be applied. However, it requires two compromises: Firstly, it effectively fixes the discount factor at \u03b3 = 1, thus removing the incentive to solve problems quickly. This is significant, for example in Section 4.2 where we show how the optimal search algorithm is only recovered when setting \u03b3 < 1. Secondly, this approach is limited to single trajectory-level rewards, losing fine-grained per-turn credit assignment.

<a id='6012e9c0-41d4-45a5-b546-d0746b026c39'></a>

Many prior works make these concessions and use GRPO in multi-turn LLM RL (Cao et al., 2025; Jiang et al., 2025; Chen et al., 2025a; Jin et al., 2025; Feng et al., 2025a). However, to develop an algorithm compatible with the full RL setting, we go back to the second view (action=response) and employ a simple variant of REINFORCE with Return Batch Normalization (ReBN). Unlike GRPO, this algorithm is compatible with per-step dense rewards and arbitrary discount factors (γ ≤ 1), thus making it significantly more flexible for optimizing LLMs in complex, multi-turn interactive settings.

<a id='6bbc106b-8279-4693-8acf-9981438618b4'></a>

²Ignoring token-level PPO clipping which has no effect if the updates are on-policy.

<a id='940665ad-06c2-48cb-bf1b-e5fce372a3ad'></a>

4

<!-- PAGE BREAK -->

<a id='b1313305-84a6-4931-a9b1-f8392b40c4a8'></a>

GEM: A Gym for Agentic LLMs

<a id='edc4ece0-0e47-4cf9-b72a-6b16f0b01dba'></a>

3.2 BASELINE ALGORITHMS
We start from the foundational on-policy³ policy-gradient method REINFORCE (Williams, 1992),
which optimizes the following objective:

$J_{\text{REINFORCE}}(\theta) = \frac{1}{N} \sum_{n=1}^{N} \sum_{t=0}^{T^{(n)}-1} G_t^{(n)} \log \pi_{\theta}(a_t^{(n)}|s_t^{(n)})$, (1)

<a id='c390f9f3-7079-4aa5-87a8-248ced1a8904'></a>

where $N$ is the batch size, $[s_0, a_0, s_1, ..., a_{T-1}]$ is a sequence of states and actions making up a trajectory in which each $s_t$ and $a_t$ is itself a sequence of tokens, and $G_t = \sum_{k=t}^{T-1} \gamma^{k-t}r_k$ is the return. Though initially designed for single-turn problems (i.e., $T^{(n)} = 1$), GRPO can be extended to multi-turn tasks by sampling a group of $M$ trajectories per initial state and normalizing the trajectory-level reward for each group$^4$:

<a id='e502347b-314f-420a-a81d-f1d05891ba79'></a>

<::J_{GRPO}(\theta) = \frac{1}{N} \sum_{n=1}^{N} \frac{1}{M} \sum_{m=1}^{M} A_{GRPO}^{(n,m)} \sum_{t=0}^{T^{(n,m)}-1} \log \pi_{\theta}(a_{t}^{(n,m)}|s_{t}^{(n,m)}) \quad (2)
: figure::>

<a id='d52bd833-13cb-4b35-aa6f-18b75f7cfb8e'></a>

where A_{GRPO}^{(n,m)}=(\sum_{t=0}^{T-1}r_{t}^{(n,m)}-mean(R))/std(R) with R=\{\sum_{t=0}^{T-1}r_{t}^{(n,m)}\}_{m\in[1,...,M]}. However, this approach has poor credit assignment for multi-turn problems because all turns in the trajectory share the same advantage estimation, and improving it typically requires tree-like sampling which leads to combinatorial explosion. To bypass the expensive sampling from each turn, we can learn a value function to estimate the return G_{t}, known as critic (Sutton and Barto, 2018), which in turn guides the policy learning in the actor-critic architecture. We can compute GAE (Schulman et al., 2015) for the advantage actor-critic (A2C) objective:

<a id='a6a6dd52-e3d0-4a22-a496-cb4e4bdd68e4'></a>

$$J_{A2C}(\theta) = \frac{1}{N} \sum_{n=1}^{N} \sum_{t=0}^{T^{(n)}-1} A_{GAE,t}^{(n)} \log \pi_{\theta}(a_t^{(n)}|s_t^{(n)}). \quad (3)$$

<a id='dd280399-84a4-470b-b20c-8d0cb3aca2e0'></a>

To retain the benefits of fine-grained and stable advantage estimation without the combinatorial explosion or learning an additional critic, we instead use _Return Batch Normalization_ (ReBN). For ReBN the per-transition returns Gᵢ are normalized over the _whole batch of transitions_:

<a id='fb222829-b379-4b6d-b89e-3f6941c78839'></a>

$$\mathcal{J}_{REINFORCE+ReBN}(\theta)=\frac{1}{N}\sum_{n=1}^{N}\sum_{t=0}^{T^{(n)}-1}A_{ReBN,t}^{(n)}log~\pi_{\theta}(a_{t}^{(n)}|s_{t}^{(n)})\quad(4)$$where $A_{ReBN,t}^{(n)}=(G_{t}^{(n)}-mean(\mathbf{G}))/std(\mathbf{G})$, with $\mathbf{G}=\{G_{t}^{(n)}\}_{n\in[1,...,N],t\in[1,...,T^{(n)}-1]}$. Each of these algorithms trains the agent by iterating between two main phases: (A) data collection and (B) policy update. We present the RL loop of Equation (4) in Algorithm 1 in Section \textbf{C} due to space constraint.

<a id='068864b4-e075-4424-a60d-48cd976312a8'></a>

## 4 EMPIRICAL STUDIES WITH GEM

In this section, we demonstrate how GEM can facilitate RL research on agentic LLMs through a series of empirical studies. These include a comprehensive apples-to-apples algorithm benchmarking across eight GEM environments (Section 4.1); analyses of the effects of the discount factor \u03b3 and tool integration (Sections 4.2 and 4.3); an examination of cross-task generalization (Section 4.4); and, finally, a demonstration of GEM's compatibility with five RL training frameworks along with their easily accessible infrastructure benefits (Section 4.5). RL results in a vision-language environment and analysis of a multi-agent environment can be found in Sections D.2 and D.3.

<a id='33f3403a-a59a-47f0-8c18-72d4ed72cb78'></a>

4.1 BENCHMARKING RL ALGORITHMS FOR LLMS
Benchmarking has been critical for the progress of RL, with OpenAI-Gym providing standardized environments that enabled systematic evaluation of algorithms (Raffin et al., 2021; Huang et al., 2022). Following this paradigm, GEM offers a unified testbed for agentic LLMs, where prior work often relied on bespoke tasks that complicate fair comparison. We benchmark all baseline algorithms introduced in Section 3.2 (GRPO, PPO5, REINFORCE, ReBN) across eight GEM environments under a unified experimental protocol. All algorithms are implemented using Oat (Liu et al., 2024) with hyperparameters detailed in Section F. Results are evaluated by mean episode return, sample efficiency, and stability.

<a id='88c4207c-9d06-4711-af64-bdedcc461413'></a>

³Orthogonally, we can also utilize proximal updates (Schulman et al., 2017) to improve sample efficiency.
⁴This is not the original GRPO because we fixed the length bias as noted by Liu et al. (2025b).
⁵PPO in this work generally refers to *turn-level PPO* instead of token-level PPO commonly seen in single-turn dialogue scenarios (Ouyang et al., 2022).

<a id='961c11d8-00e5-4c15-911f-945e68195033'></a>

5

<!-- PAGE BREAK -->

<a id='bee8925b-110f-43a3-aaab-8b713bd6788f'></a>

GEM: A Gym for Agentic LLMs<::chart: The visual content is a figure titled "Figure 4: Algorithm benchmarking using eight representative environments from GEM. All agents are trained from Qwen3-{scale}-Base models, with scale specified in each plot. rg refers to single-turn reasoning tasks from ReasoningGym; game consists of long-horizon language games; qa and math are tool-integrated multi-turn environments." It displays eight line plots arranged in a 2x4 grid. Each plot shows "Episode return" on the y-axis and "Policy iteration steps" on the x-axis. The plots illustrate the performance of different algorithms over policy iteration steps for various environments. The legend at the bottom indicates four algorithms: GRPO (blue line), PPO (orange line), REINFORCE (green line), and REINFORCE w. ReBN (Ours) (red line). Each line has a shaded area representing variability. The plots are:

1.  **rg:arc_1d (1.7B)**: Episode return from 0.0 to 0.6. REINFORCE w. ReBN and PPO show higher returns, reaching around 0.4.
2.  **game:Minesweeper (1.7B)**: Episode return from 0.0 to 0.8. REINFORCE w. ReBN and PPO reach returns around 0.7-0.8.
3.  **rg:acre (1.7B)**: Episode return from 0.0 to 1.0. REINFORCE w. ReBN and PPO achieve returns around 0.8-0.9.
4.  **rg:mini_sudoku (1.7B)**: Episode return from 0.0 to 0.8. REINFORCE w. ReBN and PPO reach returns around 0.7-0.8, while REINFORCE is lower, around 0.5-0.6.
5.  **game:GuessTheNumber (1.7B)**: Episode return from 0.0 to 1.0. All algorithms except GRPO quickly reach high returns, around 0.9-1.0.
6.  **game:Sudoku (1.7B)**: Episode return from 0.0 to 0.8. REINFORCE w. ReBN and PPO show better performance, reaching around 0.7.
7.  **qa:MixedQA+Tool (4B)**: Episode return from 0.0 to 0.8. All algorithms converge to an episode return around 0.4-0.5.
8.  **math:DeepScaleR40K+Tool (4B)**: Episode return from 0.0 to 0.8. All algorithms converge to an episode return around 0.5-0.6.::>

<a id='b7b2ead7-e438-48ba-81f6-b680ab4fca69'></a>

We present all learning curves in Figure 4. We first observe that in all three single-turn environments (labeled with **rg**), GRPO performs reasonably well, defending its effectiveness in single-step RL with verifiable rewards. However, GRPO falls short when it comes to multi-turn environments (GuessTheNumber and Sudoku), where dense per-turn rewards are available and more fine-grained credit assignment is necessary for efficient policy learning, due to a constant advantage estimation across all steps. Such effects are the most profound when the environment's reward structure is inherently non-sparse (**qa** and **math** is less so).

<a id='32a7645e-0e9e-4dde-a336-2e0e826c4839'></a>

In contrast to GRPO, REINFORCE and PPO are natively suitable for multi-turn RL. We find that vanilla REINFORCE is readily a strong baseline in most environments, but it might suffer from suboptimal convergence (e.g., two Sudoku environments). We hypothesize that this might be because the raw return calculation of vanilla REINFORCE can be sensitive to reward shaping, thus hindering exploration; we defer an in-depth ablation study to Section D.1. On the other hand, PPO is generally performant, attaining the best episode return in the complex and long-horizon Sudoku environment. This performance advantage can be attributed to a well-learned critic, but it is also deemed difficult to robustly learn an accurate critic (Van Hasselt et al., 2018; Kazemnejad et al., 2025) (as evidenced by the poor performance of PPO in Minesweeper), inviting future works to go in this direction.

<a id='db433c32-59d1-49d8-9862-012f92075304'></a>

Finally, we investigate the proposed REINFORCE variant, which incorporates a simple Return Batch Normalization (ReBN) technique. Results in both Figures 1 and 4 show that ReBN consistently improves on vanilla REINFORCE by a large margin, suggesting the empirical benefits of adaptive normalization of policy gradient coefficients. Moreover, ReBN outperforms or is comparable with PPO and GRPO in all evaluated environments, rendering it the strongest baseline without expensive computations, such as critic learning or extensive rollouts.

<a id='56fc2286-1c93-49b0-984f-06ca6b4f0335'></a>

4.2 DISCOUNT FACTOR γ MATTERS
<::Figure 5: A set of four line charts titled "(a) Effect of discount factor" and "(b) Effect of tool availability".

**Panel (a) Effect of discount factor** consists of two charts:
1.  **Left chart (Mean episode length):**
    *   Y-axis: Mean episode length (ranging from 0 to 30).
    *   X-axis: Policy iteration steps (ranging from 0 to 500).
    *   Three lines represent different discount factors (gamma):
        *   `gamma = 0.9` (red line): Starts low, peaks around 20 at ~100 steps, then drops to ~5.
        *   `gamma = 0.99` (orange line): Starts low, peaks around 20 at ~100 steps, then drops to ~5.
        *   `gamma = 0.999` (blue-green line): Starts low, peaks around 25 at ~100 steps, then fluctuates between 10 and 20.
2.  **Right chart (Mean episode return):**
    *   Y-axis: Mean episode return (ranging from 0.0 to 1.0).
    *   X-axis: Policy iteration steps (ranging from 0 to 500).
    *   Legend: `gamma = 0.9` (red), `gamma = 0.99` (orange), `gamma = 0.999` (blue-green).
    *   Three lines show the return:
        *   `gamma = 0.9` (red line): Increases rapidly to 1.0 around 150 steps.
        *   `gamma = 0.99` (orange line): Increases rapidly to 1.0 around 250 steps.
        *   `gamma = 0.999` (blue-green line): Increases rapidly to 1.0 around 350 steps.

**Panel (b) Effect of tool availability** consists of two charts, both showing "Mean Episode Success" vs "Policy iteration steps":
1.  **Left chart (math:Orz57K):**
    *   Y-axis: Mean Episode Success (ranging from 0.00 to 0.75).
    *   X-axis: Policy iteration steps (ranging from 0 to 500).
    *   Legend: Tool (red), No tool (blue-green).
    *   Two lines:
        *   "Tool" (red line): Increases to ~0.65-0.70 and plateaus.
        *   "No tool" (blue-green line): Increases to ~0.55-0.60 and plateaus.
2.  **Right chart (qa:HotpotQA):**
    *   Y-axis: Mean Episode Success (ranging from 0.0 to 0.6).
    *   X-axis: Policy iteration steps (ranging from 0 to 500).
    *   Two lines:
        *   "Tool" (red line): Increases to ~0.45-0.55 and plateaus.
        *   "No tool" (blue-green line): Increases to ~0.20-0.25 and plateaus.
: chart::>
Figure 5: (a) Average number of turns and episode return when trained with different discount factors. (b)
Comparative experiment results on tool availability.

<a id='30368f99-6f73-437d-88a4-1d42e4d9fec1'></a>

Next, we investigate the effect of the discount factor \$\gamma\$. A key motivation for REINFORCE+ReBN over GRPO is its compatibility with arbitrary discount factors. To investigate the effect of this

<a id='062dc095-08c3-4bd5-8e58-cc526b5f6e44'></a>

6

<!-- PAGE BREAK -->

<a id='a53257c1-5c3e-40a1-8ecf-4cc3f6856769'></a>

GEM: A Gym for Agentic LLMs

<a id='b9406184-bc4f-4641-ad4f-24117cb5c69a'></a>

we trained the Qwen3-1.7B-Base model (Yang et al., 2025) using REINFORCE+ReBN on the GuessTheNumber environment. In this environment the agent must guess a hidden number randomly selected between 1 and 50. At each turn the agent may guess, and receives feedback as to whether the hidden number is larger or smaller. The optimal strategy is therefore binary search.

<a id='11352946-64cd-470f-8f0a-9bc5ca5e6f91'></a>

As shown in Figure 5(a), as expected, smaller \gamma values naturally encourage solutions with fewer turns and drive convergence to the optimal turn count (log\textsubscript{2}(50) \approx 5.6)—achievable only through binary search. Example interactions are included in Section B. As discussed in Section 3.2, the natural efficiency incentive from \gamma < 1 is not compatible with GRPO. Instead, prior works using GRPO hyperparameter tune the environment's maximum number of turns to get efficient agent behavior (Xue et al., 2025).

<a id='eb05b4d7-4db4-4ae5-a068-87c117fbe214'></a>

## 4.3 TOOL-INTEGRATION IN MATH AND QUESTION-ANSWERING TASKS
GEM is designed with modular support for external tools, enabling seamless integration into a range of tasks. To empirically assess the impact of tool use, we focus on two domains: Math and Question-Answering (QA).

<a id='9a31fea0-e635-46c8-b847-02ab73fd9bd7'></a>

Table 1: Math benchmark scores for four agents, evaluated with and without tool access and RL training. Note: scores should be interpreted relative to other values here due to sensitivity to the grader code (see Section 4.3).
<table id="6-1">
<tr><td id="6-2">Qwen3-4B-Base</td><td id="6-3">Base (no tool)</td><td id="6-4">Base (with tool)</td><td id="6-5">Base + RL (no tool)</td><td id="6-6">Base + RL (with tool)</td></tr>
<tr><td id="6-7">AIME24</td><td id="6-8">10.0</td><td id="6-9">6.7</td><td id="6-a">16.7</td><td id="6-b">30.0</td></tr>
<tr><td id="6-c">AMC</td><td id="6-d">39.8</td><td id="6-e">50.6</td><td id="6-f">49.4</td><td id="6-g">67.5</td></tr>
<tr><td id="6-h">MATH500</td><td id="6-i">61.0</td><td id="6-j">62.4</td><td id="6-k">67.4</td><td id="6-l">71.0</td></tr>
<tr><td id="6-m">MinervaMath</td><td id="6-n">36.4</td><td id="6-o">30.1</td><td id="6-p">40.1</td><td id="6-q">40.4</td></tr>
<tr><td id="6-r">OlympiadBench</td><td id="6-s">29.5</td><td id="6-t">31.0</td><td id="6-u">33.5</td><td id="6-v">39.9</td></tr>
<tr><td id="6-w">Average</td><td id="6-x">35.3</td><td id="6-y">36.2</td><td id="6-z">41.4</td><td id="6-A">49.8</td></tr>
</table>

<a id='c8a1eca4-0c26-4c5e-91a4-c68dcb7d1f79'></a>

We first investigate the effect of GEM's Python tool on Math tasks. Starting from the base model Qwen3-4B-Base, we finetune on the math: Orz57K environment, training two variants: one with Python tool integration and one without. The base model and both finetuned models are then evaluated across five distinct math environments. Hyperparameter details are provided in Section F, with the training curve shown in Figure 5(b), and Pass@1 accuracy reported in Table 1.

<a id='c06289a3-aa4e-4b0d-b170-e605342a6d7d'></a>

The math grader used for reward and evaluation is based on HuggingFace's math_verify library⁶.
We found that even minor differences in grading logic across codebases yields substantial variation
in reported performance. Thus, all results should be interpreted comparatively—within a consistent
evaluation framework—rather than as absolute values. This further highlights the need for unified
benchmarking, as provided by GEM.

<a id='f19fb396-d277-4b8c-9585-b68bc0a19c86'></a>

Results in Table 1 reveal a clear and consistent pattern: across all environments, performance improves substantially after RL training compared to the base model. Furthermore, the model with access to the Python tool achieves higher final performance in every setting.

<a id='9e24ac31-32ab-4daf-8969-3d6f8e4adad2'></a>

Table 2: QA benchmark scores for the base agent and agents trained with different RL configurations. † and *
denote single-hop and multi-hop datasets, respectively.
<table id="6-B">
<tr><td id="6-C">Qwen3-4B</td><td id="6-D">Base (no tool)</td><td id="6-E">Base + RL (no tool, single env)</td><td id="6-F">Base + RL (no tool, mixed env)</td><td id="6-G">Base + RL (with tool, single env)</td><td id="6-H">Base + RL (with tool, mixed env)</td></tr>
<tr><td id="6-I">NQ†</td><td id="6-J">6.1</td><td id="6-K">15.4</td><td id="6-L">15.8</td><td id="6-M">35.0</td><td id="6-N">37.3</td></tr>
<tr><td id="6-O">TriviaQA†</td><td id="6-P">35.4</td><td id="6-Q">43.4</td><td id="6-R">44.9</td><td id="6-S">69.0</td><td id="6-T">71.9</td></tr>
<tr><td id="6-U">PopQA†</td><td id="6-V">11.3</td><td id="6-W">19.0</td><td id="6-X">19.9</td><td id="6-Y">47.1</td><td id="6-Z">48.1</td></tr>
<tr><td id="6-10">HotpotQA*</td><td id="6-11">11.1</td><td id="6-12">21.1</td><td id="6-13">22.1</td><td id="6-14">43.2</td><td id="6-15">45.5</td></tr>
<tr><td id="6-16">2wiki*</td><td id="6-17">10.0</td><td id="6-18">26.8</td><td id="6-19">30.1</td><td id="6-1a">44.5</td><td id="6-1b">46.7</td></tr>
<tr><td id="6-1c">Musique*</td><td id="6-1d">2.9</td><td id="6-1e">4.7</td><td id="6-1f">5.5</td><td id="6-1g">17.6</td><td id="6-1h">19.9</td></tr>
<tr><td id="6-1i">Bamboogle*</td><td id="6-1j">17.6</td><td id="6-1k">28.8</td><td id="6-1l">28.8</td><td id="6-1m">49.6</td><td id="6-1n">48.8</td></tr>
<tr><td id="6-1o">Average</td><td id="6-1p">10.2</td><td id="6-1q">22.7</td><td id="6-1r">23.9</td><td id="6-1s">43.7</td><td id="6-1t">45.5</td></tr>
</table>

<a id='8a4d33ce-3b0e-492d-8977-8d086ca2c85d'></a>

6 github.com/huggingface/Math-Verify

<a id='5a638f11-466e-40b3-aa6d-adfa5f33485d'></a>

7

<!-- PAGE BREAK -->

<a id='c579c749-1dc1-4cfa-b6b5-5a5a3a15f2d8'></a>

GEM: A Gym for Agentic LLMs

<a id='7a51d3a2-8df3-42ee-becd-012e4f882dd3'></a>

We also perform a parallel analysis for QA tasks, this time integrating the Search tool. We train on two environment compositions: qa:HotpotQA alone, and a mixture of both qa:HotpotQA and qa:NaturalQuestions. All other setting are the same as for the Math experiments (see above). Evaluation spans seven diverse QA environments. Results, summarized in Table 2, mirror those from the math domain: RL finetuning markedly improves performance, and models equipped with the Search tool achieve the highest accuracy in every scenario.

<a id='ebc1dc16-f1c6-43f0-8070-1eda31cbd97b'></a>

The consistency of these findings across both domains (mathematics and QA), tools (Python and Search), and multiple evaluation environments underscores the flexibility and robustness of GEM's approach to RL LLM with tool integration.

<a id='364c409d-2d76-43b5-b359-2e6029dd888a'></a>

4.4 STUDYING GENERALIZATION<::The visual content is a figure composed of four line charts, arranged in a 1x4 grid, illustrating training performance and generalization across different environments.All charts share a common x-axis label: "Training Steps".Chart 1: Title: "game:Sudoku-v0-easy". Y-axis: "Mean Episode Return" (ranging from 0.00 to 1.00). The line shows a rapid increase in mean episode return from 0.00 to approximately 0.90 within the first 100 training steps, then gradually reaching and stabilizing near 1.00 by 200 training steps, with a slight variance band.Chart 2: Title: "rg:circuit_logic". Y-axis: "Eval Accuracy" (ranging from 0.3 to 0.7). The line shows evaluation accuracy fluctuating, starting around 0.4, peaking around 0.7 near 300 training steps, and then decreasing and fluctuating between 0.3 and 0.5 towards 500 training steps.Chart 3: Title: "rg:needle_haystack". Y-axis: "Eval Accuracy" (ranging from 0.85 to 0.975). The line shows evaluation accuracy fluctuating at a high level, rapidly increasing from around 0.84 to 0.975 within the first 50 steps, and then fluctuating mostly between 0.90 and 0.975 throughout the training steps.Chart 4: Title: "rg:mini_sudoku". Y-axis: "Eval Accuracy" (ranging from 0.25 to 0.55). The line shows evaluation accuracy fluctuating, starting around 0.28, rising to approximately 0.47, then fluctuating between 0.35 and 0.525 across the training steps.Figure 6: Training on the game : sudoku-v0-easy environment generalizes to ReasoningGym.::>

<a id='1180b44f-3226-4bb0-9e4a-34a2c87a3384'></a>

GEM's environments can be used for both training and evaluation. This makes it ideal for investigating cross-environment generalization. For instance, we demonstrate training on the game: `sudoku-v0-easy` environment, while periodically evaluating on three different environments, with some encouraging initial generalization results shown in Figure 6.

<a id='e240d4c2-4adf-4fba-834a-4ec2331f3ab5'></a>

## 4.5 INTEGRATION WITH TRAINING FRAMEWORKS

Finally, we demonstrate that GEM—which takes care of the environment side—can be easily integrated with five popular frameworks that handle the training side. There has been a proliferation of frameworks focusing on the training side of LLM RL. These often rely heavily on multiple other libraries (such as vLLM for response generation (Kwon et al., 2023), and DeepSpeed for optimization (Rasley et al., 2020)). The diverse range of features and design choices make it challenging for researchers to select and adapt a suitable training framework to their specific needs.

<a id='f5b425f5-c14c-4225-8454-82368fd5d9b3'></a>

To address this GEM comes with complete, single-file training scripts showing clean integration into five widely used LLM RL frameworks: Oat (Liu et al., 2024), Verl (Sheng et al., 2024), OpenRLHF (Hu et al., 2024), ROLL (Wang et al., 2025a), and RL2 (Tan et al., 2025). These are validated in Figure 7(a) where we show the training curve for each. Despite minor differences due to underlying design choices of the frameworks (e.g., different LLM generation engines) and RL stochasticity, all curves exhibit similar trends, demonstrating that GEM is agnostic to training frameworks and validating their implementation equivalence. Furthermore, supporting a wide range of frameworks allows us to effortlessly access their advanced features. For example, enabling the asynchronous rollout in RL2 gives an immediate 2x gain in wall-clock efficiency (Figure 7(b)).

<a id='7bc74089-07ef-4664-b697-36d70c60a397'></a>

<::Figure 7: (a) Training curves on two environments showing successful integration of GEM into five existing frameworks. The figure contains two plots under the heading "(a) Cross-framework comparison". The first plot, titled "game:GuessTheNumber", shows "Mean episode return" on the y-axis and "Policy iteration steps" on the x-axis, with five lines representing "oat", "openrlhf", "rl2", "roll", and "verl". The second plot, titled "rg:LetterCounting", also shows "Mean episode return" on the y-axis and "Policy iteration steps" on the x-axis, with the same five lines. (b) Asynchronous rollout improves wall-clock efficiency of training Sudoku-solving agents based on Qwen3-4B-Base. This part of the figure contains two plots under the heading "(b) Efficiency comparison". The first plot shows an unlabeled y-axis (values from -0.1 to 0.3) against "Policy iteration steps" on the x-axis, with two lines labeled "Sync" and "Async". The second plot shows the same unlabeled y-axis against "Time (in hours)" on the x-axis, also with lines for "Sync" and "Async".: chart::>

<a id='85c2a2f3-c055-42d9-90ac-9f9146e255a5'></a>

8

<!-- PAGE BREAK -->

<a id='5438108a-6ff9-41ef-86e2-111ef834c423'></a>

GEM: A Gym for Agentic LLMs

<a id='f0919959-93a4-4fef-8646-d3e049c8c4a4'></a>

## 5 AGENT EVALUATION WITH GEM

In addition to RL training, GEM can serve as a **unified evaluation interface** to test LLM agents' performance. In this section, we present two example use cases where we evaluate agents powered by strong LLMs (GPT-5 (OpenAI, 2025), Gemini-2.5-Pro (Gemini Team, 2025) and Claude-Sonnet-4 (Anthropic, 2025a)) on two complex tasks: database operation via model context protocol (MCP) (Anthropic, 2025b) and terminal interaction via docker containers, both of which have been added to GEM following Section A.

<a id='101208c5-5e4b-4371-864b-5ec7e4edf086'></a>

5.1 GENERAL TOOL USE VIA MODEL CONTEXT PROTOCOL Modern LLM agents often need to interact with external tools, such as search engines, APIs, and code interpreters. To facilitate this, GEM is designed to be compatible with the MCP, which is an open protocol that provides a standardized way for LLMs to communicate with external tools and data sources. <::chart: The visual content consists of two combined bar and line charts, side-by-side, comparing the performance of different LLMs.Figure 8: Benchmark results on MCPMark (Postgres subset) and Terminal-Bench (subset) using GEM as a unified evalua-

Chart 1: MCPMark Results
- X-axis: GPT-5, Gemini-2.5-Pro, Claude-Sonnet-4
- Left Y-axis: Success rate (from 0.0 to 0.6)
- Right Y-axis: Number of turns (from 0 to 20)
- Data:
  - GPT-5: Success rate 0.65, Number of turns 10.0
  - Gemini-2.5-Pro: Success rate 0.35, Number of turns 14.3
  - Claude-Sonnet-4: Success rate 0.30, Number of turns 20.1

Chart 2: Terminal Bench Results
- X-axis: GPT-5, Gemini-2.5-Pro, Claude-Sonnet-4
- Left Y-axis: Success rate (from 0.0 to 0.5)
- Right Y-axis: Number of turns (from 0 to 100)
- Data:
  - GPT-5: Success rate 0.49, Number of turns 17.4
  - Gemini-2.5-Pro: Success rate 0.30, Number of turns 89.5
  - Claude-Sonnet-4: Success rate 0.33, Number of turns 42.8::>
The MCP architecture consists of an MCP host (tha II Mannlication) on MCD client

<a id='8bb3de15-1dcc-43d7-93ce-1f1c94e22a97'></a>

and an MCP server (the external tool). By
adopting this protocol, GEM allows for "plug-and-play" tool usage, where any tool that implements
the MCP server interface can be used by an agent in a GEM environment without custom integration.
This significantly simplifies the process of creating tool-augmented LLM agents and opens up a vast
ecosystem of potential tools.

<a id='16d46040-fd53-4dd5-8841-ec697c720c8f'></a>

Using a PostgreSQL MCP tool, we assess the agent's tool-augmented reasoning capabilities using 20 database tasks taken from MCPMark (Team, 2025a). We report the average success rate and the average number of turns required to complete the tasks in the left panel of Figure 87. GPT-5 attains the best success rate with the fewest interactions, while Gemini-2.5-Pro and Claude-Sonnet-4 have slightly lower and varied performance.

<a id='02385e0d-fb4d-48d4-84ed-e02c7e06df5a'></a>

5.2 TERMINAL ENVIRONMENT VIA DOCKER CONTAINER
To support a wider range of tasks, especially those involving complex software dependencies and interactions with the operating system, GEM includes support for environments running inside docker containers. The integrated terminal environment provides a sandboxed unix operating system where agents can learn to perform tasks using shell commands. This approach provides a high degree of isolation and reproducibility, ensuring that the environment is consistent across different machines.

<a id='3b695b68-4a1b-41a4-8ae9-07c652a31bdc'></a>

We assess the terminal mastery of LLM agents on 57 tasks sampled from Terminal-Bench (Team, 2025b), without any scaffolding. The right panel of Figure 8 reports the average success rate and the number of turns required to complete the tasks. GPT-5 attains the highest success rate with the most efficient interaction, followed by Claude-Sonnet-4 and Gemini-2.5-Pro. The evaluation leverages the same interaction loop used for RL training, highlighting GEM's role as a unified framework for both reinforcement learning and standardized evaluation.

<a id='0a77bc19-2c78-467d-bef6-8bec0f8cbf79'></a>

## 6 CONCLUSIONS
GEM aims to accelerate agentic LLM research by providing a decoupled and clean library that is agnostic to training frameworks, a unified agent-environment interface and a suite of standardized environments. In this paper, we introduced the design choices of GEM, the current suite of task domains and tools, features like vectorized environment execution, a simple yet general multi-turn REINFORCE algorithm implemented in five training frameworks, a comprehensive algorithm benchmarking evaluation, and in-depth analysis on several algorithmic details. We invite the community to enter the era of experience for LLM agent learning, and join us in both using and continuing to develop the GEM framework.

<a id='df25eace-6877-4995-81e1-69e86a291715'></a>

---^7Our evaluation relies on the basic response generation API rather than agent frameworks (e.g., LangChain, OpenAI Agent SDK), which may lead to deviations from the original benchmark results.

<a id='ca6ad46e-4b63-4a0c-913e-32e7fd1ff99b'></a>

9

<a id='379228fc-d572-4748-9db3-0e95527ccc58'></a>

host (the LLM application), an MCP client, tion toolkit.

<!-- PAGE BREAK -->

<a id='f30930c2-4ae6-4b78-96ac-28ad848da2e7'></a>

GEM: A Gym for Agentic LLMs

<a id='51ce2e77-a4a9-428b-93c1-db81e326277d'></a>

## REFERENCES

Naoki Abe, Alan W Biermann, and Philip M Long. Reinforcement learning with immediate rewards and linear hypotheses. *Algorithmica*, 37(4):263–293, 2003.

Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. *arXiv preprint arXiv:2402.14740*, 2024.

<a id='85eee12c-3083-4f09-aa66-c2f93f8f5a61'></a>

Marcin Andrychowicz, Anton Raichuk, Piotr Stańczyk, Manu Orsini, Sertan Girgin, Raphaël Marinier,
Leonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, et al. What matters for
on-policy deep actor-critic methods? a large-scale study. In *International conference on learning
representations*, 2021.

<a id='43768598-3f48-47d5-a3b4-e9503c7c9e9d'></a>

Anthropic. System card: Claude opus 4 & claude sonnet 4. https://www-cdn.anthropic.com/07b2a3f9902ee19fe39a36ca638e5ae987bc64dd.pdf, 2025a.

<a id='d94048b2-714c-4af1-b00b-a4522974fc57'></a>

Anthropic. Model context protocol. https://github.com/modelcontextprotocol/modelcontextprotocol, 2025b.

<a id='5f328306-3027-4aa2-9724-2a34e43aa068'></a>

Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang,
Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923,
2025.

<a id='1bf09b04-de2d-4850-9d55-b63870c8f530'></a>

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.

<a id='9c211cdf-ad13-43a1-939a-8b2099d21a57'></a>

Shiyi Cao, Sumanth Hegde, Dacheng Li, Tyler Griggs, Shu Liu, Eric Tang, Jiayi Pan, Xingyao Wang,
Akshay Malik, Graham Neubig, Kourosh Hakhamaneshi, Richard Liaw, Philipp Moritz, Matei
Zaharia, Joseph E. Gonzalez, and Ion Stoica. Skyrl-v0: Train real-world long-horizon agents via
reinforcement learning, 2025.

<a id='046a64a2-5a1d-48e5-8604-6e105a8aaf74'></a>

Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z. Pan, Wen Zhang, Huajun Chen, Fan Yang, Zenan Zhou, and Weipeng Chen. Research: Learning to reason with search for llms via reinforcement learning, 2025a.

<a id='52aeda63-7802-4b6a-9d1c-a969fe7478ea'></a>

Wentse Chen, Jiayu Chen, Hao Zhu, and Jeff Schneider. Context-lite multi-turn reinforcement learning for LLM agents. In ES-FoMo III: 3rd Workshop on Efficient Systems for Foundation Models, 2025b. URL https://openreview.net/forum?id=6CE5PLsZdW.

<a id='73469e70-08b6-47c9-86d4-d680c88c8375'></a>

Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang,
Jinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in llms,
2025a.

<a id='8684a527-f121-4569-917d-86e4f454d7f1'></a>

Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. Group-in-group policy optimization for llm agent training. arXiv preprint arXiv:2505.10978, 2025b.

<a id='7ad694a9-b9a0-4c95-b212-c31b84573355'></a>

Google Gemini Team. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025.

<a id='d8b65379-f555-4be5-98a4-ccd4c2f3fa66'></a>

Leon Guertler, Bobby Cheng, Simon Yu, Bo Liu, Leshem Choshen, and Cheston Tan. Textarena.
arXiv preprint arXiv:2504.11442, 2025.

<a id='9f7f5f74-00b8-4956-ac43-221b25ecb3a4'></a>

Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,
Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms
via reinforcement learning. *arXiv preprint arXiv:2501.12948*, 2025.

<a id='13306291-a3cd-454b-97f6-55619effd018'></a>

Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. *arXiv preprint arXiv:2405.11143*,
2024.

<a id='3e42dd36-679d-4a5b-83de-7129401a3ed9'></a>

Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam Chakraborty, Kinal Mehta, and João G.M. Araújo. Cleanrl: High-quality single-file implementations of deep reinforcement learning algorithms. Journal of Machine Learning Research, 23(274):1-18, 2022. URL http://jmlr.org/papers/v23/21-1342.html.

<a id='a5055293-64c5-450f-8b4e-aaddb240df24'></a>

10

<!-- PAGE BREAK -->

<a id='86a1d594-5132-4120-a05c-b36435653620'></a>

GEM: A Gym for Agentic LLMs

<a id='e5663235-72dd-4bbe-94a5-9a1dc91d7f6f'></a>

Dongfu Jiang, Zhuofeng Li, Yi Lu, Zhiheng Lvu, Ping Nie, Wenhu Chen, Tianyu Pang, and Chao Du.
Verltool, 2025. URL https://github.com/TIGER-AI-Lab/verl-tool.

<a id='a3b75a24-7380-41b6-ad59-0dd11c40e4c1'></a>

Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training Ilms to reason and leverage search engines with reinforcement learning. *arXiv preprint arXiv:2503.09516*, 2025.

<a id='cf4e1f2a-a3fc-41b4-bf8a-d5eb71144e31'></a>

Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy,
Aaron Courville, and Nicolas Le Roux. Vineppo: Refining credit assignment in rl training of llms.
In International conference on machine learning, 2025.

<a id='dacddc75-f454-4955-a359-4b320b540b0b'></a>

Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th symposium on operating systems principles, pages 611-626, 2023.

<a id='7e1af74c-97f9-4964-bdfa-d83f376c4598'></a>

Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024.

<a id='3ce6e3ff-2276-48cd-b736-db67781354d5'></a>

Bo Liu, Leon Guertler, Simon Yu, Zichen Liu, Penghui Qi, Daniel Balcells, Mickel Liu, Cheston Tan, Weiyan Shi, Min Lin, et al. Spiral: Self-play on zero-sum games incentivizes reasoning via multi-agent multi-turn reinforcement learning. arXiv preprint arXiv:2506.24119, 2025a.

<a id='1bd9f58c-6aba-4cf7-96f0-37db37c2b95a'></a>

Zichen Liu, Changyu Chen, Xinyi Wan, Chao Du, Wee Sun Lee, and Min Lin. Oat: A research-friendly framework for llm online alignment. https://github.com/sail-sg/oat, 2024.

<a id='f31ce1e5-5d78-4931-8c32-6c3ae209ee1e'></a>

Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: A critical perspective. In *Conference on Language Modeling (COLM)*, 2025b.

<a id='1a1f0cc1-4862-437a-81af-50cd54f443a0'></a>

OpenAI. Openai o1 system card. *arXiv preprint arXiv:2412.16720*, 2024.

<a id='da4b4741-398d-40e2-921f-7819bef0389f'></a>

OpenAI. Gpt-5 system card. https://cdn.openai.com/gpt-5-system-card.pdf, 2025.

<a id='9a71f1c4-0214-4673-9b93-9e4165606e4c'></a>

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in neural information processing systems_, 35:27730-27744, 2022.

<a id='c10100fa-511d-403b-af7b-f7546bf1245c'></a>

Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-baselines3: Reliable reinforcement learning implementations. *Journal of Machine Learning Research*, 22(268):1-8, 2021. URL http://jmlr.org/papers/v22/ 20-1364.html.

<a id='32a79254-a20a-4dc6-b0be-b9fc643cd075'></a>

Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 3505-3506, 2020.

<a id='a74f48e7-290d-47a2-8dbe-2adfa8212660'></a>

John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.

<a id='3a0e60c9-80c1-409e-9196-7f70774ed744'></a>

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. *arXiv preprint arXiv:1707.06347*, 2017.

<a id='9c494cd2-a7b4-4799-a998-ae029485c0c0'></a>

Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,
Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical
reasoning in open language models. *arXiv preprint arXiv:2402.03300*, 2024.

<a id='7d2272ea-7c50-4410-8e96-46f86e96a627'></a>

11

<!-- PAGE BREAK -->

<a id='789b5327-372d-4b30-bee2-74f2e813ad4f'></a>

GEM: A Gym for Agentic LLMs

<a id='c0d607b1-e9e6-4980-b77e-7b43aec7adaa'></a>

Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024.

<a id='d617595a-2d88-484e-9394-aae2cd4d7db3'></a>

David Silver and Richard S Sutton. Welcome to the era of experience. Google AI, 1, 2025.
Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, et al. Beyond human data: Scaling self-training for problem-solving with language models. arXiv preprint arXiv:2312.06585, 2023.
Zafir Stojanovski, Oliver Stanley, Joe Sharratt, Richard Jones, Abdulhakeem Adefioye, Jean Kaddour, and Andreas Köpf. Reasoning gym: Reasoning environments for reinforcement learning with verifiable rewards. arXiv preprint arXiv:2505.24760, 2025.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018.
Chenmien Tan, Simon Yu, Lanbo Lin, Ze Zhang, Yuanwu Xu, Chenhao Jiang, Tianyuan Yang, Sicong Xie, and Guannan Zhang. R12: Ray less reinforcement learning. https://github.com/ChenmienTan/RL2, 2025. GitHub repository.
The MCPMark Team. Mcpmark: Stress-testing comprehensive mcp use. https://github.com/eval-sys/mcpmark, 2025a.
The Terminal-Bench Team. Terminal-bench: A benchmark for ai agents in terminal environments, Apr 2025b. URL https://github.com/laude-institute/terminal-bench.
Hado Van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph Modayil. Deep reinforcement learning and the deadly triad. arXiv preprint arXiv:1812.02648, 2018.
Weixun Wang, Shaopan Xiong, Gengru Chen, Wei Gao, Sheng Guo, Yancheng He, Ju Huang, Jiaheng Liu, Zhendong Li, Xiaoyang Li, et al. Reinforcement learning optimization for large-scale learning: An efficient and user-friendly scaling library. arXiv preprint arXiv:2506.06122, 2025a.
Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, Yiping Lu, Kyunghyun Cho, Jiajun Wu, Li Fei-Fei, Lijuan Wang, Yejin Choi, and Manling Li. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning, 2025b.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229-256, 1992.
Wei Xiong, Jiarui Yao, Yuhui Xu, Bo Pang, Lei Wang, Doyen Sahoo, Junnan Li, Nan Jiang, Tong Zhang, Caiming Xiong, et al. A minimalist approach to llm reasoning: from rejection sampling to reinforce. arXiv preprint arXiv:2504.11343, 2025.
Zhenghai Xue, Longtao Zheng, Qian Liu, Yingru Li, Zejun Ma, and Bo An. Simpletir: End-to-end reinforcement learning for multi-turn tool-integrated reasoning. https://simpletir.notion.site/report, 2025. Notion Blog.
An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025.
Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. T-bench: A benchmark for tool-agent-user interaction in real-world domains. arXiv preprint arXiv:2406.12045, 2024.
Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.

<a id='65b39f0a-0e8a-401d-aefb-9e04f0ce273e'></a>

12

<!-- PAGE BREAK -->

<a id='ae4fc67d-85e0-4c0c-a537-ee53007c22b2'></a>

GEM: A Gym for Agentic LLMs
---


<a id='a73a4d75-e3fd-4640-bac1-2c60392aa5f9'></a>

## A ENVIRONMENT REGISTRATION
GEM enables rapid development of new RL environments. In this section, we illustrate two scenarios:
(i) integrating additional datasets into an existing task and (ii) defining a custom task, followed by the
procedure for registering these environments for use.

<a id='8fda8c22-6278-41a6-9ef0-561362bcaf50'></a>

The following code snippet shows how to add a new dataset for math environment, where the answer verification logic is predefined by GEM and can be reused.

<a id='e718fed4-af01-4664-9303-baf890fd3a8d'></a>

```python
import gem
from gem.envs.registration import register

register(
    "math:GSM8K-Example",
    "gem.envs.math_env:MathEnv",
    dataset_name="axon-rl/GSM-8k", # HuggingFace or local dataset path
    question_key="problem",
    answer_key="answer",
)

env = gem.make("math:GSM8K-Example") # ready to use
```

<a id='62b9ffdf-89b3-43c6-8608-629bbd0a0994'></a>

Next, we demonstrate how to build a new environment from scratch by defining the initial state distribution (in `.reset()`) and the transition and reward functions (in `.step()`) as follows.

```python
1 from gem.core import Env
2 from gem.envs.registration import register
3 from gem.utils.constants import TERMINAL_STATE
4 from gem.utils.parsing import extract_last_boxed_answer
5
6 class ReverseStringEnv (Env):
7     def __init__ (self, str_len: int = 5):
8         super().__init__()
9         self.str_len = str_len
10
11     def _get_instructions (self):
12         return (
13             "You are tasked to reverse a given string.\n"
14             "You may provide your response in any manner. Only the content wrapped inside \\
15             boxed{} will be considered as your final answer.\n"
16             f"Please reverse the string: {self.gt_str}.\n"
17         )
18
19     def reset (self, seed=None):
20         super().reset (seed)
21         characters = string.ascii_letters + string.digits # A-Z, a-z, 0-9
22         self.gt_str = "".join(random.choices (characters, k=self.str_len))
23         return self._get_instructions(), {}
24
25     def step (self, action):
26         clean_action = extract_last_boxed_answer (action)
27         if clean_action is None:
28             reward = 0
29         else:
30             reward = float (clean_action[::-1] == self.gt_str)
31         return TERMINAL_STATE, reward, True, True, {}
32
33 # Register your environment
34 register("custom:ReverseString", ReverseStringEnv)
35
36 env = gem.make("custom:ReverseString")
```

<a id='b667c238-716a-4566-b8d6-f23cce1da884'></a>

## B CASE STUDIES OF LANGUAGE GAMES

This section illustrates example interactions of game-playing agents trained on GuessTheNumber under different discount factors γ. With a near-undiscounted setting (γ = 0.999), the agent fails to complete the game within the trial budget, as it lacks incentive to obtain the terminal correctness reward as earlier as possible (see example below).

<a id='5249b5b9-b863-4dfb-ab53-f234db75665e'></a>

13

<!-- PAGE BREAK -->

<a id='719e5bd3-82d9-4152-83d4-2688361923d1'></a>

GEM: A Gym for Agentic LLMs

<a id='18a3b945-8a72-4c73-9a4a-cc8d3d3d0483'></a>

(y = 0.999, the agent exhausts all 10 trials)
State: You are playing Guess The Number. You have to guess the number between 1 and 50 (inclusive) within 50 turns. ... As you play, the history of your guesses will be appended below. Use the information to complete the game before you run out of guesses.
Enter your first guess to start the game.
At turn 1, you guessed 14, and the target number is higher than 14.
At turn 2, you guessed 22, and the target number is higher than 22.
...
At turn 10, you guessed 39, which has been already guessed before.
Enter your next guess.
Action: Let's break down the problem step by step:
1. **Initial Guess (Turn 1):** The target number is higher than 14. This means the target number is between 15 and 50 (inclusive).
2. **Initial Guess (Turn 2):** The target number is higher than 22. This means the target number is between 23 and 50 (inclusive).
...
From the final two steps, the target number is between 37 and 50 (inclusive). The final answer is \boxed{38}.

<a id='f77881a9-fc00-4935-897f-28c85a2b8b93'></a>

In contrast, when using a more myopic return estimate (y = 0.9), the agent is encouraged to minimize the number of turns, discovers the binary search strategy, and converges to the optimal policy.

<a id='6f2e6071-dbf9-486e-aa08-2f6aff2ce39a'></a>

(y = 0.9, the agent learns to conduct binary search)

**State:** You are playing Guess The Number. You have to guess the number between 1 and 50 (inclusive) within 50 turns. ... As you play, the history of your guesses will be appended below. Use the information to complete the game before you run out of guesses.
Enter your first guess to start the game.
At turn 1, you guessed 25, and the target number is lower than 25.
At turn 2, you guessed 12, and the target number is higher than 12.
At turn 3, you guessed 18, and the target number is higher than 18.
At turn 4, you guessed 21, and the target number is higher than 21.
At turn 5, you guessed 23, and the target number is lower than 23.
Enter your next guess.

**Action:** To determine the target number between 1 and 50, we can use a **binary search strategy**, which is effective for narrowing down a range when given hints like 'higher' or 'lower.'

**Step-by-Step Reasoning:**
1. **Initial Guess (Turn 1):** - The target number is lower than 25. - This means the target number is in the range [1, 24].
2. **Second Guess (Turn 2):** - The target number is higher than 12. - This means the target number is in the range [13, 24].
3. **Third Guess (Turn 3):** - The target number is higher than 18. - This means the target number is in the range [19, 24].
4. **Fourth Guess (Turn 4):** - The target number is higher than 21. - This means the target number is in the range [22, 24].
5. **Fifth Guess (Turn 5):** - The target number is lower than 23. - This means the target number is in the range [22, 22].

**Final Guess:**
The target number is in the range [22, 22], which means the target number is 22. **Final Answer:** \boxed{22}

<a id='d3929f03-4e36-40d8-8b5b-6a437f0e9269'></a>

14

<!-- PAGE BREAK -->

<a id='3c1c72db-3618-4b40-94e9-a4037ff8cd69'></a>

GEM: A Gym for Agentic LLMs

<a id='059421d1-af5b-45fc-a72b-085cbf43dfe2'></a>

C ALGORITHM

For completeness we include the full algorithm of ReBN in Algorithm 1.

<a id='1ff60e39-2c95-43f6-87dc-ecc062479e34'></a>

Algorithm 1 Multi-turn REINFORCE with Return Batch Normalization (ReBN)

Require: Policy $\pi_{\theta}$, Environment $\mathcal{M} = (S, A, P, R, \gamma, \rho)$, Batch size $B$
1: while not converged do
2:   Reset batch buffer $\mathcal{B} \leftarrow \emptyset$
3:   while $|\mathcal{B}| \le B$ do
4:     // Multi-turn episode collection
5:     Sample the initial state $s_0 \sim \rho$
6:     for turn $t = 0, 1, ..., T - 1$ until terminate do
7:       $y_t \sim \pi_{\theta}(\cdot|s_t)$
8:       $a_t \leftarrow \text{extract\_action}(y_t)$
9:       $r_t \leftarrow R(s_t, a_t)$
10:      $s_{t+1} \leftarrow P(s_t, a_t)$
11:    end for
12:    for $t = 0, 1, ..., T - 1$ do
13:      $G_t \leftarrow \sum_{k=t}^{T-1} \gamma^{k-t} r_k$
14:      Add $(s_t, y_t, G_t)$ to $\mathcal{B}$
15:    end for
16:  end while
17:  // Return Batch Normalization
18:  $\tilde{G}_i \leftarrow (G_i - \text{mean}(\mathbf{G})) / \text{std}(\mathbf{G})$
19:  // Policy optimization
20:  Update $\theta$ using Monte Carlo policy gradient $\sum_{i=1}^B \tilde{G}_i \nabla_{\theta} \log \pi_{\theta}(y_i|s_i)$
21: end while

$\triangleright$ Generate reasoning + action

$\triangleright$ Compute discounted return

$\triangleright$ Or proximal update for data reuse

<a id='9d9391ae-402b-4ab6-823c-01053be0b81a'></a>

D EXTENDED EMPIRICAL STUDIES WITH GEM<::
chart: Figure 9: Learning curves of different reward shaping strategies.

This figure displays four line graphs (a, b, c, d) arranged in a 2x2 grid, showing the performance of different reward shaping strategies over 500 policy iteration steps. A common legend indicates three strategies: 'w. ReBN' (red line), 'w.o ReBN' (orange line), and 'Neg rew' (teal/blue line).

(a) math:Orz57K environment - Mean Episode Success:
The x-axis represents Policy iteration steps (0 to 500). The y-axis represents Mean Episode Success (0.0 to 0.8). All three strategies show an increase in success rate over iterations. 'w. ReBN' and 'Neg rew' converge to a success rate between 0.6 and 0.7, while 'w.o ReBN' converges slightly lower, around 0.6.

(b) qa:HotpotQA environment - Mean Episode Success:
The x-axis represents Policy iteration steps (0 to 500). The y-axis represents Mean Episode Success (0.0 to 0.7). Similar to (a), all strategies show increasing success. 'w. ReBN' achieves the highest success rate, reaching approximately 0.5 to 0.55. 'w.o ReBN' and 'Neg rew' converge to success rates around 0.4 to 0.45.

(c) math:Orz57K environment - Mean Episode Len:
The x-axis represents Policy iteration steps (0 to 500). The y-axis represents Mean Episode Len (1.0 to 2.0), which corresponds to the average number of turns taken to solve tasks (tool calls minus one). 'w. ReBN' starts high (around 1.6-1.8) and decreases to around 1.2, then fluctuates, showing a higher episode length compared to the other two strategies. 'w.o ReBN' and 'Neg rew' both start high but quickly decrease to and stabilize around 1.0 to 1.1.

(d) qa:HotpotQA environment - Mean Episode Len:
The x-axis represents Policy iteration steps (0 to 500). The y-axis represents Mean Episode Len (1 to 5). 'Neg rew' starts at 1, sharply increases to a peak around 4.5, and then fluctuates around 4. 'w. ReBN' starts at 1, increases to around 3, then decreases to around 2. 'w.o ReBN' starts at 1, increases to around 2, and then remains relatively stable.
::>

<a id='544a1acb-d281-47a2-91f2-9c52a43c6ca0'></a>

D.1 IMPROVING LEARNING EFFICIENCY VIA RETURN BATCH NORMALIZATION (REBN)
As briefly discussed in Section 4.1, while REINFORCE demonstrates strong performance across most environments, its convergence can be suboptimal in certain cases. To investigate this further, we present an in-depth ablation study here. Following minimalist principles, we began with the vanilla REINFORCE algorithm and a simple reward scheme: r = 1 for correct answers and r = 0 otherwise. This approach has been shown effective for single-turn RL training (Singh et al., 2023; Xiong et al., 2025). However, as shown in Figure 9(c) (w.o ReBN), it failed to induce tool usage in multi-turn settings, despite significant amount of initial attempts.

<a id='ed2adc4b-c221-4cac-87ef-e7e3bc333ff9'></a>

We hypothesize that this failure arises from the absence of _negative gradients_ under 0/1 reward shaping, which are crucial for efficient learning and exploration. To address this, we introduced negative gradients in two ways: (i) assigning fixed negative rewards ($r = 1$ for correct and $r = -1$ for incorrect answers, denoted as Neg rew in Figure 9); and (ii) applying Return Batch Normalization with

<a id='245e6122-36cd-4ec6-bb50-f4ddc52fe4d0'></a>

15

<!-- PAGE BREAK -->

<a id='dbea9982-810e-457e-a540-b240d188e18c'></a>

GEM: A Gym for Agentic LLMs

<a id='2c816c41-c43a-4c5d-a0a6-2516fd44f2af'></a>

0/1 rewards, where Monte Carlo returns in REINFORCE are normalized as described in Algorithm 1 (denoted as ReBN in Figure 9). While both 0/1 and 1 reward schemes theoretically induce the same optimal policy, they might exhibit markedly different learning dynamics in practice.

<a id='45b75fd5-b2e8-41c1-99e7-1b2be18c441c'></a>

Notably, ReBN demonstrates strong and consistent performance across environments---not only in math and QA tasks (Figure 9) but also in all other settings (Figure 1). We also observe that models can be sensitive to fixed reward shaping: for example, Neg rew fails to improve tool use in math:Orz57K, yet leads to tool overuse in qa:HotpotQA, both of which are suboptimal behaviors.

<a id='a58038f9-b027-43ee-a488-7dad85145a27'></a>

D.2 RL ON VISION-LANGUAGE ENVIRONMENTS
In addition to text-only environments, we support visual elements as part of the observation for the agent to understand and take actions. As a demonstrative example, we build a visual-language environment based on Geometry3k dataset<sup>8</sup> for training reasoning agent to solve geometry math problems with images input. We RL-tune Qwen2.5-VL-3B/7B-Instruct (Bai et al., 2025) using Dr. GRPO (Liu et al., 2025b), and the learning curves are shown in Figure 10. An example reasoning trace is shown in Figure 11.

<a id='a90b6471-284d-406b-baa6-0c5bc5375f65'></a>

<::chart: Two line charts showing learning curves of vision-language agents. The left chart is titled "Training Rewards" and the right chart is titled "Validation Scores". Both charts share a common x-axis labeled "Policy iteration step", ranging from 0 to 400. The left chart's y-axis is labeled "Reward", ranging from 0.1 to 0.7. The right chart's y-axis is labeled "Score", ranging from 0.1 to 0.45. Both charts include a legend with two entries: "7B" (represented by a red line) and "3B" (represented by a dark teal line). In the "Training Rewards" chart, both 7B and 3B lines show an increasing trend, with 7B consistently higher than 3B. The 7B line starts around 0.3 and reaches approximately 0.65, while the 3B line starts around 0.1 and reaches approximately 0.6. Similarly, in the "Validation Scores" chart, both 7B and 3B lines show an increasing trend, with 7B consistently higher than 3B. The 7B line starts around 0.32 and reaches approximately 0.43, while the 3B line starts around 0.12 and reaches approximately 0.4.::>Figure 10: Learning curves of vision-language agents. We RL-tune Qwen2.5-VL-3B/7B-Instruct using Dr. GRPO on the math: Geometry3K environment and track their training rewards (left) and validation scores (right).

<a id='9746aeca-3a99-4f50-b3dc-2578fbf02125'></a>

11

<::m
n
(21+2x)°
(x+84)°
: figure::>

Problem: <image>Find $x$ so that $m || n$.

VLM Response:
To determine the value of \( x \) so that the lines \(m\) and \(n\) are parallel, we need to ensure that the corresponding angles are equal.
Since the lines \( m \) and \(n\) are parallel, the corresponding angles that are formed by the intersection of the transversal are equal.
Therefore, the angles \((21+2x)^\circ\) and \((x+84)^\circ\) are equal:

[
\((21+2x)^\circ = (x + 84)^\circ\)
]

We can set the angles equal to each other:

[
\(21 + 2x = x + 84\)
]

Next, we need to solve for \( x \). First, we'll move all \( x \) terms to one side of the equation and constant terms to the other side:

[
\(2x - x = 84 - 21\)
]

[
\(x = 63\)
]

Therefore, the value of \( x \) that makes the lines \( m \) and \(n\) parallel is \(\boxed{63}\).

Figure 11: An example problem and the response of a trained agent based on Owen2.5-VL-7B-Instruct.

<a id='5e83d795-22b1-4586-99d3-fbe31b0ea065'></a>

---
⁸https://huggingface.co/datasets/hiyouga/geometry3k.

<a id='7e1c5006-823f-469a-9155-76aa63670a53'></a>

16

<!-- PAGE BREAK -->

<a id='b9be0967-30e6-4596-951c-de23ea404615'></a>

GEM: A Gym for Agentic LLMs

<a id='7b9aef2e-45a6-4e23-beb8-81b5c7986059'></a>

<::TAU-bench Results heatmap::>TAU-bench Results

This heatmap displays the success rates (%) for different combinations of User Models and Agent Models.

**Y-axis: User Model**
*   Weak (Gemini-2.0-Flash-Lite)
*   Medium (GPT-4o-mini)
*   Strong (GPT-4o)

**X-axis: Agent Model**
*   Weak (Gemini-2.0-Flash-Lite)
*   Medium (GPT-4o-mini)
*   Strong (GPT-4o)

**Color Bar: Success Rate (%)**
*   Ranges from 0 (lightest gray) to 100 (darkest gray).

**Grid Values (Success Rate %):**

| User Model \ Agent Model | Weak (Gemini-2.0-Flash-Lite) | Medium (GPT-4o-mini) | Strong (GPT-4o) |
|--------------------------|------------------------------|----------------------|-----------------|
| Weak (Gemini-2.0-Flash-Lite) | 25.2                         | 38.3                 | 41.7            |
| Medium (GPT-4o-mini)     | 27.8                         | 38.3                 | 51.3            |
| Strong (GPT-4o)          | 44.3                         | 48.7                 | 61.7            |
<::>

<a id='28d929e1-20c7-47e1-bddd-634baa34daa3'></a>

Figure 12: Multi-agent evaluation on TAU-bench retail. Stronger user simulators (rows) consistently improve agent performance (columns) across model strengths.

<a id='ce3cd602-cc28-405d-a11e-3ce8d44a148b'></a>

D.3 MULTI-AGENT ENVIRONMENTS

GEM supports multi-agent settings where multiple agents interact within the same environment. This capability enables the development of agents that can collaborate, compete, or simulate realistic interactions with other entities.

<a id='50ad9383-3887-425d-ac18-1fa76621fb94'></a>

**Interface design.** GEM provides a `MultiAgentEnv` base class that extends the standard Gym API to support multiple agents. The `step()` and `reset()` functions operate on dictionaries keyed by agent identifiers:

<a id='3aa97d28-5262-4a79-a157-2d0d0911eae2'></a>

```python
1 from gem.envs.multiagent import MultiAgentEnv
2
3 env = MyMultiAgentEnv()
4 observations, infos = env.reset() # Dict[agent_id -> observation]
5
6 while not done:
7     actions = {agent_id: agent.act(obs) for agent_id, obs in observations.items()}
8     observations, rewards, terminations, truncations, infos = env.step(actions)
9     done = all(terminations.values())
```

<a id='1e79bd5d-c5a6-4041-9fd6-c3eae38343ee'></a>

To implement a custom environment, users inherit from `MultiAgentEnv` and implement `observe(agent)` and `_process_actions(actions)`. The framework handles agent lifecycle management and cumulative rewards tracking. Turn coordination is managed via `AgentSelector`, which supports two modes: _sequential_ (agents act one at a time in round-robin order) and _parallel_ (all agents act simultaneously). The selector determines which agents are active at each step and automatically advances turns, enabling flexible multi-agent interaction patterns without manual bookkeeping.

<a id='1a6841e8-eecd-4027-b850-ecf569f4cd3b'></a>

**TAU-bench retail integration.** We demonstrate this API by integrating the TAU-bench retail benchmark (Yao et al., 2024), which evaluates conversational agents on customer service tasks. We formulate this as a two-agent environment: an *assistant* agent using tools (order lookup, product search) and a *user* agent simulating customer behavior via an LLM. The user simulator is initialized with task instructions and generates queries; the assistant must satisfy these requests before episode termination.

<a id='60c5cd6b-f7c0-4f27-8ea1-8f507c557bb1'></a>

**Impact of user model strength.** A key question in multi-agent RL is: *how does simulated user agent capability affect trainable assistant agent learning?* We vary both user and assistant models across three levels: weak (Gemini-2.0-Flash-Lite), medium (GPT-4o-mini), and strong (GPT-4o), yielding 9 configurations to study user-assistant model interactions.

<a id='f10de67a-8e74-4ec6-8e43-9771cdf8ddf3'></a>

17

<!-- PAGE BREAK -->

<a id='d21d73fd-fe13-442d-915e-71b9836834a0'></a>

GEM: A Gym for Agentic LLMs

<a id='6c3bb781-f9f4-4166-8202-a905ca0dcaaa'></a>

Evaluating across all 115 tasks from the TAU-bench retail test set (Figure 12), we find that stronger user agents consistently improve overall success rates across all assistant agent model strengths. Notably, the strongest assistant (GPT-40) exhibits the largest absolute performance gains (20% from weak to strong user), achieving 61.7% success with a strong user simulator. Interestingly, a strong user paired with a weak assistant (44.3%) outperforms a weak user paired with a strong assistant (41.7%), demonstrating that improving the user agent is crucial for robust conversational task completion. These results motivate us to develop multi-agent RL to co-evolve user and assistant agents to achieve scalable and autonomous learning.

<a id='86e93633-79c0-45bb-852f-308a3d087d73'></a>

E RELATED WORKS

There is a significant body of work on tool-integrated language models—including SkyRL-v0 (Cao et al., 2025), VerlTool (Jiang et al., 2025), ReCall and ReSearch (Chen et al., 2025a), Search-R1 (Jin et al., 2025), ReTool (Feng et al., 2025a), and SimpleTIR (Xue et al., 2025). A common design pattern in these methods is to collect multi-turn agent-environment interactions as single continuous sequences of tokens of agent actions interleaved with environment outputs. Training then simply involves masking the environment outputs from the loss calculation.

<a id='8406a832-0a19-40fb-9aa2-cb60bd0b1f8d'></a>

However, this single-sequence approach presents two significant limitations. First, the state observa- tion is rigidly defined as the complete history of actions and outputs. This restricts the ability to use alternative state representations, such as pruning "thinking" tokens or summarizing the history to avoid exceeding context lengths. Second, this formulation inherently limits the reward structure to a single, trajectory-level signal, preventing the use of finer-grained, per-step rewards, and effectively fixing the discount factor at γ = 1. In Section 4.2 we demonstrate that γ < 1 is crucial for obtaining the optimal fastest search behavior. By contrast, with trajectory-level rewards, the natural speed incentive from γ < 1 is lost, and hence other works, such as SimpleTIR, must tune and enforce a strict turn-limit to get this behavior.

<a id='d7e220e0-8d45-4cdf-b601-b8c334b51298'></a>

To address this, our framework, GEM, is designed for maximum flexibility by collecting trajectories as a sequence of individual transitions (i.e., state, action, reward, next state) as in the full, unsimplified RL formulation. This design choice enables arbitrary state observation constructions (using observation wrappers), and also preserves compatibility with per-turn rewards and arbitrary discount factors $\gamma \leq 1$. The verl-agent framework (Feng et al., 2025b) also adopts this transition-wise approach, which enables its implementation of GiGPO (Feng et al., 2025b), an RL method that utilizes turn-level rewards. While GiGPO collapses to trajectory-level GRPO when observations are unique, it is an example of a type of algorithm that is now straightforward to implement with GEM's infrastructure.

<a id='0eef5082-6dce-4c49-9c17-3fac5727151d'></a>

There are multiple popular frameworks that focus on the agent training side (e.g., Oat (Liu et al., 2024), Verl (Sheng et al., 2024), OpenRLHF (Hu et al., 2024), ROLL (Wang et al., 2025a), and RL2 (Tan et al., 2025)). Currently, many works that build on these, including verl-agent, RAGEN (Wang et al., 2025b), Verlog (Chen et al., 2025b), and many of the works above, add environments by directly modifying the source code. This results in tight coupling between training and environments, and makes it difficult to maintain and reuse the environments for future research. As a result, each codebase tends to support only a small, ad-hoc collection of environments, making it hard to compare different methods. Even environments with the same name are often inconsistent between codebases. GEM addresses this by dealing with all the environment infrastructure, including providing a diverse suite of environments, and corresponding baselines. This makes it easy to keep training and environments decoupled, with the aim of freeing researchers from cumbersome environment development and setup, and thus enabling quicker prototyping and evaluation of new ideas.

<a id='0ac8d5b4-f568-4d0c-8097-664fc99c760e'></a>

18

<!-- PAGE BREAK -->

<a id='98d01d6e-1abb-455e-bc4d-8341fd8e0527'></a>

GEM: A Gym for Agentic LLMs

<a id='8046f5bf-5b27-4853-a841-44ea5039c585'></a>

F EXPERIMENTAL SETTINGS
All our experiments are performed on 8 × A100 GPUs and finished in about one day. The detailed
experimental configurations are shown in Table 3.
Table 3: Hyperparameter configurations used in all experiments.
<table id="18-1">
<tr><td id="18-2">Parameter</td><td id="18-3">Value</td></tr>
<tr><td id="18-4"></td><td id="18-5">ACTOR</td></tr>
<tr><td id="18-6">Maximum response length per turn</td><td id="18-7">4096 tokens</td></tr>
<tr><td id="18-8">Sampling temperature, train</td><td id="18-9">1.0</td></tr>
<tr><td id="18-a">Sampling temperature, evaluation</td><td id="18-b">0.0</td></tr>
<tr><td id="18-c">(top P, top k)</td><td id="18-d">(1.0, -1)</td></tr>
</table>

<a id='f9123ff3-49ed-4481-ab7d-a811a911457a'></a>

<table><thead><tr><th></th><th>LEARNER</th></tr></thead><tbody><tr><td>Optimizer</td><td>AdamW</td></tr><tr><td>Adam parameters (β<sub>1</sub>, β<sub>2</sub>)</td><td>(0.9, 0.95)</td></tr><tr><td>Weight decay</td><td>0.0</td></tr><tr><td>Gradient norm clipping</td><td>1.0</td></tr><tr><td>Learning rate scheduler</td><td>Constant</td></tr><tr><td>Learning rate</td><td>1 × 10<sup>-6</sup></td></tr><tr><td>Inner proximal update epoch</td><td>2</td></tr><tr><td>KL loss coefficient</td><td>0.0</td></tr><tr><td>KL penalty coefficient</td><td>0.0</td></tr><tr><td>Policy clipping parameter</td><td>0.2</td></tr><tr><td>Discount factor</td><td>0.9 (<b>game.qa</b>); 1.0 (otherwise)</td></tr><tr><td>GAE λ</td><td>0.95</td></tr><tr><td>Steps</td><td>500</td></tr></tbody></table>

<a id='04b083f4-1ee1-4243-80fc-8c53e608dec8'></a>

19