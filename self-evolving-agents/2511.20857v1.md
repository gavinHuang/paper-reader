<a id='93a6ef3c-2407-4ff9-8d6d-0bf5ee484a5a'></a>

<::logo: Google DeepMind
Google DeepMind
The logo features the multi-colored 'Google' text followed by 'DeepMind' in gray text.::>

<a id='9e89e948-6b6e-4ef3-a103-6c48b691dc34'></a>

2025-11-27

<a id='19baa762-4754-44dd-933d-ae2faff60269'></a>

Evo-Memory: Benchmarking LLM Agent
Test-time Learning with Self-Evolving Memory

<a id='3894feeb-a9cf-44bd-80a1-9e60ecd2f73e'></a>

Tianxin Wei†,¹, Noveen Sachdeva², Benjamin Coleman², Zhankui He², Yuanchen Bei¹, Xuying Ning¹, Mengting Ai¹, Yunzhe Li†,¹, Jingrui He¹, Ed H. Chi², Chi Wang², Shuo Chen², Fernando Pereira², Wang-Cheng Kang² and Derek Zhiyuan Cheng²

†Work done while at Google DeepMind, ¹University of Illinois Urbana-Champaign, ²Google DeepMind

<a id='7ae98561-8a6e-48eb-817b-26c357629fdf'></a>

Statefulness is essential for large language model (LLM) agents to perform long-term planning and problem-solving. This makes memory a critical component, yet its management and evolution remain largely underexplored. Existing evaluations mostly focus on static conversational settings, where memory is passively retrieved from dialogue to answer queries, overlooking the dynamic ability to accumulate and reuse experience across evolving task streams. In real-world environments such as interactive problem assistants or embodied agents, LLMs are required to handle continuous task streams, yet often fail to learn from accumulated interactions, losing valuable contextual insights, a limitation that calls for test-time evolution, where LLMs retrieve, integrate, and update memory continuously during deployment. To bridge this gap, we introduce Evo-Memory, a comprehensive streaming benchmark and framework for evaluating self-evolving memory in LLM agents. Evo-Memory structures datasets into sequential task streams, requiring LLMs to search, adapt, and evolve memory after each interaction. We unify and implement over ten representative memory modules and evaluate them across 10 diverse multi-turn goal-oriented and single-turn reasoning and QA datasets. To better benchmark experience reuse, we provide a baseline method, ExpRAG, for retrieving and utilizing prior experience, and further propose ReMem, an action-think-memory refine pipeline that tightly integrates reasoning, task actions, and memory updates to achieve continual improvement.

<a id='fb9bf1df-0bd7-42c1-b2f0-b93a5f4750bf'></a>

Keywords: LLMs, Agentic Memory, Test-time Learning, Self-evolving Agents, Lifelong Intelligence

<a id='aa7c2bd9-f364-4a0a-b05a-bf5f6b6a9758'></a>

## 1. Introduction

Large Language Models (LLMs) have rapidly evolved from simple chatbots into capable systems that can write code, control browsers, and perform advanced question answering (Comanici et al., 2025). These advances have been driven by improving inference, planning, and tool use, as shown by benchmarks emphasizing logical reasoning and multi-step actions. Yet a fundamental capability, *memory*, remains largely underexplored. Memory allows LLMs to maintain state across interactions, accumulate experience, and adapt strategies over time. Recent studies have introduced memory modules that track dialogue histories through compression, indexing, or retrieval (Maharana et al., 2024b), improving *conversational recall* and personalization. However, most of these systems only reuse static dialogue context rather than learning from experience to improve future reasoning or decision-making.

<a id='678503c0-4899-4ca1-b9bd-a76fd973034c'></a>

Despite these advances, existing LLM memory systems remain largely static, retrieving information passively rather than evolving through use. Current evaluations test whether models can recall past context but rarely assess their ability to _reuse experience_. In essence, agents remember what was said but not what was learned. _Conversational recall_ retrieves prior facts, whereas _experience reuse_ abstracts reasoning strategies for future tasks. Without such reuse, models repeatedly solve similar problems, as long-term assistants often recall context yet fail to adapt across sessions.

<a id='ce3e161e-0948-42f4-849b-26e4798c9de4'></a>

Corresponding author(s): twei10@illinois.edu
© 2025 Google DeepMind. All rights reserved

<a id='4e3f9fdb-0df9-40c3-8f26-29a5bf6d7fab'></a>

arXiv:2511.20857v1 [cs.CL] 25 Nov 2025

<!-- PAGE BREAK -->

<a id='5ddcae64-a44e-4ff2-b34d-cab9f79146d2'></a>

Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory

<a id='58b5d107-aff5-40fe-9163-6a40bf9b7704'></a>

<:: Illustration of different task types and experience reusing : flowchart::>Goal: Put a green cup with a fork in it on the counter.Box 1: State 'Cup is on the table' (represented by a globe icon). Action 'Find the cup' (represented by a robot icon). Ellipsis (...). State 'Fork is in the cup' (represented by a globe icon). Action 'Put them on the counter' (represented by a robot icon).An arrow from Box 1 points to a set of colorful jigsaw puzzle pieces labeled 'Reuse'. An arrow from 'Reuse' points to Box 2.Goal: Putting a cooled tomato in the microwave.Box 2: State 'Tomato is in the kitchen' (represented by a globe icon). Action 'Find the tomato' (represented by a robot icon). Ellipsis (...). State 'Tomato is cooled down' (represented by a globe icon). Action 'Put it in the microwave' (represented by a robot icon).Label for top section: Multi-turn task.Below the multi-turn tasks, a robot icon labeled 'AI' with a lightbulb above its head points to 'Self-Evolving Memory'.Below 'Self-Evolving Memory', a horizontal line separates the multi-turn and single-turn tasks.Label for bottom section: Single-turn task.Q: Solve 2x² - 5x + 1 = 0.Box 3: Action 'Identify equation' (represented by a robot icon). Ellipsis (...). Action 'Apply quadratic formula' (represented by a robot icon).An arrow from Box 3 points to a set of colorful jigsaw puzzle pieces labeled 'Reuse'. An arrow from 'Reuse' points to Box 4.Q: Solve 5x² - 1x + 7 = 0.Box 4: Action 'Identify equation' (represented by a robot icon). Ellipsis (...). Action 'Apply quadratic formula' (represented by a robot icon).Figure 2 | Illustration of different task types and experience reusing. A stateful agent encounters both multi-turn tasks (e.g., embodied manipulation) and single-turn tasks (e.g., solving equations), and should learn reusable experiences from past experiences.

<a id='7f004ffe-1f6b-42a1-a7c6-c1976c8e2a18'></a>

Several recent benchmarks have begun examining static adaptation but remain limited in scope. StreamBench (Wu et al., 2024a) evaluates sequential learning but mainly measures factual retention without reasoning or trajectory reuse. Lifelong-Bench (Zheng et al., 2025) studies lifelong learning across environments and skills but focuses on retention without modeling memory structure or updates. Other studies (Hu et al., 2025; Maharana et al., 2024b; Wu et al.) assess long-term conversational consistency but do not test how agents evolve their memory during deployment. Together, these efforts highlight a critical gap: while progress has been made on sequential reasoning, there is still no unified framework for evaluating how different memory methods retrieve, integrate, and evolve historical strategies in realistic streaming scenarios. <::A diagram illustrating two types of memory processes: Conversational Recall and Experience Reuse.On the left, under 'Conversational Recall': A blue speech bubble asks, "What are the solutions for 2x² + 3x - 1 = 0?" An arrow points to a green speech bubble stating, "For 2x² + 3x - 1, x = -2, 0.5". Below this, a thought bubble shows "X=-2, 0.5". A robot icon is depicted, next to which is an icon labeled 'FACTS'. The section is labeled 'Conversational Recall'.On the right, under 'Experience Reuse': A light blue speech bubble asks, "Quadratic formula". An arrow points to a green speech bubble displaying the "Quadratic formula: -b ± √b² - 4ac / 2a". Below this, a thought bubble shows "Quadratic formula". A robot icon with gears and a brain icon above it is depicted. The section is labeled 'Experience Reuse'.Between the two sections, a dashed vertical line separates them. Above this line, an arrow pointing left is labeled 'What', and an arrow pointing right is labeled 'How'.Figure 1 | Conversational recall retrieves past facts (e.g., solutions to 2x² + 3x − 1 = 0). Experience reuse recalls reasoning strategies (e.g., using the formula).::> Figure 1 illustrates this contrast between static recall and cumulative improvement through self-evolving memory. 

<a id='5ab70efb-ac8b-45e2-b16e-bb6c46c3b9dc'></a>

To bridge this gap, we introduce **Evo-Memory**, a comprehensive streaming benchmark and framework for evaluating *self-evolving memory* in LLM agents. Figure 2 illustrates how a self-evolving agent reuses prior experiences across both multi-turn interactive tasks and single-turn reasoning

<a id='db93e5e0-5814-4a82-a744-a7716b46dd4a'></a>

2

<!-- PAGE BREAK -->

<a id='d73e7a65-7c5a-4c0d-8d04-8a626e8c89b1'></a>

Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory

<a id='47423a61-6e2a-4912-abfd-0b1e9c6da46b'></a>

tasks. Evo-Memory restructures datasets into sequential _task streams_, requiring models to retrieve,
adapt, and evolve memory after each interaction. The benchmark covers both _multi-turn goal-oriented_
environments and _single-turn reasoning or problem-solving_ tasks, explicitly testing whether LLMs can
accumulate knowledge and refine strategies during deployment, a process we term _test-time evolution_.
We unify and implement over ten representative memory modules, including retrieval-based, workflow,
and hierarchical memory systems, to study their adaptation behavior. To further examine experience
reuse, we introduce **ExpRAG**, a simple retrieval-based baseline that leverages prior task experiences,
and further develop **ReMem**, an advanced _action-think-memory refine_ pipeline that tightly integrates
reasoning, action, and memory updates for continual improvement.

<a id='cb3d0528-fc79-40a5-a1bb-b66640ab6bc5'></a>

In summary, our contributions are threefold:

*   **Benchmark**: We present Evo-Memory, a streaming benchmark that evaluates LLM agents' ability to perform *test-time evolution* across diverse multi-turn and single-turn tasks, bridging the gap between conversational recall and experience reuse.
*   **Framework**: We provide a unified evaluation framework with memory-centric metrics for analyzing adaptation, efficiency, and stability, and will release all code and configurations for reproducibility.
*   **Analysis and Insights**: We introduce **ExpRAG**, a simple retrieval-based baseline for experience reuse, and **ReMem**, an *action–think–memory refine* pipeline that unifies reasoning, action, and memory for continual improvement, informing future designs of memory.

<a id='8edccc15-91ea-4e74-bf93-e698dcce525e'></a>

## 2. Related Work

In this section, we review existing works on test-time learning and self-evolving memory.

<a id='18e8f0d3-9fa0-4960-a7fa-55b0a8195feb'></a>

## 2.1. Test-time Learning

Test-time learning (TTL) builds upon early work on test-time adaptation (TTA) (Niu et al., 2022; Wang et al., 2021; Zhang et al., 2023), which enables models to adjust to distribution shifts during deployment. Recent advances extend TTA toward continuous self-improvement (Iwasawa and Matsuo, 2021; Liu et al., 2023), allowing models to refine their behavior through online optimization. Recent agent-based studies operationalize such continual improvement via reflection, planning, and self-evolution. Works like (Park et al., 2023; Shinn et al., 2023; Wang et al., 2023; Zhao et al., 2025; Zhou et al., 2024) and newer frameworks, including (Chen et al., 2025; Huang et al., 2025) demonstrate how agents autonomously revise plans, synthesize feedback, and co-evolve (Gao et al., 2025). These advances mark a shift from static adaptation toward adaptive, self-improving agents capable of continual learning during deployment. Building on this trend, we propose to benchmark such dynamics from a novel self-evolving memory perspective.

<a id='1b090d2b-b8e2-4daf-8299-3128e53b228d'></a>

## 2.2. Self-evolving Memory

Early LLM memory systems primarily served as _passive storage_, maintaining recent dialogues or retrieved facts to compensate for limited context windows (Asai et al., 2024a; Lewis et al., 2020; Liu, 2022; Packer et al., 2023; Zhong et al., 2023). Subsequent studies introduced richer management mechanisms, including differentiable read-write controllers (Liang et al., 2023; Modarressi et al., 2023) and evaluations under realistic conversational settings (Maharana et al., 2024a; Wu et al., 2024b). Beyond static buffers, recent work explores _policy-driven control_, where the model is explicitly optimized to decide what to store, retrieve, or overwrite (Li et al., 2025; Xu et al., 2025; Yan et al., 2025; Yu et al., 2025; Zhou et al., 2025). Meanwhile, structured memory representations have emerged to organize experiences into relational or procedural forms, as in RepoGraph (Ouyang et al., 2024),

<a id='21d2674b-1299-468b-8aa8-12fba1dcc63f'></a>

3

<!-- PAGE BREAK -->

<a id='6a5c6f91-41ed-4122-88aa-8521f4997d1d'></a>

Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory

<a id='77d7cbe4-8309-4740-b0c8-e45639017bf7'></a>

<::figure: flowchart::>Figure 3 | Overview of the ReMem agent framework. Left: Test-time evolution process where the agent iteratively searches, synthesizes, and evolves its memory across multiple tasks. The diagram shows three tasks (Task 1, Task 2, Task ...). Each task follows a sequence: first, a 'Search' step (depicted by a magnifying glass over a book with a brain icon) leads to 'Synthesis' (depicted by a robot with glowing eyes). After synthesis, an 'LLM Response' is generated, indicated by a robot icon and checkmark (success) and cross (failure) symbols. This response then leads to 'Evolve' (depicted by a book with a brain icon and sparkling stars), which also shows a bar chart indicating evolution. Dashed arrows connect the 'LLM Response' of one task back to the 'Search' step of the next task, illustrating iteration. Right: Agent architecture with three core modules—Think (reasoning and decomposition), Refine Memory (retrieve, prune, organize), and Act (execution)—that interact with the environment and learned memory. The 'ReMem Agent' (robot icon) is at the center, interacting with 'Environment' (globe icon) and 'Memory' (book with brain icon) via double-headed arrows. The agent also interacts with three modules in a loop: 'Think' (lightbulb icon) which 'Generate reasoning: Internal thinking and task decomposition', 'Act' (hand pointing icon) which 'Execute an operation or produce a final response', and 'Refine Memory' (database and document icon) which 'Explore memory utilization: retrieve, prune, organize'. Arrows indicate the flow: Think to Act, Act to Refine Memory, and Refine Memory back to Think, with dashed arrows from the ReMem Agent to Think and Refine Memory, and from Act back to ReMem Agent. (a) Test-time Evolution Setting (b) Illustrative Think-Act-Refine Loop<::>

<a id='34c4ed95-bd06-42e8-a256-5a3443c450a6'></a>

MEMO (Chhikara et al., 2025), Zep (Rasmussen et al., 2025), and Dynamic Cheatsheets (Suzgun et al., 2025). However, there remains no unified evaluation setting and framework for self-evolving memory, the ability to reuse and adapt experiences across tasks. Evo-Memory builds on this trajectory by benchmarking how LLMs not only store and recall but also evolve, reorganize, and reuse memory under streaming task settings.

<a id='3d59afa5-f03f-4037-9fbc-ad591463decd'></a>

### 3. Evo-Memory: Evaluating Self-Evolving Memory in LLM Agents

Existing evaluations of LLMs often treat memory as static recall, overlooking its role in continual adaptation. Evo-Memory provides a unified benchmark to study _self-evolving memory_, where agents retrieve, integrate, and update knowledge over time. As illustrated in Figure 3, the left side shows the test-time evolution process, and the right side outlines the REMEM agent with three modules—_Think_, _Act_, and _Refine Memory_. We first formalize the problem setting, then describe two representative implementations, ExPRAG and REMEM, used to instantiate the benchmark.

<a id='c77e5359-ba19-4f17-95ca-8f7a6024e60b'></a>

### 3.1. Problem Formulation
We formalize a general memory-augmented agent as a tuple (F, U, R, C), where F is the base LLM, U is the memory update pipeline, R is the retrieval module, and C is the contextual construction mechanism that transforms retrieved content into the final working context. In our setting, the agent processes a sequence of inputs {$x_1, x_2, ..., x_T$}, and the memory state $M_t$ evolves with the history. At time $t$, the agent receives an input $x_t$, maintains an evolving memory $M_t$, retrieves relevant elements $R(M_t, x_t)$, constructs a contextualized prompt

<a id='4baf10aa-0920-419e-9edb-42438af56f88'></a>

<::transcription of the content
: Ct = C(xt, R(Mt, xt)),::>

<a id='4536a38d-373a-4d94-8c1f-efb6ad016b9f'></a>

4

<!-- PAGE BREAK -->

<a id='6ee5d2da-e971-4ab3-b2db-afd8ba568af4'></a>

Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory

<a id='0b10e699-5e93-44f1-92af-2f8ae077c50a'></a>

and produces an output

ŷt = F(Ct).

<a id='d6961e47-f20a-4ff3-a791-7f380d305bfb'></a>

This abstraction unifies a wide spectrum of existing memory mechanisms, from retrieval-augmented
generation to dynamic, hierarchical, and workflow-based memories, under a single iterative formula-
tion.

<a id='16637fe5-36a5-4687-9afd-49f8ccea96cd'></a>

Search. Given the current input xt, the agent first retrieves relevant memory entries:

<a id='99ea00c7-7902-41e5-aa1a-70c6f2eb7dc7'></a>

R_t = R(M_t, x_t),

<a id='ad59feae-afe5-4007-9dda-8cf76e319934'></a>

where R can represent similarity search, index-based lookup, or attention over stored embeddings.
This step captures memory access policies across different algorithms.

<a id='6e0cdbde-f36f-40e0-b1ea-9ecab76092cd'></a>

**Synthesis.** The agent interprets and restructures the retrieved information R₁ into a concise working context aligned with the current input x₁. This synthesis yields a coherent text C₁, from which the final output is derived:

ŷ₁ = F(C₁).

<a id='24625624-c3ff-4158-8d60-fa062c46c5cd'></a>

**Synthesis.** The agent restructures the retrieved information Rt into a working context tailored to the current input xt. This step may involve forming a structured prompt (Wang et al., 2024), selecting key memory items (Chhikara et al., 2025; Xu et al., 2025), or merging retrieved content (Suzgun et al., 2025) into a short summary. We denote the resulting context as C₁ = C(xt, R₁), and the final output is

<a id='be960117-3c96-4a88-b812-3813e7122e39'></a>

ŷ_t = F(Ĉ_t).

<a id='63cf16c1-3fb2-46a9-bc94-21b935366ebe'></a>

**Evolve.** After obtaining $\hat{y}_t$, the agent constructs a new memory entry $m_t = h(x_t, \hat{y}_t, f_t)$ that captures the current step's experience together with the feedback $f_t$, such as whether the task was completed. The memory is then updated via:

$M_{t+1} = U(M_t, m_t).$

<a id='706ad403-8567-4501-8613-0881c9ea612c'></a>

Different algorithms instantiate _U_ differently, for example, direct append for retrieval-based memories, summarization or compression for long-term storage, or replacement for bounded-capacity stores. This unified formulation abstracts the essential cycle of _retrieval_, _synthesis_, and _evolution_ underlying all memory-based agents.

<a id='ab8bf898-c079-4e6a-8e48-2c1cc30fdc38'></a>

**Dataset Preparation.** Evo-Memory restructures conventional static datasets into *streaming task sequences*, enabling evaluation of how LLMs reuse and evolve memory over time. Each dataset can thus be transformed into a sequence τ = {(x1, y1), . . ., (xT, yT)}, forming a ground-truth trajectory in which earlier tasks provide essential information or strategies for later ones. At each step *t*, the agent processes input *xt*, retrieves and synthesizes memory, produces prediction ŷ*t*, and updates the memory state *Mt*, yielding the predicted trajectory:

<a id='a3746680-3521-4c29-822b-f7c2ee750c20'></a>

(x_1, \hat{y}_1, M_1) \rightarrow (x_2, \hat{y}_2, M_2) \rightarrow \cdots \rightarrow (x_T, \hat{y}_T, M_T).

<a id='cff16711-450b-4a06-88fb-b5a3ec4a7496'></a>

This design transforms static benchmarks into interactive evaluation streams that explicitly probe an LLM's ability to accumulate, adapt, and refine knowledge during deployment.

<a id='0fbe90c4-830b-4e4c-b3d7-2b6c48a22233'></a>

5

<!-- PAGE BREAK -->

<a id='1257afcd-bd1e-4f71-8386-88a8b1cafbc2'></a>

Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory

<a id='8ac3652b-96fb-49c4-aedd-e8723ad20993'></a>

## 3.2. ExpRAG: Experience Retrieval and Aggregation

As a simple baseline and extension, we define **ExpRAG**, a task-level retrieval-augmented agent. Each memory entry _mᵢ_ = _S_(_xᵢ_, _ŷᵢ_, _fᵢ_) encodes a structured experience text with template _S_. At step _t_, the agent retrieves _k_ similar experiences from memory according to a retrieval score _ϕ_:

<a id='0d92c7cf-7844-43b4-8885-14d6cbd1204e'></a>

<::R_t = Top-k_{m_i \in M_t} \phi(x_t, m_i).
: figure::>

<a id='bcbf758b-71e6-4d9f-984b-3a059302172c'></a>

The model conditions on these retrieved examples following the in-context learning principle:

<a id='7a2c037c-b787-4b05-a819-437471935dbd'></a>

ŷt = F(xt, Rt),

<a id='6a0983f4-def2-48ca-8fa4-d306815712aa'></a>

and appends the new experience to memory:

<a id='400c6515-7531-4f52-8806-a8913b6b046a'></a>

Mt+1 = Mt∪ {(xt, ŷt, ft)}.

<a id='356aa96b-08a5-44dc-ac4a-893a0ab56051'></a>

ExpRAG thus performs one-shot experience reuse through retrieval and aggregation. It captures how simple memory-based extensions of in-context learning behave but lacks iterative reasoning or adaptive refinement during inference.

<a id='03956994-85ed-4d81-baee-d96b6f5006c9'></a>

### 3.3. ReMem: Synergizing Reasoning, Acting, and Memory

We propose **ReMem**, a simple yet effective framework that unifies reasoning, action, and memory refinement within a single decision loop. Unlike conventional retrieval-augmented or ReAct-style methods that treat memory as static context, ReMem introduces a third dimension of *memory reasoning*, allowing the agent to actively evaluate, reorganize, and evolve its own memory during problem solving.

<a id='9818a7db-78c3-4de7-81b0-ab6c8f211f58'></a>

At each step t, given the current input xt, memory state Mt, and previous reasoning traces o1:n-1
at this step, the agent selects one of three operations:

<a id='b9aa025d-a469-4317-9e85-7b341258610f'></a>

$a_t^n \in \{\text{Think, Act, Refine}\}$.

<a id='42a71218-2f57-4f27-98ff-2dd5e9f4ba42'></a>

It then performs the operation and transitions according to:

$o_t^n = \text{Agent}(x_t, M_t, a_t^n)$

<a id='d93ba1df-b7ae-4285-8fc3-706c39631579'></a>

where $o_t^n$ denotes the output generated at step $t$ after $n$ operations, such as an intermediate reasoning trace, an external action, and memory refine thoughts.

<a id='7355906c-5b97-40c5-9fab-9255f057d950'></a>

Specifically, *Think* produces internal reasoning traces that help decompose the task and guide subsequent actions; *Act* executes an operation in the environment or outputs a response observable to the user; *Refine* performs meta-reasoning over memory, which exploiting useful experiences, pruning noise, and reorganizing $M_t$, to better support future reasoning and action. Within each step, the agent may perform multiple rounds of *Think* and *Refine*, and the step terminates once an *Act* operation is selected. This induces a Markov decision process where the state at step $t$ after $n$ operations is $s_t^n = (x_t, M_t, o_t^{1:n-1})$, the action space is {*Think*, *Act*, *Refine*}, and the transition dynamics are given by the Agent operator together with the environment response. Depending on the task, the *Act*-output of step $t$ may serve as the final answer for single-step tasks or as an intermediate result in multi-step settings, where the process continues until the overall task is completed.

<a id='6ea7cddd-8332-44be-b928-3893a290ec75'></a>

This unified formulation expands the action space of ReAct-style (Yao et al., 2022) agents by introducing an explicit memory reasoning mechanism. Through this extension, memory becomes an adaptive component that interacts with reasoning in real time rather than remaining a passive context. Under this view, the entire decision loop can also be interpreted as a Markov process, where the state

<a id='f4d8e855-bacc-438d-9fb5-07cbbd2f4d5f'></a>

6

<!-- PAGE BREAK -->

<a id='5b778fdb-8426-49aa-a131-9316877ff798'></a>

Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory

<a id='60638ca7-f220-46ec-a1ed-49d98753a89c'></a>

encapsulates the current input, memory state, and ongoing reasoning traces. Such integration yields a lightweight yet powerful paradigm for continual adaptation, where the agent learns to reason about both the task and its own knowledge state. By coupling reflection with memory evolution, ReMem establishes a new standard for adaptive, self-improving LLM agents.

<a id='2ac177a3-44f3-41cb-8584-75d4729f5642'></a>

## 4. Experiments

In this section, we evaluate leading LLMs on the Evo-Memory benchmark under our unified test-time learning pipeline, focusing on five key research questions (RQs):

*   RQ1: How do LLM agents perform on Evo-Memory across diverse domains and task types, and does REMEM enhance their test-time learning ability?
*   RQ2: What factors influence the effectiveness of memory in different tasks, and how does experience reuse improve task efficiency?
*   RQ3: How does task sequence difficulty (e.g., easy vs. hard trajectories) affect memory adaptation and generalization?
*   RQ4: How do varying feedback types impact learning dynamics and memory refinement across tasks?
*   RQ5: How does cumulative performance evolve over task sequences and time steps, reflecting continual adaptation during deployment?

<a id='3a454955-9223-4b09-95ce-b687b4fe190a'></a>

## 4.1. Experimental Setup

Evo-Memory evaluates memory mechanisms under realistic streaming multi-task conditions. In what follows, we describe the benchmark datasets, metrics, and the methods compared.

<a id='9e98a45b-7c41-4b22-bb83-21a33c767f7a'></a>

## 4.1.1. Datasets

Evo-Memory is evaluated on a diverse suite of datasets spanning factual knowledge, reasoning, mathematics, programming, and goal-oriented interaction. For factual and reasoning ability, we include **MMLU-Pro** (Zheng et al., 2024) and **GPQA-Diamond** (Rein et al., 2024), which test multi-disciplinary and graduate-level reasoning. For mathematical problem solving, we use **AIME-24** and **AIME-25** (HuggingFaceH4, 2024), containing Olympiad-style challenges requiring symbolic reasoning and exact-match evaluation. For tool-use and API grounding, we include **ToolBench** (Patil et al., 2023). For multi-turn and goal-oriented interaction, we adopt the **AgentBoard** (Zhuang et al., 2024) suite, covering **Alf World** (Shridhar et al., 2021), **BabyAI** (Chevalier-Boisvert et al., 2019), **ScienceWorld** (Wang et al., 2022), **Jericho** (Hausknecht et al., 2020), and **PDDL** tasks (Yang et al., 2023). Together, these datasets span both single-turn and interactive settings, enabling a unified evaluation of factual recall, procedural reasoning, and long-horizon adaptation. All methods are evaluated under the same *search-predict-evolve* loop

<a id='62a050ac-ecb9-4fe7-b816-176e84287111'></a>

<::transcription of the content
: (xt, Mt) --search--> Rt --synthesis--> ŷt --evolve--> Mt+1::>

<a id='3c13d20d-201a-4902-a72b-e6726b85bf05'></a>

with identical prompting templates, configurations, and memory budgets if applicable. Feedback $f_t$ is considered as the correctness signal.

<a id='6fe46f53-f601-4ba2-b4b5-81238a93b37a'></a>

### 4.1.2. Evaluation
Evo-Memory evaluates both task performance and memory quality along four dimensions. First, **answer accuracy** measures whether the model produces correct outputs in single-turn tasks. Second,

<a id='96781777-537f-4f02-a634-0ba6005c7362'></a>

7

<!-- PAGE BREAK -->

<a id='73fbb833-23dd-4a89-a0ab-7727f32621d2'></a>

Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory

<a id='6d328850-9dad-426c-88d6-f542618377de'></a>

**success rate** and **progress rate** evaluate goal completion in multi-turn tasks. Third, **step efficiency** tracks how many steps are needed to reach a goal, reflecting reasoning conciseness. Finally, **sequence robustness** tests whether performance stays stable under different task orders. Together, these metrics assess how well the agent learns, adapts, and reuses knowledge over time.

<a id='b8abe768-6f1b-4e64-b8d0-3009157504bc'></a>

### 4.1.3. Methods

We benchmark a broad range of agents and memory architectures instantiated on two strong LLM backbones: the Gemini-2.5 series (Comanici et al., 2025) (FLASH, FLASH-LITE, and PRO) and the Claude family (Anthropic, 2025) (3.5-HAIKU and 3.7-SONNET). The evaluated methods are grouped into four categories: (1) **Agent pipelines without persistent memory**, including ReAct (Yao et al., 2022) and Amem (Xu et al., 2025), which rely on short-term context or lightweight caches; (2) **Adaptive agentic memory methods**, such as SelfRAG (Asai et al., 2024b), MemOS (Li et al., 2025), Mem0 (Chhikara et al., 2025), and LangMem (LangChain contributors, 2025), which support dynamic retrieval and continual updates; (3) **Memory-based agents for procedural knowledge**, including Dynamic Cheatsheet (DC) (Suzgun et al., 2025) with two variants Cumulative (Cu) and Synthesis (RS) and Agent Workflow Memory (AWM) (Wang et al., 2024), which emphasize reusable workflows and task strategies; and (4) **Proposed evolving-memory framework**, comprising ExpRecent, ExpRAG, and ReMem, which unify reasoning, action, and memory refinement in a self-evolving loop. All methods are evaluated under a unified *search-predict-evolve* protocol to isolate the effects of memory design. Implementation and prompting details are provided in Appendix A. We exclude systems such as MemoryGpt (Zhong et al., 2023) and MemoryBank (Zhong et al., 2023) that target factual recall only, since Evo-Memory is designed to test evolving and procedural memory. Our goal is not to improve or modify the underlying LLMs themselves. Certain methods, such as MemOS and LangMem, are not fully compatible with embodied environments, and thus we exclude them from multi-turn datasets. Evo-Memory isolates the effect of search and evolution in memory mechanisms, so that observed differences reflect solely memory design rather than raw LLM capability.

<a id='590c2b79-b873-4687-89fe-2425c2a4f47f'></a>

## 4.2. Experiments
Below are the conducted experiments to answer the proposed research questions.

<a id='4956fa29-082d-4ed4-95c5-80ea56bcc8d6'></a>

## 4.3. Analysis of Results (RQ1)

Tables 1 and 2 summarize the results across single-turn and multi-turn settings. Overall, Evo-Memory demonstrates that self-evolving memory architectures provide consistent improvements. In single-turn reasoning and QA benchmarks (AIME-24/25, GPQA, MMLU-Pro, ToolBench), evolving-memory methods show consistent improvements, with REMEM achieving 0.65 average exact match and 0.85/0.71 API accuracy under Gemini-2.5 Flash. Adaptive retrieval methods enhance factual grounding, yet only evolving systems maintain consistent gains through iterative refinement. Agents with procedural knowledge perform well on structured domains such as AIME but lag in scientific reasoning and tool use, showing limited flexibility. ExPRAG serves as a simple yet highly effective baseline, outperforming several more complex designs. While improvements in single-turn settings are moderate, the overall trend remains consistent across datasets and model families.

<a id='23546284-b33b-4815-a9da-6bfa812e3fca'></a>

In multi-turn reasoning environments (AlfWorld, BabyAI, PDDL, ScienceWorld), REMEM and
ExpRAG achieve strong and stable performance on both Gemini-2.5 and Claude backbones, reach-
ing 0.92/0.96 on BabyAI and 0.95/0.62 on ScienceWorld. These results indicate that continual
reflection and refinement substantially improve procedural knowledge accumulation. Performance
gains are notably larger in multi-turn settings, underscoring that continual adaptation becomes

<a id='0feb80c0-fdb4-4256-9587-75fb5803c5fc'></a>

8

<!-- PAGE BREAK -->

<a id='830bf403-e656-44b0-b91f-bb6413fcf34c'></a>

Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory

<a id='46f09c2f-b8df-4321-9583-b1f259f1b4bd'></a>

<table id="8-1">
<tr><td id="8-2" rowspan="2">LLM Backbone</td><td id="8-3" rowspan="2">Method</td><td id="8-4" colspan="6">Exact Match ↑</td><td id="8-5">API / Acc. ↑</td><td id="8-6"></td></tr>
<tr><td id="8-7">AIME24</td><td id="8-8">AIME25</td><td id="8-9">GPQA</td><td id="8-a">MMLU-Pro (Eco.)</td><td id="8-b">MMLU-Pro (Eng.)</td><td id="8-c">MMLU-Pro (Philo.)</td><td id="8-d">ToolBench</td><td id="8-e">Avg. ↑</td></tr>
<tr><td id="8-f" rowspan="2"></td><td id="8-g">Baseline</td><td id="8-h">0.17</td><td id="8-i">0.13</td><td id="8-j">0.55</td><td id="8-k">0.84</td><td id="8-l">0.63</td><td id="8-m">0.78</td><td id="8-n">0.76/0.62</td><td id="8-o">0.54</td></tr>
<tr><td id="8-p">History</td><td id="8-q">0.13</td><td id="8-r">0.23</td><td id="8-s">0.56</td><td id="8-t">0.85</td><td id="8-u">0.64</td><td id="8-v">0.78</td><td id="8-w">0.76/0.61</td><td id="8-x">0.55</td></tr>
<tr><td id="8-y" rowspan="2"></td><td id="8-z">ReAct</td><td id="8-A">0.17</td><td id="8-B">0.10</td><td id="8-C">0.57</td><td id="8-D">0.84</td><td id="8-E">0.63</td><td id="8-F">0.76</td><td id="8-G">0.76/0.61</td><td id="8-H">0.54</td></tr>
<tr><td id="8-I">Amem</td><td id="8-J">0.27</td><td id="8-K">0.17</td><td id="8-L">0.54</td><td id="8-M">0.83</td><td id="8-N">0.63</td><td id="8-O">0.79</td><td id="8-P">0.77/0.63</td><td id="8-Q">0.56</td></tr>
<tr><td id="8-R" rowspan="7">Claude 3.7 Sonnet</td><td id="8-S">SelfRAG</td><td id="8-T">0.20</td><td id="8-U">0.10</td><td id="8-V">0.58</td><td id="8-W">0.84</td><td id="8-X">0.65</td><td id="8-Y">0.77</td><td id="8-Z">0.77/0.63</td><td id="8-10">0.55</td></tr>
<tr><td id="8-11">MemOS</td><td id="8-12">0.17</td><td id="8-13">0.20</td><td id="8-14">0.55</td><td id="8-15">0.84</td><td id="8-16">0.64</td><td id="8-17">0.76</td><td id="8-18">0.76/0.62</td><td id="8-19">0.55</td></tr>
<tr><td id="8-1a">Memo</td><td id="8-1b">0.20</td><td id="8-1c">0.13</td><td id="8-1d">0.58</td><td id="8-1e">0.84</td><td id="8-1f">0.62</td><td id="8-1g">0.77</td><td id="8-1h">0.76/0.61</td><td id="8-1i">0.55</td></tr>
<tr><td id="8-1j">LangMem</td><td id="8-1k">0.10</td><td id="8-1l">0.13</td><td id="8-1m">0.53</td><td id="8-1n">0.77</td><td id="8-1o">0.56</td><td id="8-1p">0.66</td><td id="8-1q">0.77/0.63</td><td id="8-1r">0.49</td></tr>
<tr><td id="8-1s">DC-Cu</td><td id="8-1t">0.17</td><td id="8-1u">0.23</td><td id="8-1v">0.57</td><td id="8-1w">0.79</td><td id="8-1x">0.52</td><td id="8-1y">0.65</td><td id="8-1z">0.77/0.62</td><td id="8-1A">0.52</td></tr>
<tr><td id="8-1B">DC-RS</td><td id="8-1C">0.20</td><td id="8-1D">0.20</td><td id="8-1E">0.62</td><td id="8-1F">0.79</td><td id="8-1G">0.52</td><td id="8-1H">0.60</td><td id="8-1I">0.77/0.62</td><td id="8-1J">0.52</td></tr>
<tr><td id="8-1K">AWM</td><td id="8-1L">0.03</td><td id="8-1M">0.03</td><td id="8-1N">0.53</td><td id="8-1O">0.80</td><td id="8-1P">0.56</td><td id="8-1Q">0.72</td><td id="8-1R">0.76/0.62</td><td id="8-1S">0.48</td></tr>
<tr><td id="8-1T" rowspan="3"></td><td id="8-1U">ExpRecent</td><td id="8-1V">0.13</td><td id="8-1W">0.20</td><td id="8-1X">0.61</td><td id="8-1Y">0.86</td><td id="8-1Z">0.63</td><td id="8-20">0.78</td><td id="8-21">0.82/0.66</td><td id="8-22">0.56</td></tr>
<tr><td id="8-23">ExpRAG</td><td id="8-24">0.17</td><td id="8-25">0.17</td><td id="8-26">0.70</td><td id="8-27">0.85</td><td id="8-28">0.67</td><td id="8-29">0.80</td><td id="8-2a">0.88/0.72</td><td id="8-2b">0.59</td></tr>
<tr><td id="8-2c">ReMem</td><td id="8-2d">0.13</td><td id="8-2e">0.13</td><td id="8-2f">0.67</td><td id="8-2g">0.86</td><td id="8-2h">0.65</td><td id="8-2i">0.80</td><td id="8-2j">0.87/0.71</td><td id="8-2k">0.58</td></tr>
<tr><td id="8-2l" rowspan="4"></td><td id="8-2m">Baseline</td><td id="8-2n">0.47</td><td id="8-2o">0.47</td><td id="8-2p">0.48</td><td id="8-2q">0.83</td><td id="8-2r">0.46</td><td id="8-2s">0.75</td><td id="8-2t">0.71/0.61</td><td id="8-2u">0.59</td></tr>
<tr><td id="8-2v">History</td><td id="8-2w">0.60</td><td id="8-2x">0.47</td><td id="8-2y">0.43</td><td id="8-2z">0.84</td><td id="8-2A">0.42</td><td id="8-2B">0.78</td><td id="8-2C">0.62/0.54</td><td id="8-2D">0.58</td></tr>
<tr><td id="8-2E">ReAct</td><td id="8-2F">0.30</td><td id="8-2G">0.27</td><td id="8-2H">0.05</td><td id="8-2I">0.64</td><td id="8-2J">0.16</td><td id="8-2K">0.54</td><td id="8-2L">0.64/0.57</td><td id="8-2M">0.37</td></tr>
<tr><td id="8-2N">Amem</td><td id="8-2O">0.70</td><td id="8-2P">0.57</td><td id="8-2Q">0.52</td><td id="8-2R">0.83</td><td id="8-2S">0.42</td><td id="8-2T">0.72</td><td id="8-2U">0.72/0.60</td><td id="8-2V">0.63</td></tr>
<tr><td id="8-2W" rowspan="2"></td><td id="8-2X">SelfRAG</td><td id="8-2Y">0.50</td><td id="8-2Z">0.47</td><td id="8-30">0.46</td><td id="8-31">0.83</td><td id="8-32">0.45</td><td id="8-33">0.75</td><td id="8-34">0.72/0.61</td><td id="8-35">0.59</td></tr>
<tr><td id="8-36">MemOS</td><td id="8-37">0.47</td><td id="8-38">0.47</td><td id="8-39">0.50</td><td id="8-3a">0.82</td><td id="8-3b">0.46</td><td id="8-3c">0.75</td><td id="8-3d">0.71/0.61</td><td id="8-3e">0.59</td></tr>
<tr><td id="8-3f" rowspan="5">Gemini 2.5 Flash</td><td id="8-3g">Mem0</td><td id="8-3h">0.50</td><td id="8-3i">0.47</td><td id="8-3j">0.45</td><td id="8-3k">0.83</td><td id="8-3l">0.46</td><td id="8-3m">0.74</td><td id="8-3n">0.71/0.61</td><td id="8-3o">0.59</td></tr>
<tr><td id="8-3p">LangMem</td><td id="8-3q">0.43</td><td id="8-3r">0.50</td><td id="8-3s">0.53</td><td id="8-3t">0.79</td><td id="8-3u">0.39</td><td id="8-3v">0.71</td><td id="8-3w">0.68/0.57</td><td id="8-3x">0.57</td></tr>
<tr><td id="8-3y">DC-Cu</td><td id="8-3z">0.60</td><td id="8-3A">0.40</td><td id="8-3B">0.48</td><td id="8-3C">0.79</td><td id="8-3D">0.44</td><td id="8-3E">0.69</td><td id="8-3F">0.70/0.59</td><td id="8-3G">0.58</td></tr>
<tr><td id="8-3H">DC-RS</td><td id="8-3I">0.53</td><td id="8-3J">0.37</td><td id="8-3K">0.48</td><td id="8-3L">0.80</td><td id="8-3M">0.42</td><td id="8-3N">0.69</td><td id="8-3O">0.68/0.57</td><td id="8-3P">0.56</td></tr>
<tr><td id="8-3Q">AWM</td><td id="8-3R">0.50</td><td id="8-3S">0.37</td><td id="8-3T">0.49</td><td id="8-3U">0.79</td><td id="8-3V">0.43</td><td id="8-3W">0.72</td><td id="8-3X">0.71/0.59</td><td id="8-3Y">0.56</td></tr>
<tr><td id="8-3Z" rowspan="3"></td><td id="8-40">ExpRecent</td><td id="8-41">0.47</td><td id="8-42">0.47</td><td id="8-43">0.42</td><td id="8-44">0.83</td><td id="8-45">0.39</td><td id="8-46">0.75</td><td id="8-47">0.78/0.66</td><td id="8-48">0.58</td></tr>
<tr><td id="8-49">ExpRAG</td><td id="8-4a">0.43</td><td id="8-4b">0.47</td><td id="8-4c">0.42</td><td id="8-4d">0.83</td><td id="8-4e">0.43</td><td id="8-4f">0.78</td><td id="8-4g">0.87/0.73</td><td id="8-4h">0.60</td></tr>
<tr><td id="8-4i">ReMem</td><td id="8-4j">0.60</td><td id="8-4k">0.53</td><td id="8-4l">0.51</td><td id="8-4m">0.85</td><td id="8-4n">0.46</td><td id="8-4o">0.79</td><td id="8-4p">0.85/0.71</td><td id="8-4q">0.65</td></tr>
</table>

<a id='cb68439f-fb57-4f2a-b9fe-e0a82a46412c'></a>

Table 1 | Cross-dataset results of diverse memory architectures across models on the single-turn reasoning and question answering datasets. Categories are separated by horizontal rules; results (Exact Match↑ and API/Acc↑) compare zero-shot, agentic, adaptive, procedural, and proposed memory methods.

<a id='b2054501-fcd8-4922-aedc-44621eaa1475'></a>

increasingly valuable as task horizons lengthen. While many baselines enhance retrieval grounding, they struggle to reuse long-horizon experiences and often falter in open-ended environments. Notably, lightweight variants such as ExpRECENT and ExpPRAG still perform competitively despite their simplicity, suggesting that explicit task-level utilization during test-time evolution is both promising and underexplored.

<a id='ef84ba03-5e58-4157-b4f6-5715cecf78dc'></a>

Across all experiments, evolving-memory methods demonstrate consistent gains on both Gemini and Claude backbones. Smaller models benefit particularly from self-evolving memory, suggesting that test-time refinement is a practical path to enhancing the capability of lighter LLMs. Together, these findings establish task-level memory utilization and continual reorganization as valuable directions for future research, providing a standardized reference point for developing and evaluating evolving-memory agents. Additional results across more LLM families are presented in Appendix B.1.

<a id='2872ddec-6ec6-44a4-8c21-fec0d031db75'></a>

## 4.4. Analysis of Memory Improvement (RQ2)

Figure 4 shows that REMEM's improvement strongly correlates with within-dataset task similarity (Pearson r = 0.717 on Gemini 2.5 Flash and r = 0.563 on Claude 3.7 Sonnet). Task similarity is measured by computing the average cosine distance between each task embedding and its dataset cluster center, where embeddings are obtained from the retriever encoder. A smaller average distance

<a id='a8423b38-45db-4ee4-b013-156209ed148f'></a>

9

<!-- PAGE BREAK -->

<a id='436e7146-a47c-4ec4-bc2e-bdbf874c2741'></a>

Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory

<a id='5332c04f-00fb-49ec-9581-ac8afbbef34b'></a>

<table id="9-1">
<tr><td id="9-2" rowspan="2">LLM Backbone</td><td id="9-3">Method</td><td id="9-4" colspan="2">Alf World</td><td id="9-5" colspan="2">BabyAI</td><td id="9-6" colspan="2">PDDL</td><td id="9-7" colspan="2">ScienceWorld</td><td id="9-8" colspan="2">Avg.</td></tr>
<tr><td id="9-9"></td><td id="9-a">S</td><td id="9-b">P</td><td id="9-c">S</td><td id="9-d">P</td><td id="9-e">S</td><td id="9-f">P</td><td id="9-g">S</td><td id="9-h">P</td><td id="9-i">S</td><td id="9-j">P</td></tr>
<tr><td id="9-k"></td><td id="9-l">Baseline</td><td id="9-m">0.12</td><td id="9-n">0.34</td><td id="9-o">0.61</td><td id="9-p">0.71</td><td id="9-q">0.12</td><td id="9-r">0.20</td><td id="9-s">0.24</td><td id="9-t">0.59</td><td id="9-u">0.27</td><td id="9-v">0.46</td></tr>
<tr><td id="9-w"></td><td id="9-x">History</td><td id="9-y">0.28</td><td id="9-z">0.60</td><td id="9-A">0.52</td><td id="9-B">0.64</td><td id="9-C">0.08</td><td id="9-D">0.15</td><td id="9-E">0.31</td><td id="9-F">0.71</td><td id="9-G">0.30</td><td id="9-H">0.53</td></tr>
<tr><td id="9-I"></td><td id="9-J">ReAct</td><td id="9-K">0.24</td><td id="9-L">0.56</td><td id="9-M">0.48</td><td id="9-N">0.63</td><td id="9-O">0.22</td><td id="9-P">0.33</td><td id="9-Q">0.34</td><td id="9-R">0.71</td><td id="9-S">0.32</td><td id="9-T">0.56</td></tr>
<tr><td id="9-U"></td><td id="9-V">Amem</td><td id="9-W">0.25</td><td id="9-X">0.59</td><td id="9-Y">0.53</td><td id="9-Z">0.64</td><td id="9-10">0.10</td><td id="9-11">0.16</td><td id="9-12">0.36</td><td id="9-13">0.74</td><td id="9-14">0.31</td><td id="9-15">0.53</td></tr>
<tr><td id="9-16"></td><td id="9-17">Self RAG</td><td id="9-18">0.25</td><td id="9-19">0.59</td><td id="9-1a">0.52</td><td id="9-1b">0.65</td><td id="9-1c">0.08</td><td id="9-1d">0.16</td><td id="9-1e">0.34</td><td id="9-1f">0.74</td><td id="9-1g">0.30</td><td id="9-1h">0.54</td></tr>
<tr><td id="9-1i"></td><td id="9-1j">Memo</td><td id="9-1k">0.27</td><td id="9-1l">0.61</td><td id="9-1m">0.54</td><td id="9-1n">0.66</td><td id="9-1o">0.10</td><td id="9-1p">0.19</td><td id="9-1q">0.32</td><td id="9-1r">0.70</td><td id="9-1s">0.31</td><td id="9-1t">0.54</td></tr>
<tr><td id="9-1u">Gemini 2.5 Flash</td><td id="9-1v">DC-Cu</td><td id="9-1w">0.25</td><td id="9-1x">0.59</td><td id="9-1y">0.53</td><td id="9-1z">0.64</td><td id="9-1A">0.08</td><td id="9-1B">0.17</td><td id="9-1C">0.29</td><td id="9-1D">0.71</td><td id="9-1E">0.29</td><td id="9-1F">0.53</td></tr>
<tr><td id="9-1G"></td><td id="9-1H">DC-RS</td><td id="9-1I">0.27</td><td id="9-1J">0.60</td><td id="9-1K">0.53</td><td id="9-1L">0.66</td><td id="9-1M">0.07</td><td id="9-1N">0.15</td><td id="9-1O">0.33</td><td id="9-1P">0.73</td><td id="9-1Q">0.30</td><td id="9-1R">0.54</td></tr>
<tr><td id="9-1S"></td><td id="9-1T">AWM</td><td id="9-1U">0.26</td><td id="9-1V">0.59</td><td id="9-1W">0.52</td><td id="9-1X">0.64</td><td id="9-1Y">0.08</td><td id="9-1Z">0.16</td><td id="9-20">0.33</td><td id="9-21">0.73</td><td id="9-22">0.30</td><td id="9-23">0.53</td></tr>
<tr><td id="9-24"></td><td id="9-25">ExpRecent</td><td id="9-26">0.37</td><td id="9-27">0.65</td><td id="9-28">0.53</td><td id="9-29">0.64</td><td id="9-2a">0.13</td><td id="9-2b">0.22</td><td id="9-2c">0.53</td><td id="9-2d">0.83</td><td id="9-2e">0.39</td><td id="9-2f">0.59</td></tr>
<tr><td id="9-2g"></td><td id="9-2h">ExpRAG</td><td id="9-2i">0.59</td><td id="9-2j">0.79</td><td id="9-2k">0.56</td><td id="9-2l">0.65</td><td id="9-2m">0.17</td><td id="9-2n">0.27</td><td id="9-2o">0.53</td><td id="9-2p">0.81</td><td id="9-2q">0.46</td><td id="9-2r">0.63</td></tr>
<tr><td id="9-2s"></td><td id="9-2t">ReMem</td><td id="9-2u">0.66</td><td id="9-2v">0.81</td><td id="9-2w">0.53</td><td id="9-2x">0.61</td><td id="9-2y">0.22</td><td id="9-2z">0.33</td><td id="9-2A">0.58</td><td id="9-2B">0.81</td><td id="9-2C">0.50</td><td id="9-2D">0.64</td></tr>
<tr><td id="9-2E"></td><td id="9-2F">Baseline</td><td id="9-2G">0.18</td><td id="9-2H">0.49</td><td id="9-2I">0.51</td><td id="9-2J">0.66</td><td id="9-2K">0.17</td><td id="9-2L">0.39</td><td id="9-2M">0.10</td><td id="9-2N">0.53</td><td id="9-2O">0.24</td><td id="9-2P">0.52</td></tr>
<tr><td id="9-2Q"></td><td id="9-2R">History</td><td id="9-2S">0.50</td><td id="9-2T">0.73</td><td id="9-2U">0.48</td><td id="9-2V">0.66</td><td id="9-2W">0.65</td><td id="9-2X">0.85</td><td id="9-2Y">0.32</td><td id="9-2Z">0.74</td><td id="9-30">0.49</td><td id="9-31">0.74</td></tr>
<tr><td id="9-32"></td><td id="9-33">ReAct</td><td id="9-34">0.51</td><td id="9-35">0.75</td><td id="9-36">0.57</td><td id="9-37">0.72</td><td id="9-38">0.75</td><td id="9-39">0.91</td><td id="9-3a">0.44</td><td id="9-3b">0.77</td><td id="9-3c">0.57</td><td id="9-3d">0.79</td></tr>
<tr><td id="9-3e"></td><td id="9-3f">Amem</td><td id="9-3g">0.48</td><td id="9-3h">0.73</td><td id="9-3i">0.46</td><td id="9-3j">0.64</td><td id="9-3k">0.62</td><td id="9-3l">0.84</td><td id="9-3m">0.33</td><td id="9-3n">0.73</td><td id="9-3o">0.47</td><td id="9-3p">0.73</td></tr>
<tr><td id="9-3q"></td><td id="9-3r">SelfRAG</td><td id="9-3s">0.52</td><td id="9-3t">0.75</td><td id="9-3u">0.46</td><td id="9-3v">0.64</td><td id="9-3w">0.65</td><td id="9-3x">0.84</td><td id="9-3y">0.31</td><td id="9-3z">0.74</td><td id="9-3A">0.49</td><td id="9-3B">0.74</td></tr>
<tr><td id="9-3C"></td><td id="9-3D">Mem0</td><td id="9-3E">0.51</td><td id="9-3F">0.74</td><td id="9-3G">0.48</td><td id="9-3H">0.66</td><td id="9-3I">0.65</td><td id="9-3J">0.84</td><td id="9-3K">0.37</td><td id="9-3L">0.76</td><td id="9-3M">0.50</td><td id="9-3N">0.75</td></tr>
<tr><td id="9-3O">Claude 3.7 Sonnet</td><td id="9-3P">DC-Cu</td><td id="9-3Q">0.50</td><td id="9-3R">0.74</td><td id="9-3S">0.50</td><td id="9-3T">0.67</td><td id="9-3U">0.62</td><td id="9-3V">0.84</td><td id="9-3W">0.33</td><td id="9-3X">0.75</td><td id="9-3Y">0.49</td><td id="9-3Z">0.75</td></tr>
<tr><td id="9-40"></td><td id="9-41">DC-RS</td><td id="9-42">0.50</td><td id="9-43">0.74</td><td id="9-44">0.52</td><td id="9-45">0.68</td><td id="9-46">0.62</td><td id="9-47">0.84</td><td id="9-48">0.34</td><td id="9-49">0.74</td><td id="9-4a">0.50</td><td id="9-4b">0.75</td></tr>
<tr><td id="9-4c"></td><td id="9-4d">AWM</td><td id="9-4e">0.49</td><td id="9-4f">0.73</td><td id="9-4g">0.53</td><td id="9-4h">0.68</td><td id="9-4i">0.60</td><td id="9-4j">0.82</td><td id="9-4k">0.34</td><td id="9-4l">0.74</td><td id="9-4m">0.49</td><td id="9-4n">0.74</td></tr>
<tr><td id="9-4o"></td><td id="9-4p">ExpRecent</td><td id="9-4q">0.66</td><td id="9-4r">0.83</td><td id="9-4s">0.63</td><td id="9-4t">0.73</td><td id="9-4u">0.53</td><td id="9-4v">0.76</td><td id="9-4w">0.49</td><td id="9-4x">0.82</td><td id="9-4y">0.58</td><td id="9-4z">0.79</td></tr>
<tr><td id="9-4A"></td><td id="9-4B">ExpRAG</td><td id="9-4C">0.74</td><td id="9-4D">0.89</td><td id="9-4E">0.62</td><td id="9-4F">0.72</td><td id="9-4G">0.72</td><td id="9-4H">0.89</td><td id="9-4I">0.46</td><td id="9-4J">0.76</td><td id="9-4K">0.63</td><td id="9-4L">0.82</td></tr>
<tr><td id="9-4M"></td><td id="9-4N">ReMem</td><td id="9-4O">0.92</td><td id="9-4P">0.96</td><td id="9-4Q">0.73</td><td id="9-4R">0.83</td><td id="9-4S">0.83</td><td id="9-4T">0.95</td><td id="9-4U">0.62</td><td id="9-4V">0.89</td><td id="9-4W">0.78</td><td id="9-4X">0.91</td></tr>
</table>

<a id='9ba41778-b52a-4da0-985f-cfa2f834ab4a'></a>

Table 2 | Cross-environment results across four embodied reasoning benchmarks (Alf World, BabyAI, PDDL, ScienceWorld). Each dataset reports success (S) and progress (P) rates. Bold indicates the best (including ties) per column. The last two columns show averaged S and P across datasets.

<a id='e6f6f5bb-132d-4e06-a273-a40e6a56c42d'></a>

indicates higher intra-dataset coherence and thus stronger structural similarity. Tasks with higher embedding cluster ratios, such as PDDL and Alf World, yield larger gains, suggesting that recurring task structures facilitate memory reuse and generalization. In contrast, more diverse or low-similarity datasets like AIME-25 or GPQA show smaller gains, reflecting limited transferable experiences. These findings highlight the importance of embedding organization and semantic overlap in driving effective memory evolution. Further analysis of memory pruning rates can be found in Appendix B.2.

<a id='1ab831d8-4414-49fc-b8f2-967090dd34d6'></a>

Figure 5 compares step efficiency across four environments. Evolving-memory methods consistently require fewer steps to reach completion, with REMEM achieving the strongest and most stable reductions (e.g., from 22.6 to 11.5 steps on Alf World). The lightweight ExPRAG and ExPRECENT also perform competitively, showing that simple task-level evolution can greatly improve efficiency without extra complexity. Overall, continual refinement not only boosts accuracy but also makes reasoning more focused and efficient.

<a id='b1f5d58c-e33a-4627-b5f8-d294f9601f95'></a>

10

<!-- PAGE BREAK -->

<a id='097f2b91-8d73-4c3e-8676-e597c87cd7f4'></a>

Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory

<a id='3fb661fb-71c2-4182-ad50-ff9d491c67c1'></a>

<::chart: Two scatter plots are displayed side-by-side, comparing "Memory Improvement (%)" on the y-axis against "Task Similarity" on the x-axis for two different models, Claude 3.7 Sonnet and Gemini 2.5 Flash. Each plot shows several data points, each labeled with a task name, and a dashed trend line with its Pearson correlation coefficient (r).||Plot 1: Claude 3.7 Sonnet. The title is "Claude 3.7 Sonnet" with "Pearson r = 0.563". The y-axis ranges from -40 to 100 in increments of 20. The x-axis ranges from 0.1 to 0.5 in increments of 0.1. Data points include: 'scienceworld' (approx. x=0.38, y=95), 'alfworld' (approx. x=0.43, y=80), 'babyai' (approx. x=0.47, y=55), 'pddl' (approx. x=0.51, y=30), 'toolbench' (approx. x=0.13, y=30), 'gpqa diamond' (approx. x=0.18, y=20), 'aime 2020' (approx. x=0.2, y=5), 'engineering' (approx. x=0.22, y=5), 'philosophy mics' (approx. x=0.23, y=0), and 'aime 2025' (approx. x=0.23, y=-45). A dashed blue line indicates a positive trend.||Plot 2: Gemini 2.5 Flash. The title is "Gemini 2.5 Flash" with "Pearson r = 0.717". The y-axis ranges from 0 to 175 in increments of 25. The x-axis ranges from 0.1 to 0.5 in increments of 0.1. Data points include: 'pddl' (approx. x=0.51, y=175), 'alfworld' (approx. x=0.43, y=135), 'scienceworld' (approx. x=0.37, y=90), 'babyai' (approx. x=0.47, y=10), 'toolbench' (approx. x=0.13, y=35), 'gpqa diamond' (approx. x=0.18, y=20), 'engineering' (approx. x=0.22, y=15), 'aime 2020' (approx. x=0.2, y=5), and 'philosophy mics' (approx. x=0.23, y=0). A dashed red line indicates a positive trend.||Figure 4 | ReMem performance gain over history baseline versus within-dataset task similarity.::>

<a id='854a0de0-dbe4-4658-aa6d-76a212f6d8be'></a>

<::bar chart: Average Steps to Complete Tasks. The y-axis is labeled "Average Number of Steps". The x-axis shows four benchmarks: ALFWORLD, BABYAI, PDDL, and SCIENCEWORLD. There are four bars for each benchmark, representing four methods: History (red), ExpRecent (light blue), ExpRAG (green), and ReMem (dark blue). For ALFWORLD: History: 22.6, ExpRecent: 17.5, ExpRAG: 14.3, ReMem: 11.5. For BABYAI: History: 18.8, ExpRecent: 16.0, ExpRAG: 16.1, ReMem: 14.0. For PDDL: History: 20.5, ExpRecent: 22.6, ExpRAG: 19.9, ReMem: 17.5. For SCIENCEWORLD: History: 24.9, ExpRecent: 22.2, ExpRAG: 22.2, ReMem: 17.8. Figure 5 | Average steps to complete tasks across four benchmarks. We compare four methods: History, ExpRecent, ExpRAG, and ReMem. Lower is better. ReMem consistently requires fewer steps to complete tasks across all datasets, demonstrating more efficient task execution.::>

<a id='5fb3c0a0-b4dd-4a62-ae5c-495bc9db0bc3'></a>

<table id="10-1">
<tr><td id="10-2" rowspan="2">Direction</td><td id="10-3" rowspan="2">Method</td><td id="10-4" colspan="2">Alf World</td><td id="10-5" colspan="2">ScienceWorld</td><td id="10-6" colspan="2">Avg.</td></tr>
<tr><td id="10-7">S</td><td id="10-8">P</td><td id="10-9">S</td><td id="10-a">P</td><td id="10-b">S</td><td id="10-c">P</td></tr>
<tr><td id="10-d" colspan="2">Base</td><td id="10-e">0.50</td><td id="10-f">0.73</td><td id="10-g">0.32</td><td id="10-h">0.74</td><td id="10-i">0.41</td><td id="10-j">0.74</td></tr>
<tr><td id="10-k"></td><td id="10-l">ExpRecent</td><td id="10-m">0.66</td><td id="10-n">0.82</td><td id="10-o">0.48</td><td id="10-p">0.83</td><td id="10-q">0.57</td><td id="10-r">0.83</td></tr>
<tr><td id="10-s">Easy→Hard</td><td id="10-t">ExpRAG</td><td id="10-u">0.77</td><td id="10-v">0.87</td><td id="10-w">0.37</td><td id="10-x">0.71</td><td id="10-y">0.57</td><td id="10-z">0.79</td></tr>
<tr><td id="10-A"></td><td id="10-B">ReMem</td><td id="10-C">0.91</td><td id="10-D">0.96</td><td id="10-E">0.63</td><td id="10-F">0.88</td><td id="10-G">0.77</td><td id="10-H">0.92</td></tr>
<tr><td id="10-I"></td><td id="10-J">ExpRecent</td><td id="10-K">0.72</td><td id="10-L">0.85</td><td id="10-M">0.47</td><td id="10-N">0.80</td><td id="10-O">0.60</td><td id="10-P">0.83</td></tr>
<tr><td id="10-Q">Hard→Easy</td><td id="10-R">ExpRAG</td><td id="10-S">0.87</td><td id="10-T">0.92</td><td id="10-U">0.51</td><td id="10-V">0.81</td><td id="10-W">0.69</td><td id="10-X">0.87</td></tr>
<tr><td id="10-Y"></td><td id="10-Z">ReMem</td><td id="10-10">0.94</td><td id="10-11">0.97</td><td id="10-12">0.68</td><td id="10-13">0.90</td><td id="10-14">0.81</td><td id="10-15">0.94</td></tr>
</table>

<a id='efa74e0c-bf32-4371-a0f0-7079f7184679'></a>

Table 3 | Comparison of memory-based agents under different sequence difficulty directions. Each cell reports Success (S) and Progress (P). Easy→Hard and Hard→Easy indicate task order transitions, and averages (Avg) summarize per-direction performance.

<a id='5be38f1d-916c-491a-97e1-40c9b3ed799d'></a>

11

<!-- PAGE BREAK -->

<a id='5f743635-512a-4d43-bad5-3e52a1b00c3a'></a>

Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory

<a id='9a4afea0-cd2a-4e02-8457-8be3160e47ce'></a>

<table id="11-1">
<tr><td id="11-2" rowspan="2">Model</td><td id="11-3" rowspan="2">Method</td><td id="11-4" colspan="2">Alf World</td><td id="11-5" colspan="2">ScienceWorld</td><td id="11-6" colspan="2">Avg.</td></tr>
<tr><td id="11-7">S</td><td id="11-8">P</td><td id="11-9">S</td><td id="11-a">P</td><td id="11-b">S</td><td id="11-c">P</td></tr>
<tr><td id="11-d" rowspan="9">Claude 3.7 Sonnet</td><td id="11-e">Amem</td><td id="11-f">0.49</td><td id="11-g">0.73</td><td id="11-h">0.31</td><td id="11-i">0.74</td><td id="11-j">0.40</td><td id="11-k">0.74</td></tr>
<tr><td id="11-l">Self RAG</td><td id="11-m">0.47</td><td id="11-n">0.73</td><td id="11-o">0.34</td><td id="11-p">0.73</td><td id="11-q">0.41</td><td id="11-r">0.73</td></tr>
<tr><td id="11-s">Mem0</td><td id="11-t">0.49</td><td id="11-u">0.73</td><td id="11-v">0.36</td><td id="11-w">0.74</td><td id="11-x">0.43</td><td id="11-y">0.74</td></tr>
<tr><td id="11-z">DC-Cu</td><td id="11-A">0.52</td><td id="11-B">0.75</td><td id="11-C">0.34</td><td id="11-D">0.73</td><td id="11-E">0.43</td><td id="11-F">0.74</td></tr>
<tr><td id="11-G">DC-RS</td><td id="11-H">0.51</td><td id="11-I">0.74</td><td id="11-J">0.38</td><td id="11-K">0.74</td><td id="11-L">0.45</td><td id="11-M">0.74</td></tr>
<tr><td id="11-N">AWM</td><td id="11-O">0.55</td><td id="11-P">0.76</td><td id="11-Q">0.32</td><td id="11-R">0.72</td><td id="11-S">0.44</td><td id="11-T">0.74</td></tr>
<tr><td id="11-U">ExpRecent</td><td id="11-V">0.62</td><td id="11-W">0.80</td><td id="11-X">0.34</td><td id="11-Y">0.74</td><td id="11-Z">0.48</td><td id="11-10">0.77</td></tr>
<tr><td id="11-11">ExpRAG</td><td id="11-12">0.76</td><td id="11-13">0.90</td><td id="11-14">0.27</td><td id="11-15">0.63</td><td id="11-16">0.52</td><td id="11-17">0.77</td></tr>
<tr><td id="11-18">ReMem</td><td id="11-19">0.92</td><td id="11-1a">0.96</td><td id="11-1b">0.69</td><td id="11-1c">0.91</td><td id="11-1d">0.81</td><td id="11-1e">0.94</td></tr>
<tr><td id="11-1f" rowspan="3"></td><td id="11-1g">Amem</td><td id="11-1h">0.22</td><td id="11-1i">0.57</td><td id="11-1j">0.39</td><td id="11-1k">0.75</td><td id="11-1l">0.31</td><td id="11-1m">0.66</td></tr>
<tr><td id="11-1n">SelfRAG</td><td id="11-1o">0.25</td><td id="11-1p">0.58</td><td id="11-1q">0.36</td><td id="11-1r">0.71</td><td id="11-1s">0.31</td><td id="11-1t">0.65</td></tr>
<tr><td id="11-1u">Memo</td><td id="11-1v">0.25</td><td id="11-1w">0.59</td><td id="11-1x">0.34</td><td id="11-1y">0.71</td><td id="11-1z">0.30</td><td id="11-1A">0.65</td></tr>
<tr><td id="11-1B" rowspan="6">Gemini 2.5 Flash</td><td id="11-1C">DC-Cu</td><td id="11-1D">0.20</td><td id="11-1E">0.56</td><td id="11-1F">0.36</td><td id="11-1G">0.72</td><td id="11-1H">0.28</td><td id="11-1I">0.64</td></tr>
<tr><td id="11-1J">DC-RS</td><td id="11-1K">0.21</td><td id="11-1L">0.57</td><td id="11-1M">0.36</td><td id="11-1N">0.71</td><td id="11-1O">0.29</td><td id="11-1P">0.64</td></tr>
<tr><td id="11-1Q">AWM</td><td id="11-1R">0.19</td><td id="11-1S">0.56</td><td id="11-1T">0.36</td><td id="11-1U">0.74</td><td id="11-1V">0.28</td><td id="11-1W">0.65</td></tr>
<tr><td id="11-1X">ExpRecent</td><td id="11-1Y">0.22</td><td id="11-1Z">0.57</td><td id="11-20">0.59</td><td id="11-21">0.86</td><td id="11-22">0.41</td><td id="11-23">0.72</td></tr>
<tr><td id="11-24">ExpRAG</td><td id="11-25">0.25</td><td id="11-26">0.60</td><td id="11-27">0.51</td><td id="11-28">0.78</td><td id="11-29">0.38</td><td id="11-2a">0.69</td></tr>
<tr><td id="11-2b">ReMem</td><td id="11-2c">0.57</td><td id="11-2d">0.76</td><td id="11-2e">0.50</td><td id="11-2f">0.75</td><td id="11-2g">0.54</td><td id="11-2h">0.76</td></tr>
</table>

<a id='e29c2091-a395-48d5-bcb2-cec3bc8533a9'></a>

Table 4 | Results with both successful and failed task experiences on AlfWorld and ScienceWorld.
Each cell reports Success (S) and Progress (P). Horizontal rules separate method families. Bold
numbers denote the best results per metric within each model.

<a id='c3d3fda0-4004-466a-9155-720983411c06'></a>

## 4.5. Task Sequence: Easy v.s. Hard (RQ3)

Table 3 examines how memory-based agents adapt to changes in task difficulty. Baseline methods exhibit noticeable degradation when moving from easier to harder tasks, revealing limited robustness under distribution shifts. In contrast, evolving-memory agents, particularly REMEM, maintain strong and consistent performance across both directions, reaching up to 0.94/0.97 success and progress in the Hard→Easy setting. This stability shows that continual reflection enables REMEM to retain transferable knowledge even as task complexity varies. The results further indicate that the design of task sequences, especially the ordering of difficulty levels, has a substantial impact on evaluating memory adaptation. They also suggest that well-structured task progressions can actively facilitate learning by allowing models to build on prior experiences and generalize across increasingly complex challenges. Together, these findings highlight the importance of standardized and thoughtfully organized task sequences for both fair evaluation and effective model development in future benchmarks.

<a id='7d2dd2b2-ea6b-4a41-9303-f4b2c27f70a7'></a>

## 4.6. Analysis of Feedback (RQ4)

Table 4 evaluates how agents perform when both successful and failed task experiences are stored in memory. Baseline methods experience a clear performance drop when exposed to unfiltered failures, indicating that naive memory accumulation introduces noise and disrupts subsequent retrieval. In contrast, evolving-memory approaches, particularly ReMEM, remain robust by actively refining stored experiences, achieving the highest overall success and progress rates under both Claude and Gemini backbones. These results demonstrate that selective utilization, which involves learning from successes while appropriately leveraging information from failures, is crucial for stable test-time adaptation. They further highlight that memory refinement plays a central role in handling imperfect experiences and suggest that future work should explore failure-aware strategies for memory evolution.

<a id='d71277e0-f4c0-4403-9b50-7b09da94b4ca'></a>

12

<!-- PAGE BREAK -->

<a id='ed7560f9-225e-426b-923d-6fd150cef90e'></a>

Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory

<a id='f6af5501-e37c-4028-ab8a-a1f33aae6829'></a>

<::chart: The image displays Figure 6, a collection of four line charts illustrating the cumulative success rate across four interactive agent datasets. Each chart shows the performance of "ReMem" (solid blue line) and a "History" baseline (dashed red line) as a function of "Task Number" on the x-axis and "Cumulative Success Rate" on the y-axis (ranging from 0.0 to 1.0). The rolling average shows performance trends as more task instances are evaluated.  

**Chart 1: ALFWORLD**  
The ReMem line starts near 0.0, rapidly increases to approximately 0.8-0.9, and then stabilizes around 0.9. The History baseline also starts near 0.0, rises to about 0.7-0.8, then drops and fluctuates between 0.5 and 0.6, ending around 0.5.  

**Chart 2: BABYAI**  
The ReMem line begins at 1.0, drops to about 0.8, then stabilizes and slowly decreases from 0.8-0.9 to around 0.75. The History baseline also starts at 1.0, drops significantly to around 0.7, and then gradually decreases to approximately 0.5.  

**Chart 3: PDDL**  
The ReMem line starts at 1.0, drops to around 0.7, then increases and fluctuates, ending around 0.8-0.85. The History baseline starts at 1.0, drops sharply to approximately 0.5-0.6, then fluctuates and shows a slight increase, ending around 0.6-0.65.  

**Chart 4: SCIENCEWORLD**  
The ReMem line starts near 0.0, fluctuates widely between 0.3 and 0.7, then stabilizes around 0.5-0.6, slowly increasing to approximately 0.6. The History baseline starts near 0.0, fluctuates between 0.1 and 0.3, and then slowly increases to around 0.35.  

Overall, ReMem (solid blue) consistently outperforms the History baseline (dashed red) on ALFWorld, BabyAI, PDDL, and ScienceWorld tasks.::>

<a id='c6124bad-5143-4a6e-bf75-6461f03b2adc'></a>

## 4.7. Performance w.r.t Time Steps (RQ5)

Figure 6 shows the cumulative accuracy as tasks progress across four interactive environments. The curves mainly serve to compare REMEM with the History baseline, as individual trajectories carry limited standalone meaning. Across all environments, REMEM consistently achieves faster adaptation and more stable retention over time. These results highlight that continual reflection enables REMEM to sustain performance across long task sequences, illustrating its robustness in test-time learning. More comparative results on single-turn tasks are presented in Appendix B.3.

<a id='534c05cf-77a5-49f8-996f-88f552eedc07'></a>

## 5. Conclusion

Self-evolving memory is a fundamental yet underexplored aspect of LLM capability. While prior work centers on static conversational recall, it overlooks how models accumulate and reuse experience across evolving task streams. Evo-Memory fills this gap by transforming static datasets into streaming trajectories, systematically evaluating how LLMs retrieve, adapt, and refine memory through interaction. Our results show that memory can substantially enhance performance but remains fragile in stability and procedural reuse. To foster progress, we introduce ExpRAG for experience retrieval and ReMem for interleaving reasoning, action, and memory updates. We hope Evo-Memory serves as a unified platform for building LLMs with reliable and continually improving memory.

<a id='a6d85aa5-910f-45a7-a1ba-9646073b9aa4'></a>

13

<!-- PAGE BREAK -->

<a id='26688e4b-a673-4589-bb80-47ee9e479851'></a>

Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory

<a id='4dc8aafa-32a1-4106-adc7-c33c131d928d'></a>

# References
Anthropic. Introducing Claude 4: Claude Opus 4 and Claude Sonnet 4. https://www.anthropic.com/news/claude-4, 2025. Accessed: 2025-09-10.
A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi. Self-rag: Self-reflective retrieval augmented generation. In *NeurIPS 2023 workshop on instruction tuning and instruction following*, 2023.
A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In *The Twelfth International Conference on Learning Representations, ICLR 2024*, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024a. URL https://openreview.net/forum?id=hSyW5go0v8.
A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. 2024b.
J. Chen, S. Xiao, P. Zhang, K. Luo, D. Lian, and Z. Liu. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation, 2023.
R. Chen, Z. Li, and Y. Wang. Llm-as-optimizer: Self-improving agents through differentiable feedback. *arXiv preprint arXiv:2503.04567*, 2025.
M. Chevalier-Boisvert, D. Bahdanau, S. E.-T. Lahlou, L. Willems, H. Lozano, L. Dassa, S. Kim, J. Pineau, and A. Courville. Babyai: A platform to study the sample efficiency of grounded language learning. In *International Conference on Learning Representations (ICLR)*, 2019.
P. Chhikara, D. Khant, S. Aryan, T. Singh, and D. Yadav. Mem0: Building production-ready ai agents with scalable long-term memory. *arXiv preprint arXiv:2504.19413*, 2025.
G. Comanici, E. Bieber, M. Schaekermann, I. Pasupat, N. Sachdeva, I. Dhillon, M. Blistein, O. Ram, D. Zhang, E. Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. *arXiv preprint arXiv:2507.06261*, 2025.
Y. Gao, Z. Sun, J. Huang, H. Zhang, Y. Wang, Y. Luo, and C. Finn. A survey of self-evolving agents: On path to artificial super intelligence. *arXiv preprint arXiv:2507.21046*, 2025.
M. Hausknecht, P. Ammanabrolu, M.-A. Côté, and X. Yuan. Interactive fiction games: A colossal adventure for reinforcement learning agents. In *AAAI Conference on Artificial Intelligence*, 2020.
Y. Hu, Y. Wang, and J. McAuley. Evaluating memory in llm agents via incremental multi-turn interactions. *arXiv preprint arXiv:2507.05257*, 2025.
J. Huang, R. Zhao, Y. Li, and Y. Zhou. Self-discovering agents: Autonomous skill expansion via continual interaction. *arXiv preprint arXiv:2506.08791*, 2025.
HuggingFaceH4. AIME-24: American Invitational Mathematics Examination 2024 Benchmark. https://huggingface.co/datasets/HuggingFaceH4/aime_2024, 2024. Accessed: 2025-09-10.
HuggingFaceH4. AIME-25: American Invitational Mathematics Examination 2025 Benchmark. https://huggingface.co/datasets/HuggingFaceH4/aime_2025, 2025. Accessed: 2025-09-10.
Y. Iwasawa and Y. Matsuo. T3a: Test-time template adjustments for domain generalization. In *Advances in Neural Information Processing Systems (NeurIPS)*, 2021.

<a id='add80a12-4e24-4899-8c69-8fe4661a52b6'></a>

14

<!-- PAGE BREAK -->

<a id='4a1ecc24-b51d-4361-9f81-976c8f6e59df'></a>

Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory

<a id='dc166ddf-a5fe-4e05-a78f-a7ab97dfd71d'></a>

LangChain contributors. Langchain, 2025. URL https://github.com/langchain-ai/langchain. MIT License.

<a id='1e698e03-f1ed-43b1-9148-b990b3965159'></a>

P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W. Yih, T. Rocktäschel, S. Riedel, and D. Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html.
Z. Li, S. Song, H. Wang, S. Niu, D. Chen, J. Yang, C. Xi, H. Lai, J. Zhao, Y. Wang, J. Ren, Z. Lin, J. Huo, T. Chen, K. Chen, K.-R. Li, Z. Yin, Q. Yu, B. Tang, H. Yang, Z. Xu, and F. Xiong. Memos: An operating system for memory-augmented generation (mag) in large language models. ArXiv, abs/2505.22101, 2025. URL https://api.semanticscholar.org/CorpusID:278960153.
X. Liang, B. Wang, H. Huang, S. Wu, P. Wu, L. Lu, Z. Ma, and Z. Li. Scm: Enhancing large language model with self-controlled memory framework. 2023. URL https://api.semanticscholar.org/CorpusID:258331553.
J. Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama_index.
J. Liu, N. Loo, H. Li, R. Chen, X. Chen, T. Hospedales, and Y. Wang. Ttt++: When does test-time training fail or thrive? In IEEE/CVF International Conference on Computer Vision (ICCV), 2023.
A. Maharana, D.-H. Lee, S. Tulyakov, M. Bansal, F. Barbieri, and Y. Fang. Evaluating very long-term conversational memory of llm agents. ArXiv, abs/2402.17753, 2024a. URL https://api.semanticscholar.org/CorpusID:268041615.
A. Maharana, D.-H. Lee, S. Tulyakov, M. Bansal, F. Barbieri, and Y. Fang. Evaluating very long-term conversational memory of llm agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13851-13870, 2024b.
A. Modarressi, A. Imani, M. Fayyaz, and H. Schütze. Ret-llm: Towards a general read-write memory for large language models. ArXiv, abs/2305.14322, 2023. URL https://api.semanticscholar.org/CorpusID:258841042.
Y. Niu, Z. Li, B. Du, and D. Tao. Efficient test-time adaptation via sample-efficient entropy minimization. In Advances in Neural Information Processing Systems (NeurIPS), 2022.
S. Ouyang, W. Yu, K. Ma, Z.-Q. Xiao, Z. Zhang, M. Jia, J. Han, H. Zhang, and D. Yu. Repograph: Enhancing ai software engineering with repository-level code graph. ArXiv, abs/2410.14684, 2024. URL https://api.semanticscholar.org/CorpusID:273502041.
C. Packer, V. Fang, S. G. Patil, K. Lin, S. Wooders, and J. Gonzalez. Memgpt: Towards llms as operating systems. ArXiv, abs/2310.08560, 2023. URL https://api.semanticscholar.org/CorpusID:263909014.
J. S. Park, C. O'Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein. Generative agents: Interactive simulacra of human behavior. In ACM Symposium on User Interface Software and Technology (UIST), 2023.
S. G. Patil, H. Li, T. Zhang, et al. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334, 2023.

<a id='7a4b3822-1a39-409d-8b5c-b7235b6fdcd1'></a>

15

<!-- PAGE BREAK -->

<a id='5d86de16-cdf6-40c4-9144-bba78910e16d'></a>

Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory

<a id='9a41e0f1-1a7c-4be4-b372-627a6615ab49'></a>

P. Rasmussen, P. Paliychuk, T. Beauvais, J. Ryan, and D. Chalef. Zep: A temporal knowledge graph architecture for agent memory. ArXiv, abs/2501.13956, 2025. URL https://api.semanticscholar.org/CorpusID:275907122.
D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. Gpqa: A graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024.
N. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, and S. Yao. Reflexion: language agents with verbal reinforcement learning. In Neural Information Processing Systems, 2023. URL https://api.semanticscholar.org/CorpusID:258833055.
M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and D. Fox. Alfworld: Aligning text and embodied environments for interactive learning. In International Conference on Learning Representations (ICLR), 2021.
M. Suzgun, M. Yuksekgonul, F. Bianchi, D. Jurafsky, and J. Zou. Dynamic cheatsheet: Test-time learning with adaptive memory. arXiv preprint arXiv:2504.07952, 2025.
D. Wang, E. Shelhamer, S. Liu, B. Olshausen, and T. Darrell. Tent: Fully test-time entropy minimization. In International Conference on Learning Representations (ICLR), 2021.
G. Wang, Y. Wang, Z. Wu, G. Chen, Z. Huang, H. Zhao, S. Han, V. Koltun, J. Zhu, and K. Lin. Voyager: An open-ended embodied agent with large language models. In Advances in Neural Information Processing Systems (NeurIPS), 2023.
Y. Wang, L. Yuan, K. Gopalakrishnan, A. Narayan-Chen, A. Fang, and M. Hausknecht. Scienceworld: Is your agent smarter than a 5th grader? In NeurIPS, 2022.
Z. Z. Wang, J. Mao, D. Fried, and G. Neubig. Agent workflow memory. ArXiv, abs/2409.07429, 2024. URL https://api.semanticscholar.org/CorpusID:272592995.
C.-K. Wu, Z. R. Tam, C.-Y. Lin, Y.-N. V. Chen, and H.-y. Lee. Streambench: Towards benchmarking continuous improvement of language agents. Advances in Neural Information Processing Systems, 37:107039-107063, 2024a.
D. Wu, H. Wang, W. Yu, Y. Zhang, K.-W. Chang, and D. Yu. Longmemeval: Benchmarking chat assistants on long-term interactive memory. In The Thirteenth International Conference on Learning Representations.
D. Wu, H. Wang, W. Yu, Y. Zhang, K.-W. Chang, and D. Yu. Longmemeval: Benchmarking chat assistants on long-term interactive memory. ArXiv, abs/2410.10813, 2024b. URL https://api.semanticscholar.org/CorpusID:273345961.
W. Xu, K. Mei, H. Gao, J. Tan, Z. Liang, and Y. Zhang. A-mem: Agentic memory for llm agents. arXiv preprint arXiv:2502.12110, 2025.
S. Yan, X. Yang, Z. Huang, E. Nie, Z. Ding, Z. Li, X. Ma, H. Schütze, V. Tresp, and Y. Ma. Memory-r1: Enhancing large language model agents to manage and utilize memories via reinforcement learning. arXiv preprint arXiv:2508.19828, 2025.
R. Yang, Q. Zhou, Y. Zhang, J. Zhang, X. Zhang, K. Zhang, S. Xu, W. Chen, H. Ma, W. Wang, et al. Pddlbench: Benchmarking llms on symbolic planning with pddl. arXiv preprint arXiv:2312.00754, 2023.

<a id='4f47b753-a8a5-4819-ae69-2df30095e58d'></a>

16

<!-- PAGE BREAK -->

<a id='bba3a2eb-4441-4bfe-adfa-13b0d530e2b2'></a>

Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory

<a id='6af9b2ef-c149-40e6-b110-8f3b1f4b19bb'></a>

S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations, 2022.

S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023.

H. Yu, T. Chen, J. Feng, J. Chen, W. Dai, Q. Yu, Y.-Q. Zhang, W.-Y. Ma, J. Liu, M. Wang, et al. Memagent: Reshaping long-context llm with multi-conv rl-based memory agent. arXiv preprint arXiv:2507.02259, 2025.

M. Zhang, K. Ahuja, K.-C. Lee, T. Zhang, and C. Finn. Memo: Test-time robustness via adaptation and augmentation. In Advances in Neural Information Processing Systems (NeurIPS), 2023.

R. Zhao, Y. Li, Z. Qian, X. Wang, W. Zhao, and J. Huang. Self-evolving agents: Continual test-time learning through memory and reflection. arXiv preprint arXiv:2507.21046, 2025.

J. Zheng, X. Cai, Q. Li, D. Zhang, Z. Li, Y. Zhang, L. Song, and Q. Ma. Lifelongagentbench: Evaluating llm agents as lifelong learners. arXiv preprint arXiv:2505.11942, 2025.

Y. Zheng, T. Li, H. Li, C. Zhao, J. Wang, Z. Zhang, S. Deng, N. Zhang, and H. Chen. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024.

W. Zhong, L. Guo, Q.-F. Gao, H. Ye, and Y. Wang. Memorybank: Enhancing large language models with long-term memory. ArXiv, abs/2305.10250, 2023. URL https://api.semanticscholar. org/CorpusID:258741194.

Y. Zhou, Z. Sun, J. Huang, K.-H. Lee, Y. Luo, and C. Finn. Eureka: Human-level reward design via coding large language models. arXiv preprint arXiv:2402.10654, 2024.

Z. Zhou, A. Qu, Z. Wu, S. Kim, A. Prakash, D. Rus, J. Zhao, B. K. H. Low, and P. P. Liang. MEM1: Learning to synergize memory and reasoning for efficient long-horizon agents. arXiv preprint arXiv: 2506.15841, 2025.

H. Zhuang, B. Zhang, Y. Wang, Y. Xie, Z. Wang, Q. Zhang, H. Zhang, C. Zhang, X. Zhou, J. He, et al. Agentboard: Evaluating long-term memory and generalization in large language model agents. arXiv preprint arXiv:2401.13178, 2024.

<a id='9c7d620a-12c2-4a9e-affa-1d5bbbce6efc'></a>

17

<!-- PAGE BREAK -->

<a id='c8e0908b-2926-48a7-b95e-5ecf2a69dedf'></a>

Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory

<a id='7b4090b4-a3bc-4957-a26f-25a262336e66'></a>

## Appendix

<a id='99a2203a-6f82-4b08-bc22-eec681af4c0d'></a>

Contents
<table id="17-1">
<tr><td id="17-2">A</td><td id="17-3">Experimental Details</td><td id="17-4">19</td></tr>
<tr><td id="17-5"></td><td id="17-6">A.1 Datasets</td><td id="17-7">19</td></tr>
<tr><td id="17-8"></td><td id="17-9">A.2 Configuration</td><td id="17-a">19</td></tr>
<tr><td id="17-b"></td><td id="17-c">A.3 Evaluation</td><td id="17-d">20</td></tr>
<tr><td id="17-e"></td><td id="17-f">A.4 Methods</td><td id="17-g">20</td></tr>
<tr><td id="17-h">B</td><td id="17-i">Experiments</td><td id="17-j"></td></tr>
<tr><td id="17-k"></td><td id="17-l">B.1 Additional Experiments . . . . . . . . . . . . . . . .</td><td id="17-m">21</td></tr>
<tr><td id="17-n"></td><td id="17-o">B.2 Additional Analysis of Memory Pruning . . . . . . . . . . . . . . . .</td><td id="17-p">22</td></tr>
<tr><td id="17-q"></td><td id="17-r">B.3 Additional Comparative Curves on Single-turn Tasks . . . . . . . . . . . . . . . .</td><td id="17-s">22</td></tr>
<tr><td id="17-t">C</td><td id="17-u">Prompts</td><td id="17-v">23</td></tr>
<tr><td id="17-w">D</td><td id="17-x">Limitations</td><td id="17-y">24</td></tr>
<tr><td id="17-z">E</td><td id="17-A">Use of Large Language Models</td><td id="17-B">24</td></tr>
</table>

<a id='65259814-ab56-4ab8-8854-0234da4b10e7'></a>

18

<!-- PAGE BREAK -->

<a id='84b0d4f1-8e3d-4600-a340-09a7bb5f18c9'></a>

Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory

<a id='170c8809-84a1-4e36-814e-7ae6d436d379'></a>

## A. Experimental Details

Evo-Memory evaluates memory mechanisms under realistic streaming multi-task conditions. In what follows, we describe the benchmark datasets, metrics, configurations and the methods compared in details.

<a id='e6200697-bee8-48b3-b6f3-24e02a874ab9'></a>

## A.1. Datasets
We evaluate our approach on a diverse suite of benchmarks that span factual knowledge, reasoning, mathematics, programming, and goal-oriented interaction.

<a id='6bd660ba-b08d-4b42-8d8f-a41245741635'></a>

We first introduce a suite of single-turn datasets designed to evaluate diverse reasoning abilities. MMLU-Pro (Zheng et al., 2024) extends the original MMLU benchmark with stronger robustness and challenge by filtering data leakage, reducing ambiguity, and introducing more difficult questions across domains such as engineering, philosophy, and economics, making it a more reliable testbed for assessing multi-disciplinary reasoning. GPQA-Diamond (Rein et al., 2024) is a graduate-level benchmark featuring expert-written, "Google-proof" questions in physics and related sciences, with its Diamond split being the most challenging and requiring rigorous multi-step reasoning. AIME-24 and AIME-25 (HuggingFaceH4, 2024, 2025) consist of Olympiad-style mathematics problems from the 2024 and 2025 American Invitational Mathematics Examinations, testing symbolic manipulation and problem-solving under strict exact-match criteria. Finally, ToolBench (Patil et al., 2023) assesses a model's ability to identify and configure external APIs, reflecting practical tool-use capabilities.

<a id='fb5c0aa0-999f-4458-92c0-177171962457'></a>

We then evaluate on a suite of multi-turn, goal-oriented benchmarks designed to evaluate memory in embodied and interactive environments. It includes several representative domains: **AlfWorld** (Shridhar et al., 2021) for household instruction following, **BabyAI** (Chevalier-Boisvert et al., 2019) for grounded navigation and compositional reasoning, **ScienceWorld** (Wang et al., 2022) for open-ended scientific experimentation, **Jericho** (Hausknecht et al., 2020) for text-based game exploration, and **PDDL** tasks (Yang et al., 2023) for symbolic planning. Together, these environments emphasize long-horizon reasoning, sequential decision-making, and the use of accumulated experience to achieve complex goals.

<a id='3dcfeaa5-d32b-48c1-a30c-c6fec98f34fb'></a>

Together, these datasets form a comprehensive benchmark suite that evaluates factual recall, domain expertise, mathematical reasoning, and procedural memory in interactive settings. This diversity enables a unified evaluation of both static and evolving capabilities, reflecting how LLMs learn, act, and adapt across academic and real-world scenarios.

<a id='6f7d7550-c93b-434b-baf5-6edbf7f30745'></a>

A.2. Configuration

For efficient retrieval and fair comparison across methods, we utilize the BAAI/bge-base-en-v1.5 (Chen et al., 2023) encoder as the retriever to index both queries and memory items. During inference, the current question is encoded as a query and compared with all stored memory embeddings, retrieving the top-k most relevant items (default k = 4) for contextual augmentation. This setting ensures a consistent retrieval budget across all methods. For efficiency, retrieved texts and task inputs are truncated to fit within the same prompt length constraint used by the generation models.

<a id='cc4dd54c-4668-4f7f-b69a-0e096e71a918'></a>

While all baselines adopt the same retrieval configuration, certain methods (e.g., SELF-RAG, REMEM) introduce additional reasoning modules that determine whether to retrieve and what to retrieve at each step. These adaptive behaviors operate on top of the same retrieval pool to ensure comparability.

<a id='4a937016-e8a8-4cdc-8172-92a8cfa43ba1'></a>

Across all experiments, we maintain a unified task sequence ordering within each dataset, ensuring

<a id='a3ed6a5a-b27f-454f-b68b-faaf57373a98'></a>

19

<!-- PAGE BREAK -->

<a id='bae86ac3-a28c-4e3e-b6f8-e2a26acc6cf4'></a>

Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory

<a id='0f47b54c-a97b-4f5f-ba1e-39cc01aeeeb0'></a>

consistent memory evolution dynamics for all models. Unless otherwise specified, retrieval and generation operate within the same pipeline, and the retrieved items are appended to the prompt following the order of relevance, from most to least similar.

<a id='9a5234b1-d9c2-4b76-9d59-305a69a2e0f3'></a>

We benchmark a broad range of agents and memory architectures instantiated on two strong LLM backbones: the Gemini-2.5 series (Comanici et al., 2025) (FLASH, FLASH-LITE, and PRO) and the Claude family (Anthropic, 2025) (3.5-HAIKU and 3.7-SONNET).

<a id='d91f7a48-a50e-4d0c-84f3-096b0cd6d21f'></a>

### A.3. Evaluation

Evo-Memory evaluates both task performance and memory quality along four key dimensions:
*   **Answer accuracy.** Evaluates whether the LLM produces correct outputs across tasks, reflecting its ability to incorporate past experiences into inference.
*   **Success rate.** Measures whether the LLM agent successfully completes task goals, indicating its overall effectiveness in interactive or goal-oriented settings.
*   **Step efficiency.** Tracks the number of steps required to complete a goal, assessing whether memory usage enables concise and scalable reasoning.
*   **Sequence robustness.** Examines whether the LLM maintains consistent knowledge and performance across varying task orders, reflecting its ability to stably reuse prior experiences.

<a id='8575278e-f75c-4147-8a34-fbb1a03d4c38'></a>

A.4. Methods
We benchmark Evo-Memory with a wide spectrum of agent and memory architectures to study how different designs impact *test-time memory evolution*. All methods are instantiated on two strong **LLM backbones**: Gemini-2.5 (Comanici et al., 2025) and Claude-3.5/3.7 (Anthropic, 2025). Our comparisons isolate the impact of memory architecture and update strategy. Differences in backbone capability are not the focus of the study. We group the evaluated approaches into four major families:

<a id='aa8ae110-dec1-447b-a56a-03c0edcd3e00'></a>

**Agent Pipelines without Persistent Memory.** ReAct (Yao et al., 2023) serves as a representative reasoning—action pipeline, where memory is limited to the immediate context. It generates interleaved reasoning traces and tool calls but does not explicitly store or evolve information. **Amem** extends this pipeline with a lightweight agentic memory that caches recent observations and reflections. It provides a minimal form of experience reuse without dedicated search or update policies, forming a bridge between memory-free agents and adaptive memory systems.

<a id='1be8f22d-abcc-4aa1-8f5d-ac480d92b96a'></a>

**Adaptive Agentic Memory Methods.** This group focuses on adaptive retrieval and self-evolving memory. **SelfRAG** (Asai et al., 2023) integrates dynamic retrieval and reflection to adaptively ground reasoning in prior contexts. **MemOS** (Li et al., 2025), **Mem0** (Chhikara et al., 2025), and **LangMem** (LangChain contributors, 2025) implement structured, agent-level memory systems that support read, write, and update operations. Within our unified interface, retrieval corresponds to the *search* stage and updates correspond to *evolve*. These methods represent adaptive long-term agents capable of continual refinement.

<a id='4f8e3cbe-4fb9-4952-931b-62f5669972e6'></a>

Memory-Based Agents for Procedural Memory. Dynamic Cheatsheet (DC) (Suzgun et al., 2025) and Agent Workflow Memory (AWM) (Wang et al., 2024) emphasize the reuse of procedural knowledge, encoding "how-to" information rather than static facts. We evaluate two DC variants, DC-RS (retrieval-based) and DC-Cu (curated), to analyze how workflow induction and update mechanisms

<a id='4f6d25a9-8d9b-4b7e-be9d-2e71903ba534'></a>

20

<!-- PAGE BREAK -->

<a id='76082715-1497-4480-bf14-20d7e572c745'></a>

Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory

<a id='9cee6e94-fa6c-4024-a74a-7a8de5c821e3'></a>

<::chart: Memory Pruning Rate by Dataset::>The chart titled "Memory Pruning Rate by Dataset" is a stacked bar chart showing the percentage of retained and pruned memory for different datasets. The y-axis represents "Percentage (%)" from 0 to 100. The x-axis lists the datasets: gpqa, toolbench, mmlu_pro_eng, mmlu_pro_eco, mmlu_pro_philo, aime24, and aime25. Each bar is divided into two segments: "Retained" (light blue) and "Pruned" (light coral), with their respective percentages labeled.

- **gpqa**: Retained 63.2%, Pruned 36.8%
- **toolbench**: Retained 76.2%, Pruned 23.8%
- **mmlu_pro_eng**: Retained 70.8%, Pruned 29.2%
- **mmlu_pro_eco**: Retained 80.0%, Pruned 20.0%
- **mmlu_pro_philo**: Retained 67.8%, Pruned 32.2%
- **aime24**: Retained 82.5%, Pruned 17.5%
- **aime25**: Retained 89.2%, Pruned 10.8%

Figure 7 | Memory pruning rates by dataset. Retained (blue) and pruned (coral) memory proportions show varying selectivity across benchmarks.<::>

<a id='3b97757e-baec-4735-a292-dc544e83634d'></a>

influence stability and transfer. These methods test the potential of procedural memory as reusable strategy repositories.

<a id='e78be6bd-a5de-4c0d-9d29-557c548a4d23'></a>

**Proposed: Evolving Memory Framework.** _ExpRecent_ maintains condensed episodic traces of recent task trajectories, while our **ExpRAG** family integrates the principles of retrieval-augmented reasoning with explicit _test-time evolution_. _ReMem_ applies iterative reflection and synthesis to refine memory embeddings over time. Together, these methods instantiate Evo-Memory's design philosophy, treating reasoning, acting, and memory refinement as interleaved processes that co-adapt during deployment, enabling continual self-improvement and more human-like adaptation.

<a id='1af8f8b2-e5f5-4aae-99ec-066559f62480'></a>

## B. Experiments

We provide more experiments in the following.

<a id='f4e26edb-9c79-48dd-b1e9-bde7e1c55fb6'></a>

## B.1. Additional Experiments

We further validate our findings through extensive benchmarking across multiple model families (Gemini-2.5-Flash-Lite, Claude-3.5-Haiku) and diverse datasets, as shown in Tables 5 and 6. The performance trends remain consistent across all settings. On both multi-turn embodied reasoning tasks (Alf World, BabyAI, PDDL, ScienceWorld) and single-turn reasoning tasks (AIME-24/25, GPQA, MMLU-Pro, ToolBench), ReMem consistently outperforms conventional baselines and adaptive retrieval methods across model backbones. These results confirm that the advantages of evolving-memory architectures are model-agnostic, highlighting continual task-level reflection as a general mechanism for improving adaptability in problem-solving.

<a id='7c480045-ef54-44b9-b9ef-3a509a5eaf05'></a>

21

<!-- PAGE BREAK -->

<a id='e728dcbb-db89-48c4-8201-809f69036ff7'></a>

Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory

<a id='bcf90b48-98e5-41f2-8846-de4bf24ca1ff'></a>

<::Figure 8 | Cumulative accuracy comparison across model variants and benchmarks. The figure displays six line graphs arranged in a 2x3 grid, showing "Cumulative Accuracy" on the y-axis (ranging from 0.0 to 1.0) against "Task Number" on the x-axis. Each graph includes two lines: "ReMem" (solid blue) and "History" (dashed red). The top row of graphs pertains to the "Gemini-2.5-Flash-Lite" model, while the bottom row pertains to the "Claude-3.7-Sonnet" model. The columns represent different datasets: GPQA, TOOLBENCH, and MMLU_PRO_ENG. Across all six graphs, the "ReMem" line consistently demonstrates higher cumulative accuracy and faster convergence compared to the "History" baseline as the task instances accumulate. For example, in the "Gemini-2.5-Flash-Lite - GPQA" graph, ReMem levels off around 0.4 cumulative accuracy, while History levels off around 0.35. In "Claude-3.7-Sonnet - TOOLBENCH", ReMem stabilizes around 0.68, whereas History stabilizes around 0.6. This visual trend supports the claim that ReMem achieves faster convergence and higher final accuracy across the tested models and datasets.::>

<a id='4292749c-4e21-4252-b8c8-96541377bf4d'></a>

## B.2. Additional Analysis of Memory Pruning

Figure 7 shows memory pruning rates across datasets, revealing varying selectivity in memory retention. The pruning ratios differ substantially across benchmarks, which appears related to task diversity and domain coverage. Datasets with broader domain coverage such as GPQA, which encompasses diverse problem types across engineering, physics, and other domains, exhibit higher pruning rates (36.8%), suggesting that more memories are deemed redundant across heterogeneous tasks. In contrast, datasets with more concentrated problem types like AIME show lower pruning rates (17.5% and 10.8% respectively), indicating that memories remain more relevant due to higher task similarity. This pattern suggests that the pruning mechanism effectively identifies and discards domain-irrelevant experiences, though the precise relationship between task diversity and memory selectivity warrants further investigation.

<a id='2f2a7ec0-a7eb-4cd1-ad91-007166ba552b'></a>

B.3. Additional Comparative Curves on Single-turn Tasks

Figure 8 presents cumulative accuracy curves comparing ReMem with the baseline across single-turn reasoning benchmarks and model variants. As task instances accumulate, ReMem shows consistent improvement on GPQA, ToolBench, and MMLU-PRO (Engineer) for both Gemini-2.5-Flash-Lite and Claude-3.7-Sonnet. Similar to the multi-turn results, History performs comparably at the beginning due to the cold-start phase, but ReMem quickly surpasses it as more tasks are processed, indicating the cumulative advantage of continual task-level adaptation.

<a id='d61a499a-9305-46f6-adfd-6f5a93993eef'></a>

22

<!-- PAGE BREAK -->

<a id='2b2d8c2d-17e3-4a64-b3a4-ba5e74e08aee'></a>

Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory

<a id='4d9d5116-a80d-423f-b814-65d3099141f8'></a>

C. Prompts

**Memory Prompt Template for Multi-turn Dataset**

===
ENVIRONMENT INSTRUCTIONS
===
[Detailed task environment description and rules]
Example: Go to kitchen, pick up apple, put it in bag, etc.

===
EXAMPLE DEMONSTRATIONS
===
[Static few-shot examples]
Example 1: Goal: ... | Action: ... | Observation: ...
Example 2: Goal: ... | Action: ... | Observation: ...

===
RELEVANT EXPERIENCE FROM SIMILAR TASKS
===
[Experience #1]
Goal: [similar goal]
Trajectory: [action sequence]
Correctness: [success/failure]
[Experience #2, #3, ...]

===
YOUR CURRENT TASK
===
Goal: [specific task goal]
Help: type 'check valid actions' if action fails
Help: type 'inventory' to check items

===
RECENT HISTORY
===
Observation: [initial environment state]
Action: [previous action]
Observation: [result of previous action]
Action: [previous action]
Observation: [current state]

===
OUTPUT FORMAT
===
You MUST respond in EXACTLY ONE of these formats:

Format 1 - Prune experiences:
Think-Prune: <IDs>
Remove unhelpful experiences from 'RELEVANT EXPERIENCE' section (e.g., "1,3" or "2-4")

Format 2 - Internal reasoning:
Think: <your reasoning>
Free-form explanation of your next step

Format 3 - Execute action:
Action: <exact command>
Must be valid command from ENVIRONMENT INSTRUCTIONS with exact names from RECENT HISTORY

<!-- PAGE BREAK -->

<a id='a40ab291-4a66-4f38-be58-b422959b6fe4'></a>

Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory

<a id='0392abc8-14e5-4b11-8ab5-102f2fc9582a'></a>

# Memory Prompt Template for Single-turn Dataset

You are a helpful assistant with access to LOCAL EXPERIENCE MEMORY. Each memory may contain past experience, rationales, domains, and skills. Below are some retrieved LOCAL EXPERIENCE MEMORIES:

[Retrieved/synthesized memories]

Now solve the following problem.

**Question:** [Your question here]

**Provide your output in the following format:**

*   Rationale: your short reasoning, may cite memory if useful
*   Final Answer: your final answer

<a id='9c8c09d0-de61-4484-af58-3ac680dd1631'></a>

D. Limitations

While Evo-Memory offers a comprehensive evaluation of self-evolving memory, several practical constraints remain. Due to budget and API limits, we focus on a selected set of strong LLMs rather than exhaustively covering all available models. Additional evaluations on open-weight or multilingual models could further validate the generality of our findings. Moreover, our benchmark primarily emphasizes textual and goal-oriented tasks; extending it to richer multimodal or real-world environments would provide a more complete picture of continual memory evolution. Despite these limitations, the current study already spans diverse domains, tasks, and architectures, offering a solid foundation for future extensions.

<a id='534d3e94-513f-44aa-8591-09cb0315cc84'></a>

E. Use of Large Language Models

During the preparation of this paper, we made limited and controlled use of large language models (LLMs), specifically ChatGPT, as an auxiliary writing aid. The LLM was used only for stylistic refinement, including improvements in clarity, grammar, and readability of text originally drafted by the authors. All scientific ideas, analyses, experiments, and conclusions were fully developed, written, and verified by the authors. Thus, LLMs were employed solely as a language-editing tool, without contributing to the intellectual or scientific content of the work.

<a id='fee239d2-baf3-4ff0-bb53-5118edde5351'></a>

24

<!-- PAGE BREAK -->

<a id='72ce7db1-dd96-47c3-a09e-eaf17c5326cd'></a>

Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory

<a id='bba46893-73ac-4058-a158-fc818e6a47f7'></a>

<table id="24-1">
<tr><td id="24-2" rowspan="2">LLM Backbone</td><td id="24-3" rowspan="2">Method</td><td id="24-4" colspan="2">Alf World</td><td id="24-5" colspan="2">BabyAI</td><td id="24-6" colspan="2">PDDL</td><td id="24-7" colspan="2">ScienceWorld</td><td id="24-8" colspan="2">Avg.</td></tr>
<tr><td id="24-9">S</td><td id="24-a">P</td><td id="24-b">S</td><td id="24-c">P</td><td id="24-d">S</td><td id="24-e">P</td><td id="24-f">S</td><td id="24-g">P</td><td id="24-h">S</td><td id="24-i">P</td></tr>
<tr><td id="24-j"></td><td id="24-k">Baseline</td><td id="24-l">0.12</td><td id="24-m">0.34</td><td id="24-n">0.61</td><td id="24-o">0.71</td><td id="24-p">0.12</td><td id="24-q">0.20</td><td id="24-r">0.24</td><td id="24-s">0.59</td><td id="24-t">0.27</td><td id="24-u">0.46</td></tr>
<tr><td id="24-v"></td><td id="24-w">History</td><td id="24-x">0.28</td><td id="24-y">0.60</td><td id="24-z">0.52</td><td id="24-A">0.64</td><td id="24-B">0.08</td><td id="24-C">0.15</td><td id="24-D">0.31</td><td id="24-E">0.71</td><td id="24-F">0.30</td><td id="24-G">0.53</td></tr>
<tr><td id="24-H"></td><td id="24-I">ReAct</td><td id="24-J">0.24</td><td id="24-K">0.56</td><td id="24-L">0.48</td><td id="24-M">0.63</td><td id="24-N">0.22</td><td id="24-O">0.33</td><td id="24-P">0.34</td><td id="24-Q">0.71</td><td id="24-R">0.32</td><td id="24-S">0.56</td></tr>
<tr><td id="24-T"></td><td id="24-U">Amem</td><td id="24-V">0.25</td><td id="24-W">0.59</td><td id="24-X">0.53</td><td id="24-Y">0.64</td><td id="24-Z">0.10</td><td id="24-10">0.16</td><td id="24-11">0.36</td><td id="24-12">0.74</td><td id="24-13">0.31</td><td id="24-14">0.53</td></tr>
<tr><td id="24-15"></td><td id="24-16">Self RAG</td><td id="24-17">0.25</td><td id="24-18">0.59</td><td id="24-19">0.52</td><td id="24-1a">0.65</td><td id="24-1b">0.08</td><td id="24-1c">0.16</td><td id="24-1d">0.34</td><td id="24-1e">0.74</td><td id="24-1f">0.30</td><td id="24-1g">0.54</td></tr>
<tr><td id="24-1h"></td><td id="24-1i">Mem0</td><td id="24-1j">0.27</td><td id="24-1k">0.61</td><td id="24-1l">0.54</td><td id="24-1m">0.66</td><td id="24-1n">0.10</td><td id="24-1o">0.19</td><td id="24-1p">0.32</td><td id="24-1q">0.70</td><td id="24-1r">0.31</td><td id="24-1s">0.54</td></tr>
<tr><td id="24-1t">Gemini 2.5 Flash</td><td id="24-1u">DC-Cu</td><td id="24-1v">0.25</td><td id="24-1w">0.59</td><td id="24-1x">0.53</td><td id="24-1y">0.64</td><td id="24-1z">0.08</td><td id="24-1A">0.17</td><td id="24-1B">0.29</td><td id="24-1C">0.71</td><td id="24-1D">0.29</td><td id="24-1E">0.53</td></tr>
<tr><td id="24-1F"></td><td id="24-1G">DC-RS</td><td id="24-1H">0.27</td><td id="24-1I">0.60</td><td id="24-1J">0.53</td><td id="24-1K">0.66</td><td id="24-1L">0.07</td><td id="24-1M">0.15</td><td id="24-1N">0.33</td><td id="24-1O">0.73</td><td id="24-1P">0.30</td><td id="24-1Q">0.54</td></tr>
<tr><td id="24-1R"></td><td id="24-1S">AWM</td><td id="24-1T">0.26</td><td id="24-1U">0.59</td><td id="24-1V">0.52</td><td id="24-1W">0.64</td><td id="24-1X">0.08</td><td id="24-1Y">0.16</td><td id="24-1Z">0.33</td><td id="24-20">0.73</td><td id="24-21">0.30</td><td id="24-22">0.53</td></tr>
<tr><td id="24-23"></td><td id="24-24">ExpRecent</td><td id="24-25">0.37</td><td id="24-26">0.65</td><td id="24-27">0.53</td><td id="24-28">0.64</td><td id="24-29">0.13</td><td id="24-2a">0.22</td><td id="24-2b">0.53</td><td id="24-2c">0.83</td><td id="24-2d">0.39</td><td id="24-2e">0.59</td></tr>
<tr><td id="24-2f"></td><td id="24-2g">ExpRAG</td><td id="24-2h">0.59</td><td id="24-2i">0.79</td><td id="24-2j">0.56</td><td id="24-2k">0.65</td><td id="24-2l">0.17</td><td id="24-2m">0.27</td><td id="24-2n">0.53</td><td id="24-2o">0.81</td><td id="24-2p">0.46</td><td id="24-2q">0.63</td></tr>
<tr><td id="24-2r"></td><td id="24-2s">ReMem</td><td id="24-2t">0.66</td><td id="24-2u">0.81</td><td id="24-2v">0.53</td><td id="24-2w">0.61</td><td id="24-2x">0.22</td><td id="24-2y">0.33</td><td id="24-2z">0.58</td><td id="24-2A">0.81</td><td id="24-2B">0.50</td><td id="24-2C">0.64</td></tr>
<tr><td id="24-2D"></td><td id="24-2E">Baseline</td><td id="24-2F">0.04</td><td id="24-2G">0.39</td><td id="24-2H">0.37</td><td id="24-2I">0.47</td><td id="24-2J">0.20</td><td id="24-2K">0.33</td><td id="24-2L">0.22</td><td id="24-2M">0.60</td><td id="24-2N">0.21</td><td id="24-2O">0.45</td></tr>
<tr><td id="24-2P"></td><td id="24-2Q">History</td><td id="24-2R">0.19</td><td id="24-2S">0.52</td><td id="24-2T">0.40</td><td id="24-2U">0.48</td><td id="24-2V">0.25</td><td id="24-2W">0.38</td><td id="24-2X">0.60</td><td id="24-2Y">0.84</td><td id="24-2Z">0.36</td><td id="24-30">0.56</td></tr>
<tr><td id="24-31"></td><td id="24-32">ReAct</td><td id="24-33">0.02</td><td id="24-34">0.26</td><td id="24-35">0.43</td><td id="24-36">0.55</td><td id="24-37">0.13</td><td id="24-38">0.22</td><td id="24-39">0.30</td><td id="24-3a">0.68</td><td id="24-3b">0.22</td><td id="24-3c">0.43</td></tr>
<tr><td id="24-3d"></td><td id="24-3e">Amem</td><td id="24-3f">0.16</td><td id="24-3g">0.50</td><td id="24-3h">0.42</td><td id="24-3i">0.49</td><td id="24-3j">0.23</td><td id="24-3k">0.38</td><td id="24-3l">0.59</td><td id="24-3m">0.85</td><td id="24-3n">0.35</td><td id="24-3o">0.56</td></tr>
<tr><td id="24-3p"></td><td id="24-3q">SelfRAG</td><td id="24-3r">0.16</td><td id="24-3s">0.49</td><td id="24-3t">0.43</td><td id="24-3u">0.50</td><td id="24-3v">0.22</td><td id="24-3w">0.35</td><td id="24-3x">0.57</td><td id="24-3y">0.84</td><td id="24-3z">0.34</td><td id="24-3A">0.55</td></tr>
<tr><td id="24-3B"></td><td id="24-3C">Memo</td><td id="24-3D">0.16</td><td id="24-3E">0.49</td><td id="24-3F">0.41</td><td id="24-3G">0.49</td><td id="24-3H">0.22</td><td id="24-3I">0.37</td><td id="24-3J">0.53</td><td id="24-3K">0.81</td><td id="24-3L">0.33</td><td id="24-3M">0.54</td></tr>
<tr><td id="24-3N">Gemini 2.5 Pro</td><td id="24-3O">DC-Cu</td><td id="24-3P">0.17</td><td id="24-3Q">0.50</td><td id="24-3R">0.40</td><td id="24-3S">0.47</td><td id="24-3T">0.28</td><td id="24-3U">0.40</td><td id="24-3V">0.53</td><td id="24-3W">0.82</td><td id="24-3X">0.35</td><td id="24-3Y">0.55</td></tr>
<tr><td id="24-3Z"></td><td id="24-40">DC-RS</td><td id="24-41">0.18</td><td id="24-42">0.51</td><td id="24-43">0.42</td><td id="24-44">0.50</td><td id="24-45">0.27</td><td id="24-46">0.40</td><td id="24-47">0.59</td><td id="24-48">0.85</td><td id="24-49">0.37</td><td id="24-4a">0.57</td></tr>
<tr><td id="24-4b"></td><td id="24-4c">AWM</td><td id="24-4d">0.20</td><td id="24-4e">0.52</td><td id="24-4f">0.38</td><td id="24-4g">0.46</td><td id="24-4h">0.20</td><td id="24-4i">0.37</td><td id="24-4j">0.57</td><td id="24-4k">0.83</td><td id="24-4l">0.34</td><td id="24-4m">0.54</td></tr>
<tr><td id="24-4n"></td><td id="24-4o">ExpRecent</td><td id="24-4p">0.36</td><td id="24-4q">0.61</td><td id="24-4r">0.54</td><td id="24-4s">0.64</td><td id="24-4t">0.35</td><td id="24-4u">0.47</td><td id="24-4v">0.69</td><td id="24-4w">0.89</td><td id="24-4x">0.49</td><td id="24-4y">0.64</td></tr>
<tr><td id="24-4z"></td><td id="24-4A">ExpRAG</td><td id="24-4B">0.38</td><td id="24-4C">0.64</td><td id="24-4D">0.46</td><td id="24-4E">0.53</td><td id="24-4F">0.28</td><td id="24-4G">0.43</td><td id="24-4H">0.61</td><td id="24-4I">0.84</td><td id="24-4J">0.43</td><td id="24-4K">0.61</td></tr>
<tr><td id="24-4L"></td><td id="24-4M">ReMem</td><td id="24-4N">0.51</td><td id="24-4O">0.70</td><td id="24-4P">0.56</td><td id="24-4Q">0.64</td><td id="24-4R">0.25</td><td id="24-4S">0.38</td><td id="24-4T">0.66</td><td id="24-4U">0.86</td><td id="24-4V">0.50</td><td id="24-4W">0.65</td></tr>
<tr><td id="24-4X"></td><td id="24-4Y">Baseline</td><td id="24-4Z">0.18</td><td id="24-50">0.49</td><td id="24-51">0.51</td><td id="24-52">0.66</td><td id="24-53">0.17</td><td id="24-54">0.39</td><td id="24-55">0.10</td><td id="24-56">0.53</td><td id="24-57">0.24</td><td id="24-58">0.52</td></tr>
<tr><td id="24-59"></td><td id="24-5a">History</td><td id="24-5b">0.50</td><td id="24-5c">0.73</td><td id="24-5d">0.48</td><td id="24-5e">0.66</td><td id="24-5f">0.65</td><td id="24-5g">0.85</td><td id="24-5h">0.32</td><td id="24-5i">0.74</td><td id="24-5j">0.49</td><td id="24-5k">0.74</td></tr>
<tr><td id="24-5l"></td><td id="24-5m">ReAct</td><td id="24-5n">0.51</td><td id="24-5o">0.75</td><td id="24-5p">0.57</td><td id="24-5q">0.72</td><td id="24-5r">0.75</td><td id="24-5s">0.91</td><td id="24-5t">0.44</td><td id="24-5u">0.77</td><td id="24-5v">0.57</td><td id="24-5w">0.79</td></tr>
<tr><td id="24-5x"></td><td id="24-5y">Amem</td><td id="24-5z">0.48</td><td id="24-5A">0.73</td><td id="24-5B">0.46</td><td id="24-5C">0.64</td><td id="24-5D">0.62</td><td id="24-5E">0.84</td><td id="24-5F">0.33</td><td id="24-5G">0.73</td><td id="24-5H">0.47</td><td id="24-5I">0.73</td></tr>
<tr><td id="24-5J"></td><td id="24-5K">SelfRAG</td><td id="24-5L">0.52</td><td id="24-5M">0.75</td><td id="24-5N">0.46</td><td id="24-5O">0.64</td><td id="24-5P">0.65</td><td id="24-5Q">0.84</td><td id="24-5R">0.31</td><td id="24-5S">0.74</td><td id="24-5T">0.49</td><td id="24-5U">0.74</td></tr>
<tr><td id="24-5V"></td><td id="24-5W">Mem0</td><td id="24-5X">0.51</td><td id="24-5Y">0.74</td><td id="24-5Z">0.48</td><td id="24-60">0.66</td><td id="24-61">0.65</td><td id="24-62">0.84</td><td id="24-63">0.37</td><td id="24-64">0.76</td><td id="24-65">0.50</td><td id="24-66">0.75</td></tr>
<tr><td id="24-67">Claude 3.7 Sonnet</td><td id="24-68">DC-Cu</td><td id="24-69">0.50</td><td id="24-6a">0.74</td><td id="24-6b">0.50</td><td id="24-6c">0.67</td><td id="24-6d">0.62</td><td id="24-6e">0.84</td><td id="24-6f">0.33</td><td id="24-6g">0.75</td><td id="24-6h">0.49</td><td id="24-6i">0.75</td></tr>
<tr><td id="24-6j"></td><td id="24-6k">DC-RS</td><td id="24-6l">0.50</td><td id="24-6m">0.74</td><td id="24-6n">0.52</td><td id="24-6o">0.68</td><td id="24-6p">0.62</td><td id="24-6q">0.84</td><td id="24-6r">0.34</td><td id="24-6s">0.74</td><td id="24-6t">0.50</td><td id="24-6u">0.75</td></tr>
<tr><td id="24-6v"></td><td id="24-6w">AWM</td><td id="24-6x">0.49</td><td id="24-6y">0.73</td><td id="24-6z">0.53</td><td id="24-6A">0.68</td><td id="24-6B">0.60</td><td id="24-6C">0.82</td><td id="24-6D">0.34</td><td id="24-6E">0.74</td><td id="24-6F">0.49</td><td id="24-6G">0.74</td></tr>
<tr><td id="24-6H"></td><td id="24-6I">ExpRecent</td><td id="24-6J">0.66</td><td id="24-6K">0.83</td><td id="24-6L">0.63</td><td id="24-6M">0.73</td><td id="24-6N">0.53</td><td id="24-6O">0.76</td><td id="24-6P">0.49</td><td id="24-6Q">0.82</td><td id="24-6R">0.58</td><td id="24-6S">0.79</td></tr>
<tr><td id="24-6T"></td><td id="24-6U">ExpRAG</td><td id="24-6V">0.74</td><td id="24-6W">0.89</td><td id="24-6X">0.62</td><td id="24-6Y">0.72</td><td id="24-6Z">0.72</td><td id="24-70">0.89</td><td id="24-71">0.46</td><td id="24-72">0.76</td><td id="24-73">0.63</td><td id="24-74">0.82</td></tr>
<tr><td id="24-75"></td><td id="24-76">ReMem</td><td id="24-77">0.92</td><td id="24-78">0.96</td><td id="24-79">0.73</td><td id="24-7a">0.83</td><td id="24-7b">0.83</td><td id="24-7c">0.95</td><td id="24-7d">0.62</td><td id="24-7e">0.89</td><td id="24-7f">0.78</td><td id="24-7g">0.91</td></tr>
<tr><td id="24-7h"></td><td id="24-7i">Baseline</td><td id="24-7j">0.11</td><td id="24-7k">0.33</td><td id="24-7l">0.38</td><td id="24-7m">0.52</td><td id="24-7n">0.15</td><td id="24-7o">0.32</td><td id="24-7p">0.08</td><td id="24-7q">0.37</td><td id="24-7r">0.18</td><td id="24-7s">0.39</td></tr>
<tr><td id="24-7t"></td><td id="24-7u">History</td><td id="24-7v">0.28</td><td id="24-7w">0.58</td><td id="24-7x">0.38</td><td id="24-7y">0.57</td><td id="24-7z">0.18</td><td id="24-7A">0.38</td><td id="24-7B">0.12</td><td id="24-7C">0.49</td><td id="24-7D">0.24</td><td id="24-7E">0.51</td></tr>
<tr><td id="24-7F"></td><td id="24-7G">ReAct</td><td id="24-7H">0.24</td><td id="24-7I">0.58</td><td id="24-7J">0.35</td><td id="24-7K">0.52</td><td id="24-7L">0.32</td><td id="24-7M">0.53</td><td id="24-7N">0.16</td><td id="24-7O">0.55</td><td id="24-7P">0.27</td><td id="24-7Q">0.55</td></tr>
<tr><td id="24-7R"></td><td id="24-7S">Amem</td><td id="24-7T">0.24</td><td id="24-7U">0.55</td><td id="24-7V">0.37</td><td id="24-7W">0.58</td><td id="24-7X">0.17</td><td id="24-7Y">0.35</td><td id="24-7Z">0.12</td><td id="24-80">0.45</td><td id="24-81">0.23</td><td id="24-82">0.48</td></tr>
<tr><td id="24-83"></td><td id="24-84">SelfRAG</td><td id="24-85">0.26</td><td id="24-86">0.58</td><td id="24-87">0.38</td><td id="24-88">0.59</td><td id="24-89">0.22</td><td id="24-8a">0.37</td><td id="24-8b">0.14</td><td id="24-8c">0.49</td><td id="24-8d">0.25</td><td id="24-8e">0.51</td></tr>
<tr><td id="24-8f"></td><td id="24-8g">Mem0</td><td id="24-8h">0.27</td><td id="24-8i">0.56</td><td id="24-8j">0.37</td><td id="24-8k">0.57</td><td id="24-8l">0.17</td><td id="24-8m">0.37</td><td id="24-8n">0.08</td><td id="24-8o">0.45</td><td id="24-8p">0.22</td><td id="24-8q">0.49</td></tr>
<tr><td id="24-8r">Claude 3.5 Haiku</td><td id="24-8s">DC-Cu</td><td id="24-8t">0.24</td><td id="24-8u">0.55</td><td id="24-8v">0.37</td><td id="24-8w">0.58</td><td id="24-8x">0.17</td><td id="24-8y">0.37</td><td id="24-8z">0.12</td><td id="24-8A">0.45</td><td id="24-8B">0.23</td><td id="24-8C">0.49</td></tr>
<tr><td id="24-8D"></td><td id="24-8E">DC-RS</td><td id="24-8F">0.24</td><td id="24-8G">0.55</td><td id="24-8H">0.37</td><td id="24-8I">0.58</td><td id="24-8J">0.17</td><td id="24-8K">0.37</td><td id="24-8L">0.12</td><td id="24-8M">0.45</td><td id="24-8N">0.23</td><td id="24-8O">0.49</td></tr>
<tr><td id="24-8P"></td><td id="24-8Q">AWM</td><td id="24-8R">0.24</td><td id="24-8S">0.55</td><td id="24-8T">0.37</td><td id="24-8U">0.58</td><td id="24-8V">0.17</td><td id="24-8W">0.37</td><td id="24-8X">0.12</td><td id="24-8Y">0.45</td><td id="24-8Z">0.23</td><td id="24-90">0.49</td></tr>
<tr><td id="24-91"></td><td id="24-92">ExpRecent</td><td id="24-93">0.48</td><td id="24-94">0.65</td><td id="24-95">0.40</td><td id="24-96">0.57</td><td id="24-97">0.15</td><td id="24-98">0.32</td><td id="24-99">0.32</td><td id="24-9a">0.64</td><td id="24-9b">0.34</td><td id="24-9c">0.55</td></tr>
<tr><td id="24-9d"></td><td id="24-9e">ExpRAG</td><td id="24-9f">0.65</td><td id="24-9g">0.74</td><td id="24-9h">0.54</td><td id="24-9i">0.64</td><td id="24-9j">0.43</td><td id="24-9k">0.61</td><td id="24-9l">0.42</td><td id="24-9m">0.68</td><td id="24-9n">0.51</td><td id="24-9o">0.67</td></tr>
<tr><td id="24-9p"></td><td id="24-9q">ReMem</td><td id="24-9r">0.69</td><td id="24-9s">0.80</td><td id="24-9t">0.49</td><td id="24-9u">0.60</td><td id="24-9v">0.43</td><td id="24-9w">0.61</td><td id="24-9x">0.44</td><td id="24-9y">0.75</td><td id="24-9z">0.51</td><td id="24-9A">0.69</td></tr>
</table>

<a id='6d9310aa-3eea-43f2-bb0e-04a202770ae3'></a>

Table 5 | Cross-environment results across four embodied reasoning benchmarks (Alf World, BabyAI, PDDL, ScienceWorld). Each dataset reports success (S) and progress (P) rates. Bold indicates the best (including ties) per column. The last two columns show averaged S and P across datasets.

<a id='ed878f66-9fc3-41d8-810a-109be8ac3cd8'></a>

25

<!-- PAGE BREAK -->

<a id='7e047433-0e7f-40ec-b695-49db4420002c'></a>

Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory

<a id='30f2947d-8cc2-4504-b21f-add449648428'></a>

<table id="25-1">
<tr><td id="25-2" rowspan="2">LLM Backbone</td><td id="25-3" rowspan="2">Method</td><td id="25-4" colspan="6">Exact Match ↑</td><td id="25-5" colspan="2">API / Acc. ↑</td></tr>
<tr><td id="25-6">AIME24</td><td id="25-7">AIME25</td><td id="25-8">GPQA</td><td id="25-9">MMLU-Pro (Eco.)</td><td id="25-a">MMLU-Pro (Eng.)</td><td id="25-b">MMLU-Pro (Philo.)</td><td id="25-c">ToolBench</td><td id="25-d">Avg. ↑</td></tr>
<tr><td id="25-e"></td><td id="25-f">Baseline</td><td id="25-g">—</td><td id="25-h">—</td><td id="25-i">0.36</td><td id="25-j">0.68</td><td id="25-k">0.42</td><td id="25-l">0.55</td><td id="25-m">0.81/0.64</td><td id="25-n">0.38</td></tr>
<tr><td id="25-o"></td><td id="25-p">History</td><td id="25-q">—</td><td id="25-r">—</td><td id="25-s">0.37</td><td id="25-t">0.70</td><td id="25-u">0.43</td><td id="25-v">0.55</td><td id="25-w">0.81/0.63</td><td id="25-x">0.38</td></tr>
<tr><td id="25-y"></td><td id="25-z">ReAct</td><td id="25-A">—</td><td id="25-B">—</td><td id="25-C">0.35</td><td id="25-D">0.69</td><td id="25-E">0.43</td><td id="25-F">0.54</td><td id="25-G">0.81/0.64</td><td id="25-H">0.38</td></tr>
<tr><td id="25-I"></td><td id="25-J">Amem</td><td id="25-K">—</td><td id="25-L">—</td><td id="25-M">0.34</td><td id="25-N">0.69</td><td id="25-O">0.43</td><td id="25-P">0.53</td><td id="25-Q">0.82/0.63</td><td id="25-R">0.37</td></tr>
<tr><td id="25-S"></td><td id="25-T">SelfRAG</td><td id="25-U">—</td><td id="25-V">—</td><td id="25-W">0.36</td><td id="25-X">0.70</td><td id="25-Y">0.44</td><td id="25-Z">0.56</td><td id="25-10">0.83/0.65</td><td id="25-11">0.39</td></tr>
<tr><td id="25-12"></td><td id="25-13">MemOS</td><td id="25-14">—</td><td id="25-15">—</td><td id="25-16">0.37</td><td id="25-17">0.70</td><td id="25-18">0.42</td><td id="25-19">0.55</td><td id="25-1a">0.81/0.64</td><td id="25-1b">0.38</td></tr>
<tr><td id="25-1c">Claude 3.5</td><td id="25-1d">Mem0</td><td id="25-1e">—</td><td id="25-1f">—</td><td id="25-1g">0.36</td><td id="25-1h">0.70</td><td id="25-1i">0.42</td><td id="25-1j">0.55</td><td id="25-1k">0.82/0.64</td><td id="25-1l">0.38</td></tr>
<tr><td id="25-1m"></td><td id="25-1n">LangMem</td><td id="25-1o">—</td><td id="25-1p">—</td><td id="25-1q">0.51</td><td id="25-1r">0.78</td><td id="25-1s">0.46</td><td id="25-1t">0.61</td><td id="25-1u">0.81/0.63</td><td id="25-1v">0.43</td></tr>
<tr><td id="25-1w"></td><td id="25-1x">DC-RS</td><td id="25-1y">—</td><td id="25-1z">—</td><td id="25-1A">0.36</td><td id="25-1B">0.68</td><td id="25-1C">0.41</td><td id="25-1D">0.56</td><td id="25-1E">0.82/0.64</td><td id="25-1F">0.38</td></tr>
<tr><td id="25-1G"></td><td id="25-1H">AWM</td><td id="25-1I">—</td><td id="25-1J">—</td><td id="25-1K">0.33</td><td id="25-1L">0.67</td><td id="25-1M">0.42</td><td id="25-1N">0.53</td><td id="25-1O">0.81/0.63</td><td id="25-1P">0.37</td></tr>
<tr><td id="25-1Q"></td><td id="25-1R">DC-Cu</td><td id="25-1S">—</td><td id="25-1T">—</td><td id="25-1U">0.33</td><td id="25-1V">0.65</td><td id="25-1W">0.39</td><td id="25-1X">0.54</td><td id="25-1Y">0.82/0.63</td><td id="25-1Z">0.36</td></tr>
<tr><td id="25-20"></td><td id="25-21">ExpRecent</td><td id="25-22">—</td><td id="25-23">—</td><td id="25-24">0.42</td><td id="25-25">0.69</td><td id="25-26">0.46</td><td id="25-27">0.59</td><td id="25-28">0.85/0.63</td><td id="25-29">0.40</td></tr>
<tr><td id="25-2a"></td><td id="25-2b">ExpRAG</td><td id="25-2c">—</td><td id="25-2d">—</td><td id="25-2e">0.40</td><td id="25-2f">0.73</td><td id="25-2g">0.49</td><td id="25-2h">0.61</td><td id="25-2i">0.87/0.67</td><td id="25-2j">0.41</td></tr>
<tr><td id="25-2k"></td><td id="25-2l">ReMem</td><td id="25-2m">—</td><td id="25-2n">—</td><td id="25-2o">0.39</td><td id="25-2p">0.71</td><td id="25-2q">0.47</td><td id="25-2r">0.62</td><td id="25-2s">0.87/0.68</td><td id="25-2t">0.41</td></tr>
<tr><td id="25-2u"></td><td id="25-2v">Baseline</td><td id="25-2w">0.17</td><td id="25-2x">0.13</td><td id="25-2y">0.55</td><td id="25-2z">0.84</td><td id="25-2A">0.63</td><td id="25-2B">0.78</td><td id="25-2C">0.76/0.62</td><td id="25-2D">0.54</td></tr>
<tr><td id="25-2E"></td><td id="25-2F">History</td><td id="25-2G">0.13</td><td id="25-2H">0.23</td><td id="25-2I">0.56</td><td id="25-2J">0.85</td><td id="25-2K">0.64</td><td id="25-2L">0.78</td><td id="25-2M">0.76/0.61</td><td id="25-2N">0.55</td></tr>
<tr><td id="25-2O"></td><td id="25-2P">ReAct</td><td id="25-2Q">0.17</td><td id="25-2R">0.10</td><td id="25-2S">0.57</td><td id="25-2T">0.84</td><td id="25-2U">0.63</td><td id="25-2V">0.76</td><td id="25-2W">0.76/0.61</td><td id="25-2X">0.54</td></tr>
<tr><td id="25-2Y"></td><td id="25-2Z">Amem</td><td id="25-30">0.27</td><td id="25-31">0.17</td><td id="25-32">0.54</td><td id="25-33">0.83</td><td id="25-34">0.63</td><td id="25-35">0.79</td><td id="25-36">0.77/0.63</td><td id="25-37">0.56</td></tr>
<tr><td id="25-38"></td><td id="25-39">SelfRAG</td><td id="25-3a">0.20</td><td id="25-3b">0.10</td><td id="25-3c">0.58</td><td id="25-3d">0.84</td><td id="25-3e">0.65</td><td id="25-3f">0.77</td><td id="25-3g">0.77/0.63</td><td id="25-3h">0.55</td></tr>
<tr><td id="25-3i"></td><td id="25-3j">MemOS</td><td id="25-3k">0.17</td><td id="25-3l">0.20</td><td id="25-3m">0.55</td><td id="25-3n">0.84</td><td id="25-3o">0.64</td><td id="25-3p">0.76</td><td id="25-3q">0.76/0.62</td><td id="25-3r">0.55</td></tr>
<tr><td id="25-3s">Claude 3.7 Sonnet</td><td id="25-3t">Mem0</td><td id="25-3u">0.20</td><td id="25-3v">0.13</td><td id="25-3w">0.58</td><td id="25-3x">0.84</td><td id="25-3y">0.62</td><td id="25-3z">0.77</td><td id="25-3A">0.76/0.61</td><td id="25-3B">0.55</td></tr>
<tr><td id="25-3C"></td><td id="25-3D">LangMem</td><td id="25-3E">0.10</td><td id="25-3F">0.13</td><td id="25-3G">0.53</td><td id="25-3H">0.77</td><td id="25-3I">0.56</td><td id="25-3J">0.66</td><td id="25-3K">0.77/0.63</td><td id="25-3L">0.49</td></tr>
<tr><td id="25-3M"></td><td id="25-3N">DC-RS</td><td id="25-3O">0.20</td><td id="25-3P">0.20</td><td id="25-3Q">0.62</td><td id="25-3R">0.79</td><td id="25-3S">0.52</td><td id="25-3T">0.60</td><td id="25-3U">0.77/0.62</td><td id="25-3V">0.52</td></tr>
<tr><td id="25-3W"></td><td id="25-3X">AWM</td><td id="25-3Y">0.03</td><td id="25-3Z">0.03</td><td id="25-40">0.53</td><td id="25-41">0.80</td><td id="25-42">0.56</td><td id="25-43">0.72</td><td id="25-44">0.76/0.62</td><td id="25-45">0.48</td></tr>
<tr><td id="25-46"></td><td id="25-47">DC-Cu</td><td id="25-48">0.17</td><td id="25-49">0.23</td><td id="25-4a">0.57</td><td id="25-4b">0.79</td><td id="25-4c">0.52</td><td id="25-4d">0.65</td><td id="25-4e">0.77/0.62</td><td id="25-4f">0.52</td></tr>
<tr><td id="25-4g"></td><td id="25-4h">ExpRecent</td><td id="25-4i">0.13</td><td id="25-4j">0.20</td><td id="25-4k">0.61</td><td id="25-4l">0.86</td><td id="25-4m">0.63</td><td id="25-4n">0.78</td><td id="25-4o">0.82/0.66</td><td id="25-4p">0.56</td></tr>
<tr><td id="25-4q"></td><td id="25-4r">ExpRAG</td><td id="25-4s">0.17</td><td id="25-4t">0.17</td><td id="25-4u">0.70</td><td id="25-4v">0.85</td><td id="25-4w">0.67</td><td id="25-4x">0.80</td><td id="25-4y">0.88/0.72</td><td id="25-4z">0.59</td></tr>
<tr><td id="25-4A"></td><td id="25-4B">ReMem</td><td id="25-4C">0.13</td><td id="25-4D">0.13</td><td id="25-4E">0.67</td><td id="25-4F">0.86</td><td id="25-4G">0.65</td><td id="25-4H">0.80</td><td id="25-4I">0.87/0.71</td><td id="25-4J">0.58</td></tr>
<tr><td id="25-4K"></td><td id="25-4L">Baseline</td><td id="25-4M">0.47</td><td id="25-4N">0.47</td><td id="25-4O">0.48</td><td id="25-4P">0.83</td><td id="25-4Q">0.46</td><td id="25-4R">0.75</td><td id="25-4S">0.71/0.61</td><td id="25-4T">0.59</td></tr>
<tr><td id="25-4U"></td><td id="25-4V">History</td><td id="25-4W">0.60</td><td id="25-4X">0.47</td><td id="25-4Y">0.43</td><td id="25-4Z">0.84</td><td id="25-50">0.42</td><td id="25-51">0.78</td><td id="25-52">0.31/0.26</td><td id="25-53">0.55</td></tr>
<tr><td id="25-54"></td><td id="25-55">ReAct</td><td id="25-56">0.30</td><td id="25-57">0.27</td><td id="25-58">0.05</td><td id="25-59">0.64</td><td id="25-5a">0.16</td><td id="25-5b">0.54</td><td id="25-5c">0.64/0.57</td><td id="25-5d">0.37</td></tr>
<tr><td id="25-5e"></td><td id="25-5f">Amem</td><td id="25-5g">0.70</td><td id="25-5h">0.57</td><td id="25-5i">0.52</td><td id="25-5j">0.83</td><td id="25-5k">0.42</td><td id="25-5l">0.72</td><td id="25-5m">0.72/0.60</td><td id="25-5n">0.63</td></tr>
<tr><td id="25-5o"></td><td id="25-5p">SelfRAG</td><td id="25-5q">0.50</td><td id="25-5r">0.47</td><td id="25-5s">0.46</td><td id="25-5t">0.83</td><td id="25-5u">0.45</td><td id="25-5v">0.75</td><td id="25-5w">0.72/0.61</td><td id="25-5x">0.59</td></tr>
<tr><td id="25-5y"></td><td id="25-5z">MemOS</td><td id="25-5A">0.47</td><td id="25-5B">0.47</td><td id="25-5C">0.50</td><td id="25-5D">0.82</td><td id="25-5E">0.46</td><td id="25-5F">0.75</td><td id="25-5G">0.71/0.61</td><td id="25-5H">0.59</td></tr>
<tr><td id="25-5I">Gemini 2.5 Flash</td><td id="25-5J">Mem0</td><td id="25-5K">0.50</td><td id="25-5L">0.47</td><td id="25-5M">0.45</td><td id="25-5N">0.83</td><td id="25-5O">0.46</td><td id="25-5P">0.74</td><td id="25-5Q">0.71/0.61</td><td id="25-5R">0.59</td></tr>
<tr><td id="25-5S"></td><td id="25-5T">LangMem</td><td id="25-5U">0.43</td><td id="25-5V">0.50</td><td id="25-5W">0.53</td><td id="25-5X">0.79</td><td id="25-5Y">0.39</td><td id="25-5Z">0.71</td><td id="25-60">0.68/0.57</td><td id="25-61">0.57</td></tr>
<tr><td id="25-62"></td><td id="25-63">DC-RS</td><td id="25-64">0.53</td><td id="25-65">0.37</td><td id="25-66">0.48</td><td id="25-67">0.80</td><td id="25-68">0.42</td><td id="25-69">0.69</td><td id="25-6a">0.68/0.57</td><td id="25-6b">0.56</td></tr>
<tr><td id="25-6c"></td><td id="25-6d">DC-Cu</td><td id="25-6e">0.60</td><td id="25-6f">0.40</td><td id="25-6g">0.48</td><td id="25-6h">0.79</td><td id="25-6i">0.44</td><td id="25-6j">0.69</td><td id="25-6k">0.70/0.59</td><td id="25-6l">0.58</td></tr>
<tr><td id="25-6m"></td><td id="25-6n">AWM</td><td id="25-6o">0.50</td><td id="25-6p">0.37</td><td id="25-6q">0.49</td><td id="25-6r">0.79</td><td id="25-6s">0.43</td><td id="25-6t">0.72</td><td id="25-6u">0.71/0.59</td><td id="25-6v">0.56</td></tr>
<tr><td id="25-6w"></td><td id="25-6x">ExpRecent</td><td id="25-6y">0.47</td><td id="25-6z">0.47</td><td id="25-6A">0.42</td><td id="25-6B">0.83</td><td id="25-6C">0.39</td><td id="25-6D">0.75</td><td id="25-6E">0.78/0.66</td><td id="25-6F">0.58</td></tr>
<tr><td id="25-6G"></td><td id="25-6H">ExpRAG</td><td id="25-6I">0.43</td><td id="25-6J">0.47</td><td id="25-6K">0.42</td><td id="25-6L">0.83</td><td id="25-6M">0.43</td><td id="25-6N">0.78</td><td id="25-6O">0.87/0.73</td><td id="25-6P">0.60</td></tr>
<tr><td id="25-6Q"></td><td id="25-6R">ReMem</td><td id="25-6S">0.60</td><td id="25-6T">0.53</td><td id="25-6U">0.51</td><td id="25-6V">0.85</td><td id="25-6W">0.46</td><td id="25-6X">0.79</td><td id="25-6Y">0.85/0.71</td><td id="25-6Z">0.65</td></tr>
<tr><td id="25-70"></td><td id="25-71">Baseline</td><td id="25-72">0.53</td><td id="25-73">0.43</td><td id="25-74">0.37</td><td id="25-75">0.73</td><td id="25-76">0.34</td><td id="25-77">0.60</td><td id="25-78">0.78/0.61</td><td id="25-79">0.58</td></tr>
<tr><td id="25-7a"></td><td id="25-7b">History</td><td id="25-7c">0.40</td><td id="25-7d">0.33</td><td id="25-7e">0.31</td><td id="25-7f">0.74</td><td id="25-7g">0.30</td><td id="25-7h">0.59</td><td id="25-7i">0.58/0.47</td><td id="25-7j">0.49</td></tr>
<tr><td id="25-7k"></td><td id="25-7l">ReAct</td><td id="25-7m">0.53</td><td id="25-7n">0.33</td><td id="25-7o">0.34</td><td id="25-7p">0.61</td><td id="25-7q">0.20</td><td id="25-7r">0.48</td><td id="25-7s">0.73/0.56</td><td id="25-7t">0.50</td></tr>
<tr><td id="25-7u"></td><td id="25-7v">Amem</td><td id="25-7w">0.40</td><td id="25-7x">0.33</td><td id="25-7y">0.33</td><td id="25-7z">0.72</td><td id="25-7A">0.34</td><td id="25-7B">0.59</td><td id="25-7C">0.77/0.61</td><td id="25-7D">0.54</td></tr>
<tr><td id="25-7E"></td><td id="25-7F">SelfRAG</td><td id="25-7G">0.57</td><td id="25-7H">0.37</td><td id="25-7I">0.37</td><td id="25-7J">0.73</td><td id="25-7K">0.32</td><td id="25-7L">0.62</td><td id="25-7M">0.81/0.63</td><td id="25-7N">0.57</td></tr>
<tr><td id="25-7O"></td><td id="25-7P">MemOS</td><td id="25-7Q">0.53</td><td id="25-7R">0.43</td><td id="25-7S">0.37</td><td id="25-7T">0.73</td><td id="25-7U">0.34</td><td id="25-7V">0.60</td><td id="25-7W">0.78/0.61</td><td id="25-7X">0.58</td></tr>
<tr><td id="25-7Y">Gemini 2.5 Flash-Lite</td><td id="25-7Z">Mem0</td><td id="25-80">0.53</td><td id="25-81">0.43</td><td id="25-82">0.37</td><td id="25-83">0.73</td><td id="25-84">0.34</td><td id="25-85">0.60</td><td id="25-86">0.78/0.61</td><td id="25-87">0.58</td></tr>
<tr><td id="25-88"></td><td id="25-89">LangMem</td><td id="25-8a">0.40</td><td id="25-8b">0.37</td><td id="25-8c">0.48</td><td id="25-8d">0.59</td><td id="25-8e">0.24</td><td id="25-8f">0.56</td><td id="25-8g">0.33/0.26</td><td id="25-8h">0.43</td></tr>
<tr><td id="25-8i"></td><td id="25-8j">DC-RS</td><td id="25-8k">0.53</td><td id="25-8l">0.43</td><td id="25-8m">0.34</td><td id="25-8n">0.59</td><td id="25-8o">0.18</td><td id="25-8p">0.36</td><td id="25-8q">0.73/0.56</td><td id="25-8r">0.49</td></tr>
<tr><td id="25-8s"></td><td id="25-8t">AWM</td><td id="25-8u">0.03</td><td id="25-8v">0.03</td><td id="25-8w">0.35</td><td id="25-8x">0.61</td><td id="25-8y">0.20</td><td id="25-8z">0.48</td><td id="25-8A">0.77/0.60</td><td id="25-8B">0.44</td></tr>
<tr><td id="25-8C"></td><td id="25-8D">DC-Cu</td><td id="25-8E">0.53</td><td id="25-8F">0.43</td><td id="25-8G">0.33</td><td id="25-8H">0.55</td><td id="25-8I">0.16</td><td id="25-8J">0.32</td><td id="25-8K">0.71/0.56</td><td id="25-8L">0.47</td></tr>
<tr><td id="25-8M"></td><td id="25-8N">ExpRecent</td><td id="25-8O">0.57</td><td id="25-8P">0.33</td><td id="25-8Q">0.35</td><td id="25-8R">0.76</td><td id="25-8S">0.29</td><td id="25-8T">0.62</td><td id="25-8U">0.82/0.65</td><td id="25-8V">0.58</td></tr>
<tr><td id="25-8W"></td><td id="25-8X">ExpRAG</td><td id="25-8Y">0.47</td><td id="25-8Z">0.37</td><td id="25-90">0.38</td><td id="25-91">0.79</td><td id="25-92">0.32</td><td id="25-93">0.66</td><td id="25-94">0.87/0.68</td><td id="25-95">0.61</td></tr>
<tr><td id="25-96"></td><td id="25-97">ReMem</td><td id="25-98">0.57</td><td id="25-99">0.33</td><td id="25-9a">0.38</td><td id="25-9b">0.77</td><td id="25-9c">0.34</td><td id="25-9d">0.65</td><td id="25-9e">0.86/0.67</td><td id="25-9f">0.61</td></tr>
</table>

<a id='3f9b2f7d-05ed-4f78-94fd-dfbe3744c9dd'></a>

Table 6 | Cross-dataset results of diverse memory architectures across models. Categories are separated by horizontal rules; results (Exact Match↑ and API/Acc↑) compare zero-shot, agentic, adaptive, procedural, and proposed memory methods. Dashes (—) indicate methods with poor or unreliable performance, which are therefore omitted.

<a id='a4af2c86-d935-4d86-a2bc-025c87e4744c'></a>

26