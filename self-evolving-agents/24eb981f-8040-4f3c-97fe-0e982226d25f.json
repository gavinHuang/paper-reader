{
  "responderUsername": "GitHub Copilot",
  "responderAvatarIconUri": {
    "id": "copilot"
  },
  "initialLocation": "panel",
  "requests": [
    {
      "requestId": "request_133c9e89-2f21-46e6-8764-a4d4aabcaa77",
      "message": {
        "parts": [
          {
            "range": {
              "start": 0,
              "endExclusive": 25
            },
            "editorRange": {
              "startLineNumber": 1,
              "startColumn": 1,
              "endLineNumber": 1,
              "endColumn": 26
            },
            "text": "whats this paper is about",
            "kind": "text"
          }
        ],
        "text": "whats this paper is about"
      },
      "variableData": {
        "variables": [
          {
            "kind": "file",
            "id": "vscode.implicit.selection",
            "name": "file:24eb981f-8040-4f3c-97fe-0e982226d25f.md",
            "value": {
              "uri": {
                "$mid": 1,
                "fsPath": "d:\\GPT\\paper-reader\\self-evolving-agents\\24eb981f-8040-4f3c-97fe-0e982226d25f.md",
                "_sep": 1,
                "external": "file:///d%3A/GPT/paper-reader/self-evolving-agents/24eb981f-8040-4f3c-97fe-0e982226d25f.md",
                "path": "/d:/GPT/paper-reader/self-evolving-agents/24eb981f-8040-4f3c-97fe-0e982226d25f.md",
                "scheme": "file"
              },
              "range": {
                "startLineNumber": 3,
                "startColumn": 1,
                "endLineNumber": 4,
                "endColumn": 1,
                "selectionStartLineNumber": 3,
                "selectionStartColumn": 1,
                "positionLineNumber": 4,
                "positionColumn": 1
              }
            },
            "modelDescription": "User's active selection"
          }
        ]
      },
      "response": [
        {
          "kind": "mcpServersStarting",
          "didStartServerIds": []
        },
        {
          "kind": "thinking",
          "value": "The user is asking about what the paper is about. They have the file open and have selected the title \"Real-Time Procedural Learning From Experience for AI Agents\".\n\nLooking at the paper, I can provide a concise summary of what it's about:\n\nThis paper introduces PRAXIS (Procedural Recall for Agents with eXperiences Indexed by State), a learning mechanism that allows AI agents to learn procedures from experience in real-time. The key points are:\n\n1. The problem: Most LLM-based agents can't acquire procedural knowledge after deployment\n2. The solution: PRAXIS stores consequences of actions and retrieves them by matching environmental and internal states\n3. Testing: Evaluated on REAL web browsing benchmark\n4. Results: Improves task completion accuracy, reliability, and efficiency across different foundation models\n\nLet me provide a brief, direct answer as per the communication style guidelines.",
          "id": "1767509043732",
          "generatedTitle": "Asked about paper content and selected title"
        },
        {
          "kind": "thinking",
          "value": "",
          "id": "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"
        },
        {
          "kind": "thinking",
          "value": "",
          "id": "",
          "metadata": {
            "vscodeReasoningDone": true,
            "stopReason": "text"
          }
        },
        {
          "value": "This paper introduces **PRAXIS** (Procedural Recall for Agents with eXperiences Indexed by State), a learning mechanism that enables AI agents to learn procedures from experience in real-time after deployment.\n\n**Core concept:** Instead of pre-programming procedures, PRAXIS stores past action outcomes and retrieves relevant experiences by matching both the current environment state (what the agent sees) and internal state (what it's trying to accomplish) to similar past situations.\n\n**Key contributions:**\n- **State-dependent memory system** that indexes experiences by environment + goal context\n- Tested on the Altrina web agent across the REAL benchmark (112 web browsing tasks)\n- Consistent improvements across multiple foundation models (Claude, GPT, Gemini, Llama, Qwen):\n  - **Accuracy**: 40.3% → 44.1% average\n  - **Reliability**: 74.5% → 79.0% (repeatability across runs)\n  - **Efficiency**: 25.2 → 20.2 steps to complete tasks\n\n**Why it matters:** Most existing memory systems for AI agents focus on factual memory (preferences, user info), but this addresses *procedural* learning—how to actually accomplish tasks in dynamic environments where procedures aren't fully documented and interfaces change frequently. The web is an ideal test environment since it requires multi-step procedures that adapt to changing interfaces.",
          "supportThemeIcons": false,
          "supportHtml": false,
          "baseUri": {
            "$mid": 1,
            "path": "/d:/GPT/paper-reader/",
            "scheme": "file"
          }
        }
      ],
      "agent": {
        "extensionId": {
          "value": "GitHub.copilot-chat",
          "_lower": "github.copilot-chat"
        },
        "extensionVersion": "0.36.2025121901",
        "publisherDisplayName": "GitHub",
        "extensionPublisherId": "GitHub",
        "extensionDisplayName": "GitHub Copilot Chat",
        "id": "github.copilot.editsAgent",
        "description": "Edit files in your workspace in agent mode",
        "when": "config.chat.agent.enabled",
        "metadata": {
          "themeIcon": {
            "id": "tools"
          },
          "hasFollowups": false,
          "supportIssueReporting": false
        },
        "name": "agent",
        "fullName": "GitHub Copilot",
        "isDefault": true,
        "locations": [
          "panel"
        ],
        "modes": [
          "agent"
        ],
        "slashCommands": [],
        "disambiguation": []
      },
      "timestamp": 1767509040256,
      "modelId": "copilot/claude-sonnet-4.5",
      "responseId": "response_25f9432b-0084-46a2-8f85-0cc7b5d15098",
      "result": {
        "timings": {
          "firstProgress": 8603,
          "totalElapsed": 16360
        },
        "metadata": {
          "codeBlocks": [],
          "renderedUserMessage": [
            {
              "type": 1,
              "text": "<attachments>\n<attachment id=\"file:24eb981f-8040-4f3c-97fe-0e982226d25f.md\">\nUser's active selection:\nExcerpt from 24eb981f-8040-4f3c-97fe-0e982226d25f.md, lines 3 to 4:\n```markdown\nReal-Time Procedural Learning From Experience for AI Agents\n```\n</attachment>\n<attachment filePath=\"d:\\\\GPT\\\\paper-reader\\\\self-evolving-agents\\\\24eb981f-8040-4f3c-97fe-0e982226d25f.md\">\nUser's active file for additional context:\n<a id='7cfdd488-96db-4787-833f-ff899aa365fa'></a>\r\n\r\nReal-Time Procedural Learning From Experience for AI Agents\r\n\r\n<a id='ae47ab06-18b7-493a-9825-3aed1294fcde'></a>\r\n\r\nDASHENG BI*, Altrina, USA\r\nYUBIN HU*, Altrina, USA\r\n\r\n<a id='b80a9c0e-202e-4086-b4a6-df7bdd748a59'></a>\r\n\r\nMOHAMMED N. NASIR, Altrina, USA\r\n\r\nLearning how to do things from trial and error in real time is a hallmark of biological intelligence, yet most LLM-based agents lack mechanisms to acquire procedural knowledge after deployment. We propose Procedural Recall for Agents with eXperiences Indexed by State (PRAXIS), a lightweight post-training learning mechanism that stores the consequences of actions and retrieves them by jointly matching environmental and internal states of past episodes to the current state. PRAXIS augments agentic action selection with retrieved state-action-result exemplars that are generated in real time. When evaluated on the REAL web browsing benchmark, PRAXIS improves task completion accuracy, reliability, and cost efficiency across different foundation model backbones, and shows preliminary generalization to unseen tasks in similar environments. These results demonstrate that PRAXIS enables the practical adoption of AI agents in fast-evolving stateful environments by helping them learn new procedures effectively.\r\n\r\n<a id='8a75b943-e4d8-44e7-860c-4633b2614509'></a>\r\n\r\n# 1 Introduction\r\n\r\n## 1.1 AI Agents and Procedural Learning\r\n\r\nAI agents are artificial intelligence systems capable of observing and taking actions in an environment. As adoption spreads across industries, there is a growing need for AI agents to quickly learn domain- or user-specific information. There are two main classes of information that agents are typically expected to learn:\r\n\r\n1.  **Facts**: atomic pieces of information independent of the state of the agent or environment (e.g., a user's name, preferences, organizational charts). Facts can change over time, but at any given moment they are generally context-independent.\r\n2.  **Procedures**: established conventions for doing things (e.g., \"how to troubleshoot a failed login\" or \"how to guide a customer through a sales process to the most fitting product\"). Procedures can be viewed as a sequence of state-dependent requirements or preferences over actions.\r\n\r\n<a id='cfb7dcc9-93a6-42a1-8180-f3872612ebae'></a>\r\n\r\nIn real-world applications, learning and optimizing procedures in real time are at least as important as learning facts. While frameworks such as Mem0 [4] and Letta [12] focus on long-term factual memory, effective post-training learning of procedures in Al agents remains relatively underexplored.\r\n\r\n<a id='0fe65bd2-19e0-4a72-a27b-8a9d38638b2f'></a>\r\n\r\nA naïve approach is _a priori_ procedural specification: a human writes rules or standard operating procedures (SOPs) that are included in the agent's context at inference time. This approach effectively reduces procedures to a large bundle of facts. In practice, this approach faces challenges because (1) many procedures are not fully documented, as humans are often trained by observation rather than by reading long SOPs; (2) enumerating all states and edge cases in a combinatorial space is difficult; and (3) procedures can become obsolete quickly as environments change. We argue that a more robust approach is to learn procedures _a posteriori_ from demonstrations or experience. Inspired by state-dependent memory in psychology [3, 14], we propose PRAXIS, a concrete method for procedural recall and show that it improves agent accuracy, reliability, and efficiency in web browsing settings. Our method is compatible with both experiences demonstrated by a human expert and actual trajectories generated by the AI agent itself.\r\n\r\n<a id='bdb81928-70c4-4f04-a471-9b916a25a7ba'></a>\r\n\r\n--- Both authors contributed equally to this research.\r\n\r\n<a id='80b8c531-989e-4b65-b4a8-d26b44917505'></a>\r\n\r\nAuthors' Contact Information: Dasheng Bi, dbi@altrina.com, Altrina, Menlo Park, California, USA; Yubin Hu, harvey@altrina.com, Altrina, Menlo Park, California, USA; Mohammed N. Nasir, mo@altrina.com, Altrina, Menlo Park, California, USA.\r\n\r\n<a id='dd67cdec-633c-47cf-9d96-1d462194058e'></a>\r\n\r\n1\r\n\r\n<a id='edd3c6e8-2d85-4550-93fb-62249a717cb8'></a>\r\n\r\narXiv:2511.22074v1 [cs.AI] 27 Nov 2025\r\n\r\n<!-- PAGE BREAK -->\r\n\r\n<a id='ac355a71-dd56-4b7a-aa9f-32d3a1559891'></a>\r\n\r\n2\r\n\r\n<a id='998850a5-219e-442a-9d64-0b8001567d84'></a>\r\n\r\nBi, Hu, and Nasir\r\n\r\n<a id='1a58dc58-fb9e-40d5-8144-7e850c37d326'></a>\r\n\r\n1.2 Web Agents and the Browser Environment\r\nHuman-facing web applications almost always require multi-step interactions to accomplish meaningful goals (e.g., purchasing an item online requires searching, filtering, logging in, completing forms, and checking out)—a procedure. Moreover, these procedures must also adapt to changing environments (e.g., an e-commerce site may have seasonal pop-ups or redesigned interfaces), making web browsing a natural environment to study *a posteriori* procedural learning. Importantly, even when tasks are obvious to humans, comprehensive procedures are rarely documented, and high personalization limits pretraining coverage in foundation models. As AI-based design tools increasingly generate and update web platforms, the economic value shifts to novel, previously unseen interfaces and interaction flows, pushing agents into out-of-distribution states and rendering *a priori* SOPs brittle. A post-training, state-indexed procedural memory thus becomes essential for robust web automation, allowing agents to acquire and reuse procedures precisely when new states appear.\r\n\r\n<a id='753cb0aa-75dc-4beb-8f96-637abdbf4939'></a>\r\n\r\n## 2 Related Work\r\n\r\nExternal Memory for AI Chatbots. A broad class of systems augment LLMs with non-parametric memory in the conversational environment. Retrieval-augmented generation (RAG) attaches a document store to provide factual knowledge at inference time [8]. In agentic settings, persistent memory frameworks such as Letta (formerly MemGPT) [12] provide hierarchical storage and dynamic context management. Mem0 [4] provides a queryable, cross-session memory for user preferences and long-range conversational context. Academic frameworks include MemoryBank [19] which mimics human long-term memory with continual decay and reinforcement, and A-MEM [16] which dynamically links and evolves structured notes. These approaches typically focus on factual memory for agents in conversational environments. In contrast, our method focuses on learning action policies in stateful visual environments that are significantly more complex and not entirely observable like the web environment.\r\n\r\n<a id='7083e7aa-15c2-43f2-914d-664c75c73e94'></a>\r\n\r\nExperience-Based Self-Improvement and Workflow Memories. A complementary line of work improves agents via self-reflection. Reflexion [13] maintains verbal reflections in an episodic buffer to guide subsequent trials; Self-Refine [10] iteratively critiques and edits its own outputs; and CLIN [11] performs continual task adaptation with a persistent textual memory of causal abstractions. These methods are effective but are not well-tested in visual environments and generally do not encode information of environmental state. There has also been a line of work on experience- or workflow-based memories for agents. Agent Workflow Memory [15], Synapse [18], and ExpeL [17] induce abstracted, natural language workflows from successful trajectories and retrieve them to augment prompts at test time. In contrast, our method performs local state-based recall that is grounded primarily in the live environment state, a factor not present in prior works, and secondarily to the goal. Moreover, we index memories with explicit state and action descriptors, rather than high-level trajectories, enabling precise recall and learning of minute details required for environments like the web.\r\n\r\n<a id='44666a96-8363-4a2d-b4c3-58b3216f0ace'></a>\r\n\r\n## 3 Methods\r\n### 3.1 Altrina Agent\r\nThe experiments in this study were conducted with Altrina¹, a frontier AI agent operating in computer use environments.\r\nAltrina is capable of perceiving the environment both visually and as compressed textual information, taking actions\r\n\r\n<a id='16ff8813-8e3f-45da-914f-117405bd2838'></a>\r\n\r\n---1\r\nThe agent system described in this work was previously known as Tessa in prior publications [1, 2].\r\n\r\n<!-- PAGE BREAK -->\r\n\r\n<a id='6f71a6a4-6dfb-4ceb-aa22-5a5ac03e5696'></a>\r\n\r\n3\r\n\r\n<a id='76ed5d51-4c88-4279-9ad5-7fba4dd686d6'></a>\r\n\r\nReal-Time Procedural Learning From Experience for AI Agents\r\n\r\n<a id='cec520e2-beff-4e00-afd8-6a3a28ea4776'></a>\r\n\r\nlike a human on a computer, and executing complex directives end-to-end. While Altrina's capabilities extend beyond acting in the browser, we restrict the action space to web-related actions for this study. In the browser environment, we previously showed that the baseline Altrina agent achieves state-of-the-art results on multiple benchmarks [1, 2], including WebVoyager [7] and REAL [6].\r\n\r\n<a id='2ab4222d-9af8-4f05-8cb5-2883c6e92d4f'></a>\r\n\r\nAltrina's scaffolding layer orchestrates underlying foundational models with a \"node-based\" architecture. Each node is designed for a specific function in the agentic loop. In this study, we modify the *action selection node*, which is responsible for deciding what the next agent action should be given the current state and progress towards the task objective, by adding a dedicated *procedural memory* section to its context.\r\n\r\n<a id='d84f2258-8c8e-4d6c-ba56-7a5245a5a7f2'></a>\r\n\r\n## 3.2 State-Dependent Memory\r\n\r\nMirroring state-dependent memory in psychology, which finds improved recall when internal state and external context at retrieval match those present during encoding [3, 14], we designed a state-dependent memory for Altrina in which indexing and retrieval are based on the browser's environment state and the agent's internal state.\r\n\r\nEach memory entry contains the following components:\r\n\r\n1.  $M_i^{env-pre}$, a description of the environmental state in which this memory is generated\r\n2.  $M_i^{int}$, the internal state of the agent at the time of the experience, including the overall directive the agent is trying to achieve\r\n3.  $a_i$, the action taken at the time\r\n4.  $M_i^{env-post}$, the state of the environment after the action was taken\r\n\r\nFormally, we use the following procedure (Alg. 1) to retrieve a set of memories that may be informative to the agent:\r\n\r\n<a id='95a5c279-f2b7-48b9-b348-8277ab45cfd2'></a>\r\n\r\nAlgorithm 1: Procedural Memory Retrieval\r\n\r\n**Input:** Memory environment states ${M_i^{env}}_{i=1}^n$, query environment state $Q^{env}$, memory internal states ${M_i^{int}}_{i=1}^n$, query internal state $Q^{int}$, internal state embedding function $f$, search breadth $k$, similarity threshold $\\tau$\r\n**Output:** Retrieved memory indices $\\mathcal{R}$\r\n\r\nfor $i \\leftarrow 1$ to $n$ do\r\n  $v_i \\leftarrow \\text{IoU}(M_i^{env}, Q^{env})$;\r\n  $\\ell_m \\leftarrow |M_i^{env}|$, $\\ell_q \\leftarrow |Q^{env}|$;\r\n  $\\ell_i \\leftarrow \\text{LENGTHOVERLAP}(\\ell_m, \\ell_q) = 1 - \\frac{|\\ell_m - \\ell_q|}{\\max(\\ell_m, \\ell_q)}$;\r\n  $s_i^{env} \\leftarrow v_i \\cdot \\ell_i$;\r\n  $s_i^{int} \\leftarrow \\langle f(M_i^{int}), f(Q^{int})\\rangle$;\r\n$\\mathcal{R}^{env} \\leftarrow \\text{TOPKINDICES}(s^{env}, k)$;\r\n$\\tilde{\\mathcal{R}} \\leftarrow \\text{SORTED}(\\mathcal{R}^{env}; \\text{key} = i \\mapsto s_i^{int}, \\text{descending})$;\r\n$\\mathcal{R} \\leftarrow [i \\in \\tilde{\\mathcal{R}} | s_i^{env} \\geq \\tau]$;\r\nreturn $\\mathcal{R}$;\r\n\r\n<a id='94b33bdf-a3b1-4703-907b-f3a9e933ffb5'></a>\r\n\r\n## 3.3 Benchmark\r\nA number of benchmarks exist to evaluate the performance of AI agents. MiniWoB/MiniWoB++ target short-horizon GUI primitives [9]; Mind2Web and WebVoyager target relatively straightforward workflows on live websites [5, 7]; WebArena hosts replicas of a few websites for local testing of relatively simple tasks with text-based evaluations [20].\r\n\r\n<!-- PAGE BREAK -->\r\n\r\n<a id='61d7866e-9d22-4c42-86f2-a23cf15e526d'></a>\r\n\r\n4\r\n\r\n<a id='188d9cc7-d611-4593-9bed-0860a32ce5a7'></a>\r\n\r\nBi, Hu, and Nasir\r\n\r\n<a id='f599d1a6-1fc6-4cdb-912e-cc79b32f13c0'></a>\r\n\r\nTo ensure reproducible evaluation on tasks of real-world relevance, we evaluate on deterministic clones of functional websites using the REAL benchmark [6]. REAL provides replicas of 11 commonly used sites and 112 everyday tasks of varying complexity, along with both programmatic state checks for action tasks and rubric-guided natural language evaluations for information retrieval tasks. Prior benchmarking reports that frontier models with naïve scaffolding achieve at most ~41% success, leaving substantial room for post-training procedural learning.\r\n\r\n<a id='4eba8c9a-6bd2-4e50-9617-1f351df88311'></a>\r\n\r\n## 4 Results\r\n\r\n### 4.1 Procedural memory improves agent accuracy\r\n\r\nWe benchmarked Altrina on REAL using several VLM backbones for each of the agentic compute nodes, either with or without access to procedural memory (Fig. 1). Procedural memory consistently improved the performance of Altrina (average performance across models increased from 40.3% to 44.1%, mean accuracy over five repetitions).\r\n\r\n<a id='fb8f583c-562b-4d6b-aab7-2938ea0e7975'></a>\r\n\r\n<::Bar chart showing performance on REAL benchmark for different models. The y-axis represents performance in percentage, ranging from 20% to 60%. The x-axis lists different models. For each model, two bars are displayed: the first (grey) represents performance without procedural memory, and the second (blue) represents performance with procedural memory. \r\nData:\r\n- Llama 4: Without procedural memory: 25.2%, With procedural memory: 28.9%\r\n- Qwen3-VL: Without procedural memory: 37.1%, With procedural memory: 40.7%\r\n- Gemini 2.5 Flash: Without procedural memory: 42.0%, With procedural memory: 48.6%\r\n- GPT-5: Without procedural memory: 45.9%, With procedural memory: 49.3%\r\n- Claude Sonnet 4.5: Without procedural memory: 51.2%, With procedural memory: 53.2%\r\nFig. 1. Performance on REAL benchmark for different models with and without procedural memory.\r\n: chart::>\r\n\r\n<a id='76004d55-0196-444a-b7a2-6ab6414e0503'></a>\r\n\r\nWe also measured the best-of-5 accuracy (Table 1), which reflects the agent's capability frontier under repeated attempts. Procedural memory also improved best-of-5 accuracy (average across models from 53.7% to 55.7%). Together, these results suggest that retrieved procedural memory traces provide reusable priors that improve agentic behavior on complex tasks, and that such memory traces may be generalized across similar tasks, supporting the completion of novel tasks.\r\n\r\n<a id='38aec8d1-bbc8-4d28-be8c-566aadbed139'></a>\r\n\r\nTable 1. Best-of-5 accuracy over tasks.\r\n\r\n<table id=\"3-1\">\r\n<tr><td id=\"3-2\">Model</td><td id=\"3-3\">No PM</td><td id=\"3-4\">With PM</td></tr>\r\n<tr><td id=\"3-5\">Llama 4</td><td id=\"3-6\">47.3</td><td id=\"3-7\">52.7</td></tr>\r\n<tr><td id=\"3-8\">Qwen3-VL</td><td id=\"3-9\">44.6</td><td id=\"3-a\">47.3</td></tr>\r\n<tr><td id=\"3-b\">Gemini 2.5 Flash</td><td id=\"3-c\">59.8</td><td id=\"3-d\">61.6</td></tr>\r\n<tr><td id=\"3-e\">GPT-5</td><td id=\"3-f\">56.2</td><td id=\"3-g\">57.1</td></tr>\r\n<tr><td id=\"3-h\">Claude Sonnet 4.5</td><td id=\"3-i\">60.7</td><td id=\"3-j\">59.8</td></tr>\r\n</table>\r\n\r\n<!-- PAGE BREAK -->\r\n\r\n<a id='852043d8-4ccd-4dd5-82f6-81b8d7e343a2'></a>\r\n\r\n5\r\n\r\n<a id='ab049c02-f0d4-4ce4-a4dd-f14818ca8b18'></a>\r\n\r\nReal-Time Procedural Learning From Experience for AI Agents\r\n\r\n<a id='59894777-d3e2-446b-b32e-766ee021cbb7'></a>\r\n\r\n## 4.2 Procedural memory improves agent reliability\r\nWe define reliability as the mean success rate over five repetitions of a task, averaged over all tasks in REAL having at least one successful run over the five repetitions. Procedural memory improved the reliability of Altrina from 74.5% to 79.0% averaged across all models (Fig. 2). These results suggest that retrieved procedural memory traces can suppress unwanted stochastic variance in the underlying vision-language models by biasing their decisions towards previously successful trajectories under similar states, thereby improving the reliability and repeatability of the high-level agentic behavior.\r\n\r\n<a id='ae2b0b0f-4035-41e2-b4c8-9c01ab4fe6a4'></a>\r\n\r\n<::bar chart: Agent reliability on REAL benchmark for different models with and without procedural memory. Y-axis represents percentage. X-axis represents models. Data:\r\n- Llama 4:\r\n  - Without procedural memory: 53.2%\r\n  - With procedural memory: 54.9%\r\n- Gemini 2.5 Flash:\r\n  - Without procedural memory: 70.1%\r\n  - With procedural memory: 78.8%\r\n- Qwen3-VL:\r\n  - Without procedural memory: 83.2%\r\n  - With procedural memory: 86.0%\r\n- GPT-5:\r\n  - Without procedural memory: 81.6%\r\n  - With procedural memory: 86.2%\r\n- Claude Sonnet 4.5:\r\n  - Without procedural memory: 84.4%\r\n  - With procedural memory: 89.0%\r\nFig. 2. Agent reliability on REAL benchmark for different models with and without procedural memory.::>\r\n\r\n<a id='645599da-9d8f-44bb-9ffa-2bbbb7635bc1'></a>\r\n\r\n## 4.3 Procedural memory improves agent efficiency\r\nWe measure _efficiency_ as the average number of steps taken on tasks in REAL with at least one successful run across five repetitions. Procedural memory reduces steps-to-completion on average from 25.2 to 20.2 across models (Table 2). This indicates that procedural memory recall effectively steers agentic behavior along both correct and more direct trajectories.\r\n\r\n<a id='b915943f-af69-44f5-921f-eec869817de4'></a>\r\n\r\nTable 2. Average number of steps to complete task.\r\n<table id=\"4-1\">\r\n<tr><td id=\"4-2\">Model</td><td id=\"4-3\">Base</td><td id=\"4-4\">With PM</td></tr>\r\n<tr><td id=\"4-5\">Llama 4</td><td id=\"4-6\">19.8</td><td id=\"4-7\">16.2</td></tr>\r\n<tr><td id=\"4-8\">Qwen3-VL</td><td id=\"4-9\">27.7</td><td id=\"4-a\">20.8</td></tr>\r\n<tr><td id=\"4-b\">Gemini 2.5 Flash</td><td id=\"4-c\">28.9</td><td id=\"4-d\">22.3</td></tr>\r\n<tr><td id=\"4-e\">GPT-5</td><td id=\"4-f\">24.2</td><td id=\"4-g\">20.7</td></tr>\r\n<tr><td id=\"4-h\">Claude Sonnet 4.5</td><td id=\"4-i\">25.2</td><td id=\"4-j\">21.0</td></tr>\r\n</table>\r\n\r\n<a id='8553a6d2-780f-4e39-8379-ea983a865181'></a>\r\n\r\n## 4.4 Procedural memory performance scales with retrieval breadth\r\nWe ablated the retrieval breadth hyperparameter k in Alg. 1 to study its effect on the performance of procedural memory (Fig. 3). Due to resource limitations, we only tested the effects of this ablation on the agent with a Gemini 2.5 Flash\r\n\r\n<!-- PAGE BREAK -->\r\n\r\n<a id='a2cfd8b8-b35f-49ba-a768-c75b037f3148'></a>\r\n\r\n6\r\n\r\n<a id='57bbc222-0543-4b54-b585-5279923f1382'></a>\r\n\r\nBi, Hu, and Nasir\r\n\r\n<a id='c20832ab-37d1-4cfc-9c88-a014cf585867'></a>\r\n\r\nbackbone. We found that performance generally increased in steps as we broadened our retrieval breadth, with slight decreases within each step, and converged to a plateau, suggesting that while there exists some local context crowding, procedural memory offers increasingly helpful and generalizable context to the language model at a larger scale.\r\n\r\n<a id='f0d02cdf-ca9a-4ea4-8649-9e9df56ba3f9'></a>\r\n\r\n<::line chart\r\n: Y-axis: Percentage from 40% to 52%\r\n: X-axis: Recall Breadth from 0 to 10\r\n: Data points:\r\n: (0, 42%)\r\n: (1, 44.2%)\r\n: (2, 45.2%)\r\n: (3, 49.5%)\r\n: (4, 48.2%)\r\n: (5, 48.7%)\r\n: (6, 47%)\r\n: (7, 51.5%)\r\n: (8, 51%)\r\n: (9, 49%)\r\n: (10, 49.7%)\r\n: Fig. 3. Performance as a function of retrieval breadth.\r\n::>\r\n\r\n<a id='a7ce9e77-ddb7-4349-8abe-5f1435c35c49'></a>\r\n\r\n## 5 Discussion\r\n\r\n_Personalized learning as a critical component of AI agents in the economy._ We envision a future where, instead of replacing humans, AI agents work alongside them. To encompass the diverse activities across our economy and maintain high collaboration efficiency, users will need to customize their AI agent with their own data and procedures. Even if it becomes possible to quickly and efficiently train everything into a single, universal model, we may not wish to, as each user should be able to decide whether to share their private knowledge with the world. In this context, personalized learning methods like PRAXIS that customize agents, not only on a superficial level, but also in terms of real capabilities, will be critical to the adoption of agents in the economy.\r\n\r\n<a id='185e9d49-b43b-4217-84e4-43b52422e0f1'></a>\r\n\r\nSummary of contributions. This paper introduces _state-dependent memory_, an _a posteriori_ learning mechanism that stores local interaction traces and retrieves them by jointly matching the _environment state_ and the agent's _internal objective_. When tested as an integration into the Altrina web agent, our method yields consistent improvements on the REAL web browsing benchmark across diverse VLM backbones: higher average accuracy, higher best-of-5 accuracy, better reliability, and fewer steps to completion. An ablation shows increased gains with retrieval breadth _k_. Together, our results indicate that state-dependent memory provides reusable local state-to-action priors that guide AI agents towards robust, generalizable behavior.\r\n\r\n<a id='93ae8e71-9e07-4796-ac28-c500929be7bf'></a>\r\n\r\nFuture directions. Beyond web environments. State-dependent memory is conceptually agnostic to the environment, and the same idea can be naturally extended to general cases of agentic computer use. Richer state encoding. Our proof-of-concept implementation of state-dependent memory uses basic visual and DOM feature overlap along with simple similarity metrics. A richer encoder can improve both retrieval quality and invariance to superficial changes.\r\n\r\n<!-- PAGE BREAK -->\r\n\r\n<a id='d7bd7f54-da1d-450d-a900-bea4fda16c0a'></a>\r\n\r\n7\r\n\r\n<a id='fed1062e-6e15-4ba6-baec-f7bd9318b82c'></a>\r\n\r\nReal-Time Procedural Learning From Experience for AI Agents\r\n\r\n<a id='62d3b3e5-2b3e-4e2d-a559-da5ec178bb56'></a>\r\n\r\nAdaptive retrieval mechanisms. Rather than a fixed retrieval based on state similarity heuristics, the retrieval mechanism can account for real-time factors such as uncertainty and compute budget. For highly uncertain scenarios, retrieval can also be iterative to improve quality. *From action agents to alignment agents* While the training signal in this paper is objective task success or failure, PRAXIS can use user preference as a training signal in cases where no objective standard exists. This would be accomplished by observing the user's inputs and feedback over time, steering the actions accordingly at each iteration, and converging to a procedural memory that encodes the user's preferences for how a task is done.\r\n\r\n<a id='06d06d4c-9376-4a16-af18-b69d38dff592'></a>\r\n\r\nReferences\r\n1. Altrina (formerly Tessa AI). 2025. Evolving Our State-of-the-Art Browsing Agent. https://www.altrina.com/blog/evolving-our-state-of-the-art-browsing-agent. Accessed: 2025-11-16.\r\n2. Altrina (formerly Tessa AI). 2025. Introducing Large Neurosymbolic Cognitive Models. https://www.altrina.com/blog/introducing-large-neurosymbolic-cognitive-models. Accessed: 2025-11-16.\r\n3. Gordon H. Bower. 1981. Mood and memory. American Psychologist 36, 2 (1981), 129-148. doi:10.1037/0003-066X.36.2.129 Place: US Publisher: American Psychological Association.\r\n4. Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. 2025. Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory. doi:10.48550/arXiv.2504.19413 arXiv:2504.19413 [cs].\r\n5. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. 2023. Mind2Web: Towards a Generalist Agent for the Web. doi:10.48550/arXiv.2306.06070 arXiv:2306.06070 [cs].\r\n6. Divyansh Garg, Shaun VanWeelden, Diego Caples, Andis Draguns, Nikil Ravi, Pranav Putta, Naman Garg, Tomas Abraham, Michael Lara, Federico Lopez, James Liu, Atharva Gundawar, Prannay Hebbar, Youngchul Joo, Jindong Gu, Charles London, Christian Schroeder de Witt, and Sumeet Motwani. 2025. REAL: Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites. doi:10.48550/arXiv.2504.11543 arXiv:2504.11543 [cs].\r\n7. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. 2024. WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models. doi:10.48550/arXiv.2401.13919 arXiv:2401.13919 [cs].\r\n8. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2021. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. doi:10.48550/arXiv.2005.11401 arXiv:2005.11401 [cs].\r\n9. Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. 2018. Reinforcement Learning on Web Interfaces Using Workflow-Guided Exploration. doi:10.48550/arXiv.1802.08802 arXiv:1802.08802 [cs].\r\n10. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-Refine: Iterative Refinement with Self-Feedback. doi:10.48550/arXiv.2303.17651 arXiv:2303.17651 [cs].\r\n11. Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Peter Jansen, Oyvind Tafjord, Niket Tandon, Li Zhang, Chris Callison-Burch, and Peter Clark. 2023. CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization. doi:10.48550/arXiv.2310.10134 arXiv:2310.10134 [cs].\r\n12. Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. 2024. MemGPT: Towards LLMs as Operating Systems. arXiv:2310.08560 [cs.AI] https://arxiv.org/abs/2310.08560\r\n13. Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language Agents with Verbal Reinforcement Learning. doi:10.48550/arXiv.2303.11366 arXiv:2303.11366 [cs].\r\n14. Endel Tulving and Donald M. Thomson. 1973. Encoding specificity and retrieval processes in episodic memory. Psychological Review 80, 5 (1973), 352-373. doi:10.1037/h0020071 Place: US Publisher: American Psychological Association.\r\n15. Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. 2024. Agent Workflow Memory. doi:10.48550/arXiv.2409.07429 arXiv:2409.07429 [cs].\r\n16. Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, and Yongfeng Zhang. 2025. A-MEM: Agentic Memory for LLM Agents. doi:10.48550/arXiv.2502.12110 arXiv:2502.12110 [cs].\r\n17. Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. 2023. ExpeL: LLM Agents Are Experiential Learners. doi:10.48550/arXiv.2308.10144 arXiv:2308.10144 [cs] version: 2.\r\n18. Longtao Zheng, Rundong Wang, Xinrun Wang, and Bo An. 2024. Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control. doi:10.48550/arXiv.2306.07863 arXiv:2306.07863 [cs].\r\n19. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. 2023. MemoryBank: Enhancing Large Language Models with Long-Term Memory. doi:10.48550/arXiv.2305.10250 arXiv:2305.10250 [cs].\r\n\r\n<!-- PAGE BREAK -->\r\n\r\n<a id='573ab191-add7-44ac-8e2d-f42208780e35'></a>\r\n\r\n8\r\n\r\n<a id='9f466aae-ab66-4bc2-8098-8eece6ce014f'></a>\r\n\r\nBi, Hu, and Nasir\r\n\r\n<a id='8303aca4-589a-47f4-a12f-ba3f6dc24674'></a>\r\n\r\n[20] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. 2024. WebArena: A Realistic Web Environment for Building Autonomous Agents. doi:10.48550/arXiv.2307.13854 arXiv:2307.13854 [cs] version: 4.\n</attachment>\n\n</attachments>\n<context>\nThe current date is January 4, 2026.\nTerminals:\nTerminal: powershell\nLast Command: uv sync\nCwd: D:\\GPT\\paper-reader\nExit Code: 0\nTerminal: powershell\nLast Command: & D:\\GPT\\paper-reader\\.venv\\Scripts\\Activate.ps1\nCwd: D:\\GPT\\paper-reader\nExit Code: 0\n\n</context>\n<editorContext>\nThe user's current file is d:\\GPT\\paper-reader\\self-evolving-agents\\24eb981f-8040-4f3c-97fe-0e982226d25f.md. The current selection is from line 3 to line 4.\n</editorContext>\n<reminderInstructions>\nWhen using the replace_string_in_file tool, include 3-5 lines of unchanged code before and after the string you want to replace, to make it unambiguous which part of the file should be edited.\nFor maximum efficiency, whenever you plan to perform multiple independent edit operations, invoke them simultaneously using multi_replace_string_in_file tool rather than sequentially. This will greatly improve user's cost and time efficiency leading to a better user experience. Do not announce which tool you're using (for example, avoid saying \"I'll implement all the changes using multi_replace_string_in_file\").\nDo NOT create a new markdown file to document each change or summarize your work unless specifically requested by the user.\n\n</reminderInstructions>\n<userRequest>\nwhats this paper is about\n</userRequest>\n"
            },
            {
              "type": 3,
              "cacheType": "ephemeral"
            }
          ],
          "renderedGlobalContext": [
            {
              "type": 1,
              "text": "<environment_info>\nThe user's current OS is: Windows\n</environment_info>\n<workspace_info>\nI am working in a workspace with the following folders:\n- d:\\GPT\\paper-reader \nI am working in a workspace that has the following structure:\n```\nagent.md\nparse_with_llamaparse.py\npyproject.toml\nothers/\n\t204c24c5-7411-4631-8ea6-305649dd156a.md\n\t204c24c5-conversation.md\n\t2511.14214v1.md\n\t2511.19757v1.md\n\t2512.03413v1.md\n\t2512.16922v1.md\n\t73f5769f-333c-4ba3-b751-533aeb157422.md\n\t96e34750-4a80-462f-953e-628a10a816c6.md\n\tde640784-9f94-420e-ab02-c7b8809d09e2.md\n\te0c663d0-cee2-44c3-a902-754aa3186279.md\n\tf47afba4-2787-453b-b3bb-d804f68dbd32.md\nscripts/\n\textract_pdfs.py\n\tpaper-analysis-skill.md\n\tpaper-reading.md\n\tparse_pdfs.py\n\tSKILL_LLAMAPARSE.md\n\tSKILL.md\nself-evolving-agents/\n\t24eb981f-8040-4f3c-97fe-0e982226d25f.md\n\t2508.16153v2.md\n\t2510.01051v1.md\n\t2511.20857v1.md\n\t2511.21689v1.md\n\t2512.04123v1.md\n\t2512.04868v1.md\n\t2512.05765v1.md\n\t2512.16301v1.md\n\t32960e66-927a-416e-85c8-12f089812d0c.md\n\t76083b1b-07d6-4099-9b17-e4da08599706.md\n\t77246551-482e-4312-99b4-786192a59e5e.md\n\ts10462-025-11422-4.md\n```\nThis is the state of the context at this point in the conversation. The view of the workspace structure may be truncated. You can use tools to collect more context if needed.\n</workspace_info>\n"
            },
            {
              "type": 3,
              "cacheType": "ephemeral"
            }
          ],
          "cacheKey": "file:///d%3A/GPT/paper-reader",
          "toolCallRounds": [
            {
              "response": "This paper introduces **PRAXIS** (Procedural Recall for Agents with eXperiences Indexed by State), a learning mechanism that enables AI agents to learn procedures from experience in real-time after deployment.\n\n**Core concept:** Instead of pre-programming procedures, PRAXIS stores past action outcomes and retrieves relevant experiences by matching both the current environment state (what the agent sees) and internal state (what it's trying to accomplish) to similar past situations.\n\n**Key contributions:**\n- **State-dependent memory system** that indexes experiences by environment + goal context\n- Tested on the Altrina web agent across the REAL benchmark (112 web browsing tasks)\n- Consistent improvements across multiple foundation models (Claude, GPT, Gemini, Llama, Qwen):\n  - **Accuracy**: 40.3% → 44.1% average\n  - **Reliability**: 74.5% → 79.0% (repeatability across runs)\n  - **Efficiency**: 25.2 → 20.2 steps to complete tasks\n\n**Why it matters:** Most existing memory systems for AI agents focus on factual memory (preferences, user info), but this addresses *procedural* learning—how to actually accomplish tasks in dynamic environments where procedures aren't fully documented and interfaces change frequently. The web is an ideal test environment since it requires multi-step procedures that adapt to changing interfaces.",
              "toolCalls": [],
              "toolInputRetry": 0,
              "id": "233f93f4-dc41-4497-a061-66d738fc1efa",
              "thinking": {
                "id": "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",
                "text": "The user is asking about what the paper is about. They have the file open and have selected the title \"Real-Time Procedural Learning From Experience for AI Agents\".\n\nLooking at the paper, I can provide a concise summary of what it's about:\n\nThis paper introduces PRAXIS (Procedural Recall for Agents with eXperiences Indexed by State), a learning mechanism that allows AI agents to learn procedures from experience in real-time. The key points are:\n\n1. The problem: Most LLM-based agents can't acquire procedural knowledge after deployment\n2. The solution: PRAXIS stores consequences of actions and retrieves them by matching environmental and internal states\n3. Testing: Evaluated on REAL web browsing benchmark\n4. Results: Improves task completion accuracy, reliability, and efficiency across different foundation models\n\nLet me provide a brief, direct answer as per the communication style guidelines."
              }
            }
          ],
          "modelMessageId": "090d5d05-86f4-47b4-99e6-c2b98fed375c",
          "responseId": "1c0d30a6-1be1-4444-a4e7-c85c548e0827",
          "sessionId": "2c2b7cde-ed4c-4a2f-98ef-e8806696ad5f",
          "agentId": "github.copilot.editsAgent"
        },
        "details": "Claude Sonnet 4.5 • 1x"
      },
      "responseMarkdownInfo": [],
      "followups": [],
      "modelState": {
        "value": 1,
        "completedAt": 1767509056631
      },
      "contentReferences": [],
      "codeCitations": [],
      "timeSpentWaiting": 0
    },
    {
      "requestId": "request_eea5899b-e248-4afb-9fc8-2e06dc8b451d",
      "message": {
        "parts": [
          {
            "range": {
              "start": 0,
              "endExclusive": 41
            },
            "editorRange": {
              "startLineNumber": 1,
              "startColumn": 1,
              "endLineNumber": 1,
              "endColumn": 42
            },
            "text": "show me an example of precedural learning",
            "kind": "text"
          }
        ],
        "text": "show me an example of precedural learning"
      },
      "variableData": {
        "variables": [
          {
            "kind": "file",
            "id": "vscode.implicit.selection",
            "name": "file:24eb981f-8040-4f3c-97fe-0e982226d25f.md",
            "value": {
              "uri": {
                "$mid": 1,
                "fsPath": "d:\\GPT\\paper-reader\\self-evolving-agents\\24eb981f-8040-4f3c-97fe-0e982226d25f.md",
                "_sep": 1,
                "external": "file:///d%3A/GPT/paper-reader/self-evolving-agents/24eb981f-8040-4f3c-97fe-0e982226d25f.md",
                "path": "/d:/GPT/paper-reader/self-evolving-agents/24eb981f-8040-4f3c-97fe-0e982226d25f.md",
                "scheme": "file"
              },
              "range": {
                "startLineNumber": 3,
                "startColumn": 1,
                "endLineNumber": 4,
                "endColumn": 1,
                "selectionStartLineNumber": 3,
                "selectionStartColumn": 1,
                "positionLineNumber": 4,
                "positionColumn": 1
              }
            },
            "modelDescription": "User's active selection"
          }
        ]
      },
      "response": [
        {
          "kind": "mcpServersStarting",
          "didStartServerIds": []
        },
        {
          "kind": "thinking",
          "value": "The user is asking for an example of procedural learning. Based on the paper, procedural learning is about learning sequences of state-dependent actions. Let me provide a concrete example that illustrates what procedural learning means in the context of this paper.\n\nI should provide a clear, concrete example that shows:\n1. A specific task/goal\n2. The sequence of state-dependent actions\n3. How this differs from factual learning\n\nI'll create a practical example related to the web browsing context mentioned in the paper.",
          "id": "1767509288607",
          "generatedTitle": "Provided example of procedural learning"
        },
        {
          "kind": "thinking",
          "value": "",
          "id": "",
          "metadata": {
            "vscodeReasoningDone": true,
            "stopReason": "other"
          }
        },
        {
          "kind": "thinking",
          "value": "",
          "id": "tgFJTbdKdRsuUA2SWk/9EMJ6PDD+/WOjN7ppCyj4JpM4j0pCqkNn/XKs9lUUKuEdGJNA84eh/Z4cZDBNWXW1JCpJ3cVxcfGiD9ae8h1UjNQMRAjsdR77goNwIDNvICGFauoOn6QkkWbbbRRND89+VRdZamX/Yn+fLJtopH6/sIH+ovlj7PBGM3dXy7uhvmIa1UKWCkoAr1osryjfohAhFPUsdT3BUEczhzsubeYw6KzAO+uTn2VGLF9UzWZeNj2dMf/LWvBZ37LbP9zf12fjEEg1MudOqrms0vO3Np0cSu3drs6jmhqvkGaqSAic50FH/chIfnl68nMJcXp1S2AKsrSSmnOHFO81roZS9FejAU9XdPlYjBwDQY5zcrqLhJdGKBjgMl+MfZsg7AC8ACLKI7sQblrF7ZaIv9GYq+1RRykLlvEwGnO0Qve9gvVSRgDXe78gUbeJRM+Jrum1q0uxCWRyKp+YkdbSQhy1kH08UDGhnRVq2lIY73ndwWWkXCNaOEmcQx/Wa02KCfYqQeINeVrthzcG9L77VaeaBABIAKw8QQEruUb46DzrAd2PdB8roR+3B4fnnP1TQSNPhcM5R0bqtiAUztN3sjxqeRl4feoFpCsHJZ1p+uaWRygaH8yhBOWifbYzA/AgveCxAcuzjqEYkBFYm8Co654cDGB0+DifaCNn6G90C3XXXG4B/BupMavpAwa4NDYlxdt0/YtZLkgQg0L2Gn0TsPNIcHVt07dvOEwo0CMCs0r+HUTBZzBzMKJugI4pklt3y4SiaOKck7V1EQnlQuqJOTiQ1EBV4VM9diqgAoeTDfoOOBnnIvCyt0ftZH+lU9A0zYX+u55PLCc2p1n15tL+8IpVJRlVVL1WRoHy/Vb0SdPBb/UcDU0PHBxlNCSDYRsrVzHJ7nSyVTSyndWQBhygaUFVNUnmWEUZwrdyfN0VtiAOAgOwyg7OAZkzRXRalGPdbRwnpZsJUvrqkF6wqnYH6FXL+PJe/lgmnPRIMatcc2602LvB8j7ZAOCB28eHJ4x1dvbwHsutxEabnD1CsoXmCse9Ax+Ojyf+tKGyulswa6gZcq5xn1YgLO/c3L33jsA9wevOvIAtX35+hAPLPaTH1l/4wYURNw7q5s7zrfnQclhghMioQDtldV6K01EE+uehD87Pv8oc4CZCvJzC2NuD6SzIj9trixhoKOZeIox9XO0FF2CeBOMSi+V06fhILgohVWFmhQ/CESKdxcE3aBIouyBJayF2c8v+o+5CR5c/3Vrjz7KI4bPZ8oxa0YEKZ1JQfD9ZkmMqF85gYiRAPZFxc1ybvmpMAkhMp2btkaC1cOqi9BCWKLnQuXosWaBsiMG1crEz60hJ3xgrGPW0P9pmtYb/XmhHRRPAODByHvaIxIdPrg8a224s/q8IygUTTsApPJkXHRNwG8SSaR5g1FlDFa0Lpu9deasLQsrGFYvth5MlXupOsKZ+3DiydjqfN1dxVUawG0gQNLeHrMP0yDM29o6eDmMH+z81bd+/6QBcTBWmQ6ifPysSJbll19M4RKZfVEZwol0m4DKXnU1Whm9WQ20ixRjzmrpqsbme92JE+Gw1xjEw4xQf+lA="
        },
        {
          "kind": "thinking",
          "value": "",
          "id": "",
          "metadata": {
            "vscodeReasoningDone": true,
            "stopReason": "text"
          }
        },
        {
          "value": "Here's a concrete example of procedural learning from the paper's context:\n\n## Example: Online Shopping Checkout\n\n**Factual Learning** (what most memory systems do):\n- Store: \"User's email is john@example.com\"\n- Store: \"User prefers express shipping\"\n- Store: \"Credit card ending in 1234\"\n\n**Procedural Learning** (what PRAXIS does):\nStore state-action sequences like:\n\n```\nState 1: On product page with \"Add to Cart\" button visible\n├─ Action: Click \"Add to Cart\"\n└─ Result: Cart icon shows (1) item\n\nState 2: Cart icon showing items, not yet at checkout\n├─ Action: Click cart icon\n└─ Result: Cart summary page appears\n\nState 3: Cart summary page with \"Proceed to Checkout\" button, popup appeared\n├─ Action: Close popup first (learned from past failure)\n└─ Result: Popup dismissed\n\nState 4: Cart summary page, no popups, \"Proceed to Checkout\" visible\n├─ Action: Click \"Proceed to Checkout\"  \n└─ Result: Login/guest checkout page appears\n\nState 5: Login form visible, user is not logged in\n├─ Action: Fill email field, enter password, click login\n└─ Result: Redirected to shipping address page\n\nState 6: Shipping form visible, all fields empty\n├─ Action: Fill address fields in order: name → street → city → zip\n└─ Result: Form validated, \"Continue\" button enabled\n```\n\nThe key difference: **Procedural learning captures the \"how to do X when Y\"** — knowing that you need to close popups before clicking checkout, or that fields must be filled in a specific order. It's context-dependent and learned from experience, not pre-programmed rules.\n\nIn the paper's PRAXIS system, when the agent encounters a similar state (e.g., cart page with popup), it retrieves the past experience and knows to close the popup first, improving success rate and efficiency.",
          "supportThemeIcons": false,
          "supportHtml": false,
          "baseUri": {
            "$mid": 1,
            "path": "/d:/GPT/paper-reader/",
            "scheme": "file"
          }
        }
      ],
      "agent": {
        "extensionId": {
          "value": "GitHub.copilot-chat",
          "_lower": "github.copilot-chat"
        },
        "extensionVersion": "0.36.2025121901",
        "publisherDisplayName": "GitHub",
        "extensionPublisherId": "GitHub",
        "extensionDisplayName": "GitHub Copilot Chat",
        "id": "github.copilot.editsAgent",
        "description": "Edit files in your workspace in agent mode",
        "when": "config.chat.agent.enabled",
        "metadata": {
          "themeIcon": {
            "id": "tools"
          },
          "hasFollowups": false,
          "supportIssueReporting": false
        },
        "name": "agent",
        "fullName": "GitHub Copilot",
        "isDefault": true,
        "locations": [
          "panel"
        ],
        "modes": [
          "agent"
        ],
        "slashCommands": [],
        "disambiguation": []
      },
      "timestamp": 1767509284420,
      "modelId": "copilot/claude-sonnet-4.5",
      "responseId": "response_26c9496c-410e-4e20-aa89-1285b4404fc5",
      "result": {
        "timings": {
          "firstProgress": 6319,
          "totalElapsed": 17544
        },
        "metadata": {
          "codeBlocks": [
            {
              "code": "State 1: On product page with \"Add to Cart\" button visible\n├─ Action: Click \"Add to Cart\"\n└─ Result: Cart icon shows (1) item\n\nState 2: Cart icon showing items, not yet at checkout\n├─ Action: Click cart icon\n└─ Result: Cart summary page appears\n\nState 3: Cart summary page with \"Proceed to Checkout\" button, popup appeared\n├─ Action: Close popup first (learned from past failure)\n└─ Result: Popup dismissed\n\nState 4: Cart summary page, no popups, \"Proceed to Checkout\" visible\n├─ Action: Click \"Proceed to Checkout\"  \n└─ Result: Login/guest checkout page appears\n\nState 5: Login form visible, user is not logged in\n├─ Action: Fill email field, enter password, click login\n└─ Result: Redirected to shipping address page\n\nState 6: Shipping form visible, all fields empty\n├─ Action: Fill address fields in order: name → street → city → zip\n└─ Result: Form validated, \"Continue\" button enabled\n",
              "language": "",
              "markdownBeforeBlock": "Here's a concrete example of procedural learning from the paper's context:\n\n## Example: Online Shopping Checkout\n\n**Factual Learning** (what most memory systems do):\n- Store: \"User's email is john@example.com\"\n- Store: \"User prefers express shipping\"\n- Store: \"Credit card ending in 1234\"\n\n**Procedural Learning** (what PRAXIS does):\nStore state-action sequences like:\n\n"
            }
          ],
          "renderedUserMessage": [
            {
              "type": 1,
              "text": "<attachments>\n<attachment id=\"file:24eb981f-8040-4f3c-97fe-0e982226d25f.md\">\nUser's active selection:\nExcerpt from 24eb981f-8040-4f3c-97fe-0e982226d25f.md, lines 3 to 4:\n```markdown\nReal-Time Procedural Learning From Experience for AI Agents\n```\n</attachment>\n<attachment filePath=\"d:\\\\GPT\\\\paper-reader\\\\self-evolving-agents\\\\24eb981f-8040-4f3c-97fe-0e982226d25f.md\">\nUser's active file for additional context:\n<a id='7cfdd488-96db-4787-833f-ff899aa365fa'></a>\r\n\r\nReal-Time Procedural Learning From Experience for AI Agents\r\n\r\n<a id='ae47ab06-18b7-493a-9825-3aed1294fcde'></a>\r\n\r\nDASHENG BI*, Altrina, USA\r\nYUBIN HU*, Altrina, USA\r\n\r\n<a id='b80a9c0e-202e-4086-b4a6-df7bdd748a59'></a>\r\n\r\nMOHAMMED N. NASIR, Altrina, USA\r\n\r\nLearning how to do things from trial and error in real time is a hallmark of biological intelligence, yet most LLM-based agents lack mechanisms to acquire procedural knowledge after deployment. We propose Procedural Recall for Agents with eXperiences Indexed by State (PRAXIS), a lightweight post-training learning mechanism that stores the consequences of actions and retrieves them by jointly matching environmental and internal states of past episodes to the current state. PRAXIS augments agentic action selection with retrieved state-action-result exemplars that are generated in real time. When evaluated on the REAL web browsing benchmark, PRAXIS improves task completion accuracy, reliability, and cost efficiency across different foundation model backbones, and shows preliminary generalization to unseen tasks in similar environments. These results demonstrate that PRAXIS enables the practical adoption of AI agents in fast-evolving stateful environments by helping them learn new procedures effectively.\r\n\r\n<a id='8a75b943-e4d8-44e7-860c-4633b2614509'></a>\r\n\r\n# 1 Introduction\r\n\r\n## 1.1 AI Agents and Procedural Learning\r\n\r\nAI agents are artificial intelligence systems capable of observing and taking actions in an environment. As adoption spreads across industries, there is a growing need for AI agents to quickly learn domain- or user-specific information. There are two main classes of information that agents are typically expected to learn:\r\n\r\n1.  **Facts**: atomic pieces of information independent of the state of the agent or environment (e.g., a user's name, preferences, organizational charts). Facts can change over time, but at any given moment they are generally context-independent.\r\n2.  **Procedures**: established conventions for doing things (e.g., \"how to troubleshoot a failed login\" or \"how to guide a customer through a sales process to the most fitting product\"). Procedures can be viewed as a sequence of state-dependent requirements or preferences over actions.\r\n\r\n<a id='cfb7dcc9-93a6-42a1-8180-f3872612ebae'></a>\r\n\r\nIn real-world applications, learning and optimizing procedures in real time are at least as important as learning facts. While frameworks such as Mem0 [4] and Letta [12] focus on long-term factual memory, effective post-training learning of procedures in Al agents remains relatively underexplored.\r\n\r\n<a id='0fe65bd2-19e0-4a72-a27b-8a9d38638b2f'></a>\r\n\r\nA naïve approach is _a priori_ procedural specification: a human writes rules or standard operating procedures (SOPs) that are included in the agent's context at inference time. This approach effectively reduces procedures to a large bundle of facts. In practice, this approach faces challenges because (1) many procedures are not fully documented, as humans are often trained by observation rather than by reading long SOPs; (2) enumerating all states and edge cases in a combinatorial space is difficult; and (3) procedures can become obsolete quickly as environments change. We argue that a more robust approach is to learn procedures _a posteriori_ from demonstrations or experience. Inspired by state-dependent memory in psychology [3, 14], we propose PRAXIS, a concrete method for procedural recall and show that it improves agent accuracy, reliability, and efficiency in web browsing settings. Our method is compatible with both experiences demonstrated by a human expert and actual trajectories generated by the AI agent itself.\r\n\r\n<a id='bdb81928-70c4-4f04-a471-9b916a25a7ba'></a>\r\n\r\n--- Both authors contributed equally to this research.\r\n\r\n<a id='80b8c531-989e-4b65-b4a8-d26b44917505'></a>\r\n\r\nAuthors' Contact Information: Dasheng Bi, dbi@altrina.com, Altrina, Menlo Park, California, USA; Yubin Hu, harvey@altrina.com, Altrina, Menlo Park, California, USA; Mohammed N. Nasir, mo@altrina.com, Altrina, Menlo Park, California, USA.\r\n\r\n<a id='dd67cdec-633c-47cf-9d96-1d462194058e'></a>\r\n\r\n1\r\n\r\n<a id='edd3c6e8-2d85-4550-93fb-62249a717cb8'></a>\r\n\r\narXiv:2511.22074v1 [cs.AI] 27 Nov 2025\r\n\r\n<!-- PAGE BREAK -->\r\n\r\n<a id='ac355a71-dd56-4b7a-aa9f-32d3a1559891'></a>\r\n\r\n2\r\n\r\n<a id='998850a5-219e-442a-9d64-0b8001567d84'></a>\r\n\r\nBi, Hu, and Nasir\r\n\r\n<a id='1a58dc58-fb9e-40d5-8144-7e850c37d326'></a>\r\n\r\n1.2 Web Agents and the Browser Environment\r\nHuman-facing web applications almost always require multi-step interactions to accomplish meaningful goals (e.g., purchasing an item online requires searching, filtering, logging in, completing forms, and checking out)—a procedure. Moreover, these procedures must also adapt to changing environments (e.g., an e-commerce site may have seasonal pop-ups or redesigned interfaces), making web browsing a natural environment to study *a posteriori* procedural learning. Importantly, even when tasks are obvious to humans, comprehensive procedures are rarely documented, and high personalization limits pretraining coverage in foundation models. As AI-based design tools increasingly generate and update web platforms, the economic value shifts to novel, previously unseen interfaces and interaction flows, pushing agents into out-of-distribution states and rendering *a priori* SOPs brittle. A post-training, state-indexed procedural memory thus becomes essential for robust web automation, allowing agents to acquire and reuse procedures precisely when new states appear.\r\n\r\n<a id='753cb0aa-75dc-4beb-8f96-637abdbf4939'></a>\r\n\r\n## 2 Related Work\r\n\r\nExternal Memory for AI Chatbots. A broad class of systems augment LLMs with non-parametric memory in the conversational environment. Retrieval-augmented generation (RAG) attaches a document store to provide factual knowledge at inference time [8]. In agentic settings, persistent memory frameworks such as Letta (formerly MemGPT) [12] provide hierarchical storage and dynamic context management. Mem0 [4] provides a queryable, cross-session memory for user preferences and long-range conversational context. Academic frameworks include MemoryBank [19] which mimics human long-term memory with continual decay and reinforcement, and A-MEM [16] which dynamically links and evolves structured notes. These approaches typically focus on factual memory for agents in conversational environments. In contrast, our method focuses on learning action policies in stateful visual environments that are significantly more complex and not entirely observable like the web environment.\r\n\r\n<a id='7083e7aa-15c2-43f2-914d-664c75c73e94'></a>\r\n\r\nExperience-Based Self-Improvement and Workflow Memories. A complementary line of work improves agents via self-reflection. Reflexion [13] maintains verbal reflections in an episodic buffer to guide subsequent trials; Self-Refine [10] iteratively critiques and edits its own outputs; and CLIN [11] performs continual task adaptation with a persistent textual memory of causal abstractions. These methods are effective but are not well-tested in visual environments and generally do not encode information of environmental state. There has also been a line of work on experience- or workflow-based memories for agents. Agent Workflow Memory [15], Synapse [18], and ExpeL [17] induce abstracted, natural language workflows from successful trajectories and retrieve them to augment prompts at test time. In contrast, our method performs local state-based recall that is grounded primarily in the live environment state, a factor not present in prior works, and secondarily to the goal. Moreover, we index memories with explicit state and action descriptors, rather than high-level trajectories, enabling precise recall and learning of minute details required for environments like the web.\r\n\r\n<a id='44666a96-8363-4a2d-b4c3-58b3216f0ace'></a>\r\n\r\n## 3 Methods\r\n### 3.1 Altrina Agent\r\nThe experiments in this study were conducted with Altrina¹, a frontier AI agent operating in computer use environments.\r\nAltrina is capable of perceiving the environment both visually and as compressed textual information, taking actions\r\n\r\n<a id='16ff8813-8e3f-45da-914f-117405bd2838'></a>\r\n\r\n---1\r\nThe agent system described in this work was previously known as Tessa in prior publications [1, 2].\r\n\r\n<!-- PAGE BREAK -->\r\n\r\n<a id='6f71a6a4-6dfb-4ceb-aa22-5a5ac03e5696'></a>\r\n\r\n3\r\n\r\n<a id='76ed5d51-4c88-4279-9ad5-7fba4dd686d6'></a>\r\n\r\nReal-Time Procedural Learning From Experience for AI Agents\r\n\r\n<a id='cec520e2-beff-4e00-afd8-6a3a28ea4776'></a>\r\n\r\nlike a human on a computer, and executing complex directives end-to-end. While Altrina's capabilities extend beyond acting in the browser, we restrict the action space to web-related actions for this study. In the browser environment, we previously showed that the baseline Altrina agent achieves state-of-the-art results on multiple benchmarks [1, 2], including WebVoyager [7] and REAL [6].\r\n\r\n<a id='2ab4222d-9af8-4f05-8cb5-2883c6e92d4f'></a>\r\n\r\nAltrina's scaffolding layer orchestrates underlying foundational models with a \"node-based\" architecture. Each node is designed for a specific function in the agentic loop. In this study, we modify the *action selection node*, which is responsible for deciding what the next agent action should be given the current state and progress towards the task objective, by adding a dedicated *procedural memory* section to its context.\r\n\r\n<a id='d84f2258-8c8e-4d6c-ba56-7a5245a5a7f2'></a>\r\n\r\n## 3.2 State-Dependent Memory\r\n\r\nMirroring state-dependent memory in psychology, which finds improved recall when internal state and external context at retrieval match those present during encoding [3, 14], we designed a state-dependent memory for Altrina in which indexing and retrieval are based on the browser's environment state and the agent's internal state.\r\n\r\nEach memory entry contains the following components:\r\n\r\n1.  $M_i^{env-pre}$, a description of the environmental state in which this memory is generated\r\n2.  $M_i^{int}$, the internal state of the agent at the time of the experience, including the overall directive the agent is trying to achieve\r\n3.  $a_i$, the action taken at the time\r\n4.  $M_i^{env-post}$, the state of the environment after the action was taken\r\n\r\nFormally, we use the following procedure (Alg. 1) to retrieve a set of memories that may be informative to the agent:\r\n\r\n<a id='95a5c279-f2b7-48b9-b348-8277ab45cfd2'></a>\r\n\r\nAlgorithm 1: Procedural Memory Retrieval\r\n\r\n**Input:** Memory environment states ${M_i^{env}}_{i=1}^n$, query environment state $Q^{env}$, memory internal states ${M_i^{int}}_{i=1}^n$, query internal state $Q^{int}$, internal state embedding function $f$, search breadth $k$, similarity threshold $\\tau$\r\n**Output:** Retrieved memory indices $\\mathcal{R}$\r\n\r\nfor $i \\leftarrow 1$ to $n$ do\r\n  $v_i \\leftarrow \\text{IoU}(M_i^{env}, Q^{env})$;\r\n  $\\ell_m \\leftarrow |M_i^{env}|$, $\\ell_q \\leftarrow |Q^{env}|$;\r\n  $\\ell_i \\leftarrow \\text{LENGTHOVERLAP}(\\ell_m, \\ell_q) = 1 - \\frac{|\\ell_m - \\ell_q|}{\\max(\\ell_m, \\ell_q)}$;\r\n  $s_i^{env} \\leftarrow v_i \\cdot \\ell_i$;\r\n  $s_i^{int} \\leftarrow \\langle f(M_i^{int}), f(Q^{int})\\rangle$;\r\n$\\mathcal{R}^{env} \\leftarrow \\text{TOPKINDICES}(s^{env}, k)$;\r\n$\\tilde{\\mathcal{R}} \\leftarrow \\text{SORTED}(\\mathcal{R}^{env}; \\text{key} = i \\mapsto s_i^{int}, \\text{descending})$;\r\n$\\mathcal{R} \\leftarrow [i \\in \\tilde{\\mathcal{R}} | s_i^{env} \\geq \\tau]$;\r\nreturn $\\mathcal{R}$;\r\n\r\n<a id='94b33bdf-a3b1-4703-907b-f3a9e933ffb5'></a>\r\n\r\n## 3.3 Benchmark\r\nA number of benchmarks exist to evaluate the performance of AI agents. MiniWoB/MiniWoB++ target short-horizon GUI primitives [9]; Mind2Web and WebVoyager target relatively straightforward workflows on live websites [5, 7]; WebArena hosts replicas of a few websites for local testing of relatively simple tasks with text-based evaluations [20].\r\n\r\n<!-- PAGE BREAK -->\r\n\r\n<a id='61d7866e-9d22-4c42-86f2-a23cf15e526d'></a>\r\n\r\n4\r\n\r\n<a id='188d9cc7-d611-4593-9bed-0860a32ce5a7'></a>\r\n\r\nBi, Hu, and Nasir\r\n\r\n<a id='f599d1a6-1fc6-4cdb-912e-cc79b32f13c0'></a>\r\n\r\nTo ensure reproducible evaluation on tasks of real-world relevance, we evaluate on deterministic clones of functional websites using the REAL benchmark [6]. REAL provides replicas of 11 commonly used sites and 112 everyday tasks of varying complexity, along with both programmatic state checks for action tasks and rubric-guided natural language evaluations for information retrieval tasks. Prior benchmarking reports that frontier models with naïve scaffolding achieve at most ~41% success, leaving substantial room for post-training procedural learning.\r\n\r\n<a id='4eba8c9a-6bd2-4e50-9617-1f351df88311'></a>\r\n\r\n## 4 Results\r\n\r\n### 4.1 Procedural memory improves agent accuracy\r\n\r\nWe benchmarked Altrina on REAL using several VLM backbones for each of the agentic compute nodes, either with or without access to procedural memory (Fig. 1). Procedural memory consistently improved the performance of Altrina (average performance across models increased from 40.3% to 44.1%, mean accuracy over five repetitions).\r\n\r\n<a id='fb8f583c-562b-4d6b-aab7-2938ea0e7975'></a>\r\n\r\n<::Bar chart showing performance on REAL benchmark for different models. The y-axis represents performance in percentage, ranging from 20% to 60%. The x-axis lists different models. For each model, two bars are displayed: the first (grey) represents performance without procedural memory, and the second (blue) represents performance with procedural memory. \r\nData:\r\n- Llama 4: Without procedural memory: 25.2%, With procedural memory: 28.9%\r\n- Qwen3-VL: Without procedural memory: 37.1%, With procedural memory: 40.7%\r\n- Gemini 2.5 Flash: Without procedural memory: 42.0%, With procedural memory: 48.6%\r\n- GPT-5: Without procedural memory: 45.9%, With procedural memory: 49.3%\r\n- Claude Sonnet 4.5: Without procedural memory: 51.2%, With procedural memory: 53.2%\r\nFig. 1. Performance on REAL benchmark for different models with and without procedural memory.\r\n: chart::>\r\n\r\n<a id='76004d55-0196-444a-b7a2-6ab6414e0503'></a>\r\n\r\nWe also measured the best-of-5 accuracy (Table 1), which reflects the agent's capability frontier under repeated attempts. Procedural memory also improved best-of-5 accuracy (average across models from 53.7% to 55.7%). Together, these results suggest that retrieved procedural memory traces provide reusable priors that improve agentic behavior on complex tasks, and that such memory traces may be generalized across similar tasks, supporting the completion of novel tasks.\r\n\r\n<a id='38aec8d1-bbc8-4d28-be8c-566aadbed139'></a>\r\n\r\nTable 1. Best-of-5 accuracy over tasks.\r\n\r\n<table id=\"3-1\">\r\n<tr><td id=\"3-2\">Model</td><td id=\"3-3\">No PM</td><td id=\"3-4\">With PM</td></tr>\r\n<tr><td id=\"3-5\">Llama 4</td><td id=\"3-6\">47.3</td><td id=\"3-7\">52.7</td></tr>\r\n<tr><td id=\"3-8\">Qwen3-VL</td><td id=\"3-9\">44.6</td><td id=\"3-a\">47.3</td></tr>\r\n<tr><td id=\"3-b\">Gemini 2.5 Flash</td><td id=\"3-c\">59.8</td><td id=\"3-d\">61.6</td></tr>\r\n<tr><td id=\"3-e\">GPT-5</td><td id=\"3-f\">56.2</td><td id=\"3-g\">57.1</td></tr>\r\n<tr><td id=\"3-h\">Claude Sonnet 4.5</td><td id=\"3-i\">60.7</td><td id=\"3-j\">59.8</td></tr>\r\n</table>\r\n\r\n<!-- PAGE BREAK -->\r\n\r\n<a id='852043d8-4ccd-4dd5-82f6-81b8d7e343a2'></a>\r\n\r\n5\r\n\r\n<a id='ab049c02-f0d4-4ce4-a4dd-f14818ca8b18'></a>\r\n\r\nReal-Time Procedural Learning From Experience for AI Agents\r\n\r\n<a id='59894777-d3e2-446b-b32e-766ee021cbb7'></a>\r\n\r\n## 4.2 Procedural memory improves agent reliability\r\nWe define reliability as the mean success rate over five repetitions of a task, averaged over all tasks in REAL having at least one successful run over the five repetitions. Procedural memory improved the reliability of Altrina from 74.5% to 79.0% averaged across all models (Fig. 2). These results suggest that retrieved procedural memory traces can suppress unwanted stochastic variance in the underlying vision-language models by biasing their decisions towards previously successful trajectories under similar states, thereby improving the reliability and repeatability of the high-level agentic behavior.\r\n\r\n<a id='ae2b0b0f-4035-41e2-b4c8-9c01ab4fe6a4'></a>\r\n\r\n<::bar chart: Agent reliability on REAL benchmark for different models with and without procedural memory. Y-axis represents percentage. X-axis represents models. Data:\r\n- Llama 4:\r\n  - Without procedural memory: 53.2%\r\n  - With procedural memory: 54.9%\r\n- Gemini 2.5 Flash:\r\n  - Without procedural memory: 70.1%\r\n  - With procedural memory: 78.8%\r\n- Qwen3-VL:\r\n  - Without procedural memory: 83.2%\r\n  - With procedural memory: 86.0%\r\n- GPT-5:\r\n  - Without procedural memory: 81.6%\r\n  - With procedural memory: 86.2%\r\n- Claude Sonnet 4.5:\r\n  - Without procedural memory: 84.4%\r\n  - With procedural memory: 89.0%\r\nFig. 2. Agent reliability on REAL benchmark for different models with and without procedural memory.::>\r\n\r\n<a id='645599da-9d8f-44bb-9ffa-2bbbb7635bc1'></a>\r\n\r\n## 4.3 Procedural memory improves agent efficiency\r\nWe measure _efficiency_ as the average number of steps taken on tasks in REAL with at least one successful run across five repetitions. Procedural memory reduces steps-to-completion on average from 25.2 to 20.2 across models (Table 2). This indicates that procedural memory recall effectively steers agentic behavior along both correct and more direct trajectories.\r\n\r\n<a id='b915943f-af69-44f5-921f-eec869817de4'></a>\r\n\r\nTable 2. Average number of steps to complete task.\r\n<table id=\"4-1\">\r\n<tr><td id=\"4-2\">Model</td><td id=\"4-3\">Base</td><td id=\"4-4\">With PM</td></tr>\r\n<tr><td id=\"4-5\">Llama 4</td><td id=\"4-6\">19.8</td><td id=\"4-7\">16.2</td></tr>\r\n<tr><td id=\"4-8\">Qwen3-VL</td><td id=\"4-9\">27.7</td><td id=\"4-a\">20.8</td></tr>\r\n<tr><td id=\"4-b\">Gemini 2.5 Flash</td><td id=\"4-c\">28.9</td><td id=\"4-d\">22.3</td></tr>\r\n<tr><td id=\"4-e\">GPT-5</td><td id=\"4-f\">24.2</td><td id=\"4-g\">20.7</td></tr>\r\n<tr><td id=\"4-h\">Claude Sonnet 4.5</td><td id=\"4-i\">25.2</td><td id=\"4-j\">21.0</td></tr>\r\n</table>\r\n\r\n<a id='8553a6d2-780f-4e39-8379-ea983a865181'></a>\r\n\r\n## 4.4 Procedural memory performance scales with retrieval breadth\r\nWe ablated the retrieval breadth hyperparameter k in Alg. 1 to study its effect on the performance of procedural memory (Fig. 3). Due to resource limitations, we only tested the effects of this ablation on the agent with a Gemini 2.5 Flash\r\n\r\n<!-- PAGE BREAK -->\r\n\r\n<a id='a2cfd8b8-b35f-49ba-a768-c75b037f3148'></a>\r\n\r\n6\r\n\r\n<a id='57bbc222-0543-4b54-b585-5279923f1382'></a>\r\n\r\nBi, Hu, and Nasir\r\n\r\n<a id='c20832ab-37d1-4cfc-9c88-a014cf585867'></a>\r\n\r\nbackbone. We found that performance generally increased in steps as we broadened our retrieval breadth, with slight decreases within each step, and converged to a plateau, suggesting that while there exists some local context crowding, procedural memory offers increasingly helpful and generalizable context to the language model at a larger scale.\r\n\r\n<a id='f0d02cdf-ca9a-4ea4-8649-9e9df56ba3f9'></a>\r\n\r\n<::line chart\r\n: Y-axis: Percentage from 40% to 52%\r\n: X-axis: Recall Breadth from 0 to 10\r\n: Data points:\r\n: (0, 42%)\r\n: (1, 44.2%)\r\n: (2, 45.2%)\r\n: (3, 49.5%)\r\n: (4, 48.2%)\r\n: (5, 48.7%)\r\n: (6, 47%)\r\n: (7, 51.5%)\r\n: (8, 51%)\r\n: (9, 49%)\r\n: (10, 49.7%)\r\n: Fig. 3. Performance as a function of retrieval breadth.\r\n::>\r\n\r\n<a id='a7ce9e77-ddb7-4349-8abe-5f1435c35c49'></a>\r\n\r\n## 5 Discussion\r\n\r\n_Personalized learning as a critical component of AI agents in the economy._ We envision a future where, instead of replacing humans, AI agents work alongside them. To encompass the diverse activities across our economy and maintain high collaboration efficiency, users will need to customize their AI agent with their own data and procedures. Even if it becomes possible to quickly and efficiently train everything into a single, universal model, we may not wish to, as each user should be able to decide whether to share their private knowledge with the world. In this context, personalized learning methods like PRAXIS that customize agents, not only on a superficial level, but also in terms of real capabilities, will be critical to the adoption of agents in the economy.\r\n\r\n<a id='185e9d49-b43b-4217-84e4-43b52422e0f1'></a>\r\n\r\nSummary of contributions. This paper introduces _state-dependent memory_, an _a posteriori_ learning mechanism that stores local interaction traces and retrieves them by jointly matching the _environment state_ and the agent's _internal objective_. When tested as an integration into the Altrina web agent, our method yields consistent improvements on the REAL web browsing benchmark across diverse VLM backbones: higher average accuracy, higher best-of-5 accuracy, better reliability, and fewer steps to completion. An ablation shows increased gains with retrieval breadth _k_. Together, our results indicate that state-dependent memory provides reusable local state-to-action priors that guide AI agents towards robust, generalizable behavior.\r\n\r\n<a id='93ae8e71-9e07-4796-ac28-c500929be7bf'></a>\r\n\r\nFuture directions. Beyond web environments. State-dependent memory is conceptually agnostic to the environment, and the same idea can be naturally extended to general cases of agentic computer use. Richer state encoding. Our proof-of-concept implementation of state-dependent memory uses basic visual and DOM feature overlap along with simple similarity metrics. A richer encoder can improve both retrieval quality and invariance to superficial changes.\r\n\r\n<!-- PAGE BREAK -->\r\n\r\n<a id='d7bd7f54-da1d-450d-a900-bea4fda16c0a'></a>\r\n\r\n7\r\n\r\n<a id='fed1062e-6e15-4ba6-baec-f7bd9318b82c'></a>\r\n\r\nReal-Time Procedural Learning From Experience for AI Agents\r\n\r\n<a id='62d3b3e5-2b3e-4e2d-a559-da5ec178bb56'></a>\r\n\r\nAdaptive retrieval mechanisms. Rather than a fixed retrieval based on state similarity heuristics, the retrieval mechanism can account for real-time factors such as uncertainty and compute budget. For highly uncertain scenarios, retrieval can also be iterative to improve quality. *From action agents to alignment agents* While the training signal in this paper is objective task success or failure, PRAXIS can use user preference as a training signal in cases where no objective standard exists. This would be accomplished by observing the user's inputs and feedback over time, steering the actions accordingly at each iteration, and converging to a procedural memory that encodes the user's preferences for how a task is done.\r\n\r\n<a id='06d06d4c-9376-4a16-af18-b69d38dff592'></a>\r\n\r\nReferences\r\n1. Altrina (formerly Tessa AI). 2025. Evolving Our State-of-the-Art Browsing Agent. https://www.altrina.com/blog/evolving-our-state-of-the-art-browsing-agent. Accessed: 2025-11-16.\r\n2. Altrina (formerly Tessa AI). 2025. Introducing Large Neurosymbolic Cognitive Models. https://www.altrina.com/blog/introducing-large-neurosymbolic-cognitive-models. Accessed: 2025-11-16.\r\n3. Gordon H. Bower. 1981. Mood and memory. American Psychologist 36, 2 (1981), 129-148. doi:10.1037/0003-066X.36.2.129 Place: US Publisher: American Psychological Association.\r\n4. Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. 2025. Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory. doi:10.48550/arXiv.2504.19413 arXiv:2504.19413 [cs].\r\n5. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. 2023. Mind2Web: Towards a Generalist Agent for the Web. doi:10.48550/arXiv.2306.06070 arXiv:2306.06070 [cs].\r\n6. Divyansh Garg, Shaun VanWeelden, Diego Caples, Andis Draguns, Nikil Ravi, Pranav Putta, Naman Garg, Tomas Abraham, Michael Lara, Federico Lopez, James Liu, Atharva Gundawar, Prannay Hebbar, Youngchul Joo, Jindong Gu, Charles London, Christian Schroeder de Witt, and Sumeet Motwani. 2025. REAL: Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites. doi:10.48550/arXiv.2504.11543 arXiv:2504.11543 [cs].\r\n7. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. 2024. WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models. doi:10.48550/arXiv.2401.13919 arXiv:2401.13919 [cs].\r\n8. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2021. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. doi:10.48550/arXiv.2005.11401 arXiv:2005.11401 [cs].\r\n9. Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. 2018. Reinforcement Learning on Web Interfaces Using Workflow-Guided Exploration. doi:10.48550/arXiv.1802.08802 arXiv:1802.08802 [cs].\r\n10. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-Refine: Iterative Refinement with Self-Feedback. doi:10.48550/arXiv.2303.17651 arXiv:2303.17651 [cs].\r\n11. Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Peter Jansen, Oyvind Tafjord, Niket Tandon, Li Zhang, Chris Callison-Burch, and Peter Clark. 2023. CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization. doi:10.48550/arXiv.2310.10134 arXiv:2310.10134 [cs].\r\n12. Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. 2024. MemGPT: Towards LLMs as Operating Systems. arXiv:2310.08560 [cs.AI] https://arxiv.org/abs/2310.08560\r\n13. Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language Agents with Verbal Reinforcement Learning. doi:10.48550/arXiv.2303.11366 arXiv:2303.11366 [cs].\r\n14. Endel Tulving and Donald M. Thomson. 1973. Encoding specificity and retrieval processes in episodic memory. Psychological Review 80, 5 (1973), 352-373. doi:10.1037/h0020071 Place: US Publisher: American Psychological Association.\r\n15. Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. 2024. Agent Workflow Memory. doi:10.48550/arXiv.2409.07429 arXiv:2409.07429 [cs].\r\n16. Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, and Yongfeng Zhang. 2025. A-MEM: Agentic Memory for LLM Agents. doi:10.48550/arXiv.2502.12110 arXiv:2502.12110 [cs].\r\n17. Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. 2023. ExpeL: LLM Agents Are Experiential Learners. doi:10.48550/arXiv.2308.10144 arXiv:2308.10144 [cs] version: 2.\r\n18. Longtao Zheng, Rundong Wang, Xinrun Wang, and Bo An. 2024. Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control. doi:10.48550/arXiv.2306.07863 arXiv:2306.07863 [cs].\r\n19. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. 2023. MemoryBank: Enhancing Large Language Models with Long-Term Memory. doi:10.48550/arXiv.2305.10250 arXiv:2305.10250 [cs].\r\n\r\n<!-- PAGE BREAK -->\r\n\r\n<a id='573ab191-add7-44ac-8e2d-f42208780e35'></a>\r\n\r\n8\r\n\r\n<a id='9f466aae-ab66-4bc2-8098-8eece6ce014f'></a>\r\n\r\nBi, Hu, and Nasir\r\n\r\n<a id='8303aca4-589a-47f4-a12f-ba3f6dc24674'></a>\r\n\r\n[20] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. 2024. WebArena: A Realistic Web Environment for Building Autonomous Agents. doi:10.48550/arXiv.2307.13854 arXiv:2307.13854 [cs] version: 4.\n</attachment>\n\n</attachments>\n<context>\nThe current date is January 4, 2026.\nTerminals:\nTerminal: powershell\nLast Command: uv sync\nCwd: D:\\GPT\\paper-reader\nExit Code: 0\nTerminal: powershell\nLast Command: & D:\\GPT\\paper-reader\\.venv\\Scripts\\Activate.ps1\nCwd: D:\\GPT\\paper-reader\nExit Code: 0\n\n</context>\n<editorContext>\nThe user's current file is d:\\GPT\\paper-reader\\self-evolving-agents\\24eb981f-8040-4f3c-97fe-0e982226d25f.md. The current selection is from line 3 to line 4.\n</editorContext>\n<reminderInstructions>\nWhen using the replace_string_in_file tool, include 3-5 lines of unchanged code before and after the string you want to replace, to make it unambiguous which part of the file should be edited.\nFor maximum efficiency, whenever you plan to perform multiple independent edit operations, invoke them simultaneously using multi_replace_string_in_file tool rather than sequentially. This will greatly improve user's cost and time efficiency leading to a better user experience. Do not announce which tool you're using (for example, avoid saying \"I'll implement all the changes using multi_replace_string_in_file\").\nDo NOT create a new markdown file to document each change or summarize your work unless specifically requested by the user.\n\n</reminderInstructions>\n<userRequest>\nshow me an example of precedural learning\n</userRequest>\n"
            },
            {
              "type": 3,
              "cacheType": "ephemeral"
            }
          ],
          "toolCallRounds": [
            {
              "response": "Here's a concrete example of procedural learning from the paper's context:\n\n## Example: Online Shopping Checkout\n\n**Factual Learning** (what most memory systems do):\n- Store: \"User's email is john@example.com\"\n- Store: \"User prefers express shipping\"\n- Store: \"Credit card ending in 1234\"\n\n**Procedural Learning** (what PRAXIS does):\nStore state-action sequences like:\n\n```\nState 1: On product page with \"Add to Cart\" button visible\n├─ Action: Click \"Add to Cart\"\n└─ Result: Cart icon shows (1) item\n\nState 2: Cart icon showing items, not yet at checkout\n├─ Action: Click cart icon\n└─ Result: Cart summary page appears\n\nState 3: Cart summary page with \"Proceed to Checkout\" button, popup appeared\n├─ Action: Close popup first (learned from past failure)\n└─ Result: Popup dismissed\n\nState 4: Cart summary page, no popups, \"Proceed to Checkout\" visible\n├─ Action: Click \"Proceed to Checkout\"  \n└─ Result: Login/guest checkout page appears\n\nState 5: Login form visible, user is not logged in\n├─ Action: Fill email field, enter password, click login\n└─ Result: Redirected to shipping address page\n\nState 6: Shipping form visible, all fields empty\n├─ Action: Fill address fields in order: name → street → city → zip\n└─ Result: Form validated, \"Continue\" button enabled\n```\n\nThe key difference: **Procedural learning captures the \"how to do X when Y\"** — knowing that you need to close popups before clicking checkout, or that fields must be filled in a specific order. It's context-dependent and learned from experience, not pre-programmed rules.\n\nIn the paper's PRAXIS system, when the agent encounters a similar state (e.g., cart page with popup), it retrieves the past experience and knows to close the popup first, improving success rate and efficiency.",
              "toolCalls": [],
              "toolInputRetry": 0,
              "id": "be1a738a-0c27-4e41-86c9-ae0bcc5067a8",
              "thinking": {
                "id": "tgFJTbdKdRsuUA2SWk/9EMJ6PDD+/WOjN7ppCyj4JpM4j0pCqkNn/XKs9lUUKuEdGJNA84eh/Z4cZDBNWXW1JCpJ3cVxcfGiD9ae8h1UjNQMRAjsdR77goNwIDNvICGFauoOn6QkkWbbbRRND89+VRdZamX/Yn+fLJtopH6/sIH+ovlj7PBGM3dXy7uhvmIa1UKWCkoAr1osryjfohAhFPUsdT3BUEczhzsubeYw6KzAO+uTn2VGLF9UzWZeNj2dMf/LWvBZ37LbP9zf12fjEEg1MudOqrms0vO3Np0cSu3drs6jmhqvkGaqSAic50FH/chIfnl68nMJcXp1S2AKsrSSmnOHFO81roZS9FejAU9XdPlYjBwDQY5zcrqLhJdGKBjgMl+MfZsg7AC8ACLKI7sQblrF7ZaIv9GYq+1RRykLlvEwGnO0Qve9gvVSRgDXe78gUbeJRM+Jrum1q0uxCWRyKp+YkdbSQhy1kH08UDGhnRVq2lIY73ndwWWkXCNaOEmcQx/Wa02KCfYqQeINeVrthzcG9L77VaeaBABIAKw8QQEruUb46DzrAd2PdB8roR+3B4fnnP1TQSNPhcM5R0bqtiAUztN3sjxqeRl4feoFpCsHJZ1p+uaWRygaH8yhBOWifbYzA/AgveCxAcuzjqEYkBFYm8Co654cDGB0+DifaCNn6G90C3XXXG4B/BupMavpAwa4NDYlxdt0/YtZLkgQg0L2Gn0TsPNIcHVt07dvOEwo0CMCs0r+HUTBZzBzMKJugI4pklt3y4SiaOKck7V1EQnlQuqJOTiQ1EBV4VM9diqgAoeTDfoOOBnnIvCyt0ftZH+lU9A0zYX+u55PLCc2p1n15tL+8IpVJRlVVL1WRoHy/Vb0SdPBb/UcDU0PHBxlNCSDYRsrVzHJ7nSyVTSyndWQBhygaUFVNUnmWEUZwrdyfN0VtiAOAgOwyg7OAZkzRXRalGPdbRwnpZsJUvrqkF6wqnYH6FXL+PJe/lgmnPRIMatcc2602LvB8j7ZAOCB28eHJ4x1dvbwHsutxEabnD1CsoXmCse9Ax+Ojyf+tKGyulswa6gZcq5xn1YgLO/c3L33jsA9wevOvIAtX35+hAPLPaTH1l/4wYURNw7q5s7zrfnQclhghMioQDtldV6K01EE+uehD87Pv8oc4CZCvJzC2NuD6SzIj9trixhoKOZeIox9XO0FF2CeBOMSi+V06fhILgohVWFmhQ/CESKdxcE3aBIouyBJayF2c8v+o+5CR5c/3Vrjz7KI4bPZ8oxa0YEKZ1JQfD9ZkmMqF85gYiRAPZFxc1ybvmpMAkhMp2btkaC1cOqi9BCWKLnQuXosWaBsiMG1crEz60hJ3xgrGPW0P9pmtYb/XmhHRRPAODByHvaIxIdPrg8a224s/q8IygUTTsApPJkXHRNwG8SSaR5g1FlDFa0Lpu9deasLQsrGFYvth5MlXupOsKZ+3DiydjqfN1dxVUawG0gQNLeHrMP0yDM29o6eDmMH+z81bd+/6QBcTBWmQ6ifPysSJbll19M4RKZfVEZwol0m4DKXnU1Whm9WQ20ixRjzmrpqsbme92JE+Gw1xjEw4xQf+lA=",
                "text": "The user is asking for an example of procedural learning. Based on the paper, procedural learning is about learning sequences of state-dependent actions. Let me provide a concrete example that illustrates what procedural learning means in the context of this paper.\n\nI should provide a clear, concrete example that shows:\n1. A specific task/goal\n2. The sequence of state-dependent actions\n3. How this differs from factual learning\n\nI'll create a practical example related to the web browsing context mentioned in the paper."
              }
            }
          ],
          "modelMessageId": "3f3766c6-747a-48f5-b27a-d833111817d4",
          "responseId": "ce7089a8-bf97-48c9-92f5-dc6889e1878b",
          "sessionId": "2c2b7cde-ed4c-4a2f-98ef-e8806696ad5f",
          "agentId": "github.copilot.editsAgent"
        },
        "details": "Claude Sonnet 4.5 • 1x"
      },
      "responseMarkdownInfo": [
        {
          "suggestionId": "sgt-5bad76fd-6870-43f3-a0ee-bbc74cc8451a"
        }
      ],
      "followups": [],
      "modelState": {
        "value": 1,
        "completedAt": 1767509301976
      },
      "contentReferences": [],
      "codeCitations": [],
      "timeSpentWaiting": 0
    }
  ]
}