{
  "responderUsername": "GitHub Copilot",
  "responderAvatarIconUri": {
    "id": "copilot"
  },
  "initialLocation": "panel",
  "requests": [
    {
      "requestId": "request_343a01af-5b28-40ec-ac6a-f7935d9dcccd",
      "message": {
        "parts": [
          {
            "range": {
              "start": 0,
              "endExclusive": 25
            },
            "editorRange": {
              "startLineNumber": 1,
              "startColumn": 1,
              "endLineNumber": 1,
              "endColumn": 26
            },
            "text": "whats this paper is about",
            "kind": "text"
          }
        ],
        "text": "whats this paper is about"
      },
      "variableData": {
        "variables": [
          {
            "kind": "file",
            "value": {
              "$mid": 1,
              "fsPath": "d:\\GPT\\paper-reader\\self-evolving-agents\\2512.04123v1.md",
              "_sep": 1,
              "external": "file:///d%3A/GPT/paper-reader/self-evolving-agents/2512.04123v1.md",
              "path": "/d:/GPT/paper-reader/self-evolving-agents/2512.04123v1.md",
              "scheme": "file"
            },
            "id": "file:///d%3A/GPT/paper-reader/self-evolving-agents/2512.04123v1.md",
            "name": "2512.04123v1.md"
          }
        ]
      },
      "response": [
        {
          "kind": "mcpServersStarting",
          "didStartServerIds": []
        },
        {
          "kind": "thinking",
          "value": "The user is asking me to explain what this paper is about. I have access to the paper file \"2512.04123v1.md\" which appears to be titled \"Measuring Agents in Production\". \n\nLooking at the content provided in the attachment, I can see:\n\n1. The paper is titled \"Measuring Agents in Production\"\n2. It's authored by researchers from UC Berkeley, UIUC, Stanford, IBM Research, and Intesa Sanpaolo\n3. The abstract indicates this is \"the first large-scale systematic study of AI agents in production\"\n\nLet me read through the key sections to understand what the paper is about:\n\n**Main Topic**: The paper studies how AI agents are built, evaluated, and deployed in production environments across various industries.\n\n**Research Questions**:\n- RQ1: What are the applications, users, and requirements of agents?\n- RQ2: What models, architectures, and techniques are used to build deployed agents?\n- RQ3: How are agents evaluated for deployment?\n- RQ4: What are the top challenges in building deployed agents?\n\n**Methodology**: They conducted:\n- A survey with 306 responses (filtered to 86 deployed agents for main analysis)\n- 20 in-depth case study interviews\n- Coverage of 26 different domains\n\n**Key Findings**:\n1. Production agents use simple, controllable approaches\n2. 68% execute at most 10 steps before human intervention\n3. 70% rely on prompting off-the-shelf models instead of fine-tuning\n4. 74% depend primarily on human evaluation\n5. Reliability is the top development challenge\n6. Agents are primarily used to increase productivity and automate routine tasks\n7. Most agents serve human users (92.5%)\n8. Teams favor established methods over experimental techniques\n\nThis is a comprehensive empirical study documenting the current state of AI agents in production environments, bridging the gap between research and real-world deployment.\n\nI should provide a clear, concise summary for the user.",
          "id": "1767509676678",
          "generatedTitle": "Read paper and summarized key sections"
        },
        {
          "kind": "thinking",
          "value": "",
          "id": "",
          "metadata": {
            "vscodeReasoningDone": true,
            "stopReason": "other"
          }
        },
        {
          "kind": "thinking",
          "value": "",
          "id": "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"
        },
        {
          "kind": "thinking",
          "value": "",
          "id": "",
          "metadata": {
            "vscodeReasoningDone": true,
            "stopReason": "text"
          }
        },
        {
          "value": "This paper, **\"Measuring Agents in Production\"**, is the first large-scale systematic study of how AI agents are actually built and deployed in real-world production environments.\n\n## Core Focus\n\nThe researchers surveyed 306 practitioners and conducted 20 in-depth interviews to understand:\n- Why organizations build agents\n- How they build them technically  \n- How they evaluate them\n- What challenges they face\n\n## Key Findings\n\n**1. Simple & Controlled Approaches Dominate:**\n- 68% of agents execute ≤10 steps before requiring human intervention\n- 70% use off-the-shelf models with prompting (no fine-tuning)\n- 74% rely primarily on human evaluation\n- Teams deliberately constrain autonomy to maintain reliability\n\n**2. Productivity Drives Adoption:**\n- 73% deploy agents to increase efficiency and reduce manual task time\n- Most applications span 26 diverse domains (finance, healthcare, legal, etc.) - far beyond just coding\n- 92.5% serve human users directly\n\n**3. Reliability is the Main Challenge:**\n- Ensuring correctness is the top concern\n- Teams favor human-in-the-loop evaluation over automated methods\n- Production agents use structured workflows rather than open-ended autonomy\n\n**4. Technical Patterns:**\n- Proprietary frontier models (Claude, GPT) dominate\n- Manual prompt engineering preferred over automated optimization\n- Most teams build custom implementations rather than using frameworks (85% in interviews)\n\n## Bottom Line\n\nThe paper reveals that **successful production agents today are deliberately simple, human-supervised systems** rather than fully autonomous ones. Teams trade capability for reliability, and this pragmatic approach already delivers measurable value across diverse industries.",
          "supportThemeIcons": false,
          "supportHtml": false,
          "baseUri": {
            "$mid": 1,
            "path": "/d:/GPT/paper-reader/",
            "scheme": "file"
          }
        }
      ],
      "agent": {
        "extensionId": {
          "value": "GitHub.copilot-chat",
          "_lower": "github.copilot-chat"
        },
        "extensionVersion": "0.36.2025121901",
        "publisherDisplayName": "GitHub",
        "extensionPublisherId": "GitHub",
        "extensionDisplayName": "GitHub Copilot Chat",
        "id": "github.copilot.editsAgent",
        "description": "Edit files in your workspace in agent mode",
        "when": "config.chat.agent.enabled",
        "metadata": {
          "themeIcon": {
            "id": "tools"
          },
          "hasFollowups": false,
          "supportIssueReporting": false
        },
        "name": "agent",
        "fullName": "GitHub Copilot",
        "isDefault": true,
        "locations": [
          "panel"
        ],
        "modes": [
          "agent"
        ],
        "slashCommands": [],
        "disambiguation": []
      },
      "timestamp": 1767509672906,
      "modelId": "copilot/claude-sonnet-4.5",
      "responseId": "response_f299bcce-e9c6-4fed-b2ff-89db88a1f089",
      "result": {
        "timings": {
          "firstProgress": 13000,
          "totalElapsed": 21465
        },
        "metadata": {
          "codeBlocks": [],
          "renderedUserMessage": [
            {
              "type": 1,
              "text": "<attachments>\n<attachment id=\"2512.04123v1.md\" isSummarized=\"true\" filePath=\"d:\\\\GPT\\\\paper-reader\\\\self-evolving-agents\\\\2512.04123v1.md\">\n<a id='48adc707-6e99-4f91-86ec-c5bb06ddc827'></a>\r\n\r\nMeasuring Agents in Production\r\n\r\n<a id='eb2cd812-5347-4896-b274-f1fa22b1b26c'></a>\r\n\r\nMelissa Z. Pan 1* Negar Arabzadeh 1* Riccardo Cogo 2 Yuxuan Zhu 3 Alexander Xiong 1 Lakshya A Agrawal 1\r\nHuanzhi Mao 1 Emma Shen 1 Sid Pallerla 1 Liana Patel 4 Shu Liu 1 Tianneng Shi 1 Xiaoyuan Liu 1\r\nJared Quincy Davis 4 Emmanuele Lacavalla 2 Alessandro Basile 2 Shuyi Yang 2 Paul Castro 5 Daniel Kang 3\r\nJoseph E. Gonzalez 1 Koushik Sen 1 Dawn Song 1 Ion Stoica 1 Matei Zaharia 1* Marquita Ellis 5 *\r\n\r\n1 UC Berkeley\r\n2 Intesa Sanpaolo\r\n3 UIUC\r\n4 Stanford University\r\n5 IBM Research\r\n\r\n<a id='babcd455-5622-4d4c-9799-926b143008d8'></a>\r\n\r\n# Abstract\r\nAI agents are already operating in production across many industries, yet there is limited public understanding of the technical strategies that make deployments successful. We present the first large-scale systematic study of AI agents in production, surveying 306 practitioners and conducting 20 in-depth case studies via interviews across 26 domains. We investigate why organizations build agents, how they build them, how they evaluate them, and what the top development challenges are. We find that production agents are typically built using simple, controllable approaches: 68% execute at most 10 steps before requiring human intervention, 70% rely on prompting off-the-shelf models instead of weight tuning, and 74% depend primarily on human evaluation. Reliability remains the top development challenge, driven by difficulties in ensuring and evaluating agent correctness. Despite these challenges, simple yet effective methods already enable agents to deliver impact across diverse industries. Our study documents the current state of practice and bridges the gap between research and deployment by providing researchers visibility into production challenges while offering practitioners patterns emerging from successful deployments.\r\n\r\n<a id='7ac25a55-b4b5-4868-ab1c-2c408fe696d2'></a>\r\n\r\n# 1. Introduction\r\nLarge language models (LLMs) have enabled a new class of software systems: **AI Agents**. We define AI agents as systems that combine foundation models with optional tools, memory, and reasoning capabilities to autonomously execute multi-step tasks [1-5]. Recent research has demonstrated exciting potential for AI agents across domains from drug discovery [6-8], algorithmic innovation [9-11] to gen-\r\n\r\n<a id='3084bde6-3271-4246-83e3-c4b79bac3678'></a>\r\n\r\n*Project Co-Leads.\r\nCorrespondence to: Melissa Z. Pan, Negar Arabzadeh, Marquita\r\nEllis <{melissapan, negara,mme} @berkeley.edu>.\r\n\r\n<a id='86e08693-da48-4bba-ab62-9993f58a9013'></a>\r\n\r\n<::horizontal bar chart::> The chart displays the percentage of responses for various categories, ordered from highest to lowest. The x-axis is labeled \"% of Responses\" and ranges from 0% to 100%. The y-axis lists the categories. Each bar also shows the exact percentage and the count in parentheses. The categories and their corresponding values are:\r\n- Increasing Productivity: 72.7% (48)\r\n- Reducing Human Task-Hours: 63.6% (42)\r\n- Automating Routine Labor: 50.0% (33)\r\n- Increasing Client Satisfaction: 37.9% (25)\r\n- Reducing Human Training: 34.8% (23)\r\n- Novel Technology: 33.3% (22)\r\n- Reducing Interdisciplinary Expertise: 18.2% (12)\r\n- Faster Failure Response Time: 18.2% (12)\r\n- Risk Mitigation: 12.1% (8)\r\n<::chart::>\r\n\r\n<a id='ee2e9491-2b45-4703-b976-61d3087d42bf'></a>\r\n\r\nFigure 1. Reasons practitioners build and deploy AI agents (N=66): increasing speed of task completion over the previous non-agentic (human or non-LLM automation) system (Increasing Productivity) is most often selected (73%), while improving operational stability (mitigating risk and accelerating failure recovery) is least often selected. Exact question and option descriptions are given in Appendix E.3 (N7). N=66 reflects the filtered subset of survey participants working on deployed agents who answered this question. Error bars indicate 95th-percentile intervals estimated from 1,000 bootstrap samples with replacements. The question was multi-select so participants were asked to select all/any benefits that applied to them and thus the proportions do not sum to 1.\r\n\r\n<a id='0c1d3d3c-c6dc-44a0-8c06-29ad563296fa'></a>\r\n\r\neral AI scientist [12–14]. This excitement extends beyond research prototypes to production systems. Companies report agents actively running in diverse domains far beyond the initial coding focus [15–18], including finance [19–22], insurance [23–26], and education [27–29].\r\n\r\n<a id='9d359393-2d3d-4119-9a89-7a764b4cd8b6'></a>\r\n\r\nHowever, despite widespread excitement about agent po-\r\ntential [30-33], many question their real value and success\r\nin production [34-37]: whether agents can deliver on their\r\npromise and where their future lies. For example, a re-\r\ncent study reports that 95% of agent deployments fail [37].\r\nThis stark contrast between the promise of agents and their\r\nhigh failure rate raises fundamental questions about what\r\nseparates successful deployments from failures.\r\n\r\n<a id='cf8e4d0a-c689-477a-b69d-eeea058ce876'></a>\r\n\r\n1\r\n\r\n<a id='d4fd9c02-285b-49b7-bef9-3a94ba1d3955'></a>\r\n\r\narXiv:2512.04123v1 [cs.CY] 2 Dec 2025\r\n\r\n<!-- PAGE BREAK -->\r\n\r\n<a id='d310c838-3300-4c23-be1b-aec3bbd0bd9c'></a>\r\n\r\nMeasuring Agents in Production\r\n\r\n<a id='4bd6ed7b-a8c9-4ac9-ab50-0898f9a6f50a'></a>\r\n\r\nUnfortunately, little information is publicly available on how production agents are built. Why do some agents succeed while others fail? What requirements must agents meet for production deployment? We lack a systematic understanding of what methods enable successful agents in the real world. Researchers have little visibility into real-world constraints and practices. Are agents failing because models are not capable enough, or are there other factors at play? Without understanding how agents actually work in production, researchers risk addressing the wrong problems. Meanwhile, practitioners lack consensus on how to build reliable agents and would benefit from understanding how the industry approaches these fast-evolving systems.\r\n\r\n<a id='7c5c9792-fe0c-42d2-9c47-4a6d896f21c9'></a>\r\n\r\nTo address this knowledge gap, we present Measuring Agents in Production (MAP), the first large-scale systematic study of AI agents in production. We study the practices of developers and teams behind successful real-world systems via four research questions (RQs):\r\n\r\nRQ1. What are the applications, users, and requirements of agents?\r\n\r\nRQ2. What models, architectures, and techniques are used to build deployed agents?\r\n\r\nRQ3. How are agents evaluated for deployment?\r\n\r\nRQ4. What are the top challenges in building deployed agents?\r\n\r\n<a id='98fd5f02-78e6-4e4b-9194-6be627e06f2f'></a>\r\n\r\nWe answer these questions through an online survey with 306 responses and 20 in-depth interviews with agent development teams, capturing technical details of production agent development. Our survey respondents are practitioners actively building AI agents (Figure 4a) across 26 domains, from finance and healthcare to legal (Figure 2). We filter the survey data to 86 responses that explicitly reported systems in production or pilot phases,* which we denote as deployed agents, for the results in the main paper. The full dataset with all 306 responses appears in Appendix A. For in-depth interviews, we study 20 cases from teams building deployed agents with real users, spanning organizations from global enterprises to startups (Figure 3). Our case studies and survey capture deployed agent systems serving user bases ranging from hundreds to millions of daily users (Figure 4c), providing visibility into high-impact deployments. This dual approach provides the first empirically grounded view of the technical methods, architectural patterns, and operational practices behind AI agents in production.\r\n\r\n<a id='5e4a8fca-2c9f-4439-a0db-95ff0e24f2b2'></a>\r\n\r\nTo provide first-hand data on working practices and real-\r\nworld constraints, our study focuses on practitioners actively\r\n\r\n<a id='4e7aeca4-a807-4a12-820d-9a651afeea19'></a>\r\n\r\n--- \r\n*Production systems are fully deployed and used by target end users in live operational environments. Pilot systems are deployed to controlled user groups for evaluation, phased rollout, or safety testing. Definitions for other stages appear in Appendix E.2 N5.\r\n\r\n<a id='fd376233-88dc-46a4-88c3-6a3019c68a1c'></a>\r\n\r\n<::transcription of the content: horizontal bar chart::>Horizontal bar chart showing the percentage of responses for different application domains where practitioners build and deploy Agentic AI systems. The x-axis is labeled '% of responses' ranging from 0% to 60%. The y-axis lists the domains, with each bar indicating a percentage and the count in parentheses. Each bar also displays an error bar.\r\n- Finance & Banking: 39.1% (27)\r\n- Technology: 24.6% (17)\r\n- Corporate Services: 23.2% (16)\r\n- Data Analytics: 13.0% (9)\r\n- Research & Development: 11.6% (8)\r\n- Software Development: 10.1% (7)\r\n- Legal & Compliance: 8.7% (6)\r\n- Customer Support: 8.7% (6)\r\n- Healthcare Services: 8.7% (6)\r\n- Retail: 4.3% (3)\r\n- Other: 15.9% (11)\r\n\r\nFigure 2. Application domains where practitioners build and deploy Agentic AI systems ($N$ = 69). Deployments span a broad range of industries, with the highest concentrations in finance, technology (including but not limited to software development), and corporate services. Additional lower-frequency domains (\"other\") are listed in Table 2, totaling roughly 26 distinct domains. Note that this is a multi-class classification question where each system may be assigned to multiple domain categories.\r\n\r\n<a id='1d2cc723-93f7-4e8b-8156-d39c39c0abbf'></a>\r\n\r\nbuilding agents already deployed in production that serve\r\nusers and deliver measurable value (Figure 1). This natu-\r\nrally biases toward successful deployments and experienced\r\npractitioners. Our data capture what works today, providing\r\ninsight into how the field is developing.\r\n\r\n<a id='7b4e239e-e822-4b99-ac24-3f908abe1a13'></a>\r\n\r\nOur study reveals key findings for each research question:\r\n\r\n<a id='bb9d6aba-a10c-4b4e-8724-92f8f3d13f63'></a>\r\n\r\n_RQ1:_ **Productivity gains drive agent adoption.** We find that 73% of practitioners deploy agents primarily to increase efficiency and decrease time spent on manual tasks (Figure 1). Most systems (93%) serve human users rather than other agents or systems (Figure 5a). Regarding requirements, teams prioritize output quality over real-time responsiveness: 66% allow response times of minutes or longer, compared to 34% requiring sub-minute latency (Figure 5b). In-depth interviews reveal that organizations tolerate this latency because agents that take minutes to execute still outperform human baselines.\r\n\r\n<a id='424442a0-7716-4e2e-ad9b-51a6a0fda5d1'></a>\r\n\r\nRQ2: Simple methods and workflows dominate. 70% of interview cases use off-the-shelf models without weight tuning (Figure 6), relying instead on prompting. Teams predominantly select the most capable, expensive frontier models available, as the cost and latency remain favorable compared to human baselines. We find that 79% of surveyed deployed agents rely heavily on manual prompt construction (Figure 7), and production prompts can exceed 10,000 tokens. Production agents favor well-scoped, static work-\r\n\r\n<a id='7c1868bc-1fa3-40ab-bee3-8dd32320b33c'></a>\r\n\r\n2\r\n\r\n<!-- PAGE BREAK -->\r\n\r\n<a id='b8f918e3-0c3b-4ec3-91c0-ac1e102472f1'></a>\r\n\r\nMeasuring Agents in Production\r\n---\r\n\r\n<a id='378e49ee-6eac-4c11-b2b1-1d2536a7f0df'></a>\r\n\r\nflows: 68% execute at most ten steps before requiring hu-man intervention, with 47% executing fewer than five steps. Furthermore, 85% of detailed case studies forgo third-party agent frameworks, opting instead to build custom agent ap-plication from scratch. Organizations deliberately constrain agent autonomy to maintain reliability.\r\n\r\n<a id='b7c1141e-72a6-4d57-ab1a-97a88f62043a'></a>\r\n\r\nRQ3: Human verification remains central to evaluation. The majority of deployed survey agents (74%) rely primarily on human-in-the-loop evaluation, and 52% use LLM-as-a-judge (Figure 10b). Notably, every interviewed team utilizing LLM-as-a-judge also employs human verification. Because production tasks are highly domain-specific, public benchmarks are rarely applicable. Consequently, 25% of teams construct custom benchmarks from scratch, while the remaining 75% evaluate their agents without formal benchmarks, relying instead on online-tests such as A/B testing or direct expert/user feedback. This pattern reflects the difficulty of automated evaluation for bespoke production tasks.\r\n\r\n<a id='32de3e03-f18d-45df-82f1-1f6772579060'></a>\r\n\r\nRQ4: Reliability is an unsolved challenge. Practitioners focus most on ensuring agent reliability, spanning correctness, latency, and security. The evaluation challenges described in RQ3 directly impact the ability to verify correctness to achieve reliable deployments. Latency impacts only a small subset (15%) of applications as a deployment blocker, and security represents a manageable concern that most deployed agents mitigate through action and environment constraints.\r\n\r\n<a id='56a98a4d-80b0-4d48-8142-2c0d3df9004f'></a>\r\n\r\nOur findings reveal that reliability concerns drive practitioners toward simple yet effective solutions with high controllability, including restricted environments, limited autonomy, and human oversight. For example, developers choose human-in-the-loop evaluation over fully automated techniques and manual prompt engineering over automated methods because these approaches offer control, transparency and trustworthiness. Practitioners deliberately trade-off additional agent capability for production reliability, and this design pattern already enables a broad spectrum of applications delivering real-world value (Figure 1).\r\nProduction agents represent an emerging engineering discipline. Our study documents the current state of practice and bridges the gap between research and deployment: we provide researchers visibility into real-world constraints and opportunities while offering practitioners proven patterns from successful deployments across industries.\r\n\r\n<a id='97d02f8c-f2c2-485c-bb59-ae685f693bbe'></a>\r\n\r\n## 2. Related Work\r\nTo our knowledge, we offer the first technical characterization of how agents running in production are built.\r\n\r\n<a id='5b43da50-9f53-40a9-aa4a-9e114048fd2f'></a>\r\n\r\nTable 1. Overview of application and their domains from our 20 case studies. To maintain clarity and confidentiality, similar use cases are aggregated into representative descriptions.\r\n\r\n<a id='e45e59fe-73fb-4122-b1da-a438f8f2fe38'></a>\r\n\r\n<table id=\"2-1\">\r\n<tr><td id=\"2-2\">Business Operations</td></tr>\r\n<tr><td id=\"2-3\">Insurance claims workflow automation Customer care internal operations assistance Human resources information retrieval and task assistance</td></tr>\r\n<tr><td id=\"2-4\">Communication Tech, Multi-lingual Multi-dialect</td></tr>\r\n<tr><td id=\"2-5\">Communication automation services Automotive communication services</td></tr>\r\n<tr><td id=\"2-6\">Scientific Discovery</td></tr>\r\n<tr><td id=\"2-7\">Biomedical sciences workflow automation Materials safety and regulatory analysis automation Chemical data interactive exploration</td></tr>\r\n<tr><td id=\"2-8\">Software &amp; Business Operations</td></tr>\r\n<tr><td id=\"2-9\">Data analysis and visualization Enterprise cloud engineer and business assistance Site reliability incident diagnoses and resolution Software products technical question answering</td></tr>\r\n<tr><td id=\"2-a\">Software DevOps</td></tr>\r\n<tr><td id=\"2-b\">Spark version code and runtime migration Software development life cycle assistance end-to-end Software engineer/developer slack support SQL optimization Code auto-completion and syntax error correction</td></tr>\r\n</table>\r\n\r\n<a id='f1d14b3d-bb8b-4f9f-a87f-56a7217663bf'></a>\r\n\r\nCommercial Agent Surveys. Several prior efforts exam-ine AI (agent) adoption in production from adjacent per-spectives. MIT Media Lab and NANDA Initiative [37] and Challapally et al. [38] study economic viability and executive views on return-on-investment from companies attempting to integrate agents. Shome et al. [36] analyze marketing materials for 102 commercial agents and conduct user studies with 31 participants, revealing gaps between promised and realized capabilities. Industry reports [33, 39-42] focus on organizational readiness and market trends. LangChain [43] surveyed over 1,300 professionals on agent motivations, characteristics, and challenges.\r\n\r\n<a id='dae6d6a6-d6a0-442a-bd66-2fdd93b3d256'></a>\r\n\r\nOur work differs in two key ways: (i) Scope: we study agents actively operating in production; and (ii) Focus: we collect engineering-level technical data from practitioners. Our study complements these prior works and offers a new perspective into successful agents, providing insights into where the field is heading.\r\n\r\n<a id='804d8a01-1ebc-4706-80a1-37f23e13072e'></a>\r\n\r\nResearch Agent Literature Survey. Many prior work examine LLM-powered agents from an academic perspec- tive [44-49], providing valuable taxonomies of agent de- signs and tracing the evolution of key techniques. Other agent surveys focus on specific aspects: evaluation method- ologies [50, 51], security concerns [52], and multi-agent systems [53]. In contrast, our work is an empirical study that collects primary data directly from practitioners build- ing and operating deployed agents. We do not synthesize published research via literature reviews; instead, we con- duct original survey data collection and in-depth case study\r\n\r\n<a id='5ccb9ba9-9be9-44f8-9f5f-073b8c7831c3'></a>\r\n\r\n3\r\n\r\n<!-- PAGE BREAK -->\r\n\r\n<a id='0cf4023a-3d6d-4d43-8f69-c6d77c856b83'></a>\r\n\r\nMeasuring Agents in Production\r\n---\r\n\r\n<a id='7b565675-0924-4b97-ad4d-c3f1ecd46c9c'></a>\r\n\r\ninterviews to document production practices.\r\n\r\n**Single-System Studies.** Companies publish papers and technical blogs on agentic systems [54–62] and open-source agent implementations [63–67], offering glimpses into production-related agents. Each paper naturally focuses on a single system or domain, creating an information gap: we lack understanding of common patterns, shared challenges, and design principles across deployments. We address this gap by surveying professionals across diverse industries to identify recurring engineering practices, architectural decisions, and challenges.\r\n\r\n<a id='4327a3de-1ff0-47f8-949c-084bf353a7d0'></a>\r\n\r\n### 3. Methodology\r\n\r\nTo understand how production AI agents are built and deployed, we conducted two complementary studies: a large-scale public survey and 20 in-depth case study interviews. We detail our survey design and distribution in **Section 3.1**, describe our interview protocol and case study selection in **Section 3.2**, and outline our data processing pipeline in **Section 3.3**.\r\n\r\n<a id='7494f233-90e1-45f1-ac89-5f067b6decb4'></a>\r\n\r\n## 3.1. Public Online Survey\r\n\r\nWe designed and conducted a survey for practitioners building AI agents, with a strong focus on the technical details of their systems. The survey covers system architecture, evaluation, deployment, and operational challenges through 47 questions (listed in full in Appendix E). To capture AI Agents as practitioners understand them, we invited respondents to describe systems they refer to as AI Agents or Agentic AI without imposing a prescriptive definition.\r\n\r\n<a id='28f42428-93ea-48a7-90b1-06bae7bcb96f'></a>\r\n\r\nTo ensure relevance, we implemented the survey with dynamic branching logic in Qualtrics, where certain questions only appear based on previous responses. For example, the question about the realized benefits of using agents is only shown to respondents who confirmed they chose agentic over non-agentic solutions (O3.1.1 and O3.1 in Appendix E.3). This adaptive structure, combined with the optional nature of all questions, means each question receives a different number of responses. We report the specific sample size (N) for each result throughout Sections 4–7 in figure captions. We asked participants to focus their responses on a single system if they contributed to more than one agent system (Acknowledgment E.1). Full questionnaire details are available in Appendix E, including the branching logic in Figures 26 and 27.\r\n\r\n<a id='a2e5421d-24c8-4172-971a-bbfc23b3f6e0'></a>\r\n\r\nWe distributed the survey through multiple channels to reach practitioners across the AI agent ecosystem: the Berkeley RDI Agentic AI Summit [68], the AI Alliance Agents-in- Production Meetup [69], the UC Berkeley Sky Retreat [70], and professional networks including LinkedIn, Discord, and X. We release the survey on July 28, 2025, with data collec-\r\n\r\n<a id='e6f7f016-7ebe-4164-bcc1-82d4a41b9ab2'></a>\r\n\r\n<::chart: horizontal bar chart::>\r\nCompany Maturity Level\r\nMature: 10\r\nLate: 5\r\nGrowth: 1\r\nEarly: 3\r\nSeed: 1\r\nNumber of case studies\r\n\r\nFigure 3. The distribution of source institution maturities across in-depth interview-based case studies. The minority (5/20) are from seed-stage startups (validating product-market fit), early-stage startups (proving scalable business models), and growth-stage startups (rapidly expanding market share and operations). The majority (15/20) are from late-stage and mature institutions (with established market positions). The stages are approximated from limited public information e.g. size, sector, and annual recurring revenue.\r\n<::/chart::>\r\n\r\n<a id='87a363d5-e114-41f2-a9ab-f2081bbae4ec'></a>\r\n\r\ntion for this paper ending October 29, 2025.\r\n\r\nWe received 306 valid responses. 294 participants indicated that they have directly contributed to the building and de-signing of at least one agent system. Our survey respondents are predominantly technical professionals such as software and machine learning engineers (Figure 4a). Among 111 respondents who reported their system's deployment stage, 82% indicated their systems are in production or pilot phases (Figure 4b), demonstrating rapid transition from experimen-tal prototypes to real-world deployments.\r\n\r\n<a id='db81f02e-e7ec-48b2-9485-536d47981537'></a>\r\n\r\nTo maintain focus on production systems, we filtered the data to 86 responses that explicitly reported their systems as being in production or pilot phases. Production systems are fully deployed and used by target end users in live op- erational environments, while pilot systems are deployed to controlled user groups for evaluation, phased rollout, or safety testing. This filtering excludes development proto- types, research artifacts, and retired systems, as well as participants who did not report their deployment stage. The definitions for all deployment stages appear in Appendix E.2 N5. We denote production and pilot systems as deployed agents. All survey statistics presented in this paper refer to deployed agents unless otherwise stated. Complete data for 306 valid responses across all deployment stages appears in Appendix A.\r\n\r\n<a id='166b836f-20be-412c-8540-c7ab0e9e183b'></a>\r\n\r\n### 3.2. In-depth Case Studies\r\nTo add qualitative depth to our survey findings, we conducted 20 in-depth case studies through interviews with teams building deployed agents across a diverse range of organizations.\r\n\r\n<a id='a76d208c-1a47-4fe6-bcfd-da14ff7a9985'></a>\r\n\r\nWe carefully selected our 20 cases to achieve representative\r\n\r\n<a id='5fcb657a-74fe-4598-97b4-927bd99a5d6c'></a>\r\n\r\n4\r\n\r\n<!-- PAGE BREAK -->\r\n\r\n<a id='e9382295-aae6-4fe4-9678-eae3d52b9dfb'></a>\r\n\r\n# Measuring Agents in Production\r\n\r\n<a id='6c857d2a-71bc-47c3-8f9b-9329d7eef5ce'></a>\r\n\r\n<::bar chart::>Figure 4. Overview of survey respondent and system characteristics across all agents the survey data: (a) roles of survey participants by primary contribution area (N=108), (b) deployment stages of Agentic AI systems (N=111) that survey participants contributed to, and (c) reported number of end users (N=35) for the Agentic systems survey participants contributed to. Chart (a) shows roles of survey participants by primary contribution area: Software & ML Engineers 50.9% (55), Technical Executive & Managers 31.5% (34), Academic Researchers 9.3% (10), Infrastructure Engineers 4.6% (5), Technical Writers & Learning Professionals 3.7% (4). Chart (b) shows deployment stages of Agentic AI systems: In Production 45.0% (50), Pilot Deployment 32.4% (36), Prototype 19.8% (22), Research 1.8% (2), Retired 0.9% (1). Chart (c) shows reported number of end users: 1-10: 14% (5), 10-100: 5% (2), 100-500: 23% (8), 500-1000: 20% (7), 1000-10k: 11% (4), 10k-100k: 17% (6), 100k-1m: 6% (2), 1m+: 3% (1).<::>\r\n\r\n<a id='fc291bc1-f0d3-4455-9160-a666efc571a7'></a>\r\n\r\nsamples across application diversity, organizational matu-\r\nrity, and global reach. We only interviewed systems with\r\n_real-world users_: 14 cases are in full production and 6\r\ncases are in final pilot phases. The cases span five key sec-\r\ntors: business operations (3 cases), software development\r\nand operations (7 cases), integrated business and software\r\noperations (5 cases), scientific discovery (3 cases), and com-\r\nmunication services (2 cases). Notably, these deployments\r\nextend well beyond commonly known coding agents or gen-\r\neral chatbots, demonstrating the breadth of production agent\r\napplications. The systems target both internal employees (5\r\ncases) and external enterprise consumers (15 cases). Our\r\nselection includes organizations across maturity levels, from\r\nseed-stage startups to large enterprises with global footprints\r\n(Figure 3). For companies with multiple agent deployments,\r\nwe presented only distinct use cases to maximize application\r\ndiversity. The anonymized case studies and their applica-\r\ntion domains appear in Table 1, with additional details in\r\nAppendix D.\r\n\r\n<a id='4528d5ef-7f84-436e-9303-5f9b6dcc0f70'></a>\r\n\r\nEach interview lasted 30 to 90 minutes and was conducted\r\nby teams of 2 to 5 organizationally neutral interviewers. We\r\nfollowed a semi-structured interview protocol covering 11\r\ntopic areas (detailed in Appendix D.1), including system ar-\r\nchitecture, evaluation mechanisms, deployment challenges,\r\noperational requirements, and measurable agent values. We\r\nanonymized and recorded interviews based on participant\r\npreferences, with human note-takers capturing insights. To\r\nensure accuracy, we cross-validated final summaries among\r\nall interviewers. Per our confidentiality agreements, we\r\nanonymized all data and present findings in aggregate.\r\n\r\n<a id='68612ef3-d8eb-4f9d-9898-9331f1aa1470'></a>\r\n\r\n### 3.3. Data Processing\r\nMost survey questions use structured formats (single-select, multi-select, or numeric), requiring minimal post-processing. For the free-text domain keyword responses used in Figure 2, we used LOTUS [71], a state-of-the-art unstructured data processing tool, to identify common do-\r\n\r\n<a id='e9ddd1cc-f9b7-48d7-9f13-2f6c31d87878'></a>\r\n\r\nmain categories and perform classification. This allowed us to normalize phrases from survey responses to represen- tative label sets. For instance, we grouped responses like \"healthcare,\" \"medical,\" and \"patient monitoring\" under a unified \"healthcare\" category. Details of this normalization process appear in Appendix B.1. All other figures present results from structured survey questions or interview data without requiring automated processing.\r\n\r\n<a id='757ce315-fd6e-4c72-9959-d4db0e2b4de0'></a>\r\n\r\nAs described in Section 3.1, our survey data in the main\r\npaper are filtered to deployed (production and pilot) agents,\r\nand our interviews specifically select teams building de-\r\nployed agents. All results presented in Sections 4–7 refer to\r\ndeployed agents from either survey responses or interviews,\r\nwhich we explicitly denote throughout the paper. Refer to\r\nAppendix A for unfiltered full data.\r\n\r\n<a id='035350b7-e831-4e7b-8ca4-a215ad642fa3'></a>\r\n\r\nFor all figures that include error bars, we report 95% con-\r\nfidence intervals computed using 1,000 bootstrap samples\r\nwith replacement.\r\n\r\n<a id='5979a52f-b8cf-4789-ad4b-7322e463ee12'></a>\r\n\r\n## 4. RQ1 Results: What Are The Applications, Users, and Requirements of Agents?\r\n\r\nWe now present findings from our survey and case study interviews across four central research questions. We start by examining motivations for agent adoption (Section 4.1), which agent applications successfully reach deployment (Section 4.2), who uses these systems (Section 4.3), and what latency requirements shape their design (Section 4.4). Understanding these patterns reveals where agentic systems deliver practical value and how they transform real-world applications.\r\n\r\n<a id='26f8383d-1fba-4af0-80c7-3a60334760fe'></a>\r\n\r\n## 4.1. Motivations for Choosing Agents\r\n\r\nAmong practitioners with deployed agents who evaluated alternatives, 82.6% prefer the agentic solution for deployment. We define non-agentic alternatives as existing software sys-\r\n\r\n<a id='da0c42aa-ac0b-467a-8147-221356870348'></a>\r\n\r\n5\r\n\r\n<!-- PAGE BREAK -->\r\n\r\n<a id='e85ddb68-1905-4226-a14c-dd9a19a83b11'></a>\r\n\r\nMeasuring Agents in Production\r\n---\r\n\r\n<a id='fc238a26-0aa6-41ea-a02f-318c0318e650'></a>\r\n\r\ntems, traditional approaches, or human execution. Figure 1 details the specific benefits reported by these practitioners. The top three drivers all target the reduction of human effort: increasing productivity and efficiency (72.7%), reducing human hours (63.6%), and automating routine labor (50.0%). Conversely, qualitative benefits rank lower. Increasing user satisfaction ranks in the middle tier (37.9%), followed by reducing required human expertise and training (34.8%) and enabling novel technology (33.3%). Accelerating failure response times (18.2%), reducing interdisciplinary knowledge requirements (18.2%), and risk mitigation (12.1%) rank last.\r\n\r\n<a id='5af6628b-d36e-45bd-b467-792c4df28b79'></a>\r\n\r\nThese priorities reflect pragmatic deployment realities. Organizations adopt agents to solve immediate operational problems, such as expert-expensive manual work and insufficient staffing capacity. Productivity gains are straightforward to quantify through human-hour reductions, whereas safety improvements and risk mitigation are harder to verify. This focus aligns with our findings on internal deployments (Section 4.3), where organizations tolerate higher error rates by maintaining human oversight to catch these errors.\r\n\r\n<a id='142b7110-3054-42d8-9465-8333f1f78114'></a>\r\n\r\nThe top reported reasons for using agents reveal a trend\r\nwhere certain objectives are more verifiable and measurable.\r\nFor example, time to complete a task (productivity) is con-\r\ncrete and quantifiable, while risk mitigation benefits takes\r\nlonger to verify. We examine this measurement gap and its\r\nimplications for deployed agents in Section 7.1.\r\n\r\n<a id='f4c4b879-465a-4d50-a042-6a2068484fd6'></a>\r\n\r\nRQ 1 Finding #1: Productivity gains through automating routine human tasks drive agent adoption (73%), while harder-to-verify applications like risk mitigation are less common.\r\n\r\n<a id='5fe0e468-7ac2-43ca-b95d-c100d6c3e0da'></a>\r\n\r\n## 4.2. Application Domains\r\n\r\nWe find that agents operate in diverse industries well beyond software engineering. Figure 2 shows that the three domains with the highest number of reported deployments in survey are Finance & Banking (39.1%), Technology (24.6%), and Corporate Services (23.2%). We also observe a long tail of applications in Data Analytics (13.0%), Research & De- velopment (11.6%), and other specialized domains (15.9%). This distribution demonstrates that agentic systems deliver practical value across fundamentally different industries.\r\n\r\n<a id='8bf19cd2-83cb-473d-8878-5b2581449109'></a>\r\n\r\nRQ 1 Finding #2: Deployed agents already operate across 26 diverse domains, well beyond math and coding that are popular in research, demonstrating value across different industries.\r\n\r\n<a id='3090173b-319a-4b03-99c7-5e1aaddc62fa'></a>\r\n\r\n## 4.3. Users of Agents\r\nWe find that deployed agents primarily serve human users directly. Survey data indicates that 92.5% of deployed sys-\r\n\r\n<a id='4297208b-7f42-4c9d-90ad-4d3625d85b41'></a>\r\n\r\ntems serve humans rather than other agents or software systems. As shown in Figure 5a, internal employees are the primary user base (52.2%), followed by external customers (40.3%). Only 7.5% of deployed systems serve non-human consumers.\r\n\r\n<a id='e3416c12-226a-4e1c-81c4-e95b6f47c403'></a>\r\n\r\nThe focus on internal users is a deliberate deployment\r\nchoice. Detailed case studies reveal that organizations re-\r\nstrict deployments to internal environments to mitigate un-\r\nsolved reliability and security concerns. Internal users oper-\r\nate within organizational boundaries where agent mistakes\r\nhave lower consequences and human oversight is readily\r\navailable. An example from our case study, internal Pager-\r\nDuty agents respond to employee requests, while human\r\nengineers can take over when needed.\r\n\r\n<a id='29aae936-fdea-4128-b18c-34fdea9dee13'></a>\r\n\r\nFurthermore, most systems—including external-facing ap-\r\nplications—require domain-specific knowledge to operate.\r\nFor example, insurance authorization agents support nurses,\r\nand incident response agents support site reliability engi-\r\nneers. This reflects a pattern where agents function as tools\r\nthat *augment* domain experts rather than replace them. This\r\nparadigm also enables human users to serve as the final\r\nverifiers of agent outputs, which we discuss further in Sec-\r\ntion 6.2.\r\n\r\n<a id='a1bd25dc-c3bd-4606-8a8a-ff510558adbe'></a>\r\n\r\nBeyond user type, we examine the scale of the user base. We find that end-user counts for deployed systems vary significantly. As shown in Figure 4c, 42.9% of deployments serve user bases in the hundreds. However, we also observe high-traffic deployments (25.7%) serving tens of thousands to over 1 million users daily, representing substantial user impact or possibly mature systems.\r\n\r\n<a id='addf111d-a332-4899-9e9d-c6f8525ef8bc'></a>\r\n\r\nRQ1 Finding #3: Deployed agents primarily serve human end-users (92.5%), which enables close human-oversight.\r\n\r\n<a id='eae4be98-1f40-4316-a941-fc782024a659'></a>\r\n\r\n## 4.4. Latency Requirements\r\n\r\nRelaxed latency requirements are common among deployed agents. Figure 5b shows the distribution of maximum allowable end-to-end latency. Minutes is the most common target, followed by seconds. Notably, 17.0% report no defined limit yet, and 1.9% allow hours to days.\r\n\r\n<a id='07ee3a1c-693c-4d45-97fa-dba98a544879'></a>\r\n\r\nThe latency tolerance reflects the productivity focus from Section 4.1. Agents are often used to automate tasks that typically take humans hours to complete. Consequently, an agent taking multiple minutes to respond remains orders of magnitude faster than non-agentic baselines. Interview participants emphasized this advantage: even if an agent takes five minutes, that remains more than 10x faster than assigning the task to a person on the team, especially when staffing shortages exist and the task is secondary to the user's core responsibilities. Examples include nurses examining\r\n\r\n<a id='a57ae340-05ab-4619-96ca-3f391e5fb084'></a>\r\n\r\n6\r\n\r\n<!-- PAGE BREAK -->\r\n\r\n<a id='b0466d50-c012-4906-b3f4-e16af1d9f53e'></a>\r\n\r\n# Measuring Agents in Production\r\n\r\n<a id='f780699e-ae5e-4cb4-b4be-1cc9625b0d41'></a>\r\n\r\n<::bar charts::>Figure 5 consists of two bar charts. Chart (a) is titled \"Distribution of primary end users\" and shows the percentage of responses for different end-user categories. The y-axis lists the categories: \"Internal Employees\", \"External Customers\", \"Non-Agentic Software\", and \"Other AI Agents\". The x-axis is \"% of Responses\" ranging from 0% to 80%. A legend indicates that hatched bars (///) denote \"Human user\" and solid bars denote \"Non-human user\". The data points are: Internal Employees: 52.2% (35) (Human user); External Customers: 40.3% (27) (Human user); Non-Agentic Software: 4.5% (3) (Non-human user); Other AI Agents: 3.0% (2) (Non-human user). Chart (b) is titled \"Reported tolerable end-to-end response latency for deployed systems\" and shows the percentage of systems for different latency tolerance levels. The x-axis is \"Latency Tolerance\" with categories: \"< Subsecond\", \"Seconds\", \"Minutes\", \"Hours\", \">1 day\", and \"No limit set\". The y-axis is \"Percentage (%)\" ranging from 0% to 60%. The data points are: < Subsecond: 7.5% (4); Seconds: 26.4% (14); Minutes: 41.5% (22); Hours: 5.7% (3); >1 day: 1.9% (1); No limit set: 17.0% (9).\r\n\r\nFigure 5. Overview of Agentic AI deployment characteristics in terms of primary end users and latency requirements: (a) Distribution of primary end users ($N=67$), where hatched bars (///) denote human end-users and solid bars denote non-human end-users; and (b) Reported tolerable end-to-end response latency for deployed systems ($N=53$). Over 92% of deployed agents primarily serve human users, and most systems tolerate response times on the order of minutes rather than requiring strict real-time responsiveness. \r\n\r\n<a id='06b8a69f-444d-4732-bc0c-5d143afdc43e'></a>\r\n\r\ninsurance details and software engineers responding to in-\r\nternal pager duty. Some deployed agents from case studies\r\neven batch requests hourly or overnight, further indicating\r\nlatency is not a primary constraint.\r\n\r\n<a id='91c653c0-0b9a-4f2c-9c11-4e5b8ff2ff5c'></a>\r\n\r\nHowever, this pattern breaks for real-time interactive applications. For example, practitioners building voice-driven systems report latency as their top challenge (Section 7.2) during detailed case study. These systems compete against human conversation speeds rather than task completion baselines. Among our 20 detailed case studies, only 5 require real-time responsiveness. The remaining 15 cases tolerate extended processing times: 7 involve human review with relaxed timing, 5 operate as asynchronous background processes, and 3 have hybrid operation patterns. For these systems, processing times of minutes remain acceptable because the alternative is days of human effort.\r\n\r\n<a id='66593873-65e7-40ae-b149-65d6779c16ea'></a>\r\n\r\n$\\mathcal{R}$$\\mathcal{Q}$ 1 Finding #4: Development teams prioritize agent output quality and capability by concentrating on latency-relaxed applications.\r\n\r\n<a id='671083dc-b409-49d4-b012-8e5de2e91ff1'></a>\r\n\r\n**5. *RQ2 Results*: What Models, Architectures, And Techniques Are Used To Build Agents?**\r\n\r\nHaving established *what* problems practitioners target with agentic systems, we now address *how* these systems are built. We examine five critical implementation decisions: model selection, model weights tuning, prompt construction, agent architectures, and development frameworks.\r\n\r\n<a id='c771ca04-f435-4606-ba89-6d49f860c0e7'></a>\r\n\r\nOverall, practitioners favor established, straightforward\r\nmethods over stochastic or training-intensive techniques.\r\nWe find that methods popular in research—such as fine-\r\n\r\n<a id='781ec02c-db27-4e35-a359-205bc851d07a'></a>\r\n\r\ntuning, reinforcement learning, and automated prompt op-\r\ntimization—remain uncommon in deployment. Instead,\r\nteams prioritize control, maintainability, and iteration speed.\r\n\r\n<a id='ba49bac7-27a7-4bbd-8ad5-39de0ca6b007'></a>\r\n\r\n## 5.1. Model Selection\r\n\r\nWe find that deployed agents rely heavily on proprietary models. Only 3 of 20 detailed case studies use open-source models (LLaMA 3 [72], Qwen [73], and GPT-OSS [74]), while the remaining 17 rely on closed-source frontier models (Figure 6). Of the teams using closed-source models, 10 explicitly confirm using the Anthropic Claude [75] or OpenAI GPT [76] families. Specific models mentioned during interviews include Anthropic Claude Sonnet 4 and Opus 4.1, and OpenAI o3, each representing the state-of-the-art model from each provider at the time of the interview. While other participants did not disclose specific models, most interviewees followed the pattern of using the largest and most capable state-of-the-art models as the primary model for their agents from each model family.\r\n\r\n<a id='d20a3c49-4714-47ee-9b4f-6e266e6ec3f7'></a>\r\n\r\nWe find that open-source adoption is rare and is driven by specific constraints rather than general preference. Among the three cases using open-source models, motivations include high-volume workloads where inference costs at scale are prohibitive, and regulatory requirements preventing data sharing with external providers. For example, one team from detailed case study serve continuous infrastructure maintenance agent leverages the company's existing internal compute resources (e.g., GPUs) to serve a fine-tuned open-source model to meet cost constraints.\r\n\r\n<a id='7272740b-8008-43be-9335-28dd863c7833'></a>\r\n\r\nFor the majority of cases, model selection follows a prag-\r\nmatic, empirical approach focused on downstream perfor-\r\nmance. Interviewees report that they test the top accessible\r\n\r\n<a id='dc232322-ac07-4083-90ad-dd5a0c62118e'></a>\r\n\r\n7\r\n\r\n<!-- PAGE BREAK -->\r\n\r\n<a id='0478c687-9e04-4e42-b6de-9c5947c3e428'></a>\r\n\r\nMeasuring Agents in Production\r\n\r\n<a id='a6d45a01-f22c-47e4-831c-c1ed0b762bf8'></a>\r\n\r\n<::chart: Stacked bar chart titled \"Distribution of model characteristics of case-study systems from our interviews (N=20)\".\r\n\r\nThe Y-axis is labeled \"Number of case studies\" and ranges from 0 to 20.\r\n\r\nThere are two main categories on the X-axis: \"Open source\" and \"Post training\".\r\n\r\n**Open source bar:**\r\n- The bottom segment, labeled \"Yes(3)\", represents 3 case studies.\r\n- The top segment, labeled \"No(17)\", represents 17 case studies.\r\n\r\n**Post training bar:**\r\n- The bottom segment, labeled \"Yes(6)\", represents 6 case studies.\r\n- The top segment, labeled \"No(14)\", represents 14 case studies.\r\n\r\nFigure 6. Distribution of model characteristics of case-study systems from our interviews (N=20). The left bar chart shows model source openness; “Yes” indicates open-source model usage while “No” indicates closed-source or non-disclosed model usage. The right bar chart shows whether post-training (e.g. fine-tuning, reinforcement learning) is utilized (“Yes”); “No” labels non-use or non-disclosure. Overall, most case-study systems rely on off-the-shelf closed-source models, with comparatively fewer teams using open-source models or performing additional post-training.::>\r\n\r\n<a id='23a6d1ec-011b-4352-987b-b6fb75479370'></a>\r\n\r\nstate-of-the-art models for each task and select based on per-\r\nformance. Unlike the high-volume open-source use cases,\r\nthese teams note that runtime costs are negligible compared\r\nto the human experts (e.g., medical professionals, senior\r\nengineers) that the agent augments. Consequently, they de-\r\nfault to the best-performing closed-source models regardless\r\nof inference cost. Additionally, 4 out of 20 detailed case\r\nstudies combine LLMs with specialized off-the-shelf mod-\r\nels (e.g., text-to-speech, chemistry foundation models) to\r\nhandle specific modalities.\r\n\r\n<a id='5a29c7c0-b006-445e-8628-0e21476b5f99'></a>\r\n\r\nRQ2 Finding #1: Deployed agents predominantly rely on proprietary frontier models; open-source models are used primarily to satisfy cost or regulatory constraints.\r\n\r\n<a id='074d25e8-5cb1-41e5-8a7b-1389f8f70ff7'></a>\r\n\r\nNumber of Distinct Models. While a substantial portion rely on a single model, the majority coordinate multiple models to meet functional or operational needs. Survey results show that 40.9% of deployed agents use exactly one model, while 27.3% use two, 18.2% use three, and 13.6% use four or more. Among detailed case studies, 10 out of 20 (50%) combine models to address specific functional needs. We identify two drivers: cost optimization and modality. First, teams combine models of varying sizes to balance latency, cost, and quality. For example, one agent workflow from case study routes simple subtasks like pattern recogni- tion to smaller models while reserving larger models for sub- tasks requiring higher reasoning capabilities. Second, teams integrate models to handle distinct data modalities. Com- munication agents leverage text-to-speech models alongside LLMs, while scientific agents employ domain-specific foun- dation models (e.g., chemistry) alongside general-purpose\r\n\r\n<a id='e4a89b1d-53d9-4a2d-85d2-9e4a30ef729d'></a>\r\n\r\n___ †Rankings based on public model leaderboards at the time of development.\r\n\r\n<a id='5e8e2575-9ad6-4954-80b7-ca401a25a71a'></a>\r\n\r\nreasoning models.\r\n\r\n*Operational constraints* also drive multi-model adoption.\r\nWe find that multi-model architectures can emerge from\r\nlifecycle management needs rather than complex reasoning\r\nrequirements for the agent task. Detailed case studies reveal\r\nthat teams maintain multiple models to manage agent's be-\r\nhavioral shifts from model migration. Organizations often\r\nrun legacy models alongside newer versions because agent\r\nscaffolds and evaluation suites depend on the specific be-\r\nhaviors of the older model, where sudden updates might\r\ndegrade output quality. Additionally, governance policies\r\nenforce teams to route subtasks to different model endpoints\r\nbased on user or developer access levels. Thus, architectural\r\ncomplexity often reflects strategic operations rather than\r\ntask difficulty.\r\n\r\n<a id='85bedb66-fa0e-464d-b875-1e2bc8db6e3d'></a>\r\n\r\nInterestingly, we observe a heavier tail towards agents us-ing more distinct models when we examine the full survey data, including prototyping and research agents that have not yet been deployed (Figure 16a). Deployed agents are more likely to converge on fewer number of distinct models compared to non-deployed agents, suggesting that teams explore richer multi-model combinations during early exper-imentation but consolidate onto a smaller set of models as they move toward deployment. We hypothesize this might be also reflecting the additional operational burden of main-taining many distinct model endpoints.\r\n\r\n<a id='fc672bed-6783-4552-99e5-a280500cf86e'></a>\r\n\r\n**RQ2 Finding #2:** The majority of agents coordinate multiple models, driven not only by functional needs like modality but also by operational requirements such as model migration.\r\n\r\n<a id='c8a3d61d-f309-4d6a-b19c-3a48c66d4780'></a>\r\n\r\n## 5.2. Model Weights Tuning\r\nWe observe a strong preference for prompting over model weight updates in deployed agents. We find that 14 out of 20 (70%) detailed case studies rely solely on off-the-shelf models without supervised fine-tuning (SFT) or reinforcement learning (RL) (Figure 6). Additionally, 2 teams from detailed case studies explicitly report that foundation model capabilities already meet their target use case, making fine-tuning unnecessary.\r\n\r\n<a id='e30bed45-67fe-47d8-8ba3-2800040ed0c8'></a>\r\n\r\nOnly 5 of 20 detailed case studies actively use SFT. These teams target deployment in business-specific corporate contexts where leveraging highly contextual information improves downstream performance. For example, customer product support agents benefit from fine-tuning on specific product offerings and policies. However, performance gains do not always justify the development overhead. Among the 5 detailed case studies actively using fine-tuned models, 3 consider SFT essential, while 2 apply it selectively for enterprise clients where customization requirements justify the additional cost. Three additional teams from interview\r\n\r\n<a id='c6780ee8-6282-4c38-afb9-a246cd4c79f9'></a>\r\n\r\n8\r\n\r\n<!-- PAGE BREAK -->\r\n\r\n<a id='68afb0f6-40f2-41f6-b729-9bce6cd0a314'></a>\r\n\r\nMeasuring Agents in Production\r\n---\r\n\r\n<a id='b46320fd-8234-40d6-a379-662ce9365e2f'></a>\r\n\r\ncase studies express strong interest in future adoption for\r\nsimilar reasons. Notably, we find that 4 of the 5 detailed\r\ncase studies that employ SFT do so in combination with off-\r\nthe-shelf LLMs, rather than relying on fine-tuned models\r\nexclusively.\r\n\r\n<a id='d040296e-8859-413a-963b-ea2801a139e7'></a>\r\n\r\nRegarding reinforcement learning (RL), only 1 scientific\r\ndiscovery case from our interviews uses an RL post-trained\r\nmodel. Three other teams express interest in exploring RL\r\nfor software testing in future development cycles.\r\n\r\n<a id='b6484e20-1e99-48eb-9395-f84df46c584f'></a>\r\n\r\nThis data, however, does not diminish the value of post-training models for agent applications. Interviews show that SFT and RL is challenging to implement and brittle to model upgrades. Given that off-the-shelf base models can already do most of what the agent applications need, teams prefer methods with lower integration overhead that do not increase already high development and maintenance burdens.\r\n\r\n<a id='9968e755-931b-43cc-b0ba-25a29c60094b'></a>\r\n\r\n*RQ2 Finding #3:* Practitioners rarely post-train models. When they do, they selectively apply SFT/RL to specific subtasks or clients, typically in combination with general LLMs. Teams find prompt engineering with frontier models sufficient for *many* target use-cases already.\r\n\r\n<a id='426601d7-cca8-43a4-9027-912b37c43b20'></a>\r\n\r\n## 5.3. Prompting Strategies\r\n\r\nWe find that humans dominate system-prompt construction in production systems. Our survey data reveals that 33.9% of deployed agents use *fully manual* methods with hard-coded strings. Another 44.6% use a hybrid approach where humans manually draft prompts and then use an LLM to augment or refine them, and 3.6% rely on utilizing predefined prompt templates. Only 8.9% of respondents use a prompt optimizer (e.g., DSPy [77]) to improve their agent systems, and just 3.6% report letting agents autonomously generate their own prompts.\r\n\r\n<a id='66ac405f-e013-4384-a2ab-e6f83dd710ba'></a>\r\n\r\nOur detailed case studies confirm this pattern. Only 1 out of 20 (5%) detailed case studies has explored automated prompt optimization. The remaining cases rely on primary human construction, sometime using LLMs for augmentation. While recent research [77–79] proposes automating prompts into parametric optimizations, we find these methods rare in deployment. We speculate from interview conversations with practitioners that they prioritize controllable, interpretable methods that allow for fast iteration and debugging over automated or “black-box” methods that requires additional engineering overhead.\r\n\r\n<a id='aa728afc-be99-420f-910c-5d7cee2bd1e2'></a>\r\n\r\n*RQ2 Finding #4*: Human dominates prompt construction as teams prioritize controllability. LLMs are used as secondary tools to augment human-crafted prompts, while automated prompt optimization remains rare.\r\n\r\n<a id='7055deca-4efd-42bb-9a0a-0d652ca0c198'></a>\r\n\r\n<::chart: horizontal bar chart::>  \r\n**Distribution of prompt construction strategies across deployed Agentic AI systems (N=53)**  \r\n**Y-axis (Strategies):**  \r\n* Manual + AI: 44.6% (25)  \r\n* Fully Manual: 33.9% (19)  \r\n* Prompt Optimizer: 8.9% (5)  \r\n* Predefined Template: 3.6% (2)  \r\n* Fully Autonomous: 3.6% (2)  \r\n**X-axis:** % of Responses  \r\nFigure 7. Distribution of prompt construction strategies across deployed Agentic AI systems (N=53). Overall, the prompting patterns indicate that humans remain central to prompt crafting. This was a multi-select question, so respondents could choose all prompt construction strategies that applied.  \r\n<::/chart::>\r\n\r\n<a id='6c6edd13-5cfe-4537-a140-7c0a5fd6a7e8'></a>\r\n\r\nWe find that prompt complexity increases with system maturity. Among deployed agents from survey, we observe a wide distribution: while 51.5% of systems use short instructions under 500 tokens, there is a long tail of massive prompts (Figure 8b). Specifically, 24.2% of deployed agents use prompts between 500 and 2,500 tokens, 12.1% use between 2,500 to 10,000. Notably, another 12.1% even exceed 10,000 tokens.\r\n\r\n<a id='167f3e67-a880-44df-8d7e-3fe57a4ec919'></a>\r\n\r\n*RQ2 Finding #5*: Deployment prompt lengths vary widely: while half are short (<500 tokens), a significant long tail (12%) exceeds 10,000 tokens to handle complex contexts.\r\n\r\n<a id='03bdf76b-21cf-4abd-88b8-8d4b31a58416'></a>\r\n\r\n## 5.4. Agent Architecture\r\n\r\nWe explore the core architectural patterns that support production deployment. To ensure clarity, we adopt the terminologies visualized in Figure 22: an agent completes a high-level *task*, which decomposes into logical *subtasks*, consisting of granular atomic *steps* (e.g., model calls, tool use).\r\n\r\n<a id='e905da57-7054-47b9-8ea2-46aabd9e7532'></a>\r\n\r\n**Number of Steps.** We find that production agents tend to follow _structured workflows_ with bounded autonomy. We ask survey participants how many steps their deployed systems execute within a subtask before requiring human input. Most systems operate within tight bounds: 46.7% of deployed survey agents complete only 1–4 steps, and an additional 21.7% perform 5–10 steps (Figure 8c). A smaller subset (16.7%) extends to tens of steps, while only 6.7% report systems with no explicit step limit. Interestingly when we expand the analysis to include all agents including both deployed and not yet deployed agents in Figure 16c, the distribution shifts toward substantially higher step counts. This indicates that prototypes and research systems are more likely to run tens of steps or have no explicit limit on au-\r\n\r\n<a id='5a6859f4-e713-4df1-a8fd-ef9272a3adc9'></a>\r\n\r\n9\r\n\r\n<!-- PAGE BREAK -->\r\n\r\n<a id='6a994bbb-e9c1-4eff-8a01-cc67cad96670'></a>\r\n\r\nMeasuring Agents in Production\r\n\r\n<a id='def86bdc-ff79-49ba-829f-42475c369a83'></a>\r\n\r\n<::bar chart: (a) Number of distinct models combined to solve a single logical task (N=22). The y-axis represents '% of Responses' from 0% to 50%. The x-axis represents 'Number of Distinct Models'.\r\n- 1: 41% (9 responses)\r\n- 2: 27% (6 responses)\r\n- 3: 19% (4 responses)\r\n- 4+: 14% (3 responses)\r\n\r\n(b) Distribution of prompt lengths in terms of tokens (N=33). The y-axis represents '% of Responses' from 0% to 30%. The x-axis represents 'Instruction Length (tokens)'.\r\n- 0-250: 28% (9 responses)\r\n- 250-500: 24% (8 responses)\r\n- 500-2.5k: 24% (8 responses)\r\n- 2.5k-10k: 12% (4 responses)\r\n- 10k+: 12% (4 responses)\r\n\r\n(c) Number of autonomous execution steps before user intervention (N=60). The y-axis represents '% of Responses' from 0% to 50%. The x-axis represents 'Autonomous Steps/Cycles'.\r\n- 1-4: 47% (28 responses)\r\n- 5-10: 22% (13 responses)\r\n- Tens: 17% (10 responses)\r\n- Hundreds: 5% (3 responses)\r\n- Thousands: 3% (2 responses)\r\n- No limit: 7% (4 responses)::>\r\n\r\nFigure 8. Overview of core components configurations and architectures in deployed Agentic AI systems:\r\n\r\n<a id='ef03700d-1a4a-475d-81fa-c1aac67fee41'></a>\r\n\r\ntonomous cycles, reflecting more aggressive exploration of\r\nopen-ended autonomy during early development.\r\n\r\n<a id='0aef0924-0243-406f-9a01-b89bcc5ddded'></a>\r\n\r\nPractical constraints drive this design choice. Case study participants identify problem complexity, non-determinism in agent self-planning, and latency requirements as key limiting factors. Practitioners intentionally impose limits on reasoning steps to maintain reliability and manage computational time and costs. This simplicity reflects a broader preference for predictable, controllable workflows over experimental open-ended autonomy in production environment.\r\n\r\n<a id='6c1c62b9-99ee-4f85-9913-8196cb839867'></a>\r\n\r\n**Number of Model Calls.** While distinct from logical steps (which often include non-inference actions like tool execution), we specifically analyze *model calls* to gauge the inference intensity of deployment systems. We observe that within a single subtask, deployment systems typically execute model calls on the order of tens or less. The majority (66.7%) of deployed survey agents use fewer than 10 calls per subtask, with 46.7% using fewer than 5 calls. This is followed by 33.3% using tens of calls, 9.0% in the hundreds, and 6.1% in the thousands. Interestingly, another 12.1% do not set explicit limits. This aligns with our detailed case studies, where 3 teams confirm executing tens of model calls per subtask (up to 40–50 calls).\r\n\r\n<a id='d03fbc00-6a39-491f-8116-3dab8b3de889'></a>\r\n\r\nExperimental systems are far more likely to sit in the long\r\ntail, with many agents routinely making tens of calls. In con-\r\ntrast, deployed agents concentrate in the lower-call regime,\r\nsuggesting that teams aggressively cap or refactor call bud-\r\ngets as they move from experimentation to deployment to\r\nprobably control cost, latency, and failure amplification.\r\n\r\n<a id='e173c12c-3da3-47d6-88dc-e582e2f5622f'></a>\r\n\r\nDespite the pattern of limited model calls, 31% of deployed survey agents already use various inference-time scaling techniques, compared to 44% in experimental sys-\r\n\r\n<a id='cdb0e373-03d4-49b5-a15c-eddc8d409803'></a>\r\n\r\ntems. While this figure currently includes simpler methods like composing outputs from multiple models (Section 5.1), future work may determine how advanced techniques—such as self-planning, search-based reasoning, and self-verification—perform in production, as these approaches may then lead to higher numbers of model calls and steps before human intervention.\r\n\r\n<a id='1c401b0e-5ee7-4246-91f2-7bbd4209bef2'></a>\r\n\r\nRQ2 Finding #6: Agents operate with tightly bounded autonomy: 68% of systems execute fewer than ten steps and 46.7% with less than 5 model calls before requiring human intervention.\r\n\r\n<a id='836fed38-cf91-4925-9527-55e0a201ab51'></a>\r\n\r\n**Agent Control Flow.** We observe that production agents favor predefined static workflows over full open-ended autonomy. We find that 80% of our detailed case studies utilize a structured control flow. These agents operate within well-scoped action spaces rather than freely exploring the environment to self-determine objectives.\r\n\r\n<a id='fb823634-2d41-4514-86e9-0c6fd2e34ad9'></a>\r\n\r\nDetailed case studies provide insight into these control flow patterns. Nine cases utilize various forms of agentic Retrieval-Augmented Generation (RAG) pipelines, ranging from single agents retrieving information via tool calls to sophisticated pipelines with over 20 subtasks that explicitly configure retrieval at certain steps. For example, one insurance agent follows a fixed sequence: coverage lookup, medical necessity review, and risk identification. While the agent possesses autonomy to complete each subtask (e.g., deciding if a case needs human intervention for risk identification), the high-level objective and expected output of each subtask remain fixed.\r\n\r\n<a id='e0e47c68-dfa3-4ebf-af01-1d52fec6aaa7'></a>\r\n\r\nOpen-ended autonomy remains rare. We observe only one\r\n---\r\n\r\n‡This is a simplified example workflow to illustrate the core\r\nlogic, redacted to protect the anonymity of the interviewee.\r\n\r\n<a id='52e4beb4-6b96-4a4b-be77-bf772d6281e6'></a>\r\n\r\n10\r\n\r\n<!-- PAGE BREAK -->\r\n\r\n<a id='9128c348-0122-4fdc-9e58-d0e299819a3c'></a>\r\n\r\nMeasuring Agents in Production\r\n\r\n<a id='507b344a-bdaa-41b5-96f8-3c36ce047d1b'></a>\r\n\r\ncase where agents operate with unconstrained exploration.\r\nNotably, this system runs exclusively in a sandbox environment with rigorous CI/CD verification on the final outputs,\r\navoiding direct interaction with production environment.\r\n\r\n<a id='d7bcf911-1e00-43a7-8016-bb8dc357b778'></a>\r\n\r\nHowever, we identify a growing interest in expanding au-\r\ntonomy. Four detailed case studies employ a planning and\r\nrouting agent to decompose input requests and dispatch\r\nthem to task-specialized agents. Another team specializes\r\nagents into generators and verifiers, enabling greater auton-\r\nomy through automated verification. Several teams share\r\nthat they are experimenting with flexible workflows by al-\r\nlowing agents to make autonomous decisions about next\r\nsteps or by using planning and orchestration agents.\r\n\r\n<a id='dbd56a82-8e89-4136-a1a9-7662560afc59'></a>\r\n\r\nRQ2 Finding #7: Deployment architectures favor predefined, structured workflows over open-ended autonomous planning to ensure reliability.\r\n\r\n<a id='79a8ebd7-e65a-478d-8e0a-7ad0b5a32727'></a>\r\n\r\n## 5.5. Agentic Frameworks\r\n\r\nWe find a divergence in framework adoption between survey respondents and interview case studies. Among deployed agents from the survey, two-thirds (60.7%) use third-party agentic frameworks. Reliance concentrates around three primary frameworks: LangChain/LangGraph [80, 81] leads with 25.0%, followed by CrewAI [82] at 10.7%, with LLa-MAIndex [83] and OpenAI Swarm [84] both at 3.6% (Figure 9).\r\n\r\n<a id='009ec092-1752-4dd5-939b-2300fb622ce5'></a>\r\n\r\nIn sharp contrast, our detailed case studies reveal a strong\r\npreference for custom in-house agent implementations.\r\nOnly 3 of 20 (15%) detailed case studies rely on external\r\nagent frameworks (2 use LangChain, 1 uses BeeAI). The\r\nremaining 17 teams (85%) build their agent application en-\r\ntirely in-house with direct model API calls. For example,\r\none interview case explicitly shared that their agents are\r\ntheir own implementation of ReAct loops. Notably, two ad-\r\nditional teams report starting with frameworks like CrewAI\r\nduring the experimental prototyping phase but migrating\r\nto custom in-house solutions for production deployment to\r\nreduce dependency overhead.\r\n\r\n<a id='1a19b335-37e3-4f9a-b935-ee8bb8dd5b62'></a>\r\n\r\nWe identify three core motivations for building custom so-\r\nlutions from the detailed case studies. First, *flexibility and*\r\n*control* are critical. Deployed agents often require vertical\r\nintegration with proprietary infrastructure and customized\r\ndata pipelines that rigid frameworks struggle to support.\r\nFor example, one agent-native company deploys customer-\r\nfacing agents across varied client environments, necessi-\r\ntating a bespoke orchestration layer. Second, *simplicity*\r\ndrives the decision. Practitioners report that core agent\r\nloops are straightforward to implement using direct API\r\ncalls. They prefer building minimal, purpose-built scaffolds\r\nrather than managing the dependency bloat and abstraction\r\nlayers of large frameworks. Third, *security and privacy poli-*\r\n\r\n<a id='f4a56e6a-ccde-4b41-bb06-0332883bd150'></a>\r\n\r\n<::bar chart: Y-axis: % of Responses. X-axis: No (Did Not Use Any Framework) and Yes (Used Framework). Legend: OpenAI Swarm, LlamaIndex, CrewAI, Other, LangChain/LangGraph. Data: No: 39.3%. Yes: 60.7% total, broken down into LangChain/LangGraph: 25.0%, Other: 17.9%, CrewAI: 10.7%, LlamaIndex: 3.6%, OpenAI Swarm: 3.6%. Figure 9. Frameworks reported to support critical functionality among those using open frameworks for production systems (N = 29). Additional framework options were provided in the survey; see Appendix E.3 for the complete question and options.: chart::>\r\n\r\n<a id='ad93da36-7328-49a3-afb0-3e1510441dc9'></a>\r\n\r\ncies sometime prohibit the use of certain external libraries\r\nin enterprise environments, compelling teams to develop\r\ncompliant solutions internally.\r\n\r\n<a id='973a832f-6a43-4e53-9912-509da5043296'></a>\r\n\r\nRQ2 Finding #8: Framework adoption varies significantly between survey and case study. While third-party frameworks get broad adoption in the survey (61%), interviewed teams predominantly build custom in-house implementations (85%) to maximize control and minimize dependency bloat.\r\n\r\n<a id='34b07392-7e55-4798-92da-a1a06a304532'></a>\r\n\r\n## 6. RQ3 Results: How Are Agents Evaluated For Deployment?\r\n\r\nEvaluation practices shape which agentic systems reach production and how teams iterate on deployed systems. We investigate how practitioners evaluate and test their agents to meet production deployment requirements, examining both offline evaluation during development and online evaluation in production environments. Specifically, we examine two aspects: what practitioners compare their systems against (baselines and benchmarks), and what methods they use to verify system outputs (evaluation methods).\r\n\r\n<a id='45f8d793-d67e-4945-8e6b-c8f514d6b729'></a>\r\n\r\nOur findings reveal that evaluation practices vary widely across production agents, even within the same application domain, shaped by the specific requirements of each deployment context and the availability of ground truth data. Notably, practitioners currently focus on agent output quality and correctness rather than traditional software reliability metrics. Based on 20 detailed case studies, no team reports applying standard production reliability metrics such as five 9s availability to their agent systems. Instead, evaluation centers on whether agents produce correct, high-quality responses.\r\n\r\n<a id='dccf6410-1d71-4239-a66a-50fdabc79550'></a>\r\n\r\n11\r\n\r\n<!-- PAGE BREAK -->\r\n\r\n<a id='4c30f048-711a-4806-9028-109c889df368'></a>\r\n\r\nMeasuring Agents in Production\r\n\r\n<a id='cd53db4e-52d4-4708-92b9-640c1d03e633'></a>\r\n\r\n## 6.1. Baselines and Benchmarks\r\nDuring development, teams conduct offline evaluation to assess agent performance before deployment. Figure 10a shows that 38.7% of survey respondents compare their deployed agentic systems against non-agentic baselines such as existing software systems, traditional approaches, or human execution. The remaining 61.3% do not perform baseline comparisons. Among these, 25.8% report their systems are truly novel with no meaningful baseline for comparison. While we do not know reasons behind why the remaining 35.5% did not conduct the comparison, in-depth interviews reveal that baseline comparison is often challenging even when alternatives exist. One reason is that non-agentic baselines frequently combine multiple components, making systematic technical comparison difficult. The HR support agent development team illustrates that agent solution replace a baseline process combining company document lookup, human procedures, and non-LLM HR software. While outcomes such as task completion time are measurable, isolating technical performance for comparison is challenging.\r\n\r\n<a id='d9d1c7cb-7dc2-4683-9882-69d9c2dcdd72'></a>\r\n\r\n/* Lines 891-2691 omitted */\r\n47\n</attachment>\n\n</attachments>\n<context>\nThe current date is January 4, 2026.\nTerminals:\nTerminal: powershell\nLast Command: uv sync\nCwd: D:\\GPT\\paper-reader\nExit Code: 0\nTerminal: powershell\nLast Command: & D:\\GPT\\paper-reader\\.venv\\Scripts\\Activate.ps1\nCwd: D:\\GPT\\paper-reader\nExit Code: 0\n\n</context>\n<editorContext>\nThe user's current file is d:\\GPT\\paper-reader\\self-evolving-agents\\2512.04123v1.md. \n</editorContext>\n<reminderInstructions>\nWhen using the replace_string_in_file tool, include 3-5 lines of unchanged code before and after the string you want to replace, to make it unambiguous which part of the file should be edited.\nFor maximum efficiency, whenever you plan to perform multiple independent edit operations, invoke them simultaneously using multi_replace_string_in_file tool rather than sequentially. This will greatly improve user's cost and time efficiency leading to a better user experience. Do not announce which tool you're using (for example, avoid saying \"I'll implement all the changes using multi_replace_string_in_file\").\nDo NOT create a new markdown file to document each change or summarize your work unless specifically requested by the user.\n\n</reminderInstructions>\n<userRequest>\nwhats this paper is about\n</userRequest>\n"
            },
            {
              "type": 3,
              "cacheType": "ephemeral"
            }
          ],
          "renderedGlobalContext": [
            {
              "type": 1,
              "text": "<environment_info>\nThe user's current OS is: Windows\n</environment_info>\n<workspace_info>\nI am working in a workspace with the following folders:\n- d:\\GPT\\paper-reader \nI am working in a workspace that has the following structure:\n```\nagent.md\nparse_with_llamaparse.py\npyproject.toml\nothers/\n\t204c24c5-7411-4631-8ea6-305649dd156a.md\n\t204c24c5-conversation.md\n\t2511.14214v1.md\n\t2511.19757v1.md\n\t2512.03413v1.md\n\t2512.16922v1.md\n\t73f5769f-333c-4ba3-b751-533aeb157422.md\n\t96e34750-4a80-462f-953e-628a10a816c6.md\n\tde640784-9f94-420e-ab02-c7b8809d09e2.md\n\te0c663d0-cee2-44c3-a902-754aa3186279.md\n\tf47afba4-2787-453b-b3bb-d804f68dbd32.md\nscripts/\n\textract_pdfs.py\n\tpaper-analysis-skill.md\n\tpaper-reading.md\n\tparse_pdfs.py\n\tSKILL_LLAMAPARSE.md\n\tSKILL.md\nself-evolving-agents/\n\t24eb981f-8040-4f3c-97fe-0e982226d25f.json\n\t24eb981f-8040-4f3c-97fe-0e982226d25f.md\n\t2508.16153v2.md\n\t2510.01051v1.md\n\t2511.20857v1.md\n\t2511.21689v1.md\n\t2512.04123v1.md\n\t2512.04868v1.md\n\t2512.05765v1.md\n\t2512.16301v1.md\n\t32960e66-927a-416e-85c8-12f089812d0c.md\n\t76083b1b-07d6-4099-9b17-e4da08599706.md\n\t77246551-482e-4312-99b4-786192a59e5e.md\n\ts10462-025-11422-4.md\n```\nThis is the state of the context at this point in the conversation. The view of the workspace structure may be truncated. You can use tools to collect more context if needed.\n</workspace_info>\n"
            },
            {
              "type": 3,
              "cacheType": "ephemeral"
            }
          ],
          "cacheKey": "file:///d%3A/GPT/paper-reader",
          "toolCallRounds": [
            {
              "response": "This paper, **\"Measuring Agents in Production\"**, is the first large-scale systematic study of how AI agents are actually built and deployed in real-world production environments.\n\n## Core Focus\n\nThe researchers surveyed 306 practitioners and conducted 20 in-depth interviews to understand:\n- Why organizations build agents\n- How they build them technically  \n- How they evaluate them\n- What challenges they face\n\n## Key Findings\n\n**1. Simple & Controlled Approaches Dominate:**\n- 68% of agents execute ≤10 steps before requiring human intervention\n- 70% use off-the-shelf models with prompting (no fine-tuning)\n- 74% rely primarily on human evaluation\n- Teams deliberately constrain autonomy to maintain reliability\n\n**2. Productivity Drives Adoption:**\n- 73% deploy agents to increase efficiency and reduce manual task time\n- Most applications span 26 diverse domains (finance, healthcare, legal, etc.) - far beyond just coding\n- 92.5% serve human users directly\n\n**3. Reliability is the Main Challenge:**\n- Ensuring correctness is the top concern\n- Teams favor human-in-the-loop evaluation over automated methods\n- Production agents use structured workflows rather than open-ended autonomy\n\n**4. Technical Patterns:**\n- Proprietary frontier models (Claude, GPT) dominate\n- Manual prompt engineering preferred over automated optimization\n- Most teams build custom implementations rather than using frameworks (85% in interviews)\n\n## Bottom Line\n\nThe paper reveals that **successful production agents today are deliberately simple, human-supervised systems** rather than fully autonomous ones. Teams trade capability for reliability, and this pragmatic approach already delivers measurable value across diverse industries.",
              "toolCalls": [],
              "toolInputRetry": 0,
              "id": "d4461801-bff1-4904-8d64-40ce23f18f9c",
              "thinking": {
                "id": "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",
                "text": "The user is asking me to explain what this paper is about. I have access to the paper file \"2512.04123v1.md\" which appears to be titled \"Measuring Agents in Production\". \n\nLooking at the content provided in the attachment, I can see:\n\n1. The paper is titled \"Measuring Agents in Production\"\n2. It's authored by researchers from UC Berkeley, UIUC, Stanford, IBM Research, and Intesa Sanpaolo\n3. The abstract indicates this is \"the first large-scale systematic study of AI agents in production\"\n\nLet me read through the key sections to understand what the paper is about:\n\n**Main Topic**: The paper studies how AI agents are built, evaluated, and deployed in production environments across various industries.\n\n**Research Questions**:\n- RQ1: What are the applications, users, and requirements of agents?\n- RQ2: What models, architectures, and techniques are used to build deployed agents?\n- RQ3: How are agents evaluated for deployment?\n- RQ4: What are the top challenges in building deployed agents?\n\n**Methodology**: They conducted:\n- A survey with 306 responses (filtered to 86 deployed agents for main analysis)\n- 20 in-depth case study interviews\n- Coverage of 26 different domains\n\n**Key Findings**:\n1. Production agents use simple, controllable approaches\n2. 68% execute at most 10 steps before human intervention\n3. 70% rely on prompting off-the-shelf models instead of fine-tuning\n4. 74% depend primarily on human evaluation\n5. Reliability is the top development challenge\n6. Agents are primarily used to increase productivity and automate routine tasks\n7. Most agents serve human users (92.5%)\n8. Teams favor established methods over experimental techniques\n\nThis is a comprehensive empirical study documenting the current state of AI agents in production environments, bridging the gap between research and real-world deployment.\n\nI should provide a clear, concise summary for the user."
              }
            }
          ],
          "modelMessageId": "78068feb-1361-4f48-a5ab-56c8eae26010",
          "responseId": "18758470-ec8e-44fe-abbc-31af40be5df7",
          "sessionId": "e43783dd-145b-4ede-9bf7-34bff8d6fbaf",
          "agentId": "github.copilot.editsAgent"
        },
        "details": "Claude Sonnet 4.5 • 1x"
      },
      "responseMarkdownInfo": [],
      "followups": [],
      "modelState": {
        "value": 1,
        "completedAt": 1767509694396
      },
      "contentReferences": [],
      "codeCitations": [],
      "timeSpentWaiting": 0
    }
  ]
}