<a id='a17f655e-6b9f-4fad-9931-c0f82cd7b71a'></a>

SEAL: Self-Evolving Agentic Learning for
Conversational Question Answering over Knowledge
Graphs

<a id='8ec54079-d518-459d-8819-92b8f7cb0b69'></a>

Hao Wanga, Jialun Zhongb, Changcheng Wangb, Zhujun Niec, Zheng Lie,
Shunyu Yaoa, Yanzeng Lid,b,1, Xinchi Lia,1

a_Institute of Big Data and Artificial Intelligence, China Telecom Research Institute, Beijing, 102209, China_
b_Wangxuan Institute of Computer Technology, Peking University, Beijing, 100871, China_
c_School of Artificial Intelligence, China University of Geosciences (Beijing), Beijing, 100083, China_
d_Institute of Artificial Intelligence and Future Networks, Beijing Normal University, Zhuhai, 519087, Guangdong, China_
e_Center for Cognition and Neuroergonomics, State Key Laboratory of Cognitive Neuroscience and Learning, Beijing Normal University, Zhuhai, 519087, Guangdong, China_

<a id='f4e6eb0f-925c-4782-bb01-2a73cb830dcf'></a>

## Abstract

Knowledge-based conversational question answering (KBCQA) confronts persistent challenges in resolving coreference, modeling contextual dependencies, and executing complex logical reasoning. Existing approaches, whether end-to-end semantic parsing or stepwise agent-based reasoning---often suffer from structural inaccuracies and prohibitive computational costs, particularly when processing intricate queries over large knowledge graphs. To address these limitations, we introduce SEAL, a novel two-stage semantic parsing framework grounded in self-evolving agentic learning. In the first stage, a large language model (LLM) extracts a minimal S-expression core that captures the essential semantics of the input query. This core is then refined by an agentic calibration module, which corrects syntactic inconsistencies and aligns entities and relations precisely with the underlying knowledge graph. The second stage employs template-based completion, guided by question-type prediction and placeholder instantiation, to construct a fully

<a id='5aa5ec6b-34a6-488a-90bf-bf41d09d3afd'></a>

--- 

*Corresponding Author

<a id='121e6341-20e2-46b1-a553-ee07153415bc'></a>

arXiv:2512.04868v1 [cs.CL] 4 Dec 2025

<!-- PAGE BREAK -->

<a id='499a0e65-b22c-4058-9dcd-8f64909324df'></a>

executable S-expression. This decomposition not only simplifies logical form
generation but also significantly enhances structural fidelity and linking ef-
ficiency. Crucially, SEAL incorporates a self-evolving mechanism that inte-
grates local and global memory with a reflection module, enabling continuous
adaptation from dialog history and execution feedback without explicit re-
training. Extensive experiments on the SPICE benchmark demonstrate that
SEAL achieves state-of-the-art performance, especially in multi-hop reason-
ing, comparison, and aggregation tasks. The results validate notable gains
in both structural accuracy and computational efficiency, underscoring the
framework's capacity for robust and scalable conversational reasoning.

<a id='c808b596-6ff7-41ba-b377-ceedcad8b49b'></a>

Keywords: Knowledge-based Question Answering, Agent,
Self-Improvement, Large Language Model, Semantic Parsing

<a id='d3584eed-5532-48fd-bbd3-a1fd9cb15bbb'></a>

# 1. Introduction
A Knowledge Graph (KG) is a structured representation of knowledge, typically organized as triples (head entity, relation, tail entity) to encode factual information [1]. In recent years, KGs have gained widespread attention in both academia and industry [2, 3]. Knowledge-based Question Answering (KBQA) systems are designed to query these structured KGs, using reasoning to provide accurate answers to natural language questions [4, 5]. Among KBQA methods, Semantic Parsing (SP) based approaches translate questions into structured queries (e.g., SPARQL, Cypher, etc.) for execution against the KG, offering strong interpretability and high efficiency [6, 7]. These systems are widely applied in fields such as healthcare and business, significantly reducing the technical threshold for accessing complex knowledge systems.

Knowledge-based conversational QA (KBCQA) extends this paradigm to multi-turn interactive scenarios, requiring the system to conduct continuous reasoning and to address dialog understanding challenges such as coreference resolution [8, 9]. For this task, SP remains a mainstream approach, where the goal is to convert contextual natural language queries into executable logical forms. With the emergence of large language models (LLMs) [10, 11], SP increasingly leverages their advanced language understanding capabilities [12, 13, 14], primarily through two paradigms: end-to-end logical form generation and agent-based stepwise construction.

<a id='eb15477a-84bb-40ee-ad3a-9045cf21876a'></a>

While LLMs offer significant opportunities for SP-based KBQA, and
KBCQA tasks, current methods face substantial limitations in handling struc-

<a id='99c7996f-c4a4-4af4-ba9c-1b10f5cd0c94'></a>

2

<!-- PAGE BREAK -->

<a id='1da086a7-a688-4d14-8754-1b0a5780f83b'></a>

turally complex questions [15]. Specifically, generated logical forms often fail to fully capture semantic intent in scenarios requiring multi-hop reasoning, comparison, or aggregation operations [16, 17, 18]. This limitation is particularly evident in complex logical reasoning, where LLMs tend to focus on surface-level concepts while overlooking critical structural constraints imposed by the knowledge graph. Furthermore, the entity and relation linking process suffers from an expansive candidate space due to linguistic ambiguity [19, 20], leading to exponential growth in possible combinations and high computational overhead. This issue directly impacts reasoning generalization, LLMs often generate plausible but semantically invalid forms that ignore domain-specific validity constraints. These challenges hinder the scalability of SP-based KBQA systems, and are further exacerbated in the KBCQA setting, where the system must also manage dialog history to resolve coreferences and maintain contextual coherence. In particular, coreference resolution remains a major bottleneck that if but without aligning the resolved entity with its attributes in the knowledge graph, the final answer can still be inconsistent or incorrect.

<a id='6bd87727-b3d8-4320-b11f-da81771e2cd3'></a>

<::figure: knowledge graph::>
Legend:
- Red square: Factors only considered by LLM
- Green square: Factors not considered by LLM

The diagram shows a knowledge graph with various nodes and edges. The nodes are colored either red or green.

Nodes and their connections:
-   **Brazil** (green node) is connected to **President** (red node) and also has a dashed connection to **Soccer** (green node).
-   **President** (red node) is connected to **Country** (red node) and has a dashed connection to **2015** (green node).
-   **Country** (red node) is connected to **Capital** (red node) and has a dashed connection to **Car** (red node).
-   **Capital** (red node) is connected to **GDP** (green node) and has a dashed connection to **France** (green node).
-   **France** (green node) has a dashed connection to **City** (green node).
-   **2015** (green node) is connected to **Capital** (red node).
-   **GDP** (green node) is connected to **Olympic** (red node).
-   **USA** (green node) is connected to **GDP** (green node) and **World Cup** (green node).
-   **World Cup** (green node) is connected to **Olympic** (red node) and **Food** (green node).
-   **Olympic** (red node) has a dashed connection to **Soccer** (green node).

The overall diagram is labeled "Knowledge Graph".
<::>

Complex Logical Reasoning
Find the country whose capital city is governed by a
mayor appointed by the president, and the president's
term started after 2015.
option Brazil: [x]
option France: [ ]

Coreference Resolution
Which political party does he represent, and what is
the name of the city he was born in?
option Party: Liberal Party Born In: Glicério, São Paulo: [x]
option He represents the Workers' Party and was born in São Paulo.: [ ]

Reasoning Generalization
Among those countries, which one has hosted the
Olympics and has the highest GDP in 2023?
option Regarding the Olympics, the United States had the highest GDP in 2023.: [x]
option Yes, I'm sure. The United States is correct because it has hosted both the Olympics and the World Cup.: [ ]

Figure 1: The challenges of leveraging LLMs in KBCQA.

A key research problem in KBCQA is how to leverage LLMs to address
the challenges of generating complex logical structures and the high com-

<a id='05d5b17e-d66f-49d1-8051-4526fe5a76d0'></a>

3

<!-- PAGE BREAK -->

<a id='fe65af78-3efd-4904-a6ab-1e9f197349d6'></a>

putational cost of entity linking, as shown in Figure 1. To this end, this
article introduces Self-Evolving Agentic Learning (SEAL), a two-stage SP
framework. SEAL leverages S-expressions, a structured logical form. This
clear and readable structure is particularly advantageous for representing the
complex and discrete operations required by the KBCQA task [7, 6].

<a id='80386b53-7b0f-4060-be62-9fbd72aac52f'></a>

In the first stage, the LLM generates a preliminary S-expression core,
which is then semantically calibrated by an agent to correct structural errors.
In the second stage, the LLM completes the logical structure by integrating
the validated core with predefined templates, producing an accurate and
executable S-expression. Crucially, SEAL incorporates a self-evolving mech-
anism that establishes a continuously learning agent through the synergy
of local memory, global memory, and a reflection module. This mechanism
enables the system to adaptively learn from successful past dialogs and ex-
ecution outcomes, transforming global memory from static storage into a
dynamically updateable knowledge base without explicit retraining. This
approach effectively combines the semantic understanding of LLMs with the
structural rigor of templates, improving the accuracy of complex query gen-
eration in KBCQA while maintaining high efficiency.

<a id='7c8b5a68-1087-4e86-88f9-cbf8537400a6'></a>

We summarize the contributions of this paper as follows:

* We introduce the concept of a minimal S-expression core to represent the essential semantics of a query. This core is calibrated by an agent for syntactic correctness and knowledge graph alignment, forming a robust foundation for the final query construction.
* We propose SEAL, a two-stage agentic learning framework for the KBCQA task, in which innovations are decomposition of SP into S-expression core extraction followed by agent-driven calibration and template-based composition, significantly enhancing structural accuracy and computational efficiency in complex reasoning.
* We design a self-evolving mechanism that enables continuous performance enhancement through dynamic memory updates and reflection, allowing the system to adapt to novel expressions in real-world dialogs without retraining.
* Extensive experiments on the SPICE benchmark show that our method achieves state-of-the-art results, particularly in complex reasoning tasks, with the self-evolving mechanism demonstrating significant performance

<a id='2de1d396-69b9-4123-b708-da78a0c09163'></a>

4

<!-- PAGE BREAK -->

<a id='e6d92e71-77f0-49f8-a02e-132b0ee41095'></a>

improvements as dialog progresses. The results validate significant im-
provement in both structural accuracy and efficiency.

<a id='ece4884d-8f3f-4f34-b33d-907f00aa6dba'></a>

The article is structured as follows: Section 2 reviews related works on SP in KBQA, KCBQA, and LLM-based agents. Section 3 provides preliminaries on knowledge graphs. Section 4 details our proposed method, including the reasoning, memory, and reflection modules. Section 5 presents the experimental setup, datasets, metrics, baselines, and main results. Section 6 concludes the article.

<a id='0a989836-1b71-45f2-a814-119830bc82a6'></a>

# 2. Related Work

In this section, we introduce work related to our research, covering SP-based KBQA, KBCQA, and LLM-based Agents.

## 2.1. Semantic Parsing in KBQA

Early research on KBQA focused on SP to translate natural language questions into structured queries, ensuring interpretability and logical reasoning. In an early work, a staged framework [21] decomposes query generation into entity recognition, inference chain construction, and constraint aggregation, providing a foundation for NL2GQL. Graph embeddings and constraint-based path control [22] reduce the search space in multi-hop reasoning, emphasizing structural efficiency. Query construction as a state-transition process [23] employs node identification, connection, merging, and folding operations for dynamic semantic dependencies. With the rise of LLMs, KBQA shifted towards data-driven paradigms, with few-shot prompting [24] reducing reliance on annotated data and enhancing adaptability. Query generation aligned with code synthesis paradigms [25] leverages structured programming syntax. A generation and retrieval strategy [26] improves multi-hop reasoning, while external knowledge retrieval [27] enriches logic forms, addressing knowledge incompleteness. Agentic approaches facilitate dynamic query construction, with symbolic agents [28] improving the precision of relational inference. Step-by-step reasoning within a thought action cycle [29] enables progressive refinement. A mechanism of observation, action, and reflection [30] enhances robustness. Planning, retrieval, and reasoning [31] support structured query generation on heterogeneous graphs.

<a id='249c5fe8-ecb1-4314-9c9e-889604795816'></a>

5

<!-- PAGE BREAK -->

<a id='8cd0e18f-50c6-4e0f-b360-8b853cf936c2'></a>

## 2.2. Knowledge-based Conversational QA

KBCQA presents greater challenges than single-turn tasks, particularly in modeling dialog history and integrating heterogeneous knowledge. A comprehensive survey [32] traces the evolution of CQA, identifying context modeling issues. The Complex Sequential QA (CSQA) Dataset [33] incorporates dialog history into complex reasoning, while the CONVINSE framework and the ConvMix dataset [34] enable multi-source reasoning. Datasets with SPARQL annotations [35] support consistent logic parsing across turns. We choose the S-expression to represent the logical form of questions [36]. Proposed by Gu et al. [37], S-expressions are a Lisp-based format that uses functions to express logical relationships, widely applied in recent works such as KB-BINDER [24], KB-Coder [25], and Pangu [38]. Ambiguity resolution, including pronoun coreference and ellipsis, remains a challenge, with traditional and neural approaches [39] identifying limitations for rare entities. LLM-driven disambiguation strategies [40] address ellipsis and semantic ambiguity, while CoQA, SQuAD 2.0, and QuAC [41] reveal deficiencies in dynamic dialog. Question rewriting (QR) transforms context-dependent questions into self-contained forms, with a two-stage pipeline [42] improving retrieval. Reinforcement learning [43] optimizes rewriting via QA feedback. QR variants [44] enhance context representation. The REIGN framework [45] uses data augmentation and reinforcement learning. CornNet [46] integrates LLM-based rewriting with teacher-student architectures. Dialog history modeling integrates explicit memory and entity tracking, with Dialog-to-Action [47] resolving ellipsis via memory management. Graph neural networks [48] encode evolving subgraphs. LLMs with dynamic memory [49] synthesize diverse evidence. Reinforcement learning [50] tracks entities across multi-hop graphs. The Adaptive Context Management (ACM) framework [51] adjusts context windows for relevance. The KaFSP framework [52] integrates fuzzy reasoning, and LLM scalability [53] confirms effectiveness of few-shot prompting and fine-tuning.

<a id='f8e0867b-358e-412e-8590-2577a87f2b09'></a>

### 2.3. LLM-based Agents
Leveraging vast pre-training [54] on expansive corpora and subsequent instruction fine-tuning [55] across a diverse array of tasks, LLMs exhibit exceptional capabilities in representation, reasoning, and generation, facilitating their application across a broad spectrum of language-mediated challenges. This foundation has spurred significant interest in LLM-based agents

<a id='7d3d35a3-8eda-48ef-985b-90017a232d17'></a>

6

<!-- PAGE BREAK -->

<a id='21f344e5-ddbb-48b8-8050-06c04cfbf14b'></a>

[56], which have gained widespread attention [57, 58] due to their intellec-
tual proficiency, driving advancements in adaptive task execution. These
agents integrate LLMs into sophisticated, human-like cognitive frameworks,
incorporating key components such as perception [59, 60], strategic planning
[61, 62], and actionable execution [63, 64], enabling robust adaptation to
dynamic and complex environments [65]. Furthermore, specialized modules
tailored for long-term tasks enhance their efficacy: memory systems, encom-
passing symbolic [66] and textual [67] summaries of past interactions [68],
support sustained contextual awareness, while reflection mechanisms [66] fos-
ter self-evolution and adaptability. Our approach aligns with this paradigm,
harnessing these strengths to enable continuous performance enhancement
in response to streaming data, without the need for retraining.

<a id='ce2d6ac4-42db-4baa-a27c-6ba1d3c24605'></a>

## 3. Preliminaries

### 3.1. Knowledge Graph

Let E, R denote the sets of entities and relations respectively. A knowledge graph can be represented as $G = (E, R, T)$, where $T \subseteq E \times R \times E$ is the set of facts stored in the KG. A fact in T can be represented as a triple $(e_h, r, e_t)$, indicating that a directed relation $r \in R$ holds between a head entity $e_h \in E$ and a tail entity $e_t \in E$.

<a id='ed1df1e3-5943-4434-9fc5-65cb6eac024b'></a>

3.2. KG-based conversational QA
For the KG-based conversational QA task, a dialogue d \u2208 D consists of sequential turns of questions and answers d = (q1, a1, q2, a2,..., qn, an). The types of answers ai include sets of entities, Boolean values, aggregation quantities, etc., and need to be inferred based on the question qi, dialogue history Hi = (q1, a1, ..., qi\u22121, ai-1), and the given knowledge graph G. This process is represented in Equation 1:

<a id='6d1163ad-6b0d-4957-8566-a7ae2512a279'></a>

Hi w qi w G → ai (1)

<a id='5324f12d-d4a8-4334-a6d4-5798f0c9d07b'></a>

Moreover, some recent work parse the natural language question $q_i$ and map it onto an executable logic form $f_i \in F$ (e.g., SPARQL and S-expressions) on the KG, leading to an explicit reasoning process.

<a id='8a5b12f9-7ef6-415c-98ad-5f5868951355'></a>

7

<!-- PAGE BREAK -->

<a id='e37225f5-2115-49aa-903d-ad45c12f8415'></a>

## 4. Method

### 4.1. Overview
We propose a novel SP approach for KBCQA that uses LLMs to directly generate S-expressions. However, LLM outputs often contain ungrounded surface forms, and conventional entity and relation linking methods that rely on large candidate sets are computationally expensive. To enable efficient and accurate parsing, we introduce a lightweight calibration strategy that performs syntax correction and single-candidate KG alignment.

<a id='f73455e7-6d41-4b3a-90d8-6af3d45ed7ec'></a>

8

<!-- PAGE BREAK -->

<a id='a0f6dac3-7573-4a13-8001-2a3f21294417'></a>

Table 1: S-expression Functions
<table id="8-1">
<tr><td id="8-2">Function</td><td id="8-3">Return Type</td><td id="8-4">Description</td></tr>
<tr><td id="8-5">(JOIN r e)</td><td id="8-6">Entity set</td><td id="8-7">Inner join of e with r&#x27;s second elements</td></tr>
<tr><td id="8-8">(R r)</td><td id="8-9">(Entity, Entity) set</td><td id="8-a">Reverses each tuple (x, y) to (y, x)</td></tr>
<tr><td id="8-b">(AND e1 e2...)</td><td id="8-c">Entity set</td><td id="8-d">Intersection of input sets</td></tr>
<tr><td id="8-e">*(VALUES v1 v2...)</td><td id="8-f">Value set</td><td id="8-g">Set containing values v1, v2, etc.</td></tr>
<tr><td id="8-h">*(IS_TRUE s p o)</td><td id="8-i">Boolean</td><td id="8-j">True if triple (s, p, o) exists</td></tr>
<tr><td id="8-k">*(OR e1 e2 ...)</td><td id="8-l">Entity set</td><td id="8-m">Union of input sets</td></tr>
<tr><td id="8-n">(COUNT e)</td><td id="8-o">Integer</td><td id="8-p">Cardinality of set e</td></tr>
<tr><td id="8-q">*(DISTINCT u)</td><td id="8-r">Entity set</td><td id="8-s">Deduplicated version of set u</td></tr>
<tr><td id="8-t">*(GROUP_COUNT u)</td><td id="8-u">(Entity, Value) set</td><td id="8-v">Counts of distinct entities</td></tr>
<tr><td id="8-w">*(GROUP_SUM gc1 gc2)</td><td id="8-x">(Entity, Value) set</td><td id="8-y">Sums values from two group counts</td></tr>
<tr><td id="8-z">*(ALL b1 b2 ...)</td><td id="8-A">Boolean</td><td id="8-B">True if all inputs are true</td></tr>
<tr><td id="8-C">(ARGMAX gc)</td><td id="8-D">Entity set</td><td id="8-E">x where (x,y) has maximal y</td></tr>
<tr><td id="8-F">(ARGMIN gc)</td><td id="8-G">Entity set</td><td id="8-H">x where (x,y) has minimal y</td></tr>
<tr><td id="8-I">(LT gc n)</td><td id="8-J">Entity set</td><td id="8-K">x where (x,y) in gc and y &lt;n</td></tr>
<tr><td id="8-L">(LE gc n)</td><td id="8-M">Entity set</td><td id="8-N">x where (x,y) in gc and y ≤ n</td></tr>
<tr><td id="8-O">(GT gc n)</td><td id="8-P">Entity set</td><td id="8-Q">x where (x,y) in gc and y &gt;n</td></tr>
<tr><td id="8-R">(GE gc n)</td><td id="8-S">Entity set</td><td id="8-T">x where (x,y) in gc and y ≥ n</td></tr>
<tr><td id="8-U">*(EQ gc n)</td><td id="8-V">Entity set</td><td id="8-W">x where (x,y) in gc and y = n</td></tr>
</table>
Note: * indicates new S-expression functions introduced in this work.

<a id='b4061ca8-c2a7-448b-a8e1-899e4b4501fd'></a>

9

<!-- PAGE BREAK -->

<a id='7d47ffdd-41d9-4593-be3f-af4c0e3b9096'></a>

In Table 1, we introduce a method to extract an S-expression core that captures the essential semantics of a natural language question. This decomposition simplifies the generation process by first extracting relatively independent substructures, which can then be combined by instantiating a predefined logical template (template composition).

<a id='8c63b3f3-c907-4d9f-a9b6-269533454114'></a>

The generation process follows two stages as shown in Figure 2. First, the LLM generates candidate S-expression cores. An S-expression core is a simplified substructure of an S-expression which is composed of basic operations. These cores are calibrated by an agent interacting with the knowledge graph to produce refined variants. Second, the question type is predicted to select an appropriate template, and the LLM fills placeholders with functions, constants, or core expressions to produce the final S-expression.

<a id='73acd410-cbe4-4689-8201-a02b7311a800'></a>

<::The framework of our method: flowchart::> The flowchart illustrates the framework of a method, divided into several interconnected components. The main sections are 'Dialogue', 'Local Memory', and 'Global Memory / Logic Form Generation'.  On the left, the 'Dialogue' section is composed of three main boxes:

1.  **Dialogue History** (yellow box):
    *   Who was the Marquess of Saluzzo during the period 1475-1504?
    *   Ludovico II, Marquess of Saluzzo.
    *   - Could you tell me about the parents of that person?
    *   Did you mean Giorgia Bronzini?
    *   - Do you mean Ludovico II, Marquess of Saluzzo?
    *   -Yes. Specifically, I want to know the male parent.

2.  **Entities** (blue box):
    *   Ludovico II, Marquess of Saluzzo: noble_title
    *   Ludovico I, Marquess of Saluzzo: noble_title
    *   Thomas III of Saluzzo: noble_title
    *   Marguerite of Pierrepont: noble_person
    *   Saluzzo: location

3.  **Current Query** (yellow box):
    *   -Which male person was the parent of Ludovico II, Marquess of Saluzzo ?

An arrow points from 'Current Query' to the 'Local Memory' section. Another arrow points from the 'Answer Generator' to an 'Answer' box, which contains: 'Ludovico I, Marquess of Saluzzo.'

The top-right section is labeled **'Local Memory'**. It contains:

*   Two oval shapes: 'input' which feeds into 'Coreference Resolution' and 'Ellipsis Resolution'.
*   Arrows from 'Coreference Resolution' and 'Ellipsis Resolution' point to an 'AI' icon labeled 'LLM'.
*   A box labeled **'Core Generation'**:
    *   S-expression: (AND (JOIN (R father) Ludovico_II,_Marquess_of_Saluzzo) (JOIN instance_of common_name))
    *   S-expression Core: (AND (JOIN (R father) Ludovico_II,_Marquess_of_Saluzzo) (JOIN instance_of common_name))

Arrows indicate flow from 'LLM' to 'Global Memory' and from 'Core Generation' to 'Calibration'.

The rightmost section is broadly for **'Logic Form Generation'** and includes:

*   A box labeled **'Global Memory'**:
    *   (AND (JOIN (R P22) Q1063295) (JOIN P21 Q6581097) (JOIN P31 Q502895))
*   A box labeled **'Calibration'**.
*   A box labeled **'Linking'**.
*   A 'Combine' icon, which receives input from 'Calibration' and 'Linking'.
*   A box labeled **'Reflection'**:
    *   Question Type: simple, verify, count, compare, compare_and_count, optimize
    *   Template:
    *   x1": "(AND (JOIN (R father) Ludovico_II,_Marquess_of_Saluzzo) (JOIN instance_of common_name))"
    *   Alternatives:
    *   x1": "(AND (JOIN (R father) Ludovico_II,_Marquess_of_Saluzzo) (JOIN sex_or_gender male) (JOIN instance_of common_name))..."

Further connections within the 'Logic Form Generation' section:

*   An arrow from 'Linking' to 'Calibration'.
*   An arrow from 'Combine' to 'Reflection'.

At the bottom-middle, an 'Agent' icon is connected to a 'Knowledge Graph'. Arrows indicate interaction between the 'Agent' and 'Knowledge Graph', and from the 'Agent' to the 'Answer Generator' (represented by a gear icon). The 'Answer Generator' then points to the 'Answer' box.
<::>

<a id='220719d5-12ac-4bb5-a8da-7fd30cdc4c68'></a>

## 4.2. Reasoning Module
The core extraction phase, the initial critical step of the proposed method, focuses on deriving the S-expression core that encapsulates the essential se-mantics of natural language questions. This phase comprises two key steps:

* S-expression Core Generation: LLM analyzes the question text to identify independent query objects, employing five fundamental func-

<a id='fa372d54-cc59-4bc9-8a26-0e433de4bf28'></a>

10

<!-- PAGE BREAK -->

<a id='196ed71b-7064-41fe-b582-8a42c2b16949'></a>

tions: JOIN, R, AND, VALUES, and IS_TRUE to articulate their logical relationships, thereby generating the S-expression core.

<a id='f3e994ee-f341-461a-9c10-fa2430b32a61'></a>

*   **S-expression Core Calibration**: An agent interfacing with the knowledge graph refines the generated S-expression core by correcting syntactic errors and aligning entities and relations with the knowledge graph, yielding candidate variants.

<a id='bd61c03a-554e-49b6-91d0-a44026fbacf9'></a>

The key innovation of this phase lies in decomposing the complex task of S-expression generation into independent substructure extractions, establishing a foundation for subsequent template integration, for details regarding specific expressions, refer to Appendix B. Moreover, experimental validation confirms that this phased approach substantially reduces model learning complexity and enhances generation accuracy.

<a id='6e1dece0-9c2f-4d8f-803b-24f02584172a'></a>

### 4.2.1. S-expression Core Generation
We introduce the concept of the S-expression core, referring to a simplified subclass of S-expression structures, the specific patterns that may appear in the core of S-expressions are shown in Appendix C. Such cores typically involve only basic logical functions, such as JOIN, R, AND, VALUES, and IS_TRUE. In the context of natural language questions, an S-expression core generally corresponds to the queried objects or targets within the question. In SPARQL queries, it maps to the graph patterns inside the WHERE{...} clause.

<a id='708a36bb-51be-4979-ad0a-76874f66ce49'></a>

The entities and relations referenced in the S-expression core (e.g., as arguments in JOIN or AND) are elements of the underlying KG (G = (E, R,T)). While the LLM initially generates tokens, the subsequent calibration step aligns these with grounded entities e \u2208 E and relations r \u2208 R through KG linking, ensuring semantic consistency with the structured knowledge base.

<a id='51da8c8d-4cb8-4bc4-9ec9-01cf530ba836'></a>

To formally describe this generation process, we denote a natural language question as q in Equation 2. Through a constructed prompt P_q = Prompt(q), the input is provided to a LLM. Under the parameter space Θ_LLM, the LLM generates an S-expression core sequence Core* = (s_1, s_2, ..., s_L) by maximizing the conditional likelihood:

<a id='865f8d20-e258-4f35-bf10-6d1c02f66af6'></a>

Core* = arg max P(Core | Pq; ΘLLM)
Core
(2)

<a id='dbc6a028-f5b6-4884-8eab-df6e1dcfc9b8'></a>

Where q represents the original natural language question, Pq is the prompt constructed from q for LLM inference, Core denotes a candidate

<a id='9f57273c-5026-4a81-aa6e-71095b17407d'></a>

11

<!-- PAGE BREAK -->

<a id='7d52cb60-0f51-461f-95c0-514ef397a639'></a>

sequence of S-expression tokens, and Core* is the optimal core sequence se-
lected by the model. The parameter ΘLLM represents all learnable parameters
of the LLM, optimized during pretraining and fine-tuning to capture statis-
tical patterns in natural language. Each s_l in the sequence (_s_1, _s_2,...,_s_L)
corresponds to a token in the S-expression core, such as a function, an entity,
a relation, or a constant. _L_ is the length of the sequence.

<a id='f2d15386-eb4b-48a4-be74-cef8deeaaa17'></a>

The output of this generation process, denoted as Core*, is a preliminary representation of the question's intent. However, due to the inherent ambiguity of natural language and potential hallucinations in LLM outputs, this raw core may contain syntactic errors, unlinked surface forms, or incorrect function compositions. To address these issues, we decompose the overall core generation into two key phases. First, the LLM analyzes the question to identify independent query objects and synthesizes a preliminary core expression. Second, an agent refines this candidate by calibrating the syntactic structure and aligning its entities and relations with the underlying knowledge graph, producing multiple valid variants.

<a id='5ba5f3a8-fd84-4463-baa2-9321857c3926'></a>

4.2.2. *S-expression Core Calibration*
4.2.2.1 Light Linking

<a id='3ef17719-973a-4d71-ab71-6d914fec3ecd'></a>

Targeting KBCQA tasks [36], we implement conversions between extended S-expressions and SPARQL, enabling the transformation of SPARQL queries into corresponding S-expressions in the context of KBCQA tasks. Testing confirms that all SPARQL queries in the dataset can be successfully converted to S-expressions, which can then be converted back to SPARQL while maintaining consistency with the original query results.

<a id='adc3931d-2de3-406b-9910-f9bf44fed790'></a>

The proposed method employs a LLM to generate an initial S-expression draft based on the input question and annotated examples. As the LLM lacks access to the underlying knowledge graph, it produces element representa- tions using surface names rather than canonical entity or relation identifiers. Consequently, the draft cannot be directly executed as a SPARQL query and requires a subsequent linking process. A lightweight linking strategy is adopted, which first maps each surface-named element to the most seman- tically similar entity or relation in the knowledge graph. Corrections are then applied to address two common error types which are relationship in- version and type constraint errors. After completing the linking, the final S-expression is obtained and translated into an executable SPARQL query via a custom conversion function, enabling retrieval of the final answer from

<a id='a77c4c6b-f734-47da-91eb-c865de42b976'></a>

12

<!-- PAGE BREAK -->

<a id='78b7d0a3-eb3e-445f-9800-143baf4f9c32'></a>

the knowledge graph.

<a id='7daedd06-3f21-4b09-b5fa-a72617118dcf'></a>

In the entity and relation linking phase, this method discards the traditional entity candidate approach, retaining only the single candidate with the highest semantic similarity. Specifically, the embedding model encodes knowledge graph elements into vectors, retrieving the best match via cosine similarity. Given that LLM-generated S-expressions are semantically precise, single entity candidate suffices without degrading linking performance. In contrast, the entity candidate approach which keeps three entities can produce non-empty but semantically incorrect queries, masking linking errors and leading to wrong answers. The single entity candidate strategy, by comparison, enforces stricter semantic alignment and thus ensures higher consistency.

#### 4.2.2.2 Main Procedure of Core-Calibaration

<a id='bc6549bd-1e66-4faa-b6e7-67bf48e452b9'></a>

After the LLM generates an S-expression core Core_j, a calibration phase ensures both syntactic correctness and semantic grounding within the knowledge graph. This process consists of two key steps.

<a id='45aba890-4ba5-417d-9d07-3d0d94c89bc4'></a>

The first step performs syntactic correction. The agent parses the initially generated S-expression core Core_j and corrects any syntactic errors to produce a structurally valid variant Core'_j. The correction function Corr_syn (·) detects mismatched parentheses, function argument errors, or illegal nesting structures, returning a syntactically well-formed expression:

<a id='1e2ae1be-b35e-4f19-ae30-8c86ee23035b'></a>

Core'_j = Corr_syn(Core_j) (3)

<a id='1fcc656e-9c0e-44fd-82d1-4b3d99e0a114'></a>

In Equation 3, Core_j denotes the initial S-expression core generated by the LLM, and Core'_j represents the corrected output. The correction function Corr_syn(.) performs rule-based structural validation to fix mismatched parentheses, incorrect function arity, or illegal nesting patterns.

<a id='81760d16-a7f6-4500-83b8-cc7c738bdd49'></a>

The second step conducts knowledge graph linking to replace each surface name $s$ within $Core'$, such as entity or relation names, with the most semantically similar element $x^*$ selected from the candidate set $C_s$, which is retrieved from the knowledge graph $G = (E, R, F)$. The optimal match is determined by computing cosine similarity over vector embeddings in Equation 4:

<a id='49418f3e-0468-4514-9767-d679d5ece64b'></a>

x* = arg max Cos(Embed(s), Embed(x))
x∈Cₓ

(4)

<a id='1d43f4fd-67cd-43d0-83a1-e0ef8039613a'></a>

The embedding function Embed(.) converts surface names or knowledge
graph elements into vector representations, typically implemented via pre-

<a id='bad67ab6-7219-4457-b5ea-84903c405514'></a>

13

<!-- PAGE BREAK -->

<a id='0bc9930f-3bd4-4b60-a28c-2575d3244bf6'></a>

trained language models such as an embedding model. The similarity function Cos(.,.) measures alignment between vectors, with higher values indicating stronger semantic similarity. The candidate set C_s consists of entities or relations from G potentially corresponding to the surface form s. Each x* is the best-matching element from C_s according to embedding similarity.

<a id='069cc17b-0538-45a9-93b2-6cde1744dbdf'></a>

Each surface element $s_m$ in $Core'_j$ is substituted with its optimal match $x^*_m$ to form calibrated candidates. The final set $Calibrated\_Core_j$ retains the candidate variants whose query executions return non-empty results. In this study, the candidate variants are preserved.

<a id='46c24d8f-b732-4ab4-9175-5416a20b7a1b'></a>

This calibration strategy enhances alignment between LLM-generated symbolic structures and the underlying knowledge graph by building directly upon the core-wise linking mechanism described earlier. Rather than treating the entire S-expression as a monolithic unit, calibration operates on decomposed cores, leveraging the linking strategy to ground each component's entities and relations into the knowledge graph. This design not only ensures syntactic well-formedness but also enforces semantic validity through query executability: only those variants that yield non-empty SPARQL results are retained as plausible candidates. By integrating linking as a foundational step within calibration, SEAL achieves a tighter coupling between symbolic reasoning and knowledge graph interaction, enabling robust and scalable semantic parsing in conversational settings.

<a id='bb75a1b4-3a94-4e24-88f5-11dee7a88f7f'></a>

4.3. Memory Module
We divide memory into local and global components to capture different types of information in dialog understanding. The local memory focuses on short-term contextual dependencies within the current conversation, such as coreference resolution and intent tracking. In contrast, the global memory stores structured knowledge accumulated over past interactions, enabling generalization and long-term reasoning.

<a id='a77449c4-4ff2-45ec-9507-fdbc1797fc46'></a>

### 4.3.1. Local Memory
For KBCQA tasks, coreference and ellipsis phenomena present key challenges. To ensure accurate interpretation of user intent by the LLM, our method employs the LLM for coreference resolution. Specifically, historical dialog records are provided to the LLM, enabling it to complete the user's latest question into a fully specified form based on contextual information. Specific examples of input and output can be found in Appendix A. While simply concatenating historical dialogs can also complete semantics,

<a id='afa601f2-936d-41c5-85cf-a677d1c794e8'></a>

14

<!-- PAGE BREAK -->

<a id='83c849d7-2ff2-43d2-a884-5304c7d747b9'></a>

redundant information may degrade the accuracy of keyword matching in
the subsequent question type prediction phase. Moreover, regardless of the
approach, the LLM is ultimately required to resolve coreferences and ellipses.
Therefore, performing semantic completion using the LLM in advance shifts
critical parsing steps earlier in the pipeline, thereby enhancing overall pro-
cessing efficiency.

<a id='7085d661-ea79-4abd-95df-09926d731367'></a>

4.3.2. Global Memory
The template composition stage serves as a pivotal component of the proposed framework, aiming to synthesize calibrated S-expression cores with predefined templates to construct complete and executable S-expressions. This stage comprises three primary steps: question type prediction, template selection, and replacement plan generation.

<a id='49d944db-9811-4ccc-9c19-c1829b131d33'></a>

Question type prediction employs intent analysis and keyword matching to categorize questions into predefined types such as "simple" or "verify." This categorization constrains the search space to appropriate templates retrieved from a curated template library, which is organized according to common reasoning patterns. All generated templates are listed in Appendix D.

<a id='13d4de35-2737-47b2-a1a8-a9faaef640c8'></a>

Template selection further refines the candidate space by analyzing the logical structure of the question and selecting a suitable template _Template_\* compatible with the calibrated S-expression core. Following this selection, a replacement plan is constructed to determine the correspondence between placeholders and concrete values. The replacement plan is formally defined as:

<a id='cd3bef44-eb23-4239-8455-401655ac4833'></a>

Plan* = {(P_j, Value_j) | j = 1, ..., M} (5)

<a id='363f154a-a716-497a-ac06-df43d46a8fcf'></a>

In Equation 5, $P_j$ denotes placeholders in the template, while $Value_j$ represents the substitution elements drawn from ${Constants} \cup {Functions} \cup {Calibrated\_Core_k}$. $M$ is the number of placeholder-value pairs in the replacement plan. These elements consist of calibrated cores, functions, and constants.

<a id='3c527b9e-0966-41d9-8d57-0629eb1874b8'></a>

The generation of the final S-expression is achieved by applying the replacement plan to the selected template, as expressed by:

<a id='b323e0b3-a312-4f26-aa07-292b67f44969'></a>

S-expression_final = Transform(Template*, Plan*) (6)

<a id='64ea4f24-ccb6-4857-8ad9-a324060b7131'></a>

In Equation 6, _Transform_(&middot;) is a recursive function that replaces each placeholder in _Template_\* according to _Plan_\* to produce the final S-expression.

<a id='7e7664c8-5383-48ea-9604-57e81ee0f35d'></a>

15

<!-- PAGE BREAK -->

<a id='333e3e39-3141-4971-bdef-600c5dd51927'></a>

The recursive transformation procedure can be further formalized as:

<a id='841b95b3-4040-4e40-91e6-53f36a768546'></a>

Transform(N) = F(Transform(N₁),..., Transform(Nₖ)) (7)

<a id='257cc8a9-05fe-4d95-aa8e-55700a6616d1'></a>

Where the function F corresponds to the operation associated with node N within the template, and N₁, ..., Nₖ denote its child nodes, each recursively transformed based on the plan in Equation 7.

<a id='9b57ae38-30f7-481f-b00c-66d3b4463448'></a>

The design of S-expression templates is grounded in a detailed analysis of the training corpus. Expression cores, specific functions , and constants are abstracted into placeholders , resulting in a versatile and reusable template library. These templates encapsulate typical logical operations such as set union, difference, deduplication, grouping, counting, and extremum compu- tation, effectively reducing syntactic and semantic errors in LLM-generated outputs.

<a id='8eef3c8e-40c1-4b2b-b784-a913894d6d2e'></a>

Adopting a template-based strategy facilitates generalization and reduces reliance on exhaustive learning by encouraging consistency through reusable syntactic patterns. In KBCQA tasks, certain equality comparison queries in SPARQL references often omit necessary FILTER components, resulting in structurally incomplete S-expressions. This observation highlights the importance of ensuring query completeness during template design to enhance the robustness of generated expressions in practical scenarios.

<a id='5fdc7475-2ee9-4b2f-9371-eff4de74e3eb'></a>

4.4. Reflection Module
Each S-expression template is associated with a predefined question type, allowing efficient type-based filtering during inference. Question type prediction is performed by prompting the LLM with three examples per type. Although the task appears straightforward, semantic overlap frequently causes confusion, particularly among types involving numerical reasoning. To address this issue, a hybrid strategy is employed that integrates keyword-based heuristics to refine ambiguous type predictions—for example, reclassifying certain comparison-oriented questions as count-augmented variants when numerical quantification cues are detected in the utterance.

<a id='d6cf3e2b-41b1-48d6-a33b-6ede78c48b38'></a>

Given the predicted type, the optimal template and its corresponding
replacement plan are determined by maximizing the conditional probability
over the candidate space :

<a id='e04a3501-3d4b-4b2b-ae97-a2fae4dd0298'></a>

(Tem*, Plan*) = arg max P((T, P) | Qcom, {C_Core_k}, Exa; Θ_LLM^ref)
(T,P)∈T_c×P(T)
(8)

<a id='add4785e-19ee-458b-b241-95fb793ee8ab'></a>

16

<!-- PAGE BREAK -->

<a id='4b117572-ecbb-46ec-87d5-9af9994aa142'></a>

In Equation 8, the candidate template set $T_c$, comprising templates $Tem_m$, is determined through filtering based on the predicted question type. For a given template $T$, $P(T)$ denotes the set of all possible replacement plans compatible with $T$. Each $P \in P(T)$ represents a specific assignment of values to the placeholders in $T$, and the pair $(T, P)$ denotes a candidate template-plan combination considered during selection. $Q_{com}$ is the complete form of the current question, $C\_Core_k$ denotes the set of calibrated S-expression cores from the current dialog, $Exa$ represents the set of in-context examples, and $\Theta_{LLM}^{ref}$ denotes the parameters of the LLM used in the reflection phase. Examples of related inputs and outputs can be found in Appendix F.

<a id='1b904963-749e-4899-b192-a1cd41f54694'></a>

To resolve potential ambiguity among compare_and_count, compare, and count, heuristic-based adjustment rules are introduced. In Equation 9, Typefinal is the corrected question type after heuristic adjustment, H(.) is a rule-based function that refines the initial prediction by analyzing keyword patterns and syntactic cues in Qcom, Typepre denotes the initially predicted type, and Qcom is the complete natural language form of the current question:

<a id='cdbdb09b-631d-4e45-a9cc-eb3c289e48e0'></a>

Type_final = H(Type_pre, Q_com) (9)

<a id='c5c4881b-cfa2-4b2c-947c-07cf458909f9'></a>

The LLM is provided with up to four examples per candidate template
to facilitate accurate template selection. This structured methodology sim-
plifies the selection process while leveraging the semantic transparency of
S-expressions.

<a id='bef15a4c-5793-4174-9acd-44268ce58924'></a>

## 4.5. Self-Evolving Mechanism

Most existing methods rely on fixed KG and static parsing rules. Such systems struggle to effectively adapt to the novel expressions continuously emerging in real-world dialog. To overcome this limitation, we introduce a self-evolving mechanism. It establishes a continuously learning agent through the close synergy of local memory, global memory, and a reflection module.

In each dialog turn, the local memory module maintains the current context state, which includes resolved entities and completed semantic intent. This maintenance ensures precise input comprehension and semantic consistency, particularly when handling multi-turn dependencies. The global memory module structurally stores knowledge distilled from successful past dialogs. Knowledge is typically organized as question type to relevant question sam-ple pairs. This mechanism supports the long-term reuse of historically val-idated logical forms for frequent queries. It guides S-expression generation,

<a id='f311088d-9362-4b0b-b672-4ba7f30335d6'></a>

17

<!-- PAGE BREAK -->

<a id='2aebbb4b-704d-4283-81aa-f4500cb7c28c'></a>

significantly boosting parsing accuracy and query efficiency. Following S-expression execution, the reflection module performs post-analysis. Analysis includes syntactic validation, predicate-relation alignment checks, and result non-nullity tests. If this module detects errors, such as entity linking failure or structural invalidity, it logs the cause. It then triggers a correction loop to generate an alternative logical form.

<a id='afa3354b-0845-44ea-8ef8-21e2c7c046e8'></a>

The reflection module validates a generated S-expression to check if it is syntactically well-formed, exhibits consistent alignment between natural language predicates and knowledge graph relations, and executes to produce a non-empty result. Upon such validation, the S-expression, along with its question type and surface-form pattern, is serialized and incorporated into the global memory as a new template instance. This update is performed incrementally and selectively: only execution-verified logical forms with high confidence are retained, ensuring that the global memory evolves through the accumulation of reliable and reusable knowledge rather than unverified or noisy hypotheses.

<a id='3434303b-9f11-4197-8a51-a8ed47f46d4b'></a>

The self-evolving mechanism transforms global memory from static stor-age into a dynamically updateable KG. Validated knowledge is incrementally written through reflection. This closed-loop process, which involves perceiv-ing the current input, retrieving relevant historical patterns, reflecting on execution outcomes, and updating the KG, enables adaptive learning with-out explicit retraining. As dialog progresses, the system's capacity to handle similar or complex queries progressively strengthens. This demonstrates sus-tained evolutionary performance.

<a id='bba88c04-0103-4056-9d2b-43c818cee0a1'></a>

# 5. Experiments
## 5.1. Experimental Setup
In this section, we first introduce the datasets and evaluation metrics used for evaluation. Then, we present baseline methods for comparison and finally explain the implementation details.

<a id='42f30221-5fd5-4a38-9fec-a372d2ec2750'></a>

**Datasets.** We conduct the experiments on SPICE[36], a conversational semantic parsing dataset over Wikidata [69] derived from CSQA [33] benchmark. Each conversation instance in the SPICE dataset is a natural language user-system QA sequence. Additionally, SPARQL parsing for mapping natural language to KG query statements is also provided, exploring the paths and entities on the underlying KG. For experiments, we select nine of the

<a id='673ecbe3-e4ae-44b8-b315-7ca4336da5b3'></a>

18

<!-- PAGE BREAK -->

<a id='4b7902ee-e6d1-4571-83ae-cf17b6e972b6'></a>

ten types of questions from SPICE, covering simple questions, logical reasoning, and comparative reasoning. We discard the "Clarification" subset as it lacks corresponding SPARQL queries. To reduce cost, we randomly sample conversation instances from the complete conversation, meanwhile ensuring that each subtype of questions contains at least 50 samples.

<a id='b4accce7-3a04-4836-8082-3d434b2a17e2'></a>

**Evaluation Metrics.** Following previous works [36], we choose marco-F1 and Accuracy score as the execution-based evaluation metrics for the experiments, which are used to evaluate the questions with answer types of entity sets and numerical values (boolean values), respectively. The marco-F1 metric averages the F1 score of each question subset to prevent the evaluation bias caused by imbalanced instance amount.

<a id='4c87476a-6b9f-4ef7-b9f7-bc69729d9070'></a>

**Baselines.** We compare SEAL with two types of baselines: For supervised methods, we utilize BertSP [36] and DCG [70] for comparison. These approaches use the AllenNLP tool  for NER and global look-up (denoted as GL) for type linking. For the unsupervised type, we adapt KB-Binder [24], a strong single-turn semantic parsing-based KBCQA baseline to the conversational question answering setting. Additionally, we follow [71] to utilize LLMs to generate logic forms directly by providing them linked entities and relations. The details of the baselines can be found in Sec 2.

<a id='b0012c2c-7ae9-4179-b472-141343f3b987'></a>

**Implementation Details.** The question taxonomy consists of six types: *simple, verify, count, compare, compare_and_count*, and *optimize*, following the annotation schema of the SPICE dataset. For question type prediction, we employ the Qwen2.5-32B-Instruct model prompted with three in-context examples per type. All other components—coreference resolution, S-expression core generation, template selection, and replacement plan generation—are implemented using the DeepSeek-V3 model.

<a id='d0c89740-91f0-41a8-b920-7a6cc9e2440d'></a>

Two key parameters are explored in the experiment. The first concerns the linking strategy during the binding of surface forms to knowledge graph elements: we compare a top-1 approach (retaining only the highest-similarity candidate) against a top-k strategy (retaining k=3 entity or relation candidates per mention, as commonly used in prior work). The second parameter is the number of candidate variants preserved during calibration (either the first variant yielding a non-empty query result or the top three variants). These retention strategies influence both the accuracy and efficiency of the linking process.

<a id='9831c55e-0f55-4b91-b87d-49b66060ad05'></a>

¹https://github.com/allenai/allennlp

<a id='eb2eb491-e47b-4294-9b7a-4308cad9ac11'></a>

19

<!-- PAGE BREAK -->

<a id='75cb9184-c9d9-432e-a78f-e01b99437a38'></a>

5.2. _Main Results_

5.2.1. _S-expression Core Extraction-based Method_

When retaining only one variant, a comparison between selecting the single best candidate and selecting multiple candidates shows that the multiple candidate strategy issues slightly more SPARQL queries, though the difference is minimal. This result supports the earlier observation that S-expression cores generated by LLMs are generally semantically accurate.

<a id='8c809859-0104-4b3d-8a09-ac0817603035'></a>

We present the main experimental results in Table 2, comparing our method SEAL with state-of-the-art supervised and unsupervised approaches across various question types. The bold values indicate the best performance for each task category. As shown, SEAL achieves competitive results in both supervised and unsupervised settings, particularly excelling in complex reasoning tasks. Specifically, in *Logical Reasoning*, SEAL obtains a m-F1 of 73.08, surpassing all baselines and demonstrating strong capability in handling multi-hop logic. In *Quantitative Reasoning*, it achieves 64.45, outperforming KB-Binder by a large margin despite lacking supervision. For *Comparative Reasoning*, SEAL reaches 41.06, significantly higher than KB-Binder's 12.17, indicating its robustness in comparative queries. On simple questions, SEAL performs well across variants, achieving 78.49 on direct questions and 70.03 on ellipsis-based ones, showing effective coreference resolution. In the verification tasks, SEAL also achieves the highest accuracy at 85.97 for boolean verification and 70.12 for count-based comparison, further validating its generalization ability. Overall, SEAL achieves an AC of 66.83, significantly outperforming KB-Binder (36.66) and approaching the performance of supervised models like LLMGT (65.65), while requiring no labeled data.

<a id='0abdae3e-4e4d-421e-813d-742d7678fbc0'></a>

5.3. *Efficiency Analysis*

Under the setting of retaining three candidate variants, the total num-ber of SPARQL queries remains low despite a slight increase. Compared to KB-BINDER, our method mitigates the long-tail issue of high query counts, demonstrating improved efficiency. This advantage stems from two key fac-tors: (1) decomposing the full S-expression linking task into multiple core-level subtasks reduces the base of exponential growth, and (2) core expres-sions are semantically simpler and easier to link, making it more likely to find non-empty results early and avoid unnecessary queries, thereby further reducing SPARQL overhead.

<a id='0b319f29-6323-46f4-9bde-1396fb82033f'></a>

20

<!-- PAGE BREAK -->

<a id='29ea1ca6-e5df-4403-a64b-7ed292b33331'></a>

Table 2: Main results
<table id="20-1">
<tr><td id="20-2" rowspan="2">Question Type</td><td id="20-3" rowspan="2">Case num</td><td id="20-4" colspan="2">Supervised</td><td id="20-5" colspan="3">Unsupervised</td></tr>
<tr><td id="20-6">DCGGL</td><td id="20-7">BertSP GL</td><td id="20-8">LLMGT</td><td id="20-9">KB-Binder</td><td id="20-a">SEAL</td></tr>
<tr><td id="20-b"></td><td id="20-c"></td><td id="20-d">m-F1</td><td id="20-e">m-F1</td><td id="20-f">m-F1</td><td id="20-g">m-F1</td><td id="20-h">m-F1</td></tr>
<tr><td id="20-i">Logical Reasoning</td><td id="20-j">421</td><td id="20-k">54.21</td><td id="20-l">24.08</td><td id="20-m">89.61</td><td id="20-n">46.85</td><td id="20-o">73.08</td></tr>
<tr><td id="20-p">Quantitative Reasoning</td><td id="20-q">220</td><td id="20-r">89.67</td><td id="20-s">73.23</td><td id="20-t">21.01</td><td id="20-u">15.41</td><td id="20-v">64.45</td></tr>
<tr><td id="20-w">Comparative Reasoning</td><td id="20-x">329</td><td id="20-y">79.86</td><td id="20-z">71.44</td><td id="20-A">5.87</td><td id="20-B">12.17</td><td id="20-C">41.06</td></tr>
<tr><td id="20-D">Simple Question (Coref)</td><td id="20-E">698</td><td id="20-F">82.53</td><td id="20-G">72.19</td><td id="20-H">85.73</td><td id="20-I">41.07</td><td id="20-J">71.53</td></tr>
<tr><td id="20-K">Simple Question (Direct)</td><td id="20-L">739</td><td id="20-M">85.12</td><td id="20-N">68.87</td><td id="20-O">92.69</td><td id="20-P">40.71</td><td id="20-Q">78.49</td></tr>
<tr><td id="20-R">Simple Question (Ellipsis)</td><td id="20-S">181</td><td id="20-T">77.57</td><td id="20-U">60.33</td><td id="20-V">61.24</td><td id="20-W">39.48</td><td id="20-X">70.03</td></tr>
<tr><td id="20-Y"></td><td id="20-Z"></td><td id="20-10">AC</td><td id="20-11">AC</td><td id="20-12">AC</td><td id="20-13">AC</td><td id="20-14">AC</td></tr>
<tr><td id="20-15">Verification (Boolean)</td><td id="20-16">385</td><td id="20-17">73.19</td><td id="20-18">39.45</td><td id="20-19">91.89</td><td id="20-1a">64.03</td><td id="20-1b">85.97</td></tr>
<tr><td id="20-1c">Quantitative Reasoning (Count)</td><td id="20-1d">482</td><td id="20-1e">62.72</td><td id="20-1f">50.24</td><td id="20-1g">58.33</td><td id="20-1h">39.00</td><td id="20-1i">70.12</td></tr>
<tr><td id="20-1j">Comparative Reasoning (Count)</td><td id="20-1k">336</td><td id="20-1l">63.90</td><td id="20-1m">47.52</td><td id="20-1n">5.06</td><td id="20-1o">7.44</td><td id="20-1p">22.02</td></tr>
<tr><td id="20-1q">Overall</td><td id="20-1r">3791</td><td id="20-1s">74.72</td><td id="20-1t">57.33</td><td id="20-1u">65.65</td><td id="20-1v">36.66</td><td id="20-1w">66.83</td></tr>
</table>

<a id='6a6801c8-6021-4486-b308-3c5c0c635347'></a>

As shown in Figure 3, compared to the approach of retaining a single best candidate, the method of retaining multiple candidates (k=3) improves recall by considering a broader set of options, though it slightly reduces preci- sion due to increased noise. It performs better overall on complex questions. Building on this, retaining multiple variants further enhances recall, espe- cially for questions involving multiple conditions or complex relations, by reducing semantic drift during the calibration phase. However, for simple questions with clear intent, retaining too many variants introduces noise, complicating the selection of the correct answer and thus lowering overall performance.

<a id='2d512192-3bea-428f-a4df-fcd41d1c208a'></a>

<::chart: line chart with two lines showing Normalized Frequency vs. Query Count Range. The y-axis is 'Normalized Frequency' from 0.0 to 1.0. The x-axis is 'Query Count Range' with labels 0, 2, 4, 6, 8, 10+. There are two data series:

Series 1: Top-k, 1 Variable (red line)
- Query Count Range 0: 0.002
- Query Count Range 2: 0.207
- Query Count Range 4: 0.725
- Query Count Range 6: 0.037
- Query Count Range 8: 0.023
- Query Count Range 10+: 0.006

Series 2: Top-k, 3 Variables (blue line)
- Query Count Range 0: 0.001
- Query Count Range 2: 0.007
- Query Count Range 4: 0.218
- Query Count Range 6: 0.214
- Query Count Range 8: 0.497
- Query Count Range 10+: 0.061::>

Figure 3: SPARQL query count for each S-expression core extraction parameter setting

<a id='6c164097-9075-4ff3-bd7c-fdb1a492fa70'></a>

21

<!-- PAGE BREAK -->

<a id='6fcd035c-751b-4308-8a49-93a4f0ea27a1'></a>

Although our framework relies on a library of predefined S-expression templates, we observe that the LLM can generalize beyond these constraints when necessary. For instance, in questions requiring logical disjunction over three or more independent query cores, the model autonomously constructs nested expressions such as (OR (OR x1 x2) x3), which are not explicitly covered by any single template in the global memory. This behavior demonstrates the LLM's capacity for compositional synthesis and robust generalization in out-of-template scenarios. Additional examples are provided in Appendix E.

<a id='2c5e4d00-b6ed-405b-9ca6-7af7cbc42d03'></a>

5.4. *Structure Accuracy*

As shown in Table 3, the core extraction method consistently outperforms direct generation in structural accuracy. For all question types beyond Simple Questions, it achieves higher structural overlap and parsing success rates. This is because complex S-expression structures are harder to learn directly, whereas the two-stage approach helps reduce syntax and structural errors. This leads to outputs more closely aligned with the correct expressions.

<a id='6298c7ae-76bc-4a4e-9f50-86d0a490fc53'></a>

Table 3: Comparison of core extraction and KB-BINDER (structure overlap and parsing
success) (%)
<table id="21-1">
<tr><td id="21-2" rowspan="2">Q. Type</td><td id="21-3" colspan="2">Struct. Overlap (%)</td><td id="21-4" colspan="2">Parse Success (%)</td></tr>
<tr><td id="21-5">Core Ext.</td><td id="21-6">KB-BINDER</td><td id="21-7">Core Ext.</td><td id="21-8">KB-BINDER</td></tr>
<tr><td id="21-9">Simple Q. (Direct)</td><td id="21-a">71.7</td><td id="21-b">64.7</td><td id="21-c">97.2</td><td id="21-d">94.9</td></tr>
<tr><td id="21-e">Simple Q. (Coref.)</td><td id="21-f">67.6</td><td id="21-g">53.3</td><td id="21-h">96.4</td><td id="21-i">91.8</td></tr>
<tr><td id="21-j">Comp. Reasoning</td><td id="21-k">30.7</td><td id="21-l">4.9</td><td id="21-m">85.7</td><td id="21-n">41.0</td></tr>
<tr><td id="21-o">Comp. Reasoning (Count)</td><td id="21-p">26.2</td><td id="21-q">0.0</td><td id="21-r">85.7</td><td id="21-s">24.7</td></tr>
<tr><td id="21-t">Quant. Reasoning (Count)</td><td id="21-u">42.1</td><td id="21-v">3.3</td><td id="21-w">74.1</td><td id="21-x">51.9</td></tr>
<tr><td id="21-y">Logical Reasoning</td><td id="21-z">33.5</td><td id="21-A">15.7</td><td id="21-B">91.2</td><td id="21-C">63.4</td></tr>
<tr><td id="21-D">Verification</td><td id="21-E">95.6</td><td id="21-F">26.8</td><td id="21-G">95.8</td><td id="21-H">48.3</td></tr>
<tr><td id="21-I">Simple Q. (Ellipsis)</td><td id="21-J">61.9</td><td id="21-K">75.1</td><td id="21-L">97.8</td><td id="21-M">97.8</td></tr>
<tr><td id="21-N">Quant. Reasoning</td><td id="21-O">28.2</td><td id="21-P">15.0</td><td id="21-Q">40.9</td><td id="21-R">27.7</td></tr>
<tr><td id="21-S">Total</td><td id="21-T">54.8</td><td id="21-U">32.2</td><td id="21-V">88.1</td><td id="21-W">66.0</td></tr>
</table>

<a id='36e70a66-605e-4ce8-960c-b313bebdc9b8'></a>

5.5. _Ablation Study_
Table 4 presents the results of an ablation study on the SEAL model,
evaluating performance through F1 score, accuracy (AC), and overall per-

<a id='a705be90-93ce-4e15-b778-d83c55de7af5'></a>

22

<!-- PAGE BREAK -->

<a id='4cea13e1-85b4-4362-ae19-f684df46b644'></a>

Table 4: Performance metrics of ablated variants
<table id="22-1">
<tr><td id="22-2"></td><td id="22-3">F1</td><td id="22-4">AC</td><td id="22-5">Overall</td></tr>
<tr><td id="22-6">SEAL</td><td id="22-7">66.44</td><td id="22-8">59.37</td><td id="22-9">64.08</td></tr>
<tr><td id="22-a">w/o core extraction</td><td id="22-b">41.34</td><td id="22-c">35.39</td><td id="22-d">39.36</td></tr>
<tr><td id="22-e">w/o entity candidate</td><td id="22-f">39.32</td><td id="22-g">46.16</td><td id="22-h">41.60</td></tr>
<tr><td id="22-i">w/o calibration</td><td id="22-j">61.78</td><td id="22-k">56.21</td><td id="22-l">59.93</td></tr>
<tr><td id="22-m">w/o local memory</td><td id="22-n">61.89</td><td id="22-o">56.69</td><td id="22-p">60.16</td></tr>
</table>

<a id='016cc117-fb3b-464c-a845-b5d291129e84'></a>

formance score. The overall performance score is computed as an average. The baseline SEAL model, integrating all components, serves as a reference point with robust performance. The study assesses the impact of removing individual components (core extraction, entity candidate generation, calibration, and local memory), where each removed module is replaced by a default method to ensure fair comparison. The omission of entity candidate generation affects precision, possibly due to compensatory effects in candidate selection. Calibration removal reduces output reliability, highlighting its optimization role, while the absence of local memory slightly diminishes performance stability, though less than calibration. The reliability is measured through three independent runs, referred to as the reliability experiment, to assess consistency across trials. Comparative analysis indicates a hierarchical dependency, with core extraction and entity candidate generation exerting the greatest influence due to their core functions, while calibration and local memory enhance robustness through synergy. The results affirm the interdependent nature of these components, as their individual removal consistently degrades performance, validating their collective importance to the SEAL framework.

<a id='b38eead8-640b-4aa4-85ef-6b34d0fd9f86'></a>

## 5.6. Low Resource Senario Study
Table 5 presents the results of an ablation study for zero-shot and few-shot settings, with evaluation metrics F1 score, accuracy (AC), and overall performance score. KB-Binder, in the few-shot setting, shows an F1 score of 32.61, an AC of 36.82, and an overall score of 34.02, serving as the baseline reference. SEAL-base, also in the few-shot setting, shows an F1 of 41.34, an AC of 35.39, and an overall score of 39.36, indicating superior performance compared to few-shot KB-Binder. KB-Binder lacks data in the zero-shot setting, while SEAL-self-evolving, under zero-shot, shows an F1 of 34.42,

<a id='ffbce9dd-6aef-4b4f-8a39-2d7f3d8a325f'></a>

23

<!-- PAGE BREAK -->

<a id='63a127f8-b515-4da9-8699-4ccd1d59ec2f'></a>

Table 5: Performance comparison between SEAL variants and baselines under few-shot
and zero-shot settings

<a id='ab9a1990-d779-486a-b8e3-36ce9b168aff'></a>

<table id="23-1">
<tr><td id="23-2"></td><td id="23-3">F1</td><td id="23-4">AC</td><td id="23-5">Overall</td></tr>
<tr><td id="23-6">KB-Binder (few-shot)</td><td id="23-7">32.61</td><td id="23-8">36.82</td><td id="23-9">34.02</td></tr>
<tr><td id="23-a">SEALbase (few-shot)</td><td id="23-b">41.34</td><td id="23-c">35.39</td><td id="23-d">39.36</td></tr>
<tr><td id="23-e">KB-Binder (zero-shot)</td><td id="23-f">21.67</td><td id="23-g">15.96</td><td id="23-h">19.76</td></tr>
<tr><td id="23-i">SEALself_evolving (zero-shot)</td><td id="23-j">34.42</td><td id="23-k">34.10</td><td id="23-l">34.32</td></tr>
</table>

<a id='b153a795-249d-440f-b5a0-ed70b4fdfb14'></a>

an AC of 34.10, and an overall score of 34.32, demonstrating performance
improvement through the self-improvement mechanism. Comparative anal-
ysis reveals that SEAL-base excels in the few-shot task, whereas SEAL-self-
improvement shows potential enhancement in the zero-shot task, validating
the efficacy of the self-improvement mechanism in data-scarce scenarios.

<a id='e2b1fc7c-5f76-4ab4-8186-3b08b047c50c'></a>

5.7. Evaluating the Self-Evolving Mechanism To measure the effectiveness of the self-evolving mechanism, we conducted a series of experiments. The experimental results are shown in Figure 4. <::chart: The visual content presents Figure 4, which consists of three line charts displaying the impact of different factors on F1 score under the Self-Evolving mechanism. Each chart shares a common Y-axis labeled "F1 Score". (a) Impact of S-expression length: This chart's X-axis is labeled "S-expression Length (elements)" with categories: 0-8, 8-16, 16-24, 24-32, >32. It shows two lines: "F1 - KB-Binder" (blue) and "F1 - SEAL" (red). Data points for F1 - KB-Binder are: (0-8, 1.000), (8-16, 0.778), (16-24, 0.775), (24-32, 0.219), (>32, 0.410). Data points for F1 - SEAL are: (0-8, 1.000), (8-16, 0.875), (16-24, 0.822), (24-32, 0.854), (>32, 0.559). (b) Impact of dialogue turn: This chart's X-axis is labeled "Dialogue Turn Range" with categories: 0-3, 3-6, 6-9, 9-12, 12-16. It shows two lines: "F1 - SEAL w/o Mem" (blue) and "F1 - SEAL" (red). Data points for F1 - SEAL w/o Mem are: (0-3, 1.000), (3-6, 0.854), (6-9, 0.786), (9-12, 0.841), (12-16, 0.675). Data points for F1 - SEAL are: (0-3, 1.000), (3-6, 0.851), (6-9, 0.853), (9-12, 0.862), (12-16, 0.770). (c) Impact of cumulative dialogue context coverage: This chart's X-axis is labeled "Context Coverage (%)" with categories: 0-20, 20-40, 40-60, 60-80, 80-100. It shows two lines: "F1 - SEAL w/o Mem" (blue) and "F1 - SEAL" (red). Data points for F1 - SEAL w/o Mem are: (0-20, 0.191), (20-40, 0.237), (40-60, 0.269), (60-80, 0.340), (80-100, 0.370). Data points for F1 - SEAL are: (0-20, 0.187), (20-40, 0.240), (40-60, 0.399), (60-80, 0.532), (80-100, 0.516). Figure 4: Impact of different factors on F1 under the Self-Evolving mechanism. (a) S-expression length; (b) dialog turn depth; (c) cumulative dialog context coverage::>

<a id='c1c586f6-e07b-41fb-9163-8b64d4687896'></a>

In the first experiment, we segmented all test samples into intervals based on the length of S-expressions (0-8, 8-16, ..., >32) and calculated the F1 scores for SEAL and KB-Binder for each interval. The results are shown in Figure 4 (a). The query length refers to the number of elements in the target S-expression, serving as a metric for semantic logic complexity. SEAL demonstrated superior stability and adaptability across varying complexities. While both models performed well in the short query stage (<16 elements), KB-Binder's performance dropped sharply as complexity increased. Specifically, in the 24-32 element interval, its F1 score decreased from 0.822 to 0.219,

<a id='702f98b7-0c7f-4424-8a1c-71d0492c6743'></a>

24

<!-- PAGE BREAK -->

<a id='d33265c7-718b-4d11-ab1f-9aa9042a33fe'></a>

indicating a high susceptibility to long-range dependencies and error accumulation. In contrast, F1 score of SEAL avoided this decline, maintaining 0.854 in the same range, showcasing robust parsing for complex structures. Even with extremely long expressions (>32 elements), SEAL scored 0.559, outperforming KB-Binder's 0.410 (a relative improvement of 36.3%). This validates the S-expression core extraction mechanism of SEAL and its modular two-stage design, which prevents the performance collapse common in end-to-end long sequence generation.

<a id='acc4256c-a329-43c7-9a4c-b5f63f17e329'></a>

In the second experiment, to verify whether the self-evolving method possesses the ability to continually optimize performance as the dialog progresses, we segmented the entire sequence of dialog turns into several ranges (0-3, 3-6, ..., 12-16 turns) and evaluated the F1 performance within each interval. Here, dialog turn refers to the position of the current question within the multi-turn dialog. The experimental results are shown in Figure 4(b).

<a id='55521c8a-2bfe-4139-a947-bcf036e229fa'></a>

In the early phase (0-3 turns), the F1 of both SEAL and the ablation model (SEAL w/o memory) achieved 1.0. However, SEAL's advantage became evident from the 6th turn. In the 6-9 turn range, SEAL maintained stability (0.853), while the memory-ablated version dropped to 0.786. SEAL then peaked at 0.862 in the 9-12 turn range, contrasting sharply with the ablation model's limited rebound (0.841). In the final stage (12-16 turns), SEAL's F1 (0.770) remained superior to the ablated version's 0.675 (14% gap). This trend confirms that SEAL achieves self-evolving behavior by using local memory for contextual consistency and global memory to retrieve successful patterns, thereby continuously enhancing semantic parsing capability during complex dialogs.

<a id='65f37902-c7d1-4a6b-8928-8b6f82317177'></a>

In the third experiment, to measure the impact of the available propor-tion of cumulative dialog history data (data stream) on F1 performance in multi-turn dialogs and to quantify the contribution of memory, we segmented the entire dialog dataset into several intervals based on the proportion of vis-ible context (0-20%, 20-40%, 40-60%, 60-80%, 80-100%). The model's F1 performance was then evaluated within each interval. The results are shown in Figure 4(c). The system's performance changes based on the amount of historical dialog observed. The results show that in the early stages (<40% data stream), the performance of both versions (with and without memory) is similar, indicating that the memory mechanism is not yet influential during the initial dialog turns. However, starting from 40%, SEAL (with memory) shows a performance leap, while SEAL w/o memory exhibits gradual growth. In the 60-80% interval, SEAL's F1 peaks at 0.532, surpassing the memory-

<a id='9c566945-11d9-4f2d-8459-863c69363e84'></a>

25

<!-- PAGE BREAK -->

<a id='46bf7044-1a7f-4216-af61-d64259933830'></a>

ablated version (0.340) by 56%. Even in the final stage (80-100% interval),
where F1 values slightly decrease, likely due to noise accumulation from long
dialogs, SEAL maintains a clear advantage.

<a id='52fb82f5-9a41-4c6b-ad85-b1f42a721385'></a>

These results collectively demonstrate that SEAL's global memory effectively utilizes structured knowledge from the dialog history, significantly improving semantic parsing in mid-to-late turns. As visible context increases, the results show a clear benefit from self-evolving functionality as SEAL continuously improves its inference capability through accumulated experience. Conversely, lacking the memory module, the ablated model cannot sustain this learning, reflected in slower performance growth. This directly validates the proposed self-evolving architecture, showing system performance improves with increasing dialog turns.

<a id='f077c7de-f714-4e25-bfbc-7e340b99ad87'></a>

5.8. *Case Study*
To validate the effectiveness of the SEAL framework in complex dialogue scenarios, we conducted an analysis of a typical multi turn question answering case. This case involves identifying implicit semantic constraints, resolving cross turn coreference, and integrating user feedback. Successfully handling such scenarios requires dynamic dialogue state tracking and iterative refinement of the reasoning path. Traditional methods based on static templates, such as KB Binder, often fail in such situations due to their lack of contextual awareness. In contrast, SEAL achieves more robust reasoning through its self-evolving mechanism, which integrates global memory, local memory, and semantic calibration.

<a id='04eab0b8-4de4-46a1-ac07-cc946836e67b'></a>

26

<!-- PAGE BREAK -->

<a id='1191eebf-ac8e-4d2e-abf6-04e4e3165e1d'></a>

Implicit semantic constraint reasoning
Question:
Which male person was the parent of Ludovico II?

<a id='4e4314f4-69bb-44f2-b799-bd8063eae4f8'></a>

SEAL:
1)Input: "male person" detected as implicit constraint
2)Template Matching: Matches to (JOIN P21 Q6581097)
3)Predicted S-expression: (AND (JOIN (R P22) Q1063295)
<< (JOIN P21 Q6581097) >> (JOIN P31 Q502895))
4)Reasoning: Explicitly models "male" as a constraint on the
parent entity.
5)Calibrated S-expression: (AND (JOIN (R P22) Q1063295)
(JOIN P21 Q6581097) (JOIN P31 Q502895))

KB-Binder:
1)Input: "male person" ignored (no calibration)
2)Template Matching: Uses default "parent" template → P40
3)Predicted S-expression:(AND (JOIN (R <<P40 >>) Q1063295)
(JOIN P31 Q502895))
4)Reasoning: Uses incorrect relation (P40 = has child), and omits
the gender constraint.

<a id='1383bd6b-50ec-4b4e-9ac8-621bb5796cf7'></a>

<::Global memory box content:
Global memory:
1)"results": ["Q3750875", "Q3756712", "Q3767600",
"Q3856501"]
2)"coreference_resolved_question": "Who are the children of
Ludovico II, Marquess of Saluzzo?"::>


<a id='44acd7dc-e480-4966-9709-d123182954c8'></a>

Accurate Parsing under Multi-turn References and User Corrections
Question:
User:"Who are the children of Ludovico II?
→ System lists: Gian Gabriele I, Francesco, Giovanni Ludovico
User:Who are siblings of that one?
System (implicitly) Did you mean Francesco?
User:No, I meant Giovanni Ludovico. Could you tell me the answer for that?"

<a id='af8c96eb-e515-4aa7-8256-f10fea94102b'></a>

SEAL:
1)Input: Full multi-turn context with explicit
user correction
2)Coreference Resolution:
→ Resolves "that one" → Giovanni Ludovico
(Q3767600)
→ Uses dialogue history + negative feedback
("No, I meant...")
3)Template Matching:
→ Matches to template "x1": (AND (JOIN (R
sibling) x1)...)
→ Replaces x1 →

<a id='23420b59-1bb8-40fb-b6ba-268fbe259ede'></a>

Giovanni_Ludovico_Marquess_of_Saluzzo
4)Calibration:
→ Maps "sibling" → Wikidata relation P7
→ Ensures correct direction: subject → object
5)Predicted S-expression:
→ (AND (JOIN (R P7) <<Q3767600>>)
(JOIN P31 Q502895))
6)SPARQL:
→ (AND (JOIN (R P7) <<Q3767600>>)

<a id='adcf9c6b-0112-4e6e-91bc-b791c6d11d98'></a>

Giovanni_Ludovico_Marquess_of_Saluzzo
4)Calibration:
→ Maps "sibling" → Wikidata relation P7
→ Ensures correct direction: subject →
object
5)Predicted S-expression:
→ (AND (JOIN (R P7) <<Q3767600>>)
(JOIN P31 Q502895))
6)SPARQL:
→ (AND (JOIN (R P7) <<Q3767600>>)
(JOIN P31 Q502895))
7)Interpretation: Correctly retrieves
Giovanni's siblings

<a id='c399d2c4-2f3e-4320-85a6-1b8b3f5b864d'></a>

KB-Binder:
1) Input: Same multi-turn context
2) Coreference Handling:
→ No explicit coreference module; likely binds "that one" to first/last mentioned entity or system's prior guess (e.g., Francesco)
→ Ignores user's explicit correction
3) Template Matching:
→ Uses generic pattern without context-aware replacement
4) Calibration: None — direct mapping from surface form
5) Predicted S-expression:
→ (AND (JOIN (R P7) Q3767600) (JOIN P31 Q502895))
→ Appears correct, but inconsistent with actual SPARQL
6) SPARQL:
→ SELECT ?x WHERE { ?x wdt:P7 wd:Q3767600. ?x wdt:P31 wd:Q502895 }
→ Wrong direction: "x has sibling Giovanni"
7) Interpretation: Returns entities for whom Giovanni is a sibling

<a id='4d9937af-4189-474b-8239-cb2574f03c4d'></a>

Figure 5: Comparison of SEAL and KB-Binder on a multi-turn QA example from the SPICE dataset.

<a id='8190c625-d56a-4091-b7b3-dd8f74401b9f'></a>

As illustrated in Figure 5, the dialogue begins with a seemingly simple query: "Who are the children of Ludovico II, Marquess of Saluzzo?" However, the user's actual intent is to identify his male offspring, as all subsequent references and reasoning revolve around male heirs. This query involves two key semantic components: retrieving the "child" relationship and imposing an implicit constraint that the result must be a male person. KB-Binder ignores this gender constraint during initial parsing, defaulting to a generic "parent" template (P40) without modeling the gender restriction, which may return non-target entities such as female relatives or individuals of unknown sex, thereby contaminating the candidate set for later coreference resolution. In contrast, SEAL detects "male person" as an implicit constraint through calibration, explicitly models it as a type restriction (e.g., P31: Q502895), and incorporates it into the S-expression, ensuring that only the four male children, namely Gian Gabriele I, Francesco, Giovanni Ludovico, and Michele Antonio, are returned. This calibrated result is then stored in global memory, providing a clean and semantically consistent foundation for subsequent multi-turn reasoning.

<a id='840ef266-c14d-4d84-8a5a-acd29cddf7c6'></a>

In the following turn, the user asks, "Who are siblings of that one?"
The system initially infers the referent as Francesco, but the user corrects

<a id='12017520-0dab-4740-bf86-c54ca8881618'></a>

27

<!-- PAGE BREAK -->

<a id='81343e23-3d09-4dff-8ab9-83eff0066735'></a>

it explicitly: "No, I meant Giovanni Ludovico." In particular, "Giovanni Ludovico" does not appear in the user's input but was listed in the previous answer. KB-Binder lacks cross-turn memory and relies solely on surface-form entity linking, making it prone to incorrectly bind "that one" to the first-mentioned entity or the system's prior guess, such as Francesco, and does not respond to explicit user correction. In contrast, SEAL resolves the coreference by leveraging global memory to access the previously retrieved list of children and their corresponding QIDs, including Q3767600 for Giovanni Ludovico, thereby constraining the search space to a reliable candidate set. Local memory records the negative feedback ("No, I meant..."), and reflection enables dynamic elimination of incorrect options, confirming Giovanni Ludovico as the intended subject. Based on this accurate entity identification, SEAL generates a correctly oriented SPARQL query: SELECT ?x WHERE { wd:Q3767600 wdt:P7 ?x . ?x wdt:P31 wd:Q502895 }, retrieving all siblings including Michele Antonio, who was never mentioned by the user. Meanwhile, KB-Binder either fails to resolve the correct entity or produces a query with reversed relation direction (?x wdt:P7 wd:Q3767600), resulting in incomplete or logically inconsistent results. This case demonstrates how SEAL achieves robust multi-turn understanding through a synergistic pipeline of implicit constraint calibration, memory-augmented coreference resolution, and feedback-driven refinement.

<a id='970c2258-b110-4e5a-8856-9e685ecfc64b'></a>

This case study demonstrates how SEAL evolves its reasoning continu-
ously across multiple turns of dialogue. It starts with single turn semantic
calibration, then coordinates global memory, local memory, and a reflec-
tion mechanism. This multilayered architecture for memory and calibration
allows the system to genuinely comprehend the user's implicit intentions
within the flow of conversation. This capability enables SEAL to outper-
form baselines like KB-Binder, which lack mechanisms for cross-turn context
integration and feedback-driven refinement.

<a id='3a44fd7b-8c41-4648-9e0e-c475b0250843'></a>

## 6. Conclusion

By decomposing complex logical form generation into core extraction and template composition, and leveraging agent-based calibration to address syntactic and linking limitations of LLMs, SEAL's two-stage framework based on S-expression core extraction markedly enhances the accuracy of complex logical form generation. SEAL improves structure overlap and parsing success rates by 22.6% and 22.1%, respectively compared to KB-BINDER.

<a id='da21d3dc-f80b-4750-8863-de96582bbde4'></a>

28

<!-- PAGE BREAK -->

<a id='dc9abd19-60a8-4008-95a1-2b6fc9a9084d'></a>

The S-expression core extraction method for semantic parsing shows robust experimental performance, yet several avenues warrant further exploration. To enhance scalability, the current S-expression to SPARQL transformation, constrained by specific syntactic patterns and datasets, could be generalized through versatile transformation functions validated across diverse benchmarks. Additionally, the calibration strategy, which targets non-empty query outputs, may inadvertently revise valid empty queries, reducing precision; refining the agent's semantic understanding to accurately distinguish such cases could improve robustness. Furthermore, the LLM's reliance on prompts for S-expression comprehension could be strengthened through targeted training on S-expression generation tasks to deepen syntactic and semantic proficiency. Finally, the computational overhead from multiple LLM invocations during core extraction could be mitigated by employing smaller models for subtasks like coreference resolution or question type classification, thereby improving efficiency.

<a id='6f5b8f9e-f9b7-4f19-96b4-a3968abb128a'></a>

References

1.  A survey on knowledge graphs: Representation, acquisition, and appli-
cations | IEEE journals & magazine | IEEE xplore.
2.  Xiaohan Zou. A survey on application of knowledge graph. Journal of
Physics: Conference Series, 1487(1):012016, 2020.
3.  Sebastian R Bader, Irlan Grangel-Gonzalez, Priyanka Nanjappa, Maria-
Esther Vidal, and Maria Maleshkova. A knowledge graph for industry
4.0. In European Semantic Web Conference, pages 465-480. Springer,
2020.
4.  Yunshi Lan, Gaole He, Jinhao Jiang, Jing Jiang, Wayne Xin Zhao, and
Ji-Rong Wen. Complex knowledge base question answering: A survey.
IEEE Transactions on Knowledge and Data Engineering, 35(11):11196-
11215, 2022.
5.  Dennis Diefenbach, Vanessa Lopez, Kamal Singh, and Pierre Maret.
Core techniques of question answering systems over knowledge bases: a
survey. Knowledge and Information systems, 55(3):529-569, 2018.
6.  Tengfei Feng and Liang He. Rgr-kbqa: Generating logical forms for ques-
tion answering using knowledge-graph-enhanced large language model.

<a id='d0cc92ed-45d9-46bb-a76c-c95069fa3ae2'></a>

29

<!-- PAGE BREAK -->

<a id='94462050-4641-4780-a312-30a5fb1c08de'></a>

In Proceedings of the 31st International Conference on Computational Linguistics, pages 3057-3070, 2025.

[7] Daehwan Nam and Gary Geunbae Lee. Semantic parsing with candi- date expressions for knowledge base question answering. arXiv preprint arXiv:2410.00414, 2024.

[8] Ruicheng Liu, Rui Mao, Anh Tuan Luu, and Erik Cambria. A brief sur- vey on recent advances in coreference resolution. Artificial Intelligence Review, 56(12):14439-14481, 2023.

[9] Kenton Lee, Luheng He, Mike Lewis, and Luke Zettlemoyer. End-to-end neural coreference resolution. In Martha Palmer, Rebecca Hwa, and Se- bastian Riedel, editors, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 188-197, Copenhagen, Denmark, 2017. Association for Computational Linguistics.

[10] Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng Gao. Large language models: A survey. arXiv preprint arXiv:2402.06196, 2024.

[11] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 1(2), 2023.

[12] Minhao Zhang, Yongliang Ma, Yanzeng Li, Ruoyu Zhang, Lei Zou, and Ming Zhou. Two is better than one: Answering complex questions by multiple knowledge sources with generalized links. arXiv preprint arXiv:2309.05201, 2023.

[13] Yirui Zhan, Yanzeng Li, Minhao Zhang, and Lei Zou. A progressive question answering framework adaptable to multiple knowledge sources. In Asia-Pacific Web (APWeb) and Web-Age Information Management (WAIM) Joint International Conference on Web and Big Data, pages 362-367. Springer, 2024.

[14] Jihong Wang, Yichen Zhang, and Wei Liu. Question answering system based on the combination of large language model and knowledge graph. Applied Intelligence, 55(15):1000, 2025.

<a id='42572ba6-2afc-48b1-81b8-db11081007d3'></a>

30

<!-- PAGE BREAK -->

<a id='fa4397bb-9077-4326-bc78-ee7c728a6af4'></a>

[15] Bicheng Xua, Rong Penga, Yongchang Dinga, and Lin Fanga. An interpretable logic kbqa method based on open-source large language models.
[16] Aishwarya Kamath and Rajarshi Das. A survey on semantic parsing. arXiv preprint arXiv:1812.00978, 2018.
[17] Sijia Wei, Wenwen Zhang, Qisong Li, and Jiang Zhao. Semantic parsing for question answering over knowledge graphs. arXiv preprint arXiv:2401.06772, 2023.
[18] Zhengxiao Du, Chang Zhou, Jiangchao Yao, Teng Tu, Letian Cheng, Hongxia Yang, Jingren Zhou, and Jie Tang. Cogkr: Cognitive graph for multi-hop knowledge reasoning. IEEE Transactions on Knowledge and Data Engineering, 35(2):1283-1295, 2021.
[19] Liqiang Wen, Guanming Xiong, Tong Mo, Bing Li, Weiping Li, and Wen Zhao. Clear-kgqa: Clarification-enhanced ambiguity resolution for knowledge graph question answering. arXiv preprint arXiv:2504.09665, 2025.
[20] Yanzeng Li, Sen Hu, Wenjuan Han, and Lei Zou. Cord: a three-stage coarse-to-fine framework for relation detection in knowledge base question answering. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, pages 4069-4073, 2023.
[21] Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao. Semantic parsing via staged query graph generation: Question answering with knowledge base. In Chengqing Zong and Michael Strube, editors, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1321-1331, Beijing, China, 2015. Association for Computational Linguistics.
[22] Junwei Bao, Nan Duan, Zhao Yan, Ming Zhou, and Tiejun Zhao. Constraint-based question answering with knowledge graph. In Yuji Matsumoto and Rashmi Prasad, editors, Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2503-2514, Osaka, Japan, 2016. The COLING 2016 Organizing Committee.

<a id='b63ccd05-1135-400c-8d88-d61077d6a3e2'></a>

31

<!-- PAGE BREAK -->

<a id='ee20ba1a-f25d-4f63-b0aa-e44b486d6562'></a>

[23] Sen Hu, Lei Zou, and Xinbo Zhang. A state-transition framework to answer complex questions over knowledge base. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2098-2108, Brussels, Belgium, 2018. Association for Computational Linguistics.
[24] Tianle Li, Xueguang Ma, Alex Zhuang, Yu Gu, Yu Su, and Wenhu Chen. Few-shot in-context learning for knowledge base question answering.
[25] Zhijie Nie, Richong Zhang, Zhongyuan Wang, and Xudong Liu. Code-style in-context learning for knowledge-based question answering. In Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan, editors, Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 18833-18841. AAAI Press, 2024.
[26] Haoran Luo, Haihong E, Zichen Tang, Shiyao Peng, Yikai Guo, Wentai Zhang, Chenghao Ma, Guanting Dong, Meina Song, Wei Lin, Yifan Zhu, and Luu Anh Tuan. ChatKBQA: A generate-then-retrieve framework for knowledge base question answering with fine-tuned large language models. In Findings of the Association for Computational Linguistics ACL 2024, pages 2039-2056.
[27] Derong Xu, Xinhang Li, Ziheng Zhang, Zhenxi Lin, Zhihong Zhu, Zhi Zheng, Xian Wu, Xiangyu Zhao, Tong Xu, and Enhong Chen. Harnessing large language models for knowledge graph question answering via adaptive multi-aspect retrieval-augmentation.
[28] Yu Gu, Xiang Deng, and Yu Su. Don't generate, discriminate: A proposal for grounding language models to real-world environments. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4928-4949, Toronto, Canada, 2023. Association for Computational Linguistics.
[29] Guanming Xiong, Junwei Bao, and Wen Zhao. Interactive-KBQA:

<a id='7c3aa2da-ecc3-430b-aacb-ae4b71fc5588'></a>

32

<!-- PAGE BREAK -->

<a id='250db3f4-b7b6-48ca-9e49-758c9f1e8ae7'></a>

Multi-turn interactions for knowledge base question answering with large language models. In *Proc. of ACL*, pages 10561–10582.

[30] Lei Sun, Zhengwei Tao, Youdi Li, and Hiroshi Arakawa. ODA: Observation-driven agent for integrating LLMs and knowledge graphs.

[31] Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. Reasoning on graphs: Faithful and interpretable large language model reasoning. In *The Twelfth International Conference on Learning Representations, ICLR 2024*, Vienna, Austria, May 7–11, 2024. OpenReview.net, 2024.

[32] Vaishali Vadhavana, Krishna Patel, Brinda Patel, Bansari Patel, Naina Parmar, and Vaibhavi Patel. Conversational question answering systems: A comprehensive literature review. In *2024 International Conference on Inventive Computation Technologies (ICICT)*, pages 1088–1095. ISSN: 2767-7788.

[33] Amrita Saha, Vardaan Pahuja, Mitesh M. Khapra, Karthik Sankaranarayanan, and Sarath Chandar. Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, *Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18)*, the 30th innovative *Applications of Artificial Intelligence (IAAI-18)*, and the 8th *AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)*, New Orleans, Louisiana, USA, February 2–7, 2018, pages 705–713. AAAI Press, 2018.

[34] Philipp Christmann, Rishiraj Saha Roy, and Gerhard Weikum. Conversational question answering on heterogeneous sources. In Enrique Amigó, Pablo Castells, Julio Gonzalo, Ben Carterette, J. Shane Culpepper, and Gabriella Kazai, editors, *SIGIR '22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval*, Madrid, Spain, July 11–15, 2022, pages 144–154. ACM, 2022.

[35] Laura Perez-Beltrachini, Parag Jain, Emilio Monti, and Mirella Lapata. Semantic parsing for conversational question answering over knowledge graphs. In Andreas Vlachos and Isabelle Augenstein, editors, *Proceedings of the 17th Conference of the European Chapter of the Association*

<a id='70f86049-e2e5-4da0-97b9-a8b9a87364be'></a>

33

<!-- PAGE BREAK -->

<a id='63475925-0dfe-448b-afaa-979382c483a4'></a>

for Computational Linguistics, pages 2507-2522, Dubrovnik, Croatia, 2023. Association for Computational Linguistics.

[36] Laura Perez-Beltrachini, Parag Jain, Emilio Monti, and Mirella Lapata. Semantic parsing for conversational question answering over knowledge graphs. In Andreas Vlachos and Isabelle Augenstein, editors, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2507-2522, Dubrovnik, Croatia, 2023. Association for Computational Linguistics.

[37] Yu Gu, Sue Kase, Michelle Vanni, Brian M. Sadler, Percy Liang, Xifeng Yan, and Yu Su. Beyond I.I.D.: three levels of generalization for question answering on knowledge bases. In Jure Leskovec, Marko Grobelnik, Marc Najork, Jie Tang, and Leila Zia, editors, WWW '21: The Web Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021, pages 3477-3488. ACM / IW3C2, 2021.

[38] Yu Gu, Xiang Deng, and Yu Su. Don't generate, discriminate: A proposal for grounding language models to real-world environments. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4928-4949, Toronto, Canada, 2023. Association for Computational Linguistics.

[39] Hongming Zhang, Xinran Zhao, and Yangqiu Song. A brief survey and comparative study of recent development of pronoun coreference resolution in English. In Maciej Ogrodniczuk, Sameer Pradhan, Massimo Poesio, Yulia Grishina, and Vincent Ng, editors, Proceedings of the Fourth Workshop on Computational Models of Reference, Anaphora and Coreference, pages 1-11, Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics.

[40] Disambiguation in conversational question answering in the era of LLM: A survey.

[41] Mark Yatskar. A qualitative comparison of CoQA, SQUAD 2.0 and QuAC. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of

<a id='59248dd2-7f26-4338-ad04-fa3c9a19c16c'></a>

34

<!-- PAGE BREAK -->

<a id='feac1787-c9c0-4ea5-bcd5-1efff97fde16'></a>

the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2318-2323, Minneapolis, Minnesota, 2019. Association for Computational Linguistics.

[42] Svitlana Vakulenko, Shayne Longpre, Zhucheng Tu, and Raviteja Anantha. Question rewriting for conversational question answering.

[43] Zhiyu Chen, Jie Zhao, Anjie Fang, Besnik Fetahu, Oleg Rokhlenko, and Shervin Malmasi. Reinforced question rewriting for conversational question answering. In Yunyao Li and Angeliki Lazaridou, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 357-370, Abu Dhabi, UAE, 2022. Association for Computational Linguistics.

[44] Gonçalo Raposo, Rui Ribeiro, Bruno Martins, and Luísa Coheur. Question rewriting? assessing its importance for conversational question answering. volume 13186, pages 199-206.

[45] Magdalena Kaiser, Rishiraj Saha Roy, and Gerhard Weikum. Robust training for conversational question answering models with reinforced reformulation generation.

[46] Lihui Liu, Blaine Hill, Boxin Du, Fei Wang, and Hanghang Tong. Conversational question answering with language models generated reformulations over knowledge graph. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 839-850. Association for Computational Linguistics.

[47] Daya Guo, Duyu Tang, Nan Duan, Ming Zhou, and Jian Yin. Dialog-to-action: Conversational question answering over a large-scale knowledge base. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 2946-2955, 2018.

[48] Parag Jain and Mirella Lapata. Conversational semantic parsing using dynamic context graphs. In Houda Bouamor, Juan Pino, and Kalika

<a id='452d6395-2e18-4fb5-b72e-eeeb21ed32c0'></a>

35

<!-- PAGE BREAK -->

<a id='b118cae4-76d8-4515-b105-34d3dc746fbc'></a>

Bali, editors, *Proceedings of the 2023 Conference on Empirical Meth-ods in Natural Language Processing*, pages 8667-8679, Singapore, 2023. Association for Computational Linguistics.

[49] Parag Jain and Mirella Lapata. Integrating large language models with graph-based reasoning for conversational question answering.

[50] Xirong Xu, Tao Xu, Ziming Wang, Haochen Li, Li Zhu, and Xiaopeng Wei. Reinforcement learning from constraints and focal entity shifting in conversational KGQA. 36(4):2015-2028.

[51] Manoj Madushanka Perera, Adnan Mahmood, Kasun Eranda Wijethi-lake, and Quan Z. Sheng. Towards adaptive context management for in-telligent conversational question answering. In Quan Z. Sheng, Gill Dob-bie, Jing Jiang, Xuyun Zhang, Wei Emma Zhang, Yannis Manolopoulos, Jia Wu, Wathiq Mansoor, and Congbo Ma, editors, *Advanced Data Min-ing and Applications*, pages 360-375. Springer Nature.

[52] Junzhuo Li and Deyi Xiong. KaFSP: Knowledge-aware fuzzy semantic parsing for conversational question answering over a large-scale knowl-edge base. In Smaranda Muresan, Preslav Nakov, and Aline Villavicen-cio, editors, *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 461-473, Dublin, Ireland, 2022. Association for Computational Linguistics.

[53] Daya Guo, Duyu Tang, Nan Duan, Ming Zhou, and Jian Yin. Dialog-to-action: Conversational question answering over a large-scale knowledge base. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, *Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018*, December 3-8, 2018, Montréal, Canada, pages 2946-2955, 2018.

[54] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yu-peng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yi-fan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language models, 2025.

<a id='8219d92c-a8f1-42b9-96b1-8ff76beab271'></a>

36

<!-- PAGE BREAK -->

<a id='03bfef87-b5f9-47d8-a515-c8a46f69607c'></a>

[55] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wain-wright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022.

[56] Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, Bohan Wu, Binqi Chen, Ziyue Qiao, Qingqing Long, Rongcheng Tu, Xiao Luo, Wei Ju, Zhiping Xiao, Yifan Wang, Meng Xiao, Chenwu Liu, Jingyang Yuan, Shichang Zhang, Yiqiao Jin, Fan Zhang, Xian Wu, Hanqing Zhao, Dacheng Tao, Philip S. Yu, and Ming Zhang. Large language model agent: A survey on methodology, applications and challenges. CoRR, abs/2503.21460, 2025.

[57] Nikita Mehandru, Brenda Y. Miao, Eduardo Rodriguez Almaraz, Madhumita Sushil, Atul J. Butte, and Ahmed M. Alaa. Evaluating large language models as agents in the clinic. npj Digit. Medicine, 7(1), 2024.

[58] Zhendong Chu, Shen Wang, Jian Xie, Tinghui Zhu, Yibo Yan, Jinheng Ye, Aoxiao Zhong, Xuming Hu, Jing Liang, Philip S. Yu, and Qingsong Wen. LLM agents for education: Advances and applications. CoRR, abs/2503.11733, 2025.

[59] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied multimodal language model. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 8469-8488. PMLR, 2023.

<a id='a0fcef8f-592a-46fe-9d88-bddd1afb36b9'></a>

37

<!-- PAGE BREAK -->

<a id='4ba74f3e-3aab-4997-94c1-8d16cb97c112'></a>

[60] Sipeng Zheng, Jiazheng Liu, Yicheng Feng, and Zongqing Lu. Steve-eye: Equipping llm-based embodied agents with visual perception in open worlds. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024.
[61] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. Understanding the planning of LLM agents: A survey. CoRR, abs/2402.02716, 2024.
[62] Hui Wei, Zihao Zhang, Shenghua He, Tian Xia, Shijia Pan, and Fei Liu. Plangenllms: A modern survey of LLM planning capabilities. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 19497-19521. Association for Computational Linguistics, 2025.
[63] Callie Y. Kim, Christine P. Lee, and Bilge Mutlu. Understanding large-language model (llm)-powered human-robot interaction. In Dan Grollman, Elizabeth Broadbent, Wendy Ju, Harold Soh, and Tom Williams, editors, Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction, HRI 2024, Boulder, CO, USA, March 11-15, 2024, pages 371-380. ACM, 2024.
[64] Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Li Erran Li, Ruohan Zhang, Weiyu Liu, Percy Liang, Li Fei-Fei, Jiayuan Mao, and Jiajun Wu. Embodied agent interface: Benchmarking llms for embodied decision making. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024.
[65] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen,

<a id='93bf19ae-4b0e-4677-86a7-e49379a548a2'></a>

38

<!-- PAGE BREAK -->

<a id='2e7b1779-4453-4fe4-bd56-7f141bd5e78b'></a>

Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie
Tang. Agentbench: Evaluating llms as agents. In The Twelfth Inter-
national Conference on Learning Representations, ICLR 2024, Vienna,
Austria, May 7-11, 2024. OpenReview.net, 2024.

<a id='04539bf5-564a-40bd-96d7-45e70c4d3811'></a>

66. Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. *Chatdb: Augmenting llms with databases as their symbolic memory*. *CoRR, abs/2306.03901, 2023*.
67. Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang. *Think-in-memory: Recalling and post-thinking enable llms with long-term memory*. *CoRR, abs/2311.08719, 2023*.
68. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. *Voyager: An open-ended embodied agent with large language models*. *Trans. Mach. Learn. Res., 2024, 2024*.
69. Denny Vrandecic. *Wikidata: a new platform for collaborative data collection*. In Alain Mille, Fabien Gandon, Jacques Misselis, Michael Rabinovich, and Steffen Staab, editors, *Proceedings of the 21st World Wide Web Conference, WWW 2012, Lyon, France, April 16-20, 2012 (Companion Volume)*, pages 1063-1064. ACM, 2012.
70. Parag Jain and Mirella Lapata. *Conversational semantic parsing using dynamic context graphs*. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages 8667-8679, Singapore, 2023. Association for Computational Linguistics.
71. Phillip Schneider, Manuel Klettner, Kristiina Jokinen, Elena Simperl, and Florian Matthes. *Evaluating large language models in semantic parsing for conversational question answering over knowledge graphs*. In Ana Paula Rocha, Luc Steels, and H. Jaap van den Herik, editors, *Proceedings of the 16th International Conference on Agents and Artificial Intelligence, ICAART 2024, Volume 3, Rome, Italy, February 24-26, 2024*, pages 807-814. SCITEPRESS, 2024.

<a id='185efc64-1694-4ce4-b03d-22ef22a157a9'></a>

39

<!-- PAGE BREAK -->

<a id='4092c2f9-98b1-478c-a8a9-ee724e67a8a0'></a>

Appendix A.

<a id='72687d86-881e-416d-9905-3524989db7fa'></a>

SPICE is a multi-turn knowledge graph question-answering dataset fea-
turing coreference and ellipsis in dialogues. To enhance LLM's understanding
of question intent, coreference and ellipsis are resolved by providing the LLM
with historical user-system interactions, focusing on the user's final question,
and reformulating them into a complete, standalone query. This resolved
question replaces the original incomplete one for subsequent processing.

<a id='02e16fb0-dc00-46ae-a5f8-330e041dbadb'></a>

Input:
* Which people emerged victorious in La Madrid Challenge by La
Vuelta 2016 and La Madrid Challenge by La Vuelta 2015?
* Giorgia Bronzini, Shelley Olds, Kirsten Wild
* Which television programs are that person a screenwriter of?
* Did you mean Giorgia Bronzini?
* No, I meant Shelley Olds. Could you tell me the answer for that?
% [ENTITIES]
% La Madrid Challenge by La Vuelta 2016:
% cycling_race_class_defined_by_the_International_Cycling_Union;
% La Madrid Challenge by La Vuelta 2015:
% cycling_race_class_defined_by_the_International_Cycling_Union;
% Giorgia Bronzini: common_name;
% Shelley Olds: common_name;
% Kirsten Wild: common_name
Output:
* Which television programs is Shelley Olds a screenwriter of?

<a id='c1a0ebd3-12d5-4a79-a50f-9e7e045b609c'></a>

## Appendix B.

This article presents the 'S-expression core,' a concise subset of S-expressions limited to JOIN, R, AND, VALUES, and IS_TRUE functions. In natural language queries, S-expression cores denote objects of inquiry or comparison. In SPARQL queries, they map to graph patterns in the WHERE{...} clause. Training data S-expression cores are derived from these patterns. Figure 5 shows these mappings with examples of questions and S-expression cores.

<a id='e50f17ab-b8ac-43cf-98d4-1a9b79818a15'></a>

40

<!-- PAGE BREAK -->

<a id='8c55e4a6-eaaa-41b1-b4eb-a2ab6b48ad32'></a>

Question: Which male person was the parent of Ludovico II, Marquess of Saluzzo?
S-expression:
(AND
(JOIN (R father)
Lu-
dovico_II,_Marquess_of_Saluzzo) (JOIN instance_of common_name))
S-expression Core:
(AND (JOIN (R father) Lu-
dovico_II,_Marquess_of_Saluzzo) (JOIN instance_of common_name))
Question: How many people have Gian Gabriele I of Saluzzo as their sibling?
S-expression:
(COUNT (AND (JOIN (R brother)
Gian_Gabriele_I_of_Saluzzo) (JOIN instance_of common_name)))
S-expression Core:
(AND (JOIN (R brother)
Gian_Gabriele_I_of_Saluzzo) (JOIN instance_of common_name))

<a id='8e4e6322-60c0-4f74-846f-e91daaaac093'></a>

<::S-Expression: (COUNT (DISTINCT (OR (AND (JOIN location Versailles) (JOIN instance_of Esperanto_congress)) (AND (JOIN location Versailles) (JOIN instance_of statute))))))::>
<::Diagram with arrows pointing from two lower boxes to the upper box.::>
<::Question: How many Esperanto congresses and statutes are situated at Versailles? Query Target: Esperanto congresses situated at Versailles Statutes situated at Versailles::>
<::SPARQL SELECT (COUNT(DISTINCT ?x) AS ?count) WHERE { ?x wdt:P276 wd:Q621 . ?x wdt:P31 wd:Q13403102 . UNION ?x wdt:P276 wd:Q621 . ?x wdt:P31 wd:Q820655 . }::>
Figure B.6: S-expression

<a id='560d7132-6bd3-4826-b732-d5e8de212fdb'></a>

Appendix C.

After the LLM generates the S-expression core, an Agent is required to calibrate the generated core, so we summarize the common patterns of S-expression cores as follows.

<a id='ec494039-4183-4776-8984-4a35f88b0300'></a>

41

<!-- PAGE BREAK -->

<a id='71810c20-602c-4376-a7e8-b9c7ea292a66'></a>

S-Core pattern of expression
1. (IS_TRUE x x x)
2. (JOIN x x)
3. (JOIN (R x) x)
4. (AND (JOIN x x) (JOIN x x))
5. (AND (JOIN x (VALUES x ...)) (JOIN x x))
6. (AND (JOIN (R x) x) (JOIN x x))
7. (AND (JOIN (R x) (VALUES x ...)) (JOIN x x))
8. (AND (JOIN x (JOIN x x)) (JOIN x x))
9. (AND (JOIN (R x) (JOIN x x)) (JOIN x x))
10. (AND (JOIN x x) (JOIN x x) (JOIN x x))
11. (AND (JOIN (R x) x) (JOIN (R x) x) (JOIN x x))
12. (AND (JOIN (R x) x) (JOIN x x) (JOIN (R x) x))

<a id='2f9774aa-47f5-4e6f-92da-7c45e5d25863'></a>

Appendix D.

During the organizing of S expression templates, we identified an error in the SPICE dataset. For questions involving equality relations, the corresponding SPARQL queries often lack the necessary FILTER component to screen for equality, resulting in incomplete S-expressions. For example, the correct template should have the compare function set to EQ, but the actual template is (GROUP _COUNT x1).

<a id='415561e9-d658-42ad-be4a-bdf84a84c60b'></a>

The following describes the two primary steps for generating complete S-expressions from templates.

<a id='a2af115f-a07f-4954-88b7-868c57871877'></a>

42

<!-- PAGE BREAK -->

<a id='9c812b6e-4128-46e0-b078-b0bfe6d4f874'></a>

<table id="42-1">
<tr><td id="42-2">Operation</td><td id="42-3">S-based Query Syntax</td></tr>
<tr><td id="42-4">single</td><td id="42-5">x1(OR x1 x2)(DISTINCT x1)(DIFF x1 x2)*(GROUP_COUNT x1)*(GROUP_SUM (GROUP_COUNT x1) (GROUP_COUNT x2))</td></tr>
<tr><td id="42-6">verify</td><td id="42-7">(ALL x1)(ALL x1 x2)(ALL x1 x2 x3)...</td></tr>
<tr><td id="42-8">count</td><td id="42-9">(COUNT x1)(COUNT (DISTINCT x1))(COUNT (DISTINCT (OR x1 x2)))*(COUNT (GROUP_COUNT x1))*(COUNT (GROUP_SUM (GROUP_COUNT x1)))(GROUP_COUNT x2)</td></tr>
<tr><td id="42-a">compare</td><td id="42-b">(compare (GROUP_COUNT x1) number)(compare (GROUP_COUNT x1 x2))(compare (GROUP_SUM (GROUP_COUNT x1)))(GROUP_COUNT x2) number)(compare (GROUP_SUM (GROUP_COUNT x1)))(GROUP_COUNT x2) (OR x3 x4))</td></tr>
<tr><td id="42-c">compare_and_count</td><td id="42-d">(COUNT (compare (GROUP_COUNT x1) number))(COUNT (compare (GROUP_COUNT x1) x2))(COUNT (compare (GROUP_SUM)))(GROUP_COUNT x1 (GROUP_COUNT x2) number))(COUNT (compare (GROUP_SUM)))(GROUP_COUNT x1 (GROUP_COUNT x2) (OR x3 x4)))</td></tr>
<tr><td id="42-e">optimize</td><td id="42-f">(optimize (GROUP_COUNT x1))(optimize (GROUP_SUM (GROUP_COUNT x1)))(GROUP_COUNT x2))</td></tr>
</table>

<a id='1bc2e475-3896-4a87-b847-ca7d58c565d7'></a>

## Appendix E.

Notably, while we refer to the process as template "selection," the model retains adaptive flexibility, often generating correct templates beyond the candidate set based on syntactic context, as illustrated in the example below.

<a id='a19658cb-8248-423b-98be-e13a0efb1ab5'></a>

43

<!-- PAGE BREAK -->

<a id='ebf1b1e2-2025-4c3e-b299-9d385a40a40b'></a>

Correct S-expression:
(AND (JOIN (R field_of_this_occupation) (VALUES assistant_coach
association_football_manager association_football_player)) (JOIN
instance_of sport))
Predicted S-expression Core:
(AND (JOIN (R field_of_this_occupation) assistant_coach) (JOIN
instance_of sport)),
(AND
(JOIN
(R
field_of_this_occupation)
association_football_manager) (
JOIN instance_of sport)),
(AND (JOIN (R field_of_this_occupation) association_football_player)
(
JOIN instance_of sport))
Generated Template:
(OR (OR x1 x2) x3) - Not in the template library

<a id='b4479531-24b8-4b6a-8a7e-5675de2a3603'></a>

Appendix F.

We provide the LLM with the question, candidate templates (determined by the predicted question type), and calibrated S-expression core variants, enabling it to select an appropriate template and generate a substitution plan. The final S-expression is obtained by replacing placeholders in the selected template with the generated plan. An example of input and output is provided below.

<a id='6543a44d-346b-4a3c-8d23-418216d9d784'></a>

44

<!-- PAGE BREAK -->

<a id='3db02290-1482-49c5-afb8-16ad52fef56b'></a>

Input
Question: How many administrative territories are the narrative locations of at least 840 applications or works of art?
Candidate Templates (Predicted question type: compare_and_count):
* (COUNT (compare (GROUP_COUNT x1) number)),
* (COUNT (compare (GROUP_COUNT x1) x2)),
* (COUNT (compare (GROUP_SUM (GROUP_COUNT x1) (GROUP_COUNT x2)) number)),
* (COUNT (compare (GROUP_SUM (GROUP_COUNT x1) (GROUP_COUNT x2)) (OR x3 x4)))

Candidate S-expression Core:
* (AND (JOIN (R narrative_location) (JOIN instance_of application)) (JOIN instance_of administrative_territorial_entity)),
* (AND (JOIN (R narrative_location) (JOIN instance_of work_of_art)) (JOIN instance_of administrative_territorial_entity))

Output
Template: (COUNT (compare (GROUP_SUM (GROUP_COUNT x1) (GROUP_COUNT x2)) number))
Replacement Plan: {
"variables": {
"x2": "(AND (JOIN (R narrative_location) (JOIN instance_of work_of_art)) (JOIN instance_of administrative_territorial_entity))",
"x1": "(AND (JOIN (R narrative_location) (JOIN instance_of applica-tion)) (JOIN instance_of administrative_territorial_entity))"
},
"constants": {
"number": 840
},
"functions": {
"compare": "GE"
}
}

<a id='a93a9d1d-f19a-43bb-ac54-93dc0ab56917'></a>

45