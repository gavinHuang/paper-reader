<a id='d2d79c72-ec53-4457-8a4e-7efc6d5e7b4c'></a>

# Adaptation of Agentic AI

Pengcheng Jiang<sup>1*</sup>, Jiacheng Lin<sup>1*</sup>, Zhiyi Shi<sup>1,4*</sup>, Zifeng Wang<sup>1</sup>, Luxi He<sup>3</sup>, Yichen Wu<sup>4</sup>, Ming Zhong<sup>1</sup>,
Peiyang Song<sup>6,7</sup>, Qizheng Zhang<sup>2</sup>, Heng Wang<sup>1</sup>, Xueqiang Xu<sup>1</sup>, Hanwen Xu<sup>5</sup>, Pengrui Han<sup>1</sup>, Dylan Zhang<sup>1</sup>,
Jiashuo Sun<sup>1</sup>, Chaoqi Yang<sup>1</sup>, Kun Qian<sup>12</sup>, Tian Wang<sup>12</sup>, Changran Hu<sup>7</sup>, Manling Li<sup>10</sup>, Quanzheng Li<sup>4</sup>, Hao
Peng<sup>1</sup>, Sheng Wang<sup>5</sup>, Jingbo Shang<sup>8</sup>, Chao Zhang<sup>9</sup>, Jiaxuan You<sup>1</sup>, Liyuan Liu<sup>1</sup>, Pan Lu<sup>2</sup>, Yu Zhang<sup>11</sup>,
Heng Ji<sup>1</sup>, Yejin Choi<sup>2</sup>, Dawn Song<sup>7</sup>, Jimeng Sun<sup>1</sup>, Jiawei Han<sup>1+</sup>

<a id='19dc5eed-a3f3-4499-8956-8c9a48058f88'></a>

1 UIUC
2 Stanford
3 Princeton
4 Harvard
5 UW
6 Caltech
7 UC Berkeley
8 UCSD
9 Georgia Tech
10 Northwestern
11 TAMU
12 Unity

<a id='e096ef1a-e3e9-498b-8c07-bf0f0730ea0f'></a>

Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.

<a id='52eb2791-b980-48ec-97c0-04e074e7bb53'></a>

Github Repository: https://github.com/pat-jj/Awesome-Adaptation-of-Agentic-AI

<a id='59f7f2ce-bd16-4b6a-8eb4-4ce16afc3e40'></a>

<::Overview of adaptations in agentic AI: flowchart::>Figure 1 Overview of adaptations in agentic AI. Agent: the foundation models serving as orchestration and reasoning modules; Tool: callable components other than the agent model that operate independently, e.g., APIs, ML models, subagents, or memory. We categorize these adaptations into two: agent adaptation (A1 & A2): adapting agent models, and tool adaptation (T1 & T2): adapting tools for agents. See more details in §3. The figure is divided into two main sections: 'Agent Adaptation' and 'Tool Adaptation', each accompanied by a diagram and a set of related adaptation types.### Agent AdaptationThis section includes a diagram and two boxes detailing A1 and A2.The diagram shows a flow from 'INPUT' to 'Agent' (depicted with a fire icon), then interacting bidirectionally with 'Tool' (depicted with a snowflake icon), which in turn interacts bidirectionally with 'Environment / Offline Data'.- A red dashed arrow labeled 'A1' points from 'Agent' to 'Tool'.- A red dashed arrow labeled 'A2' points from 'Agent' to 'Environment / Offline Data'.- **A1: Tool Execution Signaled**- SFT & Off-Policy: Toolformer, ToolLLM, ...- RLVR Methods: DeepRetrieval, Prover-V2, ...- **A2: Agent Output Signaled**- w/o Tools: DeepSeek-R1, Kimi-1.5, ...- w/ Tools: ReTool, Search-R1, ...### Tool AdaptationThis section includes a diagram and two boxes detailing T1 and T2.The diagram shows a flow from 'INPUT' to 'Agent' (depicted with a snowflake icon), then interacting bidirectionally with 'Tool' (depicted with a fire icon), which in turn interacts bidirectionally with 'Environment / Offline Data'.- A red dashed arrow labeled 'T1' points from 'Environment / Offline Data' to 'Tool'.- A red dashed arrow labeled 'T2' points from 'Tool' to 'Agent'.- **T1: Agent-Agnostic**- Classic ML Tools: HuggingGPT, ViperGPT, ...- Subagent-as-Tool: SWE-Grep, Tab-RL, ...- **T2: Agent-Supervised**- Subagent-as-Tool: s3, AgentFlow, ...- Agentic Memory: Reflexion, Memento, ...

<a id='9385bbb1-ae8c-46e6-aff9-fa926cf39e95'></a>

arXiv:2512.16301v1 [cs.AI] 18 Dec 2025

<!-- PAGE BREAK -->

<a id='80cf4802-88aa-4ff5-aad9-49cdf1e7f423'></a>

Adaptation of Agentic AI

<a id='6986125b-2223-4b01-a982-2909954c581d'></a>

<::Adaptation of Agentic AI: flowchart::>Adaptation of Agentic AI
- Background (§2)
  - Agentic AI Systems (§2.1)
  - Adaptation (§2.2)
    - Prompt Engineering (§2.2.1)
    - Fine-Tuning (§2.2.2)
- Overview (§3)
  - Mathematical Notations (§3.1)
  - Adaptation Paradigms of Agentic AI (§3.2)
  - Illustrative Examples (§3.3)
- Agent Adaptation (§4)
  - A1: Tool Execution Result as Signal (§4.1)
    - Earlier Works: SFT & Off-Policy Methods (§4.1.1)
    - RLVR-Based Methods (§4.1.2)
  - A2: Agent Output as Signal (§4.2)
    - Adaptation w/o Tools (§4.2.1)
    - Adaptation w/ Tools (§4.2.2)
- Tool Adaptation (§5)
  - T1: Agent-Agnostic Tool Adaptation (§5.1)
    - Foundational Systems and Architectures (§5.1.1)
    - Categories and Training Methods (§5.1.2)
  - T2: Agent-Supervised Tool Adaptation (§5.2)
    - Earlier Methods (§5.2.1)
    - Subagent-as-Tool (§5.2.2)
    - Agentic Memory and Others (§5.2.3)
- Comparison (§6)
  - A1 & A2 (§6.2)
  - T1 & T2 (§6.3)
  - Strategic Recommendations (§6.5)
- Applications (§7)
  - Deep Research (§7.1)
  - Software Development (§7.2)
  - Computer Use (§7.3)
  - Drug Discovery & Development (§7.4) ...
- Opportunities (§8)
  - Co-Adaptation (§8.1)
  - Continual Adaptation (§8.2)
  - Safe Adaptation (§8.3)
  - Efficient Adaptation (§8.4)
Figure 2 The structure of this paper.

<a id='77016a19-18bd-4bb2-a969-cd2227273f61'></a>

## 1 Introduction

The rapid progress of foundation models, such as large language models (LLMs), has catalyzed the rise of agentic AI systems: autonomous AI systems capable of perceiving their environment, invoking external tools, managing memory, and executing multi-step plans toward completing complex tasks [1–4]. Agentic AI demonstrates remarkable potential in applications ranging from scientific discovery [5, 6] to software development and clinical research [7–9]. However, current agentic AI systems still struggle with challenges such as unreliable tool use, limited long-horizon planning, domain-specific reasoning gaps, robustness issues in real-world environments, and poor generalization to unexplored environments where the agent lacks prior interaction experience [10–13]. These limitations reveal that even highly capable foundation models often require additional adaptation to specialize for particular tasks or real-world scenarios. This motivates the need for *adaptation in agentic AI systems*, whereby the components of an agentic system are modified or optimized so that the agent achieves higher task performance, improved reliability, and better generalization across diverse scenarios.

<a id='be3a2b32-deea-40fb-b4e3-31e044b9268c'></a>

Building on this motivation, we conduct a comprehensive survey on the adaptation in agentic AI systems, aiming to systematically analyze how components in agentic AI systems are modified to overcome current limitations. Compared with existing surveys on modern AI agents [1, 14-18], this paper centers specifically on adaptation in agentic AI. To structure this rapidly expanding literature, we introduce a unified framework that organizes adaptation in agentic AI into four core paradigms spanning both agent adaptation and tool adaptation, as shown in Figure 1. This framework clarifies the underlying design space, highlights the trade-offs between different

<a id='2e214c4a-7cf2-4b1e-b897-45ef059fb3c9'></a>

2

<!-- PAGE BREAK -->

<a id='a791b7b0-9698-4df2-b921-d2b99a69918a'></a>

Adaptation of Agentic AI

<a id='05fdeeab-54f2-4a36-b279-90716f935116'></a>

adaptation strategies, and provides practical guidance for choosing or transitioning between paradigms based on
supervision signals, task requirements, and system-level constraints.

<a id='82968db4-6aa9-4616-8385-87f925f9b640'></a>

In our framework, we conclude adaptation strategies for agentic AI into two dimensions according to which component is optimized (§3). The first dimension, which we term **Agent Adaptation**, focuses on modifying the agent's internal parameters, representations, or behavioral policies to better align with task requirements. This includes both traditional fine-tuning approaches [19] and modern reinforcement learning methods that leverage environment feedback [20, 21]. The second dimension, **Tool Adaptation**, shifts the optimization target from the agent to its external tools, e.g., retrievers, planners, memory modules, and specialized models, enabling frozen agents to benefit from an adaptive operational environment [22, 11, 23]. Within these two broad paradigms, we further identify four distinct adaptation strategies, forming a comprehensive taxonomy that organizes the rapidly evolving landscape of agentic AI research:

<a id='0ec498a9-ae0e-4f77-87f4-5616d7286e19'></a>

*   **A1: Tool Execution Signaled Agent Adaptation** (§3.2.1, §4.1): The agent is optimized using verifiable outcomes produced by external tools it invokes. This paradigm captures settings where correctness signals arise directly from tool execution, such as code sandbox results, retrieval relevance scores, or API call outcomes.
*   **A2: Agent Output Signaled Agent Adaptation** (§3.2.2, §4.2): The agent is optimized using evaluations of its own outputs, e.g., final answers, plans, or reasoning traces, possibly after incorporating tool results. This paradigm includes both tool-free outcome-based learning and tool-augmented adaptation driven by answer correctness or preference scores.
*   **T1: Agent-Agnostic Tool Adaptation** (§3.2.3, §5.1): Tools are trained independently of the frozen agent. These tools include retrievers, domain-specific models, and other pretrained components that can be used as plug-and-play modules orchestrated by the frozen agent.
*   **T2: Agent-Supervised Tool Adaptation** (§3.2.4, §5.2): The agent remains fixed while its tools are adapted using signals derived from the agent's outputs. This paradigm includes reward-driven retriever tuning, adaptive rerankers, search subagents, and memory-update modules trained to better support the frozen agent.

<a id='aee1b443-1bf0-4772-bd3e-86b416a67f58'></a>

It is worth noting that these four strategies are not mutually exclusive: state-of-the-art systems increasingly combine multiple adaptation paradigms to achieve optimal performance [24-26]. For instance, a deep research system might employ T1-style retrieval tools (pre-trained dense retrievers), T2-style adaptive search agents (trained via frozen LLM feedback), and Al-style reasoning agents (fine-tuned with execution feedback) in a cascaded architecture [6].

<a id='d1213c6a-8230-4c8f-9000-d79a3f92affb'></a>

In §6, we further emphasize that the choice among these paradigms involves fundamental trade-offs along several dimensions.

(1) **Cost and flexibility**: Agent adaptation (A1/A2) typically requires substantial computational resources for training billion-parameter models but offers maximal flexibility, while tool adaptation (T1/T2) optimizes external components at lower cost but may be constrained by the frozen agent's capabilities [27, 22].

(2) **Generalization**: T1 tools trained on broad data distributions often generalize well across agents and tasks [23, 28], whereas Al methods may overfit to specific environments unless carefully regularized [20].

(3) **Modularity**: T2 approaches enable independent tool upgrades without agent retraining [29, 22], facilitating continuous system improvement, while A1/A2 methods may suffer from catastrophic forgetting when adapting to new tasks.

<a id='2a159c0f-7d23-4cb2-a227-180b0252997f'></a>

**Scope and contributions.** This paper provides the first comprehensive taxonomy of adaptation strategies for agentic AI, systematically organizing recent advances across agent adaptation (A1, A2) and tool adaptation (T1, T2). We offer several key contributions:

*   A unified conceptual framework that clarifies the associations and distinctions between adaptation paradigms and their underlying principles (Figure 2).
*   Detailed technical surveys of representative methods within each category, documenting their training objectives, architectural choices, and empirical performance across diverse benchmarks.
*   Systematic comparison of adaptation strategies along dimensions of cost, flexibility, generalization capability, and modularity.

<a id='7d65b0c0-1c09-4789-856c-c3434ea67ab2'></a>

3

<!-- PAGE BREAK -->

<a id='1030fc34-5b65-4686-9ff2-b8ccbb1b4040'></a>

Adaptation of Agentic AI
---

<a id='f9b05717-47aa-426d-bad6-bf74d86aca5d'></a>

* Demonstrating how adaptation strategies are tailored to domain applications, spanning deep research, software development, computer use, and drug discovery (§7).
* Identification of open challenges and future research directions, including unified agent-tool co-adaptation frameworks, theoretical understanding of adaptation dynamics, and standardized evaluation protocols (§8).

<a id='b4318170-b54a-4b91-a0ef-d2a192211ee8'></a>

**Organization.** The remainder of this paper is organized as follows. Section 2 provides foundational concepts, introducing the core components of agentic AI systems and the two primary forms of adaptation (prompt engineering and fine-tuning). Section 3 presents an overview of adaptation paradigms under our proposed framework, formalizing the four paradigms (A1, A2, T1, T2) and illustrating them with concrete examples. Sections 4 and 5 present our main taxonomy, systematically reviewing agent adaptation (A1, A2) and tool adaptation (T1, T2) methods respectively. Section 6 compares these paradigms along key dimensions. Section 7 examines real-world applications across multiple domains. Finally, Section 8 discusses open challenges and future research directions. Throughout, we emphasize the complementary nature of agent and tool adaptation, arguing that the most effective agentic systems will strategically combine both paradigms to achieve robust, efficient, and generalizable performance across diverse tasks and environments.

<a id='2f359a5c-26cb-4b4b-af55-c3c2757f3592'></a>

## 2 Background
In this section, we provide the background to facilitate a better understanding of the concepts discussed throughout this survey. Specifically, we first introduce the fundamental components of *Agentic AI Systems* (§2.1). We then discuss different forms of *Adaptation* (§2.2), which enable agentic AI systems to better adjust their behaviors and capabilities to specific tasks or application scenarios.

<a id='b942039c-34c3-4479-a7da-38468be84a29'></a>

## 2.1 Agentic AI Systems

Agentic AI systems refer to autonomous artificial intelligence systems capable of perceiving, reasoning, acting, and continuously improving through interaction with their environment. Such systems are designed to perform complex, open-ended tasks that require adaptive decision-making, contextual understanding, and iterative problem solving. In this survey, we primarily focus on _single-agent systems_, which provide a controlled yet expressive framework to study how an individual agent perceives, plans, and acts within an environment. Single-agent settings serve as the foundational building blocks of more complex _multi-agent systems_, in which multiple agents coordinate, cooperate, or compete to achieve shared or opposing goals. Comprehensive overviews of AI agent architectures and their extensions to multi-agent scenarios can be found in recent surveys such as [1, 2, 30].

<a id='630cdc27-9fbd-4e8b-b7b8-f1970241abb0'></a>

At the core of an agentic AI system lies a **foundation model**, typically implemented as a large language model (LLM) or multimodal model that functions as the agent's reasoning and control center. This foundation model provides the fundamental abilities for understanding, reasoning, planning, and interaction. Complementing this core are several additional components that extend the agent's autonomy and enable it to operate effectively in complex and dynamic environments:

*   **Planning Module**: Decomposes complex goals into actionable steps and organizes their sequential or hierarchical execution. Depending on the degree of feedback integration, planning can be conducted in two main modes. _Static planning_ methods, such as Chain-of-Thought [31] and Tree-of-Thought [32], enable structured reasoning through single-path or multi-path task decomposition. In contrast, _dynamic planning_ approaches, such as ReAct [33] and Reflexion [34], incorporate feedback from the environment or past actions, allowing the agent to iteratively refine its plans and improve performance in long-horizon or partially observable scenarios.
*   **Tool Use**: Enables the agent to interact with external resources and computational systems, extending its capabilities beyond the limitations of its internal knowledge. Typical tools include web search engines, APIs, code execution environments, Model Context Protocols (MCPs), and browser automation frameworks [35, 3]. Effective tool use involves selecting appropriate tools, constructing task-specific inputs, invoking external functions, and integrating their outputs into the agent's reasoning and decision-making process, thereby enhancing performance in real-world and computationally intensive scenarios.

<a id='ede5eb60-dcc6-454d-92cd-b34be32fedfd'></a>

4

<!-- PAGE BREAK -->

<a id='8caa20d8-30e8-44b1-aac3-2d14969e1357'></a>

Adaptation of Agentic AI

<a id='249ce944-bd37-4e7b-9036-c3831b4e9021'></a>

*   **Memory Module:** Allows the agent to retain, retrieve, and utilize past information for context-aware reasoning and long-term consistency. Memory is typically divided into *short-term memory*, which stores contextual information generated during the current task, and *long-term memory*, which persists across sessions to accumulate reusable knowledge and experience [1, 36]. To access relevant information from long-term memory, many systems employ retrieval-augmented generation (RAG) mechanisms that retrieve and integrate stored knowledge into the agent's reasoning process. Designing an effective memory module involves challenges such as how to structure stored information, when and what to retain, how to retrieve relevant knowledge efficiently, and how to seamlessly integrate it into ongoing reasoning and decision-making.¹

<a id='68addc7c-7696-4d77-bae7-cf40177a1fa8'></a>

## 2.2 Adaptation

Adaptation is a crucial aspect of agentic AI systems, enabling them to operate effectively across diverse and complex tasks. It allows an agent to adjust its behaviors, decision strategies, and internal representations to better align with the requirements of a specific domain, task, or operational environment. Without such adaptive mechanisms, agents may struggle to generalize beyond their initial design or handle dynamic, real-world conditions. These mechanisms can be broadly categorized into prompt-based adaptation (&sect;2.2.1) and fine-tuning-based adaptation (&sect;2.2.2).

<a id='cb75bc92-3eff-460e-9884-6bd8c5ce97bd'></a>

## 2.2.1 Prompt Engineering

Prompt engineering serves as a lightweight form of adaptation that guides the behavior of an agentic AI system without modifying its underlying model parameters. Instead of retraining the core model, the agent's behavior is shaped by carefully crafted input prompts that define goals, constraints, and contextual instructions. Through prompt design, an agent can be steered toward specific reasoning patterns, task formulations, or action strategies, enabling rapid adaptation across diverse tasks and environments.

<a id='885c98fa-7ed0-4f0f-8199-8ab184a3e5e1'></a>

A *prompt* refers to the input context provided to the agent's core model, typically consisting of instructions,
examples, or task descriptions that specify the desired behavior. By modifying or composing prompts, an agent
can be adapted to new goals or environments without any additional model training, making this approach highly
efficient and easily transferable across tasks. Such prompt-based adaptation has been widely adopted in recent
agentic systems, such as CAMEL [37], AutoGen [38], MetaGen [39] and ChatDev [40]. For a comprehensive
overview of prompt engineering techniques and their design principles, we refer readers to the survey by Sahoo
et al. [41].

<a id='9b83e3cd-e61f-4573-bbd8-30f781109dc9'></a>

### 2.2.2 Fine-Tuning
In contrast to prompt engineering, *fine-tuning* achieves adaptation by updating the internal parameters of the core model. Through exposure to task-specific data, *fine-tuning* enables the model to internalize new knowledge, reasoning patterns, or behavioral tendencies that better align with the target domain or task objectives.

<a id='9aeb5114-50e5-4816-b40e-9937352d7273'></a>

Fine-tuning can be performed at different granularities depending on data availability, computational cost, and the desired degree of adaptation. *Full fine-tuning* updates all model parameters using labeled data, providing maximal flexibility but often requiring substantial resources. Alternatively, *parameter-efficient fine-tuning* (PEFT) methods, such as low-rank adaptation (LoRA) [19], update only a small subset of parameters. These approaches offer a practical balance between efficiency and performance, enabling large agentic systems to be specialized for particular tasks or environments without extensive retraining. For a comprehensive overview of PEFT methods, we refer readers to the survey by Han et al. [42].

<a id='8b632e28-eee7-49e6-8c91-9362b14fe32a'></a>

Fine-tuning for adapting agents encompasses several major training paradigms. Supervised Fine-Tuning (SFT) [43] performs imitation learning on curated demonstrations. Preference-based methods, such as Direct Preference Optimization (DPO) [44] and its extensions [45], align the model with human or automated preference signals. Reinforcement-learning-based approaches, including algorithms such as Proximal Policy Optimization (PPO) [46] and Group Relative Policy Optimization (GRPO) [47], further adapt agents by optimizing their behavior

<a id='53e427b9-0c1d-47f2-a385-f8c4358641f1'></a>

'While memory is a fundamental component of an agent, this survey classifies _adaptive memory systems_ under the Tool Adaptation
paradigm (specifically T2, discussed in &sect;5.2.3). We frame them as external, optimizable tools, such as retrievers or reflective databases,
that are "tuned" using the frozen agent's outputs as supervision.

<a id='215471eb-a232-4bff-83c3-61ef7448178c'></a>

5

<!-- PAGE BREAK -->

<a id='3d28937b-15f9-4273-8ea7-61fb860e2f8a'></a>

Adaptation of Agentic AI

<a id='21b9fad9-7b44-4a9f-9e3f-0b12562d9dc0'></a>

<::Icon: A stylized letter 'A' in a rounded square, with flames rising from its top left corner.:> Agent Adaptation
A1 Tool Execution Signaled
Agent Adaptation
<::Diagram: A two-part diagram separated by a dashed vertical line. The left side, labeled 'SFT', shows two parallel processing paths. The top path is x -> A (orange rounded square) -> a* -> T (green rounded square) -> y'. The bottom path is x -> A (orange rounded square) -> a -> T (green rounded square) -> y. A red double-headed arrow connects a* and a, labeled 'SFT'. A red curved arrow points from the 'A' in the bottom path to a*. The right side, labeled 'RL', shows a single processing path: x -> A (orange rounded square) -> a -> T (green rounded square) -> y -> magnifying glass icon. A red curved arrow points from 'y' back to 'A', labeled 'RL'.::>

<a id='67894d6d-54fc-4e68-8018-ac6b324100bd'></a>

A2 Agent Output Signaled Agent Adaptation<::flowchart: The diagram illustrates two phases of agent adaptation. The first phase, on the left, shows an agent 'A' receiving input 'x'. It interacts with a task 'T', sending action 'a' and receiving observation 'y'. The output 'o' from 'A' is compared to an optimal output 'o*', with a feedback loop (red arrow) to 'A' for Supervised Fine-Tuning (SFT). A dashed arrow from 'T' to the 'a'-'y' interaction is labeled "not optimized". The second phase, on the right, separated by a dashed line, shows a similar setup. Agent 'A' receives 'x' and interacts with task 'T' (sending 'a' and receiving 'y'). The action 'a' from 'A' to 'T' is highlighted in red. The output 'o' from 'A' is fed into a component represented by a magnifying glass icon, with a feedback loop (red arrow) to 'A' for Reinforcement Learning (RL).::>

<a id='05cd9b40-d0b0-4ffa-a69c-61d8cd5005c0'></a>

<::flowchart: Tool Adaptation

This diagram illustrates two stages of tool adaptation. The top section shows a stylized 'T' icon with a flame, followed by the text "Tool Adaptation" and a horizontal line.

The bottom section, labeled "T1 Agent-Agnostic Tool Adaptation", depicts a process:
1. A dashed-border box containing a stylized 'T' is connected by a red arrow labeled "Any Tuning" to a solid-border box also containing a stylized 'T'. This represents a tuning process for the tool.
2. A large gray arrow points to the right, leading to the final adapted system.
3. The final adapted system consists of two connected components: a stylized 'T' (tool) in a green box above a stylized 'A' (agent) in an orange box.
4. Input 'x' flows into 'A', and output 'o' flows out from 'A'.
5. There is a bidirectional interaction between 'T' and 'A': an arrow labeled 'a' points from 'A' to 'T', and an arrow labeled 'y' points from 'T' to 'A'.
::>

<a id='d2eece74-efc5-464e-ad30-e6eae2200bdc'></a>

T2 Agent-Supervised Tool Adaptation<::A diagram illustrating two methods for agent-supervised tool adaptation: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). The diagram is divided into two main sections by a dashed vertical line.The left section, labeled "SFT", shows two similar agent-tool interaction setups. In each setup, an Agent (represented by a peach-colored box labeled 'A') receives an input 'x' and produces an output 'o' (or 'o*'). The Agent 'A' also sends an action 'a' to a Tool (represented by a green-colored box labeled 'T'), and the Tool 'T' sends a response 'y' (or 'y'') back to the Agent 'A'. A red curved arrow labeled "SFT" points from the Tool 'T' in the first setup to the Tool 'T' in the second setup, with a double-headed red arrow connecting 'y' and 'y'' between the two setups, indicating a supervised learning signal.The right section, labeled "RL", shows a single agent-tool interaction setup. An Agent 'A' receives input 'x' and produces output 'o'. The Agent 'A' sends an action 'a' to a Tool 'T', and the Tool 'T' sends a response 'y' back to the Agent 'A'. A red curved arrow labeled "RL" originates from the output 'o' (represented by a magnifying glass icon) and points back to the Tool 'T', indicating a reinforcement learning signal for tool adaptation.: diagram::>--- 

<a id='bf59ce68-1fe0-4b07-b4cd-6d870fcf3d37'></a>

Figure 3 Illustration of Four Adaptation Paradigms (A1, A2, T1, and T2). In all the panels, letters highlighted in Red denote the components directly being optimized during adaptation. The red arrows show the sources of adaptation signals. The dotted black lines separate the cases of supervised fine-tuning (SFT) and reinforcement learning (RL).

<a id='8e140312-6cb0-40ce-b4ca-fb0ea51332b6'></a>

through interaction with evaluative environments. These families of methods form the core techniques for adapting foundation-model-based agents to specialized tasks and deployment settings. For a more comprehensive review of these approaches, see the survey by Zhang et al. [48].

<a id='6be5f6f1-58dd-4b57-a93e-64abca5c7736'></a>

We next introduce a general framework that categorizes existing agentic AI adaptation approaches. This framework, presented in the following section, forms the conceptual foundation for the rest of this survey.

<a id='0556f491-3c51-48a6-aadf-f574bc48f99f'></a>

## 3 Overview of Adaptation Paradigms of Agentic AI
In this section, we provide an overview of the adaptation paradigms that form the analytical basis of this paper. Our objective is to establish a unified framework for categorizing existing studies on agentic AI systems according to **what is adapted** (the agent or the tool) and **how the adaptation signal is obtained**. We summarize these perspectives into four canonical paradigms, which together capture the major directions of adaptation explored in recent literature.

<a id='332bd2c7-cc58-46ef-a241-726f747d981c'></a>

To facilitate a clear understanding of these paradigms, this section proceeds in three parts. We first introduce the

<a id='43e3131e-59f9-4e5f-a51d-8fc9bec7370c'></a>

6

<!-- PAGE BREAK -->

<a id='1e0786ca-8a5f-4e88-afac-ce8bd8c0e522'></a>

Adaptation of Agentic AI
---

<a id='c4e1bb80-d870-4e31-ad0d-a280abafdc90'></a>

mathematical notations that are used throughout this paper (§3.1). We then provide formal expressions for the four paradigms (§3.2). Finally, we present illustrative examples that help clarify how each paradigm operates and how they differ in adaptation mechanisms (§3.3).

<a id='b8b2013f-c40d-4fef-99a1-dc02a594cefe'></a>

## 3.1 Mathematical Notations
To ensure consistency in the subsequent formalization, here, we introduce the key mathematical notations used throughout this paper. We organize the notations into three conceptual categories that together define the adaptation process of agentic AI systems: the adaptation targets, the adaptation data sources, and the adaptation objectives.

<a id='3b343ba8-2f6d-4f49-9fba-c31796e800b1'></a>

**Adaptation Targets.** This category specifies the entities that undergo adaptation within an agentic AI system.

*   **Agent** ($\mathcal{A}$): The foundation model that serves as the core reasoning and decision-making component of the system, parameterized by $\theta$. Adaptation of the agent can occur through *parameter updates*, *prompt refinement*, or other modifications to its internal policy.
*   **Tool** ($\mathcal{T}$): The set of external callable components that extend the agent's capabilities beyond its internal parameters. Tools can include retrievers, planners, executors, simulators, or other computational modules. In this paper, we also **categorize the memory module within** $\mathcal{T}$, since memory can be viewed as a dynamic and updatable database that interacts with and learns from the agent's outputs. Typically, the retrieval process for accessing stored information is performed through a dedicated retriever or search tool, which allows the agent to query and integrate relevant past knowledge into its reasoning process.

<a id='8c19a290-f7a9-4f6d-9247-28d81ee7f3ec'></a>

Adaptation Data Sources. This category describes the sources from which the adaptation signals are obtained.
*   **Offline Data** (*D*): Offline data that serve as alignment references or supervision sources for improving either the agent or the tool. These data may include human-labeled demonstrations, synthetic trajectories, or logs of prior interactions.
*   **Environment** (*E*): The external environment in which the agent or tool interacts and receives feedback. It provides online experience signals that reflect task performance or execution quality.

<a id='5030885a-d0bf-4cc4-97e9-efd184cee34d'></a>

**Adaptation Objectives.** Having defined the adaptation targets and data sources, we next describe the objective that guides the adaptation process, which quantifies performance or alignment quality.
* **Objective Function** $\mathcal{O}(\cdot)$: The objective function optimized during adaptation, which evaluates how effectively the agent–tool system performs according to the designated evaluation protocol. For example, the objective for offline data $\mathcal{D}$ may correspond to supervised or imitation learning losses such as supervised fine-tuning (SFT) or behavior cloning. When adaptation relies on interactions with the environment $\mathcal{E}$, the objective is typically defined by outcome-based metrics such as task success rate.

<a id='7c62c5b4-2adc-4ea6-bd24-705a95a03e64'></a>

These notations provide a unified foundation for expressing how adaptation operates at both the agent and tool levels, which we formalize in the subsequent subsection.

<a id='84443e0f-3aad-4c57-9f93-daf9aa171034'></a>

## 3.2 Four Adaptation Paradigms of Agentic AI

Building upon the mathematical notations introduced earlier, we now present the four adaptation paradigms proposed in this paper, which together form a unified framework for classifying existing approaches to agentic AI adaptation. In this framework, adaptation is first categorized by the optimization target, namely the _agent_ or the _tool_. For _agent adaptation_, we further differentiate paradigms based on the type of optimization signal used, which may originate from tool-execution feedback (A1) or from evaluations of the agent's own final output (A2). For _tool adaptation_, the distinction instead concerns whether the adaptation process involves the agent, where tools may be optimized independently of any agent (T1) or adapted under the supervision of a fixed agent (T2). Taken together, these considerations give rise to four paradigms, A1, A2, T1, and T2, which collectively characterize the principal modes of adaptation explored in agentic AI research.

<a id='3a5d6657-4ec6-4d75-8bd4-ea6f7d5dd8fd'></a>

7

<!-- PAGE BREAK -->

<a id='225df67c-d89f-4568-9abc-c83d6a3022da'></a>

Adaptation of Agentic AI

<a id='908c112e-b441-4690-980a-98666c57b289'></a>

### 3.2.1 A1: Tool Execution Signaled Agent Adaptation
This paradigm focuses on improving the agent A through feedback signals derived from the execution results of external tools T. It captures scenarios where the agent interacts with tools in a verifiable manner, allowing the tool outcomes to serve as a measurable basis for optimization.

<a id='74602976-ad59-4b9d-8496-aa4714b923d4'></a>

**Agent-Tool Interaction Process.** The agent receives an input x (e.g., a user query or task description) and generates a structured tool call or action a = A(x), which may include the tool name, arguments, and calling context. The tool set T then executes this call to produce a result y = T(a). The pair (a, y) represents a single agent-tool interaction, and the overall process can be summarized as
x -> A -> T -> y.

<a id='94bfc841-59ec-4733-b382-884c528b281e'></a>

This pipeline captures how the agent leverages tools to complete tasks. For simplicity and without loss of generality,
we describe the interaction using a single tool invocation; multi-turn tool use follows as a direct extension of the
formulation.

<a id='4e5250b8-71ab-46bd-98a5-72de109fa8ec'></a>

**Optimization Objective.** Given this interaction process, the general optimization goal is to adjust the agent *A* to generate high-quality tool call action *a* such that the tool-executed outcomes achieve better performance. Formally,

(A1) *A*<sup>*</sup> = arg max<sub>*A*</sub> *O*<sub>tool</sub>(*A*, *T*),
(1)

<a id='6de707a7-d1e2-42aa-80c7-bd1b72e96e45'></a>

where O_tool measures the quality or correctness of the outputs obtained from invoking T, such as tool execution success rate or retrieval scores. This optimization can be instantiated in two primary forms: (1) by imitating collected successful tool-call trajectories, or (2) by generating actions interactively and using the resulting tool feedback to optimize A via Reinforcement Learning.

<a id='65c348de-2c0b-4df8-b419-34865e1dec65'></a>

• **Supervised Fine-Tuning (SFT)**. When explicit target actions are available, the agent learns to imitate successful tool-using behaviors from recorded trajectories without performing online interaction. Let $D_{succ} = \{(x, a^*)\}$ denote a dataset of input $x$ and reference action $a^*$ that is known to lead to a correct or desirable tool outcome ($y'$). The supervised objective is formulated as:

<a id='549ee7f9-9308-4d88-80e9-72e2609fbacd'></a>

A* = arg min~A~ E~(x,a*)~Dsucc~ [l(A(x), a*)] \equiv arg max~A~ E~(x,a*)~ [log p~A~(a*|x)], (2)

<a id='5556cf08-0647-4f02-b670-b47fcaa3d2ad'></a>

where ℓ denotes the cross-entropy loss used for next-token prediction in language models.

<a id='b7f03924-57be-4645-9454-4d1f467fb225'></a>

• **Reinforcement Learning (RL)**. Alternatively, the agent can acquire adaptation signals through interactions with the environment, where it executes tool calls and receives evaluative feedback from the resulting outcomes.
The process follows:

$x \xrightarrow{A} a \xrightarrow{T} y$, with reward $R = O_{tool}(y)$.

<a id='c6ee9d60-5331-4670-a500-effbea8f44ff'></a>

Here, the agent $\mathcal{A}$ generates an action or tool call $a$ based on input $x$, the tool $\mathcal{T}$ executes $a$ to produce a result $y$, and the evaluation function $\mathcal{O}_{tool}$ assigns a scalar feedback $R$ indicating task success or quality. The optimization objective can be expressed as:

<a id='211e38d7-9a91-4fa4-b882-db62cd3cee36'></a>

J(A) = E$_{x\sim\mathcal{D}_0, a\sim A(\cdot|x), y=\mathcal{T}(a)}$ [$\mathcal{O}_{tool}(y)$],
(3)

<a id='7b5a3e69-5123-44fb-bc79-8956d41ec017'></a>

where $\mathcal{D}_0$ denotes the input distribution.

<a id='37d49207-953a-4fba-bf7d-4f06a0fb04b4'></a>

### 3.2.2 A2: Agent Output Signaled Agent Adaptation

Unlike the Al paradigm, where the adaptation signal is derived from tool-execution outcomes, the A2 paradigm obtains its optimization signal from the agent's own final output. For simplicity and without loss of generality, the following description focuses on a single-turn interaction; multi-turn tool use, where the agent invokes multiple tool calls and integrates multiple intermediate results, extends naturally from the formulation.

<a id='ab3c0b10-f009-4502-b70a-9273f3f8104e'></a>

8

<!-- PAGE BREAK -->

<a id='00116e8e-dfe0-4eba-9c86-71ccdca1f392'></a>

Adaptation of Agentic AI

<a id='b11b371a-6776-4467-af76-2cdab81c7ff8'></a>

**Agent–Tool Interaction Process.** In the A2 paradigm, the agent first generates a tool call _a_ from the input _x_, the tool _T_ executes this call and returns an executed result _y_, and the agent then integrates _x_ and _y_ to produce the final output _o_:

_x_ &#x2192;<sup>_A_</sup> _a_ &#x2192;<sup>_T_</sup> _y_ &#x2192;<sup>_A_</sup> _o_,

<a id='cec6b0de-25a4-483a-86df-a5fcf760fa77'></a>

where $o = \mathcal{A}(x, a, y)$. This formulation naturally includes the special case where the agent produces $o$ directly without calling any tools.

<a id='9b05a43c-c1af-4c09-9b5b-75259ebfd7c8'></a>

**Optimization Objective.** The goal of A2 adaptation is to optimize the agent such that its final output aligns with correctness, quality, or alignment criteria. Formally:

(A2) $A^* = \underset{A}{\operatorname{arg\,max}} \mathcal{O}_{agent}(A,T)$, (4)

<a id='f83d817a-2a75-4e26-8e38-a010a1b18823'></a>

where Oagent evaluates the final output o generated by the agent. Similarly, A2 paradigm optimization also includes two main forms:

<a id='27bd4cda-7cfe-4c37-8350-28def68baafc'></a>

• **Supervised Fine-Tuning (SFT)**. Let $\mathcal{D}_{ans} = \{(x, y, a^*, o^*)\}$ denote a dataset consisting of the input $x$, optional intermediate tool outputs $y$, the reference tool call $a^*$, and the corresponding target final output $o^*$. A key characteristic of the A2 paradigm is that its adaptation signal comes solely from the final agent output. However, supervising only the final output $o^*$, i.e., optimizing

<a id='2ecd04a6-2257-4279-a7b4-aaa455a1bd87'></a>

A* = arg min_A E_{(x,y,a*,o*)~D_{ans}} [l(A(x,a*,y),o*)] = arg max_A E_{(x,y,a*,o*)} [log p_A(o*|x, a*,y)], (5)

<a id='886178ac-0e45-440f-8eb4-2cd3ff6d3147'></a>

is insufficient for learning tool-use behavior: the agent could improve its final-answer likelihood without ever invoking tools, since the supervision provides no incentive to produce the correct tool call. Therefore, for A2-style SFT to effectively support tool-using agents, it must combine final-output supervision with tool-call supervision, effectively integrating A2-style supervision with the A1-style imitation of tool-use trajectories. The supervised objective then becomes:

<a id='791f313b-4996-4d45-bceb-059c80875ddf'></a>

$$A^* = \underset{\mathcal{A}}{\text{arg min}} \mathbb{E}_{(x,y,a^*,o^*) \sim \mathcal{D}_{\text{ans}}} \left[ \ell(\mathcal{A}(x), a^*) + \ell(\mathcal{A}(x, a^*, y), o^*) \right],$$

<a id='59a44f9c-07ad-4407-bade-156885382941'></a>

which is equivalently written as:

$$\underset{A}{\arg \max} \mathbb{E}_{(x,y,a^*,o^*)} [\log p_A(a^*|x) + \log p_A(o^*|x, a^*, y)]. \quad (6)$$

<a id='87147f83-55f6-4919-99e1-1296fa3f08bc'></a>

Here, $\ell$ denotes the cross-entropy loss used in next-token prediction for LLMs. The first term teaches the agent to make correct tool calls (A1-style imitation), while the second term supervises the agent's final answer generation (A2-style imitation). This formulation naturally includes the special case where no tools are invoked, in which case $a^*$ and $y$ are empty.

<a id='cc1dcace-263e-466d-880a-0f7148f797f9'></a>

• **Reinforcement Learning (RL)**. When explicit target outputs are unavailable, the agent learns from feedback assigned to its final response. The interaction follows:

<a id='5d6210c1-7461-4a1e-86ef-25087aa712a3'></a>

x $\stackrel{A}{\mapsto}$ a $\stackrel{T}{\mapsto}$ y $\stackrel{A}{\mapsto}$ o, with reward R = $\mathcal{O}_{agent}(o)$.

The optimization objective becomes:

J($\mathcal{A}$) = $\mathbb{E}_{x \sim \mathcal{D}_0, a \sim \mathcal{A}(\cdot|x), y=\mathcal{T}(a), o=\mathcal{A}(x,a,y)} [\mathcal{O}_{agent}(o)]$, 

<a id='e8f2acd1-4883-40dc-9ca7-1cab06a8d046'></a>

where $\mathcal{D}_0$ is the distribution of task inputs. Here, the agent receives rewards based solely on the quality of its final output, irrespective of how many intermediate tool calls were invoked.

<a id='6f0a0c76-980c-41ab-a259-8bf127c4e57a'></a>

### 3.2.3 T1: Agent-Agnostic Tool Adaptation

In the T1 paradigm, the agent _A_ is kept fixed, and adaptation is applied to only the external tool set _T_. This setting arises naturally when the agent is a powerful and robust closed-source API (such as GPT, Claude, or Gemini) that usually cannot be fine-tuned, or when the goal is to enhance the fixed agent by training specialized tools to complement the frozen agent, such as retrievers, rerankers, planners, simulators, or additional foundation models. In this sense, a "tool" in T1 primarily refers to a trainable model, regardless of whether it is a traditional machine-learning model or a large-scale foundation model.

<a id='80f5d39d-0f1f-4a75-8d3c-70d9e5bd859c'></a>

9

<!-- PAGE BREAK -->

<a id='f4be9f28-09cf-443a-b922-582be0862c18'></a>

Adaptation of Agentic AI

<a id='6e810581-099c-4480-8997-def4be6174e5'></a>

**Optimization Objective.** The goal of T1 is to optimize the tool in an agent-agnostic manner:
(T1) $\mathcal{T}^{*} = \arg \max_{\mathcal{T}}\mathcal{O}_{tool}(\mathcal{T})$

<a id='29fe8e48-69ac-4a09-a725-a0a287ca1415'></a>

where Otool(_T_) evaluates the quality of tool-produced results, often through metrics such as retrieval accuracy,
ranking quality, simulation fidelity, or downstream task success. Since the agent is fixed and only _T_ is trainable,
T1 reduces to standard model training under various learning paradigms, such as supervised learning, contrastive
learning, or reinforcement learning.

<a id='ccba9b3f-d0ee-48af-8a5a-f2d740e7e6b7'></a>

### 3.2.4 T2: Agent-Supervised Tool Adaptation
In the T2 paradigm, tool adaptation is guided by the frozen agent _A_. Unlike T1, where tools are trained independently of any agent, T2 explicitly aims to adapt or construct a tool that complements the fixed agent and enhances its overall capability. This setting reflects a practical motivation: when the main agent is a powerful closed-source foundation model, it is often preferable to train auxiliary tools around it rather than modifying the agent itself.

<a id='dcea8b6e-30cf-4cf9-be59-04242e40e6cb'></a>

**Agent-Tool Interaction Process.** Without loss of generality, we describe the interaction using a single-turn example, noting that multi-turn processes can be extended naturally from this. The agent receives input _x_ and produces a tool call _a_ = _A_(_x_). The tool _T_ executes this call to return a result _y_ = _T_(_a_), and the agent integrates (_x_,_a_,_y_) or (_x_,_y_) to produce the final output _o_:
x _→^A_ a _→^T_ y _→^A_ o.

<a id='43c87eed-85de-4563-8a01-7cfac640dee8'></a>

**Optimization Objective.** The tool is optimized to improve the performance of the fixed agent-tool system:

(T2) $T^* = \arg \max_T \mathcal{O}_{agent}(\mathcal{A}, \mathcal{T}),$

<a id='7b0f1ca4-a8f3-4479-8595-d164a5147628'></a>

where $O_{\text{agent}}$ evaluates how effectively the agent performs when equipped with tool $T$. This objective emphasizes that T2 adapts the tool specifically to the needs of the given agent. Tool adaptation in T2 generally takes two forms:

*   **Supervised Learning.** In the supervised setting, the frozen agent provides signals that indicate how the tool should improve. The core idea is to adjust the tool so that its future outputs $T(a)$ become more helpful for the agent's downstream reasoning. This can be instantiated in several ways. For example:

    *   **Quality-Weighted Training.** The agent's final output $o$ induces a quality score $w = w(o)$ that reflects the desirability or correctness of the agent's behavior. The tool is trained by weighting each trajectory according to this score:

    $T^* = \arg \min_T \mathbb{E}_{(a,y,o)}[w(o) \ell(T(a), y)],$

    where $\ell$ is a task-specific loss encouraging the tool's output to improve. If $w(o)$ takes binary values \{0,1\}, this reduces to a data-selection scheme where only trajectories associated with desirable agent outputs $o$ are used to train the tool.

    *   **Output-Consistency Training.** The agent's final output $o$ induces an implicit supervision target $\tau = \phi(a,y,o)$, which prescribes how the tool output should change to better support the agent. The tool is updated by:

    $T^* = \arg \min_T \mathbb{E}_{(a,y,o)}[\ell(T(a), \tau)],$

    where the mapping $\phi(a,y,o)$ extracts a learning target from the relationship between $y$ and $o$. This encourages the tool to produce outputs that more effectively align with the agent's downstream reasoning.

*   **Reinforcement Learning (RL).** The tool is updated using a scalar reward based on the final quality of the agent's output. Let $R = O_{\text{agent}}(o)$ denote the reward assigned to the final output $o = A(x,a,y)$. The RL objective becomes:

    $J(T) = \mathbb{E}_{x \sim D_0, a=A(x), y=T(a), o=A(x,a,y)}[O_{\text{agent}}(o)],$

where $D_0$ is the distribution of task inputs.

<a id='1dbddd25-537f-4073-965c-d21c1592095c'></a>

10

<!-- PAGE BREAK -->

<a id='90a984a9-a7ba-4ca7-b425-908622822229'></a>

Adaptation of Agentic AI

<a id='bcd1ae93-1709-440d-b86c-09d379bc22c1'></a>

Memory as a Special Case of T2. In this paper, the memory module storing long-term memory is treated as a tool within the T2 paradigm. Although memory serves a conceptual role distinct from conventional executable tools, its update mechanism aligns precisely with the agent-driven tool adaptation view. During interaction, the frozen agent produces a final output _o_ that reflects its reasoning over both the input and retrieved memory contents. This final output _o_ is then used to update the memory module through a fixed or learnable write function: $\mathcal{M}\leftarrow Update(\mathcal{M},o)$. where $\mathcal{M}$ denotes the memory store. This process corresponds directly to T2: the agent remains fixed, the adaptation signal originates from the agent's own output, and the memory module is optimized to better support future agent reasoning.

<a id='07563d33-43cd-4f16-bbd9-964a0fdd1104'></a>

Thus, adaptive memory systems naturally fall under the T2 paradigm: the tool $\mathcal{T}$ being optimized is the memory module, and the supervision signal arises entirely from the behavior of a fixed agent $\mathcal{A}$ interacting with and benefiting from that memory.

<a id='cb6ba8de-79b7-4cdd-a027-e51b3ed80ceb'></a>

### 3.3 Illustrative Examples
To make the above adaptation paradigms more concrete, this section provides illustrative examples drawn from two representative application settings: retrieval-augmented generation (RAG) and code-execution-based tasks. These two settings are chosen because they highlight the central role of tool use in agentic AI systems, while exhibiting distinct tool-agent interaction patterns and evaluation protocols.

<a id='c3ba5f95-e86c-4c41-a398-3a6cc02eda7a'></a>

For each application, we present a pair of examples that correspond to the A1 and A2 paradigms. The paired examples share the same form of tool-call action, i.e., document retrieval or executing code, respectively. This allows us to clearly contrast tool-feedback-based agent adaptation (A1) with agent-output-based agent adaptation (A2). Finally, we provide a T2 example in the RAG setting. By examining these examples side by side, readers can develop an intuitive understanding of how these paradigms differ in objectives, update signals, and learning dynamics.

<a id='12fe2f0e-8db6-4556-b37c-6050a4133d12'></a>

### 3.3.1 Agent Adaptation Examples Across Two Applications

We now illustrate the A1 and A2 paradigms through two representative tool-use settings: retrieval-augmented generation (RAG) and code-execution-based question answering. For each application, we begin by describing the underlying problem setup, followed by a pair of examples that instantiate A1 and A2 under the same form of tool-call action. This allows a clean comparison between adaptation driven by tool-execution feedback (A1) and adaptation driven by agent final outputs (A2).

<a id='610cec59-0bb6-4c64-bec7-d9e588f52ae5'></a>

**Retrieval-Augmented Generation (RAG) Setting.** In the RAG setting, the agent receives a query and performs a retrieval action to obtain relevant documents from a database. Formally, the agent produces a retrieval query _a_, the retriever returns a set of documents _y_, and the agent synthesizes these documents together with the original query to generate a final answer _o_.

*   **A1 example.** DeepRetrieval [21] optimizes the agent using feedback signals computed directly from retrieval quality. After generating a retrieval query _a_, the retriever returns documents _y_, and metrics such as recall or nDCG are computed from _y_ and used as the reward for updating the agent. Since the adaptation signal depends solely on the tool-execution outcome, this represents the A1 paradigm.
*   **A2 example.** Search-R1 [49] follows the full RAG pipeline, where the agent first retrieves documents and then integrates them into its context to produce a final answer _o_. The adaptation signal is computed from the correctness or quality of this final answer by calculating exact matching accuracy. Because the optimization is guided by the agent's final output rather than the retrieval result alone, this falls under the A2 paradigm.

<a id='93e2a407-46e2-481b-888f-83cbf03d1b80'></a>

**Code-Execution-Based Task Setting.** In code-execution-based tasks, the agent receives a problem description and produces executable code as the tool-call action. The sandbox executes the code and returns an execution result _y_, which the agent may optionally use to generate a final answer _o_.

<a id='04e5aa8e-ffc6-46d0-928a-2f1a3c776ddb'></a>

11

<!-- PAGE BREAK -->

<a id='c603f57c-bd79-4bd3-9ad5-f4d736afd4d4'></a>

Adaptation of Agentic AI
---

<a id='d6f4c194-1196-4094-ad09-252f7ab0fba3'></a>

• A1 example: DeepSeek-R1 (code) [24]. During reinforcement learning, DeepSeek-R1 generates code that is executed inside a sandbox. The execution output, such as test-case pass rate or numerical correctness, is used directly as the reward for policy optimization. Since adaptation is based entirely on the tool's execution result, this example fits the A1 paradigm.

<a id='a7c7be77-778e-4fa9-ac67-82ebb0ca6bcb'></a>

*   **A2 example:** ReTool [50] also generates executable code, but the sandbox result is fed back into the agent as additional context. The agent then produces a final answer *o*, whose correctness determines the reward. Because the adaptation signal depends only on the final output of the agent after integrating tool feedback, this corresponds to the A2 paradigm.

<a id='385c571a-cf20-4de5-8620-4f3b64d50cea'></a>

### 3.3.2 Tool Adaptation Examples in the RAG Setting

In many practical systems, the central agent is instantiated as a powerful closed-source API model (such as GPT-, Claude-, or Gemini-style models) that already exhibits strong performance and robustness across a wide range of tasks. Training from an open-source model to match this level of robustness is extremely challenging: the data quality must be carefully curated, scaling laws suggest that much larger models and much more data are required for competitive performance, and such models demand substantial training infrastructure. As a result, a convenient and often more feasible strategy is to treat the closed-source API model as a fixed agent and instead adapt auxiliary tools around it. In the RAG setting, this motivates tool adaptation for components such as retrievers, which can be optimized to complement the fixed agent.

<a id='0490a3fa-17b5-490b-8c12-6b8e6db8cd0e'></a>

T1 examples.

(1) **Classic Dense Retrievers.** Under the T1 paradigm, tools are trained independently of any specific agent and can be plugged into frozen LLM agents without further co-adaptation. A canonical example is a standard dense retriever, such as a bi-encoder trained with contrastive learning to map queries and documents into a shared embedding space for vector similarity search. Once trained, such a retriever can be used as a standalone retrieval tool: given a tool call action _a_ (a retrieval query), the dense retriever returns a ranked document set _y_ = _T_(_a_) optimized for recall or semantic relevance. A closed-source agent (e.g., GPT-, Claude-, or Gemini-style) can then consume _y_ to perform downstream reasoning and answer synthesis, despite the agent itself never participating in the retriever's training.

(2) **Learned Subagents as Agent-Agnostic Tools.** Beyond classic dense retrievers, once agent adaptation has produced strong retrieval-oriented models, these learned models can themselves be reused as tools under the T1 paradigm. For example, a model trained in the DeepRetrieval [21] style can be deployed purely as a high-quality subagent that rewrites queries for improved retrieval over specific document databases. Given a tool call action _a_ (a retrieval query), the subagent returns a reformulated query or a curated document set _y_ = _T_(_a_) with enhanced recall or relevance, which is then consumed by a fixed closed-source agent that performs the final reasoning and answer synthesis.

<a id='ac3f4781-6a23-475c-ab2a-e489a841200a'></a>

**T2 examples:** Under the T2 paradigm, the tool is adapted using supervision signals derived directly from a fixed agent's final outputs. In the RAG setting, both s3 [27] and AgentFlow [51] provide representative examples. The tool (a learnable search subagent) is updated based on the fixed agent's output signal so that its behavior becomes increasingly aligned with what the fixed agent needs for successful downstream reasoning.

<a id='df762715-4cd7-4d45-a2e6-39803aa3dd69'></a>

Concretely, in s3 [27], given a question x, the learnable subagent T takes x as input and internally generates a retrieval query a' = T(x). This query is executed on a static search engine to retrieve a document set y, which is then inserted into the frozen agent's context. The fixed agent consumes (x,y) and produces a final answer o = A(x, y). An evaluation function O_agent(o) (e.g., answer correctness) assigns a scalar reward, which is then used to update the tool so that its future retrieval behaviors yield document sets that more effectively support the agent's downstream reasoning. Thus, s3 directly realizes the T2 objective with the agent-output signaled tool adaptation specifically for the fixed agent's downstream performance. AgentFlow [51] further extends this idea by training a more expressive planning-oriented subagent capable of multi-tool decision-making, applying T2 supervision signal to align a richer planning policy with the fixed agent's final-output preferences.

<a id='82531bd8-4562-4633-b825-8db3f720fcd8'></a>

With these illustrative examples in place, we next move on to systematically review the literature associated with each of the four adaptation paradigms.

<a id='ef6142c6-f29e-471e-9cd3-300c953c1093'></a>

12

<!-- PAGE BREAK -->

<a id='3a038293-786f-441d-82b9-48d7b1ab66d6'></a>

Adaptation of Agentic AI

<a id='74a2a33c-eabf-471f-9e94-57f1cf1b962f'></a>

<::timeline chart: The chart displays a development timeline of AI methods from 2022 to 2026. The timeline is structured horizontally with branches for different years and vertical lines indicating specific months or points in time where AI methods were introduced. The methods are often associated with logos or icons. A red arrow points from 2022 to 2023 next to ChatGPT. The caption is below the timeline.Year 2022:  ChatGPT (Dec)Year 2023:  Toolformer (Mar), TRICE (June), Gorilla (June), ToolAlpaca (June), ToolLLM (Sep), LeReT (Dec), ToolFlow (Dec), RLEF (Dec), DeepSeek-Prover-V1.5 (Dec), TP-LLaMA (Dec), LeDex (Dec), AutoTools (Dec)Year 2024:  NEXT (Mar), CodeAct (Mar), RetPO (Mar), DeepRetrieval (June), R1-Code-Interpreter (June), Code-R1 (June), Rec-R1 (June), Router-R1 (Sep), Goedel-Prover-V2 (Sep), FTRL (Sep), AlphaProof (Dec), Orion (Dec)Year 2025:  DeepSeek-R1 (Mar), Kimi-1.5 (Mar), RAGEN (Mar), Tool-N1 (June), SQL-R1 (June), Kimina-Prover (June), DeepSeek-Prover-V2 (June), Leanabell-Prover-V2 (Sep), ToolExpander (Dec), olmOCR 2 (Dec)Year 2026:  The timeline extends to 2026 without specific entries shown.: timeline chart::>Figure 4 Development timeline of AI methods (agent adaptation with tool-execution result as signal).

<a id='e088b5b1-beec-4a00-96be-46bf506c53c9'></a>

## 4 Agent Adaptation
Agent adaptation refers to the mechanisms through which agents refine their behavior and decision-making capabilities based on feedback from their interactions with tools, environments, or their own outputs. This process is pivotal for enhancing the autonomy, reasoning, and generalization abilities of agents across diverse tasks. Broadly, agent adaptation can be categorized into two paradigms: A1, which leverages tool execution results as feedback signals, and A2, which focuses on evaluating the agent's own outputs.

<a id='5e8f782d-b3bc-4797-b4ad-feced0ce8341'></a>

Formally, let A denote an agent, parameterized by its internal configuration or policy (which includes prompt templates or model weights), and let T represent the set of tools accessible to the agent. The agent's performance under a given configuration is evaluated by an objective function O(·), which provides feedback based on either tool performance or agent output quality. Accordingly, the two adaptation paradigms can be formalized as optimization objectives:

<a id='2fa0c679-b7ef-4cc0-8709-f2fe0b679a7f'></a>

(A1) $A^* = \arg \max_A \mathcal{O}_{\text{tool}}(A, \mathcal{T})$, (A2) $A^* = \arg \max_A \mathcal{O}_{\text{agent}}(A, \mathcal{T})$,

<a id='e248e0f0-79d3-4983-b148-a1e478dc953a'></a>

where Otool quantifies the correctness or utility of outcomes derived from tool execution, such as successful code compilation or retrieval precision, and Oagent measures the quality of the agent's generated outputs, including reasoning validity, factual accuracy, or alignment with human preferences. Here, A* denotes the optimized agent configuration that maximizes the corresponding feedback objective.

<a id='43d41357-5ce8-4070-a09f-18bbf027ce8d'></a>

## 4.1 A1: Tool Execution Result as Signal

Tool execution result as signal refers to a class of adaptation mechanisms where an agent leverages the actual outcomes of external tool invocations (such as execution correctness, functional success, or numerical improvement)

<a id='a8228370-f2a6-4309-94e5-5ee4d19c606c'></a>

13

<!-- PAGE BREAK -->

<a id='04844977-d240-4448-8343-1cc12e166acf'></a>

Adaptation of Agentic AI

<a id='d19196d0-02fd-4a18-9bdb-af4026f2c0ee'></a>

as feedback to refine its behavior. Here, the tool or environment serves as an objective source of feedback, forming
a verifiable signal that can drive both supervised and reinforcement-based learning.

<a id='a784228c-e5f1-4069-90af-99b915eda83d'></a>

Table 1 A1 Methods (Tool Execution Signaled): Earlier Methods (SFT & DPO) and Recent RLVR-based Methods
<table id="13-1">
<tr><td id="13-2">Time</td><td id="13-3">Method</td><td id="13-4">Venue</td><td id="13-5">Task(s)</td><td id="13-6">Tool(s)</td><td id="13-7">Agent Backbone</td><td id="13-8">Tuning</td><td id="13-9">Links</td></tr>
<tr><td id="13-a" colspan="8">SFT &amp; Off-Policy Methods</td></tr>
<tr><td id="13-b">2023.02</td><td id="13-c">Toolformer</td><td id="13-d">NeurIPS&#x27;23</td><td id="13-e">QA, Math</td><td id="13-f">Calculator, QA system, Search Engine, Translation System, Calendar</td><td id="13-g">GPT-J</td><td id="13-h">SFT</td><td id="13-i">Document and GitHub icons</td></tr>
<tr><td id="13-j">2023.05</td><td id="13-k">TRICE</td><td id="13-l">NAACL&#x27;24</td><td id="13-m">Math Reasoning, QA</td><td id="13-n">Calculator, WikiSearch, Atlas QA Model, NLLB Translator</td><td id="13-o">ChatGLM, Alpaca, Vicuna</td><td id="13-p">SFT, Contrastive Learning</td><td id="13-q">Document and GitHub icons</td></tr>
<tr><td id="13-r">2023.05</td><td id="13-s">Gorilla</td><td id="13-t">NeurIPS&#x27;24</td><td id="13-u">Tool-Calling, API Retrieval</td><td id="13-v">APIS</td><td id="13-w">LLaMA</td><td id="13-x">SFT</td><td id="13-y">Document and GitHub icons</td></tr>
<tr><td id="13-z">2023.06</td><td id="13-A">ToolAlpaca</td><td id="13-B">arXiv</td><td id="13-C">Multi-Turn Tool-Use</td><td id="13-D">Simulated APIs</td><td id="13-E">Vicuna</td><td id="13-F">SFT</td><td id="13-G">PDF and GitHub logos</td></tr>
<tr><td id="13-H">2023.07</td><td id="13-I">ToolLLM</td><td id="13-J">ICLR&#x27;24</td><td id="13-K">Tool-Calling, API Planning, Multi-Tool Reasoning</td><td id="13-L">Real-World APIs</td><td id="13-M">LLaMA, Vicuna</td><td id="13-N">SFT</td><td id="13-O">PDF and GitHub logos</td></tr>
<tr><td id="13-P">2024.01</td><td id="13-Q">NEXT</td><td id="13-R">ICML&#x27;24</td><td id="13-S">Program Repair</td><td id="13-T">Code Executor</td><td id="13-U">PaLM2</td><td id="13-V">SFT</td><td id="13-W">PDF logo</td></tr>
<tr><td id="13-X">2024.02</td><td id="13-Y">CodeAct</td><td id="13-Z">ICML&#x27;24</td><td id="13-10">Coding</td><td id="13-11">Code Executor</td><td id="13-12">LLaMA2, Mistral</td><td id="13-13">SFT</td><td id="13-14">PDF and GitHub logos</td></tr>
<tr><td id="13-15">2024.02</td><td id="13-16">RetPO</td><td id="13-17">NAACL&#x27;25</td><td id="13-18">IR</td><td id="13-19">Retriever</td><td id="13-1a">LLaMA2-7B</td><td id="13-1b">SFT, DPO</td><td id="13-1c">PDF and GitHub logos</td></tr>
<tr><td id="13-1d">2024.03</td><td id="13-1e">CYCLE</td><td id="13-1f">OOPSLA&#x27;24</td><td id="13-1g">Coding</td><td id="13-1h">Code Executor</td><td id="13-1i">CodeGen, StarCoder</td><td id="13-1j">SFT</td><td id="13-1k">(PDF icon)</td></tr>
<tr><td id="13-1l">2024.05</td><td id="13-1m">AutoTools</td><td id="13-1n">WWW&#x27;25</td><td id="13-1o">Tool-Calling</td><td id="13-1p">APIs</td><td id="13-1q">GPT4, LLaMA3, Mistral</td><td id="13-1r">SFT</td><td id="13-1s">(PDF and GitHub icons)</td></tr>
<tr><td id="13-1t">2024.06</td><td id="13-1u">TP-LLaMA</td><td id="13-1v">NeurIPS&#x27;24</td><td id="13-1w">Tool-Calling</td><td id="13-1x">APIs</td><td id="13-1y">LLaMA2</td><td id="13-1z">SFT, DPO</td><td id="13-1A">(PDF icon)</td></tr>
<tr><td id="13-1B">2024.10</td><td id="13-1C">ToolFlow</td><td id="13-1D">NAACL&#x27;25</td><td id="13-1E">Tool-Calling</td><td id="13-1F">APIs</td><td id="13-1G">LLaMA3.1</td><td id="13-1H">SFT</td><td id="13-1I">(PDF icon)</td></tr>
<tr><td id="13-1J">2024.10</td><td id="13-1K">LeReT</td><td id="13-1L">ICLR&#x27;25</td><td id="13-1M">IR</td><td id="13-1N">Dense Retriever</td><td id="13-1O">LLaMA3, Gemma2</td><td id="13-1P">DPO-like (IPO)</td><td id="13-1Q">(PDF and GitHub icons)</td></tr>
<tr><td id="13-1R" colspan="8">RLVR Methods</td></tr>
<tr><td id="13-1S">2024.05</td><td id="13-1T">LeDex</td><td id="13-1U">NeurIPS&#x27;24</td><td id="13-1V">Coding</td><td id="13-1W">Code Executor</td><td id="13-1X">StarCoder &amp; CodeLlaMA</td><td id="13-1Y">SFT, PPO</td><td id="13-1Z">PDF icon</td></tr>
<tr><td id="13-20">2024.08</td><td id="13-21">DeepSeek-Prover-V1.5</td><td id="13-22">ICLR&#x27;25</td><td id="13-23">Formal Theorem Proving</td><td id="13-24">Lean 4 Prover</td><td id="13-25">DeepSeek-Prover-V1.5-RL</td><td id="13-26">SFT, GRPO</td><td id="13-27">PDF icon, GitHub icon</td></tr>
<tr><td id="13-28">2024.10</td><td id="13-29">RLEF</td><td id="13-2a">ICML&#x27;25</td><td id="13-2b">Coding</td><td id="13-2c">Code Executor</td><td id="13-2d">LLaMA3.1</td><td id="13-2e">PPO</td><td id="13-2f">PDF icon</td></tr>
<tr><td id="13-2g">2025.01</td><td id="13-2h">DeepSeek-R1-Zero (Code)</td><td id="13-2i">Nature</td><td id="13-2j">Coding</td><td id="13-2k">Code Executor</td><td id="13-2l">DeepSeek-V3-Base</td><td id="13-2m">GRPO</td><td id="13-2n">PDF icon</td></tr>
<tr><td id="13-2o">2025.02</td><td id="13-2p">DeepRetrieval</td><td id="13-2q">COLM&#x27;25</td><td id="13-2r">Web Search, IR, Text2SQL</td><td id="13-2s">Search Engine, Retrievers, SQL exec.</td><td id="13-2t">Qwen2.5, LLaMA3.2</td><td id="13-2u">PPO, GRPO</td><td id="13-2v">red paper, blue github logo</td></tr>
<tr><td id="13-2w">2025.03</td><td id="13-2x">Code-R1</td><td id="13-2y">—</td><td id="13-2z">Coding</td><td id="13-2A">Code Executor</td><td id="13-2B">Qwen2.5</td><td id="13-2C">GRPO</td><td id="13-2D">blue github logo</td></tr>
<tr><td id="13-2E">2025.03</td><td id="13-2F">ReZero</td><td id="13-2G">arXiv</td><td id="13-2H">Web Search, IR</td><td id="13-2I">Web Search Engine</td><td id="13-2J">LLaMA3.2</td><td id="13-2K">GRPO</td><td id="13-2L">red paper, blue github logo</td></tr>
<tr><td id="13-2M">2025.03</td><td id="13-2N">Rec-R1</td><td id="13-2O">TMLR&#x27;25</td><td id="13-2P">Recommendation Optimization</td><td id="13-2Q">Recommendation System</td><td id="13-2R">Qwen2.5, LLaMA3.2</td><td id="13-2S">GRPO</td><td id="13-2T">red paper, blue github logo</td></tr>
<tr><td id="13-2U">2025.04</td><td id="13-2V">SQL-R1</td><td id="13-2W">NeurIPS&#x27;25</td><td id="13-2X">Text2SQL Search</td><td id="13-2Y">SQL Engine</td><td id="13-2Z">Qwen2.5, OmniSQL</td><td id="13-30">SFT, GRPO</td><td id="13-31">red paper, blue github logo</td></tr>
</table>

<a id='79f14749-ea51-461c-a469-8a7376597220'></a>

Continued on next page

<a id='a921fc34-feda-400a-8f72-abaaafab105e'></a>

14

<!-- PAGE BREAK -->

<a id='5bd6101c-19f4-4447-a79d-0553e761bc9d'></a>

Adaptation of Agentic AI

<a id='28409378-0f50-4efb-a83d-f0289d20fe52'></a>

Table 1 – Continued from previous page
<table id="14-1">
<tr><td id="14-2">Time</td><td id="14-3">Method</td><td id="14-4">Venue</td><td id="14-5">Task(s)</td><td id="14-6">Tool(s)</td><td id="14-7">Agent Backbone</td><td id="14-8">Tuning</td><td id="14-9">Links</td></tr>
<tr><td id="14-a">2025.04</td><td id="14-b">Kimina-Prover</td><td id="14-c">arXiv</td><td id="14-d">Formal Theorem Proving</td><td id="14-e">Lean 4 Compiler, Numina Lean Server</td><td id="14-f">Qwen2.5</td><td id="14-g">SFT, GHPO</td><td id="14-h">PDF icon, GitHub icon</td></tr>
<tr><td id="14-i">2025.04</td><td id="14-j">DeepSeek-Prover-V2</td><td id="14-k">arXiv</td><td id="14-l">Formal Theorem Proving</td><td id="14-m">Lean 4 Compiler</td><td id="14-n">DeepSeek-V3</td><td id="14-o">SFT, GRPO</td><td id="14-p">PDF icon, GitHub icon</td></tr>
<tr><td id="14-q">2025.05</td><td id="14-r">Tool-N1</td><td id="14-s">arXiv</td><td id="14-t">Tool-Calling</td><td id="14-u">Tool APIs</td><td id="14-v">Qwen2.5</td><td id="14-w">GRPO</td><td id="14-x">PDF icon, GitHub icon</td></tr>
<tr><td id="14-y">2025.05</td><td id="14-z">R1-Code-Interpreter</td><td id="14-A">arXiv</td><td id="14-B">Coding</td><td id="14-C">Code Execution Sandbox</td><td id="14-D">Qwen2.5</td><td id="14-E">GRPO</td><td id="14-F">PDF icon, GitHub icon</td></tr>
<tr><td id="14-G">2025.06</td><td id="14-H">Router-R1</td><td id="14-I">NeurIPS&#x27;25</td><td id="14-J">Multi-Round Routing</td><td id="14-K">LLM Routing Pool</td><td id="14-L">Qwen2.5, LLaMA3.2</td><td id="14-M">PPO</td><td id="14-N">(PDF and GitHub icons)</td></tr>
<tr><td id="14-O">2025.07</td><td id="14-P">Leanabell-Prover-V2</td><td id="14-Q">arXiv</td><td id="14-R">Formal Theorem Proving</td><td id="14-S">Lean 4 Verifier</td><td id="14-T">Kimina, DeepSeek-V2</td><td id="14-U">SFT, DAPO</td><td id="14-V">(PDF and GitHub icons)</td></tr>
<tr><td id="14-W">2025.08</td><td id="14-X">Goedel-Prover-V2</td><td id="14-Y">arXiv</td><td id="14-Z">Formal Theorem Proving</td><td id="14-10">Lean Compiler</td><td id="14-11">Qwen3</td><td id="14-12">SFT, GRPO</td><td id="14-13">(PDF and GitHub icons)</td></tr>
<tr><td id="14-14">2025.08</td><td id="14-15">FTRL</td><td id="14-16">arXiv</td><td id="14-17">Multi-Step Tool-Use</td><td id="14-18">Simulated APIs</td><td id="14-19">Qwen3</td><td id="14-1a">GRPO</td><td id="14-1b">(PDF and GitHub icons)</td></tr>
<tr><td id="14-1c">2025.09</td><td id="14-1d">Tool-R1</td><td id="14-1e">arXiv</td><td id="14-1f">Tool-Augmented Reasoning, QA</td><td id="14-1g">Code Execution, Multimedia Tools</td><td id="14-1h">Qwen2.5</td><td id="14-1i">GRPO</td><td id="14-1j">(PDF and GitHub icons)</td></tr>
<tr><td id="14-1k">2025.09</td><td id="14-1l">WebGen-Agent</td><td id="14-1m">arXiv</td><td id="14-1n">Website Generation</td><td id="14-1o">VLM, GUI Agent, Code Executor</td><td id="14-1p">Qwen2.5-Code, Qwen3</td><td id="14-1q">SFT, Step-GRPO</td><td id="14-1r">(PDF and GitHub icons)</td></tr>
<tr><td id="14-1s">2025.10</td><td id="14-1t">ToolExpander</td><td id="14-1u">arXiv</td><td id="14-1v">Tool-Calling</td><td id="14-1w">Tool APIs</td><td id="14-1x">Qwen2.5</td><td id="14-1y">SFT, GRPO</td><td id="14-1z">(PDF icon)</td></tr>
<tr><td id="14-1A">2025.10</td><td id="14-1B">AlphaProof</td><td id="14-1C">Nature</td><td id="14-1D">Formal Theorem Proving</td><td id="14-1E">Lean Solver</td><td id="14-1F">Transformer (3B Enc-Dec)</td><td id="14-1G">SFT, AlphaZero, TTRL</td><td id="14-1H">(PDF icon)</td></tr>
<tr><td id="14-1I">2025.10</td><td id="14-1J">olmOCR2</td><td id="14-1K">arXiv</td><td id="14-1L">Document OCR</td><td id="14-1M">Synthetic Document Verifier</td><td id="14-1N">Qwen2.5-VL</td><td id="14-1O">SFT, GRPO</td><td id="14-1P">(PDF and GitHub icons)</td></tr>
<tr><td id="14-1Q">2025.11</td><td id="14-1R">Orion</td><td id="14-1S">arXiv</td><td id="14-1T">IR</td><td id="14-1U">Retrievers</td><td id="14-1V">LFM2</td><td id="14-1W">GRPO</td><td id="14-1X">(PDF icon)</td></tr>
</table>

<a id='8b1c6ee9-b8bf-4303-a515-8f7aec24b291'></a>

### 4.1.1 Earlier Works: SFT & Off-Policy Methods
Early AI-type methods typically focus on SFT or DPO, which aim to teach agents using pre-collected data. These methods begin by collecting a set of model responses or trajectories involving tool usage, and then use this data for supervised fine-tuning or DPO. These AI methods share a common foundation in leveraging objective environment-grounded outcomes, but differ in the form, source, and utilization of their feedback signals. The evolution of these imitation-based AI-type methods primarily centers on the transformation in **how training signal is obtained and utilized**.

<a id='0b45a8fd-2f6d-47f2-a32f-6de46fa74103'></a>

The earliest representative, **Toolformer** [4] (NeurIPS 2023), introduced the idea of using tool outcomes as **self-supervised learning signals**. The model automatically inserts candidate API calls into text, executes them, and measures whether the returned result improves token prediction likelihood. A call is retained if it significantly reduces perplexity, formalized as $L_i^- - L_i^+ \ge \tau_f$, where the reduction quantifies a self-supervised *tool execution result signal*. This implicit signal anchors learning in the correctness of tool usage, enabling the model to autonomously discover when external APIs improve performance.

<a id='4b959757-ca8e-42e6-bca9-18a1eb183058'></a>

However, since the training is based on self-supervised feedback, Toolformer remains limited in precision when applied to real executable environments. This limitation motivated a series of subsequent approaches that sought to introduce more reliable, externally grounded learning signals. Building upon this insight, the evolution of AI-type methods can be viewed through three progressively grounded paradigms of learning:
* **Alignment with golden answers:** supervision comes from correct responses or expert trajectories.
* **Alignment with golden formats:** correctness is defined structurally or syntactically.

<a id='5111bdd5-8f7f-4e8d-818d-f12968cea8ec'></a>

15

<!-- PAGE BREAK -->

<a id='668d24a4-6be6-4c09-9e78-5c37c37b51da'></a>

Adaptation of Agentic AI

<a id='3826be35-fdf5-43b4-8e47-dbf8216f4bfa'></a>

*   **Alignment with direct tool execution:** learn from verifiable outcomes produced by executing tools, allowing supervision to emerge from actual tool behavior rather than predefined labels.

<a id='0ddbe31c-7807-4456-b023-5e2486cb751b'></a>

Alignment with golden answers. Early AI-type approaches focused on aligning models with correct final outputs, typically defined by task-specific ground truths or verified expert solutions.

<a id='6dcbe76c-ab98-4da8-b051-8138c7c993f4'></a>

TRICE [52] (NAACL 2024) is a two-stage framework designed to teach LLMs when and how to use tools selectively. The first stage uses supervised fine-tuning to provide the model with a preliminary ability to imitate tool-use behavior. The core of the method lies in the second stage, "Reinforcement Learning with Execution Feedback (RLEF)". In this stage, the agent is trained using a reward signal derived directly from tool execution. The system collects a set of candidate responses for a given task, some of which involve tool calls. A reward strategy then scores each response by comparing its execution result against the ground-truth answer. The model is then reinforced, using a ranking loss, to align its preferences with the high-reward responses, effectively learning from the successful or failed outcomes of tool execution to mitigate excessive reliance on tools and improve accuracy. This design provides a clear instance of learning grounded in correctness alignment, where rewards are explicitly tied to whether the tool-executed outcome matches the golden answer.

<a id='7abc97eb-1d27-4cf6-93aa-abc26d44e3bb'></a>

**ToolAlpaca** [53] represents one of the earliest closed-loop implementations of AI-type adaptation, where the model refines its tool-use capability directly through iterative interaction with executable environments. The model first generates a tool-call candidate, for example an API invocation, which is then executed within the environment. The system records the runtime outcome of each execution, including returned values, errors, or completion states, to determine the correctness of the model's action. This design establishes an automated self-improvement loop consisting of four key stages: *generate, execute, evaluate, and finetune*. Through repeated cycles, ToolAlpaca progressively aligns its internal representation with the actual semantics and behavior of the tools it uses. This closed-loop process embodies the essence of the AI paradigm, where *tool execution results* themselves serve as the primary adaptation signal. ToolAlpaca demonstrates the ability to *generalize* across unseen tools and to adapt its calling strategy according to contextual requirements. By grounding updates in correctness relative to observed outcomes, it implicitly aligns its learning with the "*golden answer*" paradigm.

<a id='0a24e027-5156-460a-b94f-bbf839793910'></a>

TP-LLaMA [54] (NeurIPS 2024) is an inference trajectory optimization framework designed to improve tool-augmented LLMs by learning from errors. The authors observe that prior models like ToolLLaMA [10] are trained via SFT exclusively on successful expert trajectories from the ToolBench dataset, which ignores valuable information contained in failed exploration paths of the decision trees. To address this, their method consists of two stages: first, the model undergoes standard SFT on successful trajectories, similar to the baseline. Second, the framework leverages the "failed paths" by constructing a novel preference dataset called ToolPreference. This dataset is built using a step-wise method: for any decision node along a successful path, the "preferred" output (Yw) is the expert's correct next step, while the dispreferred output (yi) is any corresponding failed branch originating from that same node. The model is then trained on these preference pairs using DPO. This approach explicitly uses the execution feedback from failed attempts as a training signal, enabling the model to learn from failure and significantly enhancing its decision-making, generalization, and reasoning efficiency. TP-LLaMA therefore transforms failure signals into preference-aligned supervision, reinforcing learning according to correctness with respect to expert trajectories.

<a id='3deff68d-51bb-4dc5-97ed-b62a0d7208eb'></a>

**Alignment with golden formats.** A complementary branch of AI-type evolution shifted from output-based correctness to format-based structural correctness, emphasizing syntactic and logical validity of tool calls rather than explicit task answers.

<a id='71d248fb-792e-47e9-be90-d605babfdb46'></a>

Gorilla [55] (NeurIPS 2024) is a retrieval-augmented LLaMA-based language model fine-tuned to generate correct API calls across a large and changing set of machine learning APIs. During training, it leverages self-instructed instruction–API pairs and, optionally, a document retriever to adapt to test-time documentation changes. A crucial component of Gorilla's evaluation and feedback loop is the use of Abstract Syntax Trees (ASTs). Both the model-generated API calls and reference API calls are converted into ASTs, and correctness is determined by checking whether the reference API forms a subtree of the generated AST. Compared to direct text matching, AST-based evaluation is more robust: differences in parameter order or optional arguments do not lead to false negatives, as ASTs focus on the logical structure of the API call. In the context of AI-type adaptation, this serves as a form of

<a id='5cb89aac-ac11-4800-9847-6e69198664ee'></a>

16

<!-- PAGE BREAK -->

<a id='0c801f84-37f2-4e90-96c5-d9e4d22d5ead'></a>

Adaptation of Agentic AI

<a id='1a0df32b-59bc-4735-ab01-e9a4519e6703'></a>

tool execution result signal, providing the model with structured feedback on whether its API call was functionally correct, which can then be used to guide learning and improve tool-use performance. Gorilla therefore exemplifies the golden-format paradigm, where correctness is defined by adherence to canonical structural representations rather than output values.

<a id='a429dc9c-aaaf-4e97-94b2-f392b222b8ca'></a>

ToolFlow [56] (NAACL 2025) is a data synthesis pipeline designed to enhance the tool-calling capabilities of LLMs through the generation of natural and coherent dialogues. Traditional SFT approaches rely on synthetically generated tool-call data. However, previous methods often suffer from low diversity and limited coherence because tools are sampled randomly and dialogues are synthesized as single-turn interactions, ignoring multi-turn dependencies. ToolFlow addresses these limitations by introducing two key strategies: *Graph-based Sampling* and *Planned Generation*. The Graph-based Sampling strategy constructs a tool graph based on parameter and return-value similarities between tools. Nodes represent tools, and edges indicate their relevance, allowing the selection of tool subsets that are likely to interact effectively. This facilitates the generation of complex user requirements involving multiple interconnected tools. The Planned Generation strategy enables the LLM to first create a high-level dialogue plan that organizes user requests across multiple turns, including both tool-call tasks and non-tool interactions. This ensures logical consistency and natural flow throughout the dialogue, resulting in more realistic training data. Overall, ToolFlow provides a systematic approach for generating high-quality multi-turn tool-call dialogues that closely reflect real-world interaction scenarios, and exemplifies structural alignment through graph-based format consistency.

<a id='dbc7cf3b-3f43-4368-98ef-d830b3425777'></a>

Alignment with direct tool execution. The most advanced stage of AI evolution centers on learning directly from verifiable environment signals, where tool execution outcomes themselves become the supervision source.

<a id='c42fee34-f9a9-4450-885a-d7b0d67e2056'></a>

CodeAct [57] (ICML 2024) represented a paradigm in which LLMs learn tool use through direct interaction with executable code environments. Instead of producing textual or JSON-based commands, the model generates executable code actions that are run within a sandboxed environment. The environment returns explicit execution feedback—such as success or failure signals and resulting outputs—which are then used as supervision to refine the model. This process grounds the learning objective in the verifiable outcomes of tool execution, rather than in human-annotated correctness or model-predicted preferences. Through this execution-based feedback loop, CodeAct effectively aligns model behavior with the underlying causal mechanisms of tools.

<a id='11c0cb01-4aa8-4aab-a131-7c9cf55e57a9'></a>

NExT [58] (ICML 2024) is a method designed to teach LLMs to reason about code execution, specifically for program repair tasks. The approach utilizes an iterative self-training loop based on a "Sample-Filter-Train" process. The core of this method lies in its filtering step, which uses a tool's execution result as the primary training signal. An external tool, takes the agent's generated code output and validates it by running a set of unit tests. The binary pass-or-fail diagnostic from this execution serves as a reward signal. Only the candidate solutions (both rationale and code) that successfully pass all unit tests are deemed correct and are collected into a new synthetic dataset, which is then used to finetune the agent for the next iteration. This iterative refinement, guided strictly by the tool's verification, progressively improves the model's ability to generate accurate fixes and high-quality, execution-aware rationales.

<a id='d6196825-1ed6-412f-bba0-2f158684998b'></a>

ToolLLM [10] and AutoTools [59] (WWW 2025) push this paradigm toward autonomous tool learning. Specifically, AutoTools consists of two core stages: *tool encapsulation* and *tool programming*. In the tool encapsulation stage, the LLM automatically parses raw API documentation and transforms each tool into a callable function, including structured docstrings, argument specifications, and usage examples. Syntax and runtime correctness are verified through an integration verification procedure, which checks not only individual functions but also input-output dependencies among related tools. Verified functions are aggregated into a function library for subsequent use. In the tool programming stage, the LLM directly generates executable programs that sequentially call these functions, resolve intermediate outputs, and ultimately solve user queries. This design allows the model to flexibly integrate multiple tools using a unified programming language, instead of relying on specialized tokens or handcrafted formats. To further enhance model expertise, AutoTools introduces *AutoTools-Learning*, a multi-task learning approach that trains the LLM on three synthetic tasks: (1) documentation understanding, (2) relevance learning for selecting appropriate tools, and (3) function learning for generating correct multi-step tool programs. The training dataset comprises 34k high-quality examples synthesized from public sources and existing benchmarks, providing

<a id='d186afb1-74eb-488f-af29-fd52988bdfef'></a>

17

<!-- PAGE BREAK -->

<a id='d87cb329-c1b8-4a33-8228-5f25768f52a3'></a>

Adaptation of Agentic AI

<a id='09fc039e-b989-474e-b2db-5c88f4197f2a'></a>

fine-grained supervision for both tool understanding and programmatic reasoning. Through this combination of automated tool encapsulation, programmatic integration, and multi-task training, AutoTools represents a significant step towards fully autonomous AI-type adaptation. Execution outcomes of each tool call serve as direct feedback for iterative self-improvement, enabling LLMs to handle complex, multi-step tool-use tasks without explicit human intervention.

<a id='7eb27792-dbb3-42f4-b08a-5f0265207f1c'></a>

LeReT [60] (ICLR 2025) is a reinforcement learning framework for improving multi-hop retrieval in LLM pipelines. A query-generating LLM πr produces diverse search queries using few-shot prompt ensembles, and retrieved documents are scored with a reward function R. Queries are converted into preference pairs (yᵢ, yⱼ), where the higher-reward query is preferred. The model is then fine-tuned using preference-based reinforcement learning, specifically *Identity Policy Optimization* (IPO). IPO is a method for directly optimizing a model to reflect human or reward-based preferences. Instead of modeling the probability of a preferred output as in DPO, IPO explicitly enforces that the model's implicit reward difference between preferred and dispreferred outputs matches a target margin. Formally, it minimizes the squared deviation of the reward difference from a fixed margin:

<a id='754b1e8a-5982-47df-8fbf-c637204508ae'></a>

$$ \mathcal{L}_{\text{IPO}} = \mathbb{E}_{(x, y_w, y_l) \sim D_p} \left[ \left( \tilde{r}_\phi(x, y_w) - \tilde{r}_\phi(x, y_l) - 0.5\tau^{-1} \right)^2 \right], $$

<a id='ee2eec1a-b213-43ac-bc71-b1c11cdfcc16'></a>

where $\tilde{r}_{\phi}(x,y) = \log \frac{\pi_{\phi}(y|x)}{\pi_{\text{ref}}(y|x)}$, $y_w$ and $y_l$ denote the preferred and dispreferred queries, $x$ is the context, and $\tau$ is a margin hyperparameter controlling the target difference between rewards. Intuitively, IPO encourages the model to assign higher scores to better outputs and ensures the difference reaches a predefined magnitude, providing a direct and stable training signal. LeReT can be applied iteratively, using the fine-tuned model to generate better exploration data in subsequent iterations. This method improves both retrieval accuracy and downstream generation quality, and can adapt to arbitrary off-the-shelf retrievers without modifying the generator model. As such, it represents the culmination of environment-grounded AI learning, where reward optimization is explicitly tied to real-world, verifiable signals. Similarly, **RetPO [61]** (NAACL 2025) trains a query reformulation model to translate LLM-generated queries into retriever-optimal queries. The training signal is ingenious in its simplicity: GPT-4 generates multiple candidate rewrites, each is evaluated by running it through an off-the-shelf retriever (e.g., BM25), and the retrieval performance (measured by how well the retrieved documents support the correct answer) serves as the reward. A smaller, open-source LM is then trained via DPO to produce high-reward rewrites.

<a id='09e4b96c-c700-481a-888d-d85bd3093079'></a>

Overall, the evolution of Al-type methods reflects a gradual shift from implicit, self-supervised feedback to explicit, execution-grounded learning signals. Early methods such as Toolformer demonstrated the feasibility of using tool execution as a self-supervised signal, but their reliance on internal likelihood metrics limited their external validity. Subsequent approaches strengthened the supervision source by grounding model optimization in correctness relative to golden answers or expert trajectories, allowing models to learn more directly from verified outcomes. The next wave of methods, such as Gorilla and ToolFlow, advanced this idea by emphasizing structural and syntactic alignment, ensuring that generated tool calls conform to canonical formats even under distributional shifts. Finally, environment-grounded methods fully integrated external feedback loops, enabling models to learn directly from verifiable execution signals and real-world rewards. This progression illustrates a clear trend toward deeper coupling between model reasoning and environment interaction, where feedback transitions from probabilistic imitation to causally grounded supervision. Such grounding not only enhances tool reliability and adaptability but also marks a key step toward autonomous, self-improving LLM agents capable of operating robustly in open, dynamic environments.

<a id='fa427bea-0029-45fc-8339-41e909df6ca6'></a>

Despite these advances, these methods rely on SFT or DPO, where model updates are derived from pre-collected trajectories or candidate responses. While effective for exploiting existing data, these approaches are inherently constrained in exploration and may fail to fully capture the dynamic nature of interactive environments.

<a id='fb8ff1de-4c02-445c-8356-c197030d4605'></a>

### 4.1.2 RLVR-Based Methods
Reinforcement learning with verifiable reward (RLVR) marks a pivotal stage in the evolution of AI-type adaptation, where models learn directly from online interaction with tools and environments. Unlike SFT or DPO approaches that rely on pre-collected trajectories or candidate responses, RLVR-based methods enable LLMs to iteratively explore, execute, and refine their actions based on immediate environment feedback. This paradigm allows adaptation to be

<a id='59bd9f1a-f627-48a8-a123-b3ce6bd1127d'></a>

18

<!-- PAGE BREAK -->

<a id='89039426-ff3c-4e4f-856a-bd7d8c36fdee'></a>

Adaptation of Agentic AI

<a id='3dc34b7c-48f6-4d21-b57a-4ce92a0027bc'></a>

dynamic, context-aware, and tightly coupled with the specific execution environment, spanning diverse domains
such as web search, code generation, multi-tool reasoning, and downstream applications.

<a id='55374137-8e06-41d5-b76b-0c407cc5ee9f'></a>

**Web search and information retrieval tools** optimized query generation and retrieval using environment-derived rewards. **DeepRetrieval** [21] (COLM 2025) marked a key turning point in AI-type adaptation, introducing RLVR to train LLMs as search agents that learn directly from retrieval outcomes. It formalizes query reformulation as an MDP where the user query is the state, the rewritten query is the action, and retrieval metrics—such as Recall@K, NDCG, or SQL execution accuracy—serve as the reward. The policy is optimized via KL-regularized PPO:

<a id='f7dcbedb-b38a-48e3-859d-c6765760c0cc'></a>

π̂ = arg max₋π E₋q,q'~π₍ [r(q, q') − β log π(q'|q) / πₓₑₓ(q'|q)]

<a id='aceb7fe7-480c-4e41-b7f9-cf9538084712'></a>

where $r(q, q') = r_{retrieval}(q,q') + r_{format}(q')$ jointly captures retrieval effectiveness and syntactic validity. This unified formulation enables the same framework to adapt seamlessly across literature search, QA-style retrieval, local corpus search with dense or BM25 retrievers, and text-to-SQL database querying. Empirically, DeepRetrieval achieved roughly a *threefold improvement* in recall (65.1% vs. previous state-of-the-art 24.7%) on literature search tasks using real-world search engines, while maintaining impressive performance across other retrieval and SQL domains. These results established reinforcement learning on environment rewards as a general, scalable, and cost-efficient paradigm for retrieval-based agent adaptation. **ReZero** [62], a successor to DeepRetrieval, extends this idea with GRPO-based reinforcement learning that rewards adaptive retries after failed searches. By introducing retry-aware reward shaping, it improves agent persistence and robustness in dynamic or partially observable web environments, further validating the effectiveness of DeepRetrieval's reinforcement-driven approach. **Orion** [63] further extends DeepRetrieval by moving from single-step reformulation to multi-turn adaptive search. Using GRPO with turn-level rewards based on normalized similarity and rank, Orion trains models to iteratively refine, pivot, or backtrack through structured think-search cycles. This yields strong multi-hop retrieval performance with compact 350M-1.2B models, showing that effective multi-step search strategies can be learned without large controllers.

<a id='8a786bb8-b486-4fa3-bb8e-a7e92f4660e5'></a>

Code-based tools provided deterministic or sandboxed execution environments for reasoning and task completion. LeDex [64] (NeurIPS 2024) applied reinforcement learning using a PPO-based algorithm with a novel reward function that considers both the correctness of the refined code (via unit test results and CodeBLEU) and the quality of the explanation (via semantic similarity). RLEF [20] (ICML 2025) framed code synthesis as a multi-turn interactive task, where the LLM generates a solution, receives automatic feedback from executing the code on public test cases, and updates its subsequent generations accordingly. The process is formalized as a partially observable Markov Decision Process (MDP), with actions corresponding to token-level code generation and rewards determined by the success of private test cases, optimized using PPO. Code-R1 [65] built a reliable, scalable, and sandboxed reward pipeline to minimize reward false positives caused by faulty tests, unsolvable prompts, or mismatched execution environments. The key finding is that reward quality matters more than data quantity --- clean, verified datasets and secure sandboxed execution are essential for effective code RL training. R1-Code-Interpreter [66] introduced a general framework for training LLMs to effectively use a Code Interpreter through multi-stage reinforcement learning. Unlike prior works limited to math or retrieval tasks, it identifies a key challenge that task heterogeneity causes sparse and unstable rewards during RL. To address this, the authors propose a multi-stage curriculum learning approach that prioritizes samples based on their improvement potential. Tool-R1 [67] proposed a sample-efficient reinforcement learning framework that performs multi-step reasoning through executable Python code. It introduces a *dynamic sample queue* to cache and reuse high-quality trajectories and employs *outcome-driven rewards* based on code execution success and LLM-judged correctness.

<a id='4e7b748f-9888-4971-9f91-9907035a3e8a'></a>

**Formal theorem proving** [68-76] has emerged as a canonical domain for RLVR under the Al paradigm, as proof assistants provide *ground-truth*, *tool-execution-signaled* feedback at every step. In this setting, the agent proposes one or more tactics (i.e., proof steps), a formal proof checker (the tool) deterministically verifies their validity, and the resulting validated proof-state transition is returned to the agent. This verification outcome---e.g., whether a tactic is accepted, whether it advances the proof state, or whether a complete proof is achieved---serves directly as a verifiable reward signal for policy optimization. Compared to code-execution RLVR, where unit tests may be sparse or incomplete, theorem proving offers step-wise semantic verification with minimal ambiguity, enabling denser rewards and substantially easing long-horizon credit assignment. Recent systems such as **AlphaProof** [77] (Nature 2025), **DeepSeek-Prover-V2** [78] (ICLR 2025), **Kimina-Prover** [72], and **Leanabell-Prover-V2** [79] leverage this

<a id='bc5d351e-6894-4472-a8d9-109d7d6ccc2a'></a>

19

<!-- PAGE BREAK -->

<a id='55ebc6aa-7281-4892-945b-2ef2df369547'></a>

Adaptation of Agentic AI

<a id='270f7558-4b79-4fed-b328-034d0b1fb03d'></a>

verifier feedback to train multi-step proof search policies via reinforcement learning, while a complementary line of work augments the native proof checker feedback with auxiliary guidance signals to prioritize trajectories, shape exploration, or stabilize optimization on top of verifier-grounded rewards [80–84]. While RLVR is well suited for learning proof strategies under a fixed prover snapshot, formal theorem proving also highlights a broader adaptation challenge: formal libraries (e.g., MATHLIB [85]) and large, actively evolving formalization projects [86] built by the Lean community grow continuously, expanding the available premise space. Addressing this non-stationarity often requires complementary continual or low-resource adaptation mechanisms beyond pure RLVR, which we discuss in _Continual Adaptation_ (§8.2).

<a id='cb015b4d-364d-4bd1-baf8-d98cd9b5b467'></a>

Multi-tool reasoning systems incorporated multiple tools in sequential or compositional pipelines, which used environment feedback to guide action selection. Router-R1 [87] (NeurIPS 2025) trained a policy language model to coordinate multiple large language models through a RL framework that formulates multi-round routing and aggregation as a sequential decision process. During training, the policy LLM learns to alternate between internal reasoning and external model selection, dynamically invoking different LLMs from a routing pool to solve complex tasks. FTRL [88] proposed an automated strategy for constructing tool-use training environments through a multi-stage pipeline, enabling the creation of diverse and comprehensive training settings without external toolsets. Building on these environments, FTRL introduces a feedback-driven training framework that improves a model's tool-use capabilities by leveraging a verifiable reward function, which balances tool invocation accuracy and task completion using only environmental feedback. Nemotron-Research-Tool-N1 (Tool-N1) [25] is a series of LLMs trained with R1-style reinforcement learning to enhance tool-calling capabilities in multi-tool reasoning scenarios. At each action step, the model produces explicit reasoning enclosed in <think> tags, followed by structured tool calls in <tool_call> tags, effectively separating internal reasoning from external tool execution. WebGen-Agent [89] introduced a novel framework for interactive website code generation and integrated a visual-language model for assessing website appearance via screenshots and a GUI-agent for testing functional correctness. To enhance smaller open-source models, the authors propose Step-GRPO with Screenshot and GUI-agent Feedback, a step-level reinforcement learning method that uses appearance and functionality scores as dense rewards. ToolExpander [90] enhanced GRPO-based RL for single-turn tool tasks, specifically targeting small-scale, resource-constrained LLMs. It introduced Dynamic Multi-Round Hard Sampling to replace difficult samples with high-quality few-shot examples during training, reducing the proportion of hard samples and improving learning stability. Additionally, the Self-Exemplifying Thinking mechanism allows the model to autonomously generate and analyze few-shot examples, receiving a small extra reward to encourage self-guided learning.

<a id='9ba7246d-a251-4a8e-a400-9a2c826070f9'></a>

**More tasks** A1-type training is also applied to many other downstream tasks achieving promising results. **Rec-R1 [91]** (TMLR 2025) is a reinforcement learning framework that directly optimizes LLMs for recommendation tasks using feedback from downstream recommendation systems, rather than relying on imitation of other models or synthetic supervised data. By casting LLM generation as a policy and using recommendation metrics (e.g., NDCG, Recall) as reward signals, Rec-R1 enables closed-loop adaptation of LLM outputs, aligning generation with actual recommendation performance. **SQL-R1 [92]** is a recent work that addresses the Natural Language to SQL (NL2SQL) task by leveraging reinforcement learning to enhance reasoning capabilities in complex database scenarios. SQL-R1 introduces a tailored RL reward function comprising format, execution, result, and length rewards to guide the model toward generating SQL queries that accurately reflect user intent. **olmOCR 2 [93]** is a state-of-the-art open-source OCR system for converting digitized print documents into naturally ordered plain text. This reinforcement learning-based method is a key departure from the project's first version, **olmOCR 1 [94]**, which was trained using supervised fine-tuning to mimic the static outputs of a teacher model. Critically, this new approach moves beyond traditional edit distance metrics, which often fail to capture practical correctness or handle layout ambiguities. Instead, the model is trained using a diverse set of binary unit tests as the reward signal, covering text presence/absence, reading order, table accuracy, and math formula rendering.

<a id='bed851c7-627d-4fb9-bba0-daf40af64a5d'></a>

In summary, RLVR-based A1 methods represent a substantial evolution, directly engaging with interactive environ-
ments to iteratively improve performance. RLVR-based A1 methods leverage environment-derived reward signals to
guide policy optimization, often integrating advanced techniques such as KL-regularized PPO, GRPO, and dynamic
sampling. The unifying principle is that models learn through trial-and-error, receiving immediate feedback from the
environment to refine both reasoning and tool-use strategies. Despite their effectiveness, RLVR-based approaches

<a id='4eaf7de8-54ac-4b3a-9501-c90e58ee9cdc'></a>

20

<!-- PAGE BREAK -->

<a id='3adcaf56-bbf6-4353-905f-251d8e47ed9c'></a>

Adaptation of Agentic AI

<a id='5790f51f-1864-4767-bd40-cd7551e0695a'></a>

<::Development timeline of A2 methods (agent adaptation with agent output as signal).: timeline chart::>2022
2023
ChatGPT (arrow pointing to 2023)
Mar.: Self-Refine
June: TextGrad, RPG, Re-ReST (UCLA)
Sep.: FireAct, Self-RAG (W)
2024
Mar.: CYCLE
Sep.: A²FM (OPPO), TT-SI, GRACE, KnowRL, Empower
2025
Dec. (from 2024): SCoRe, RISE, DeepResearcher, ToolRL, ReTool
Mar.: DeepRAG, Self-Challenging, Magistral, Agent Lightning
June: DeepSeek-R1, Kimi-1.5, Agent-R, RAGEN, R1-Searcher, Search-R1, ReSearch, metaTextGrad, EHRMind, AutoRefine, ZeroSearch, StepSearch
Sep.: VeriTool
2026
Figure 5 Development timeline of A2 methods (agent adaptation with agent output as signal).

<a id='3c045f73-37f5-484b-8624-72e9ad04be8f'></a>

generally require careful reward design, computational resources for interactive training, and mechanisms to stabilize learning.

<a id='f51f1488-237f-47ef-8f63-5a0bdfb114c0'></a>

## 4.2 A2: Agent Output as Signal

Different from A1-type adaptation, which leverages feedback obtained from tool executions or external environments, the A2 paradigm focuses on using evaluations of the agent's own outputs as the optimization signal. In this setting, the learning or adjustment process is driven by assessing the quality of the agent's generated outputs. Such evaluations may come from human judgments, automated metrics, or environment-based rewards, and are used to update or refine the agent policy. Under this paradigm, adaptation can occur in two primary settings:

<a id='79181f1d-6ce5-4d38-a8a4-40965dfcdd9b'></a>

*   **Agent Adaptation w/o Tools:** The agent relies on the evaluation of its reasoning or problem-solving outputs without involving external tools. This direction mainly focuses on improving intrinsic reasoning abilities, such as mathematical reasoning, coding, or logical inference, by optimizing the model based on evaluations of its generated solutions.

<a id='461712e2-81cf-40eb-9341-3aa8310a4e28'></a>

• Agent Adaptation w/ Tools: The agent's generated outputs are assessed in conjunction with tool interactions, providing feedback on how effectively the agent plans, selects, and executes tool usage. This line of adaptation aims to enhance the agent's capability in coordinating and utilizing tools, using evaluation signals derived from task outcomes that depend on tool-mediated actions.

<a id='42a7a21e-9736-421d-a0a0-31f9bffa2ed5'></a>

21

<!-- PAGE BREAK -->

<a id='3c402e61-afaa-4678-a543-ba194df7bd8c'></a>

Adaptation of Agentic AI

<a id='e4a6375d-049c-4cd1-9e31-369232c3245b'></a>

Table 2 A2 Methods: Tool Adaptation w/ Agent Supervision
<table id="21-1">
<tr><td id="21-2">Time</td><td id="21-3">Method</td><td id="21-4">Venue</td><td id="21-5">Task(s)</td><td id="21-6">Tool(s)</td><td id="21-7">Agent Backbone</td><td id="21-8">Tuning</td><td id="21-9">Links</td></tr>
<tr><td id="21-a" colspan="8">w/o Tools</td></tr>
<tr><td id="21-b">2023.03</td><td id="21-c">Self-Refine</td><td id="21-d">NeurIPS&#x27;23</td><td id="21-e">Dialogue, Math, Coding</td><td id="21-f">—</td><td id="21-g">GPT3.5, GPT4, CODEX</td><td id="21-h">Prompt Engineering</td><td id="21-i">(PDF icon) (GitHub icon)</td></tr>
<tr><td id="21-j">2024.06</td><td id="21-k">TextGrad</td><td id="21-l">Nature</td><td id="21-m">Code Optimization, Molecule Optimization, etc.</td><td id="21-n">—</td><td id="21-o">GPT3.5, GPT4o</td><td id="21-p">Prompt Engineering</td><td id="21-q">(PDF icon) (GitHub icon)</td></tr>
<tr><td id="21-r">2024.07</td><td id="21-s">RISE</td><td id="21-t">NeurIPS&#x27;24</td><td id="21-u">Math</td><td id="21-v">—</td><td id="21-w">LLaMA2, LLaMA3, Mistral</td><td id="21-x">SFT</td><td id="21-y">(PDF icon) (GitHub icon)</td></tr>
<tr><td id="21-z">2024.09</td><td id="21-A">SCoRe</td><td id="21-B">ICLR&#x27;25</td><td id="21-C">Math, Coding, QA</td><td id="21-D">—</td><td id="21-E">Gemini1.0 Pro, Gemini1.5 Flash</td><td id="21-F">REINFORCE</td><td id="21-G">(PDF icon), (GitHub icon)</td></tr>
<tr><td id="21-H">2025.01</td><td id="21-I">DeepSeek-R1-Zero (Math)</td><td id="21-J">Nature</td><td id="21-K">Math</td><td id="21-L">—</td><td id="21-M">DeepSeek-V3</td><td id="21-N">GRPO</td><td id="21-O">(PDF icon)</td></tr>
<tr><td id="21-P">2025.01</td><td id="21-Q">Kimi k1.5</td><td id="21-R">arXiv</td><td id="21-S">Math, Coding</td><td id="21-T">—</td><td id="21-U">Kimi k1.5</td><td id="21-V">GRPO</td><td id="21-W">(PDF icon), (GitHub icon)</td></tr>
<tr><td id="21-X">2025.05</td><td id="21-Y">EHRMind</td><td id="21-Z">arXiv</td><td id="21-10">EHR-based Reasoning</td><td id="21-11">—</td><td id="21-12">LLaMA3</td><td id="21-13">SFT, GRPO</td><td id="21-14">(PDF icon)</td></tr>
<tr><td id="21-15">2025.05</td><td id="21-16">metaTextGrad</td><td id="21-17">NeurIPS&#x27;25</td><td id="21-18">QA, Math, Word Sorting</td><td id="21-19">—</td><td id="21-1a">Qwen3-235B-A22B, Claude-3.5-Sonnet</td><td id="21-1b">Prompt Engineering</td><td id="21-1c">(PDF icon), (GitHub icon)</td></tr>
<tr><td id="21-1d">2025.06</td><td id="21-1e">Magistral</td><td id="21-1f">arXiv</td><td id="21-1g">Math, Coding</td><td id="21-1h">—</td><td id="21-1i">Magistral</td><td id="21-1j">PPO, GRPO</td><td id="21-1k">(PDF icon)</td></tr>
<tr><td id="21-1l">2025.10</td><td id="21-1m">GRACE</td><td id="21-1n">arXiv</td><td id="21-1o">Embedding Tasks</td><td id="21-1p">—</td><td id="21-1q">Qwen2.5, Qwen3, LLaMA3.2</td><td id="21-1r">GRPO</td><td id="21-1s">(PDF icon, GitHub icon)</td></tr>
<tr><td id="21-1t">2025.10</td><td id="21-1u">KnowRL</td><td id="21-1v">arXiv</td><td id="21-1w">Knowledge Calibration</td><td id="21-1x">—</td><td id="21-1y">LLaMA3.1, Qwen2.5</td><td id="21-1z">REINFORCE++</td><td id="21-1A">(PDF icon, GitHub icon)</td></tr>
<tr><td id="21-1B">2025.10</td><td id="21-1C">Empower</td><td id="21-1D">arXiv</td><td id="21-1E">Coding</td><td id="21-1F">—</td><td id="21-1G">Gemma3</td><td id="21-1H">SFT</td><td id="21-1I">(PDF icon, GitHub icon)</td></tr>
<tr><td id="21-1J" colspan="8">w/ Tools</td></tr>
<tr><td id="21-1K">2023.10</td><td id="21-1L">FireAct</td><td id="21-1M">arXiv</td><td id="21-1N">QA</td><td id="21-1O">Search API</td><td id="21-1P">GPT3.5, LLaMA2, CodeLLaMA</td><td id="21-1Q">SFT</td><td id="21-1R">PDF and GitHub icons</td></tr>
<tr><td id="21-1S">2023.10</td><td id="21-1T">Self-RAG</td><td id="21-1U">ICLR&#x27;24</td><td id="21-1V">QA, Fact Verification</td><td id="21-1W">Retriever</td><td id="21-1X">LLaMA2</td><td id="21-1Y">SFT</td><td id="21-1Z">PDF and GitHub icons</td></tr>
<tr><td id="21-20">2024.06</td><td id="21-21">RPG</td><td id="21-22">EMNLP&#x27;24</td><td id="21-23">QA, Reasoning</td><td id="21-24">Search Engine, Retriever</td><td id="21-25">LLaMA2, GPT3.5</td><td id="21-26">SFT</td><td id="21-27">PDF and GitHub icons</td></tr>
<tr><td id="21-28">2024.06</td><td id="21-29">Re-ReST</td><td id="21-2a">EMNLP&#x27;24</td><td id="21-2b">QA, VQA, Sequential Decision, Coding</td><td id="21-2c">Tool APIs</td><td id="21-2d">Various Models</td><td id="21-2e">DPO</td><td id="21-2f">PDF and GitHub icons</td></tr>
<tr><td id="21-2g">2025.01</td><td id="21-2h">Agent-R</td><td id="21-2i">arXiv</td><td id="21-2j">Various Tasks</td><td id="21-2k">Monte Carlo Tree Search</td><td id="21-2l">Qwen2.5, LLaMA3.2</td><td id="21-2m">SFT</td><td id="21-2n">PDF and GitHub icons</td></tr>
<tr><td id="21-2o">2025.02</td><td id="21-2p">RAS</td><td id="21-2q">arXiv</td><td id="21-2r">QA</td><td id="21-2s">Retriever</td><td id="21-2t">LLaMA2, LLaMA3.2</td><td id="21-2u">SFT</td><td id="21-2v">Adobe PDF and GitHub icons</td></tr>
<tr><td id="21-2w">2025.03</td><td id="21-2x">R1-Searcher</td><td id="21-2y">arXiv</td><td id="21-2z">QA</td><td id="21-2A">Retriever</td><td id="21-2B">LLaMA3.1, Qwen2.5</td><td id="21-2C">REINFORCE++</td><td id="21-2D">Adobe PDF and GitHub icons</td></tr>
<tr><td id="21-2E">2025.03</td><td id="21-2F">Search-R1</td><td id="21-2G">COLM &#x27;25</td><td id="21-2H">QA</td><td id="21-2I">Search Engine, Retriever</td><td id="21-2J">Qwen2.5</td><td id="21-2K">PPO, GRPO</td><td id="21-2L">Adobe PDF and GitHub icons</td></tr>
<tr><td id="21-2M">2025.03</td><td id="21-2N">ReSearch</td><td id="21-2O">NeurIPS &#x27;25</td><td id="21-2P">QA</td><td id="21-2Q">Search Engine, Retriever</td><td id="21-2R">Qwen2.5</td><td id="21-2S">GRPO</td><td id="21-2T">Adobe PDF and GitHub icons</td></tr>
<tr><td id="21-2U">2025.04</td><td id="21-2V">ReTool</td><td id="21-2W">arXiv</td><td id="21-2X">Math</td><td id="21-2Y">Code Interpreter</td><td id="21-2Z">Qwen2.5</td><td id="21-30">PPO</td><td id="21-31">Adobe PDF and GitHub icons</td></tr>
</table>
Continued on next page

<a id='0ab08e10-4da3-41b8-b928-24f5c4b16fa3'></a>

22

<!-- PAGE BREAK -->

<a id='29cf585a-0442-43d0-ad60-21d9247c2c5f'></a>

Adaptation of Agentic AI

<a id='d0df8f2c-a6bb-4c39-b577-75289b589322'></a>

Table 2 – Continued from previous page
<table id="22-1">
<tr><td id="22-2">Time</td><td id="22-3">Method</td><td id="22-4">Venue</td><td id="22-5">Task(s)</td><td id="22-6">Tool(s)</td><td id="22-7">Agent Backbone</td><td id="22-8">Tuning</td><td id="22-9">Links</td></tr>
<tr><td id="22-a">2025.04</td><td id="22-b">DeepResearcher</td><td id="22-c">arXiv</td><td id="22-d">QA, Reasoning, Deep Research</td><td id="22-e">Web Search API, Web Browser</td><td id="22-f">Qwen2.5</td><td id="22-g">GRPO</td><td id="22-h">PDF icon, GitHub icon</td></tr>
<tr><td id="22-i">2025.04</td><td id="22-j">ToolRL</td><td id="22-k">arXiv</td><td id="22-l">Tool Calling</td><td id="22-m">Tool APIs</td><td id="22-n">Various Models</td><td id="22-o">GRPO</td><td id="22-p">PDF icon, GitHub icon</td></tr>
<tr><td id="22-q">2025.05</td><td id="22-r">AutoRefine</td><td id="22-s">NeurIPS&#x27;25</td><td id="22-t">QA</td><td id="22-u">Retriever</td><td id="22-v">Qwen2.5</td><td id="22-w">GRPO</td><td id="22-x">PDF icon, GitHub icon</td></tr>
<tr><td id="22-y">2025.05</td><td id="22-z">ZeroSearch</td><td id="22-A">arXiv</td><td id="22-B">QA</td><td id="22-C">Search Engine, Web Search</td><td id="22-D">Qwen2.5, LLaMA3.2</td><td id="22-E">REINFORCE, GPRO, PPO, SFT</td><td id="22-F">PDF icon, GitHub icon</td></tr>
<tr><td id="22-G">2025.05</td><td id="22-H">StepSearch</td><td id="22-I">EMNLP&#x27;25</td><td id="22-J">QA</td><td id="22-K">Search Engine, Retriever</td><td id="22-L">Qwen2.5</td><td id="22-M">StePPO</td><td id="22-N">(PDF icon) (GitHub icon)</td></tr>
<tr><td id="22-O">2025.06</td><td id="22-P">Self-Challenging</td><td id="22-Q">arXiv</td><td id="22-R">Multi-Turn Function-Calling, Calculation</td><td id="22-S">Code Interpreter, Web Browser</td><td id="22-T">LLaMA3.1</td><td id="22-U">REINFORCE, SFT</td><td id="22-V">(PDF icon)</td></tr>
<tr><td id="22-W">2025.06</td><td id="22-X">MMSearch-R1</td><td id="22-Y">arXiv</td><td id="22-Z">QA, VQA</td><td id="22-10">Image Search, Web Browser, Retriever</td><td id="22-11">Qwen2.5</td><td id="22-12">REINFORCE, SFT</td><td id="22-13">(PDF icon) (GitHub icon)</td></tr>
<tr><td id="22-14">2025.07</td><td id="22-15">DynaSearcher</td><td id="22-16">arXiv</td><td id="22-17">QA</td><td id="22-18">Document Search, KG Search</td><td id="22-19">Qwen2.5, LLaMA3.1</td><td id="22-1a">GRPO</td><td id="22-1b">(PDF icon) (GitHub icon)</td></tr>
<tr><td id="22-1c">2025.07</td><td id="22-1d">CodePRM</td><td id="22-1e">ACL&#x27;25</td><td id="22-1f">Coding</td><td id="22-1g">Code Executor</td><td id="22-1h">Qwen2.5-Coder</td><td id="22-1i">SFT</td><td id="22-1j">(PDF icon)</td></tr>
<tr><td id="22-1k">2025.08</td><td id="22-1l">Agent Lightning</td><td id="22-1m">arXiv</td><td id="22-1n">Text2SQL, Math</td><td id="22-1o">SQL Executor, Retriever, Calculator</td><td id="22-1p">LLaMA3.2</td><td id="22-1q">LightningRL</td><td id="22-1r">(PDF and Git icons)</td></tr>
<tr><td id="22-1s">2025.08</td><td id="22-1t">MedResearcher- R1</td><td id="22-1u">arXiv</td><td id="22-1v">Medical QA</td><td id="22-1w">Medical Retriever, Web Search API, Document Reader</td><td id="22-1x">MedResearcher-R1</td><td id="22-1y">SFT, GRPO</td><td id="22-1z">(PDF and Git icons)</td></tr>
<tr><td id="22-1A">2025.09</td><td id="22-1B">VerlTool</td><td id="22-1C">arXiv</td><td id="22-1D">Math, QA, SQL, Visual, Web Search, Coding</td><td id="22-1E">Code Interpreter, Search Engine, SQL Executor, Vision Tools</td><td id="22-1F">Qwen2.5, Qwen3</td><td id="22-1G">GRPO</td><td id="22-1H">(PDF and Git icons)</td></tr>
<tr><td id="22-1I">2025.10</td><td id="22-1J">A²FM</td><td id="22-1K">arXiv</td><td id="22-1L">Web Navigation, Math, QA</td><td id="22-1M">Search Engine, Crawl, Code Executor</td><td id="22-1N">Qwen2.5</td><td id="22-1O">APO, GRPO</td><td id="22-1P">(PDF and Git icons)</td></tr>
<tr><td id="22-1Q">2025.10</td><td id="22-1R">TT-SI</td><td id="22-1S">arXiv</td><td id="22-1T">Tool Calling</td><td id="22-1U">Tool APIs</td><td id="22-1V">Qwen2.5</td><td id="22-1W">Test-Time Fine-Tuning</td><td id="22-1X">(PDF icon)</td></tr>
</table>

<a id='f8c5bd7d-9ce6-48b3-aac7-884fc90055f7'></a>

### 4.2.1 Agent Adaptation w/o Tools
A major breakthrough in the paradigm of output-based agent adaptation emerged with the introduction of the DeepSeek-R1 framework [24] (Nature 2025), which demonstrated that *reinforcement learning with verifiable reward (RLVR)* can effectively enhance the reasoning capabilities of large agents. In this framework, a strong base model serves as the core reasoning engine, while reinforcement learning encourages the generation of outputs that are logically consistent and verifiably correct. The training process focuses primarily on reasoning-intensive domains such as mathematics and code generation, where the quality of outputs can be automatically evaluated through deterministic correctness signals. This approach not only improved model reasoning but also revealed a scalable pathway for further enhancing agent intelligence beyond supervised fine-tuning.

<a id='079f27cf-bfb2-4312-b879-7ff796d9c26e'></a>

Concurrently, **Kimi-1.5** [95] advanced this paradigm by scaling reinforcement learning for multi-modal agents
and introducing simplified yet effective policy optimization strategies. By leveraging large-scale reasoning data
and efficient reward modeling, Kimi-1.5 achieved strong performance across a range of reasoning benchmarks,

<a id='3ca74aa5-3716-4173-8bfc-f4c014c2d66d'></a>

23

<!-- PAGE BREAK -->

<a id='b3d27e7f-1a44-4258-a054-9608a87bdd58'></a>

Adaptation of Agentic AI

<a id='827bc65d-ab48-4452-976d-7a172684d87c'></a>

matching or surpassing prior state-of-the-art models. Together, these works sparked a new wave of research into the so-called **R1 paradigm**, where reinforcement learning is used to refine the reasoning process of agentic systems based on verifiable output evaluations. Following this development, a number of subsequent studies have extended the paradigm to various applications and reasoning settings.

<a id='79593532-e0bd-49c2-ac0c-e6e6c3386f15'></a>

Following the emergence of the R1 paradigm, a series of subsequent works further extended the idea of optimizing agent reasoning through evaluations of model outputs, without relying on external tools. These studies explored diverse learning signals, objectives, and task domains, collectively enriching the landscape of output-based adaptation for reasoning enhancement.

<a id='56100a79-bae1-443c-9297-539682687133'></a>

Empower [96] proposed a self-supervised fine-tuning framework for assistive language models, where the optimization objective centers on maximizing human empowerment rather than explicit correctness. By using only offline text data, the method aligns agents to assist human users in multi-turn coding tasks, encouraging context-sensitive and cooperative behavior without the need for additional feedback or verifiable rewards.

<a id='1f46623e-ddd3-444e-8f6a-f121f0d7546a'></a>

KnowRL [97] introduced a reinforcement-based approach to strengthen self-knowledge calibration. Instead of focusing on task-specific correctness, KnowRL trains agents to assess their own confidence and feasibility in producing reliable answers. Through internally generated rewards derived from self-assessment, the model enhances its awareness of what it knows and what it does not, improving reliability and consistency across reasoning domains.

<a id='4879c720-ffa1-479f-ad15-f640958947cd'></a>

GRACE [98] reimagines contrastive learning as a form of reward-guided optimization, transforming contrastive objectives into policy signals that encourage explicit, interpretable reasoning. By treating positive-negative sample distinctions as reward feedback, GRACE bridges generative reasoning and representation learning, yielding improved embedding alignment and more transparent rationales. A related study, Rec-R1 [91], applies reinforcement optimization to product re-ranking tasks, demonstrating that reinforcement signals derived from task-specific output evaluations can improve discriminative performance while preserving general reasoning ability. EHRMind [99] extends the reinforcement-with-verifiable-reward framework to clinical reasoning scenarios. Targeting electronic health record (EHR) interpretation, the study highlights the limitations of RLVR alone, noting that domain-specific reasoning often requires prior knowledge alignment through SFT. EHRMind combines a lightweight SFT warm-up phase with subsequent RLVR optimization, effectively stabilizing training and improving interpretability in medical tasks such as clinical calculation, patient-trial matching, and disease diagnosis. This finding underscores a broader insight: while RL-based adaptation enhances reasoning quality, SFT remains a critical foundation for domain grounding and stable agent adaptation.

<a id='61552396-b471-488d-a7f9-45bbe33eaca1'></a>

Before the emergence of the R1 paradigm, several studies had already explored output-based adaptation strategies that optimize reasoning quality through self-generated feedback, without modifying model parameters via traditional supervised signals. These early attempts laid the conceptual foundation for reinforcement-driven reasoning improvement later seen in R1-style frameworks.

<a id='bc2e0cf8-f1f9-4421-930d-0addd7bf9d3e'></a>

Self-Refine [100] (NeurIPS 2023) introduced an iterative refinement framework in which the same language model acts as both generator and critic. The model first produces an initial response and then evaluates and revises it based on self-generated textual feedback. This process, inspired by human-style revision, improves output quality across diverse domains such as dialogue, mathematics, and code generation. Notably, Self-Refine requires no supervised data, auxiliary models, or reinforcement learning, which shows that structured self-feedback alone can lead to measurable gains in reasoning accuracy and output preference.

<a id='62ca790d-04c8-444b-be33-bd24f2bb3803'></a>

Building upon this direction, **SCoRe** [100] (ICLR 2025) proposed a reinforcement learning approach for enabling language models to self-correct using entirely self-generated data. Unlike conventional SFT, which struggles to teach effective correction behavior, SCoRe employs multi-turn online reinforcement learning to encourage models to iteratively refine their reasoning under their own distribution of responses. By combining reward regularization and self-generated correction traces, the method significantly improves self-correction ability on mathematical and reasoning benchmarks, demonstrating that reinforcement learning can effectively operationalize self-reflection into a stable learning signal.

<a id='9b746762-6181-4134-9ae2-ad98e5c870f2'></a>

Backpropagating language-model feedback. TextGrad [101] (Nature 2025) introduced a general framework for agent self-improvement through *textual gradient descent (TGD)*. Instead of relying on numerical gradients or

<a id='7ffa4eb1-47f9-4398-95ba-27a14c9a8cb1'></a>

24

<!-- PAGE BREAK -->

<a id='f4d27554-dfae-4054-a1e0-52ae998fe764'></a>

Adaptation of Agentic AI

<a id='33f560d9-1bac-416a-9c2b-6d27c2df8e7f'></a>

verifiable environment rewards, TextGrad propagates language-model feedback in the form of natural-language critiques that describe how to improve the model's outputs. These feedback messages act as "textual gradients," allowing optimization across black-box LLM systems without requiring access to their internal parameters. This method formalizes self-refinement as a differentiable-like process and generalizes earlier outcome-based approaches such as DeepSeek-R1. Empirically, TextGrad improves GPT-4o's zero-shot code accuracy on LEETCODE-HARD (from 26% to 36%), raises MMLU-Physics performance from 91.2% to 95.1%, and enhances the multi-tool agent CHAMELEON by 7.7%. Conceptually, TextGrad exemplifies the A2 paradigm, extending reinforcement-style adaptation from scalar rewards to structured linguistic feedback. It unifies prompt tuning, reasoning refinement, and compound-agent optimization under a single abstraction of backpropagating language feedback, marking a step toward interpretable and parameter-agnostic agent adaptation. Building on this, **metaTextGrad** [102] (NeurIPS 2025) applies the A2 paradigm recursively to the optimizer itself, using validation feedback to automatically refine the optimizer's prompts and structure for better task alignment.

<a id='586bb9d7-7ed2-4bbf-bb7e-cae0e7d0ad47'></a>

### 4.2.2 Agent Adaptation w/ Tools
Following the rise of the R1 paradigm, the idea of using agent outputs as optimization signals has expanded beyond pure reasoning tasks to encompass tool-using agents. These studies extend output-based RL to settings where agents must decide when and how to invoke external tools, integrating real-time feedback from retrieval systems, APIs, or executable environments. This marks a shift from reasoning-centric refinement to _tool-grounded adaptation_, where outcome feedback from the external world provides rich and verifiable learning signals.

<a id='aa798c29-1949-4577-afce-4a59514c270f'></a>

Retrieval-based tool learning. Different from earlier work like **Self-RAG** [103] (ICLR 2024) and its successors [104–106], which introduces an distillation-SFT-based paradigm to teach models to use retrieval tools for search, a recent major line of work investigates how RL can improve the use of retrieval tools for question answering. **R1-Searcher** [107], **Search-R1** [49] (COLM 2025), **ReSearch** [108] (NeurIPS 2025), and their successors [109–115] all extend the R1 paradigm by enabling LLMs to autonomously generate and refine search queries during multi-turn reasoning. R1-Searcher [107] proposes a two-stage RL framework that incentivizes the use of external search APIs, enhancing factual accuracy and reducing hallucinations in open-domain QA. Trained with multi-turn RL, the model learns to balance reasoning and retrieval, achieving up to 24% improvement over strong RAG baselines. Similarly, Search-R1 [49] formulates search invocation as a reinforcement optimization problem, where retrieved evidence and final correctness jointly form the outcome-based reward. ReSearch [108] trains LLMs to _reason with search_ via reinforcement learning, without any supervised data on reasoning steps. It integrates search queries and retrieved results directly into the reasoning chain using tags such as <think>, <search>, and <result> and optimizes the model with GRPO to decide _when_ and _how_ to search. Trained on multi-hop QA tasks, ReSearch yields 9–22% absolute gains over iterative RAG baselines and exhibits emergent reflection and self-correction behaviors during RL training These works demonstrate that reasoning-oriented RL can naturally extend to retrieval-augmented contexts, allowing LLMs to internalize when external information is needed.

<a id='ede54722-5ac1-4798-b0c1-5c5e688c5b5f'></a>

**Code- and execution-based tool learning.** A parallel direction focuses on code-based environments where tools provide executable feedback. CodePRM [116] (ACL 2025) introduces a process reward model that scores reasoning steps based on code execution results, forming a *Generate–Verify–Refine* pipeline that dynamically corrects reasoning errors during inference. ReTool [50] advances this idea by integrating real-time code execution into RL rollouts, teaching models when and how to invoke computational tools such as interpreters to optimize mathematical and symbolic reasoning.

<a id='8f142d89-3713-44ea-ada7-a9b02ffc4b1f'></a>

__General multi-tool and agentic learning.__ Beyond retrieval, several studies generalize this principle to agents interacting with diverse APIs and environments. **Test-Time Self-Improvement (TT-SI) [117]** introduces on-the-fly self-improvement, where the agent identifies uncertain test cases and generates new training data for them, performing fine-tuning directly at inference time. **Agent Lightning [118]** provides a flexible RL framework that decouples agent execution from training, allowing reinforcement optimization to handle complex, multi-agent, and multi-tool workflows with minimal code modification. **Re-ReST [119]** extends self-training with reflection, using environment feedback such as unit test results to refine low-quality trajectories, yielding large gains on HotpotQA and AlfWorld. In a similar spirit, **Self-Challenging Agents [120]** introduce a self-generated curriculum: the model first generates novel tool-use tasks as a challenger, then solves them via RL as an executor, achieving

<a id='6db696d5-e92f-4253-9bae-ab173ce15148'></a>

25

<!-- PAGE BREAK -->

<a id='c57beb95-6249-465f-aa34-43f985b2060e'></a>

Adaptation of Agentic AI

<a id='e0c19f39-5525-443a-88ff-0231509f8b1f'></a>

more than a twofold improvement on multi-turn tool-use benchmarks. **Agent-R** [121] further formalizes iterative self-reflection via model-guided critique construction, continuously correcting failed trajectories using Monte Carlo Tree Search (MCTS) rollouts, which improves performance by 5.6% across interactive environments. Finally, **AFM** [122] unifies reasoning and acting within a cost-regularized RL framework, dynamically choosing between internal reasoning, tool invocation, or direct answering, thereby improving both efficiency and accuracy in hybrid reasoning benchmarks. Addressing system-level scalability in a similar vein, **VerlTool** [123] introduces a unified infrastructure for agentic RL. By decoupling the RL workflow from a modular tool server and implementing asynchronous rollouts, it eliminates synchronization bottlenecks, enabling the efficient training of multi-modal agents across disparate tasks ranging from visual reasoning to software engineering.

<a id='8b04efda-b936-42c7-ac49-2583b650060e'></a>

## 5 Tool Adaptation

Tool adaptation represents another emerging trend for enhancing AI agents' performance on specific tasks, marking a conceptual shift from optimizing the agent itself to optimizing its ecosystem. Instead of modifying the agent's parameters through fine-tuning or reinforcement learning, this paradigm targets the external components—the "tools" that mediate perception, computation, and interaction. These tools may encompass pre-trained models, retrievers, planners, or executors that the agent can invoke through language or code. Consequently, tool adaptation focuses on improving the agent's operational environment rather than its internal cognition. Methods in this category typically (1) employ pre-trained machine learning models—developed via environment feedback or data imitation—as plug-and-play components, ranging from simple classifiers to complex LLM-based sub-agents as discussed in §4; or (2) leverage the agent's outputs as supervision or reinforcement signals to train, align, or refine the tool itself. This perspective re-frames intelligent systems as co-adaptive ecosystems, where frozen agents and adaptive tools evolve symbiotically toward higher task efficiency, modularity, and generalization.

<a id='ecf3d5d6-d032-433e-84a5-a845b83be44e'></a>

Formally, let A denote an agent, parameterized by its internal configuration or policy (which include prompt templates or model weights) and T denote a tool or a set of tools that can be trained or optimized based on task feedback. The adaptation process can be characterized by two complementary paradigms:

<a id='dfc1772d-d1a5-43dd-babc-fa67ea54b6b3'></a>

(T1) T* = arg max O_tool(T),
(T2) T* = arg max O_agent(A, T),

<a id='4ed89115-5b99-4cc1-b4cf-cd3687f9e7a0'></a>

where Otool quantifies task-specific or environment-driven improvements that are independent of the agent, such as retrieval accuracy or planning efficiency, while Oagent incorporates agent-derived supervision, where the agent's outputs provide learning signals to refine or align the tool. Here, T* denotes the optimized tool configuration that maximizes the respective objective, illustrating how tool adaptation complements agent-level optimization within the broader agent-tool ecosystem.

<a id='a4acee28-6218-4d2a-adf5-8e82c7ac7819'></a>

## 5.1 T1: Agent-Agnostic Tool Adaptation
The foundational architecture for tool-augmented systems is to use pre-trained models as plug-and-play tools for frozen agents. The agent orchestrates tool usage through prompting alone, never updating its parameters, while leveraging tools that were trained independently on diverse data sources before deployment.

<a id='eaa55ca9-4285-4d44-95c0-ab1eae3ea8e7'></a>

### 5.1.1 Foundational Systems and Architectures
Early systems exemplifying the tool-adaptation paradigm established the architectural foundations for how frozen agents can effectively orchestrate or invoke external models. These pioneering works demonstrate distinct mechanisms—functional, prompt-based, code-based, and graph-based—that together shaped the design space for modern tool-augmented AI systems.

<a id='4da88964-4b26-4bd0-b3fb-955e97504656'></a>

**Operator-Learning Tools: Neural Operators** [124] (JMLR). Before large-scale LLM-based orchestration emerged, Neural Operators represented a seminal example of *agent-agnostic tool learning*: models trained to approximate mappings between infinite-dimensional function spaces, serving as differentiable surrogates for complex simulators. Unlike conventional neural networks tied to discrete grids, Neural Operators are **discretization-invariant**—they learn

<a id='06e695c3-556d-48cb-8681-a88cbc71b53b'></a>

26

<!-- PAGE BREAK -->

<a id='41dba368-0637-4ea0-9c37-bd6b800fb33d'></a>

Adaptation of Agentic AI

<a id='3a74c4ab-9972-42ae-8838-13ddba100892'></a>

the underlying operator itself, not its finite discretization—and can generalize across resolutions and geometries. The **Fourier Neural Operator (FNO)** achieves $O(J \log J)$ inference via spectral convolution and outperforms classical solvers on Navier–Stokes, Darcy flow, and elasticity equations by orders of magnitude in speed. Conceptually, FNO and its variants (Graph-, Low-rank-, Multipole-NO) mark the first wave of “frozen tools” that agents can query repeatedly for reasoning, planning, or control without retraining. In modern agentic pipelines, they are used as plug-in surrogates—fast, differentiable black-box functions invoked within decision or inference loops.

<a id='b2c8459e-6722-4566-9c70-486ae4c5dd4e'></a>

HuggingGPT [125] (NeurIPS 2023) pioneered the orchestration paradigm by enabling ChatGPT to command 1000+ machine learning models from HuggingFace Hub without any fine-tuning. The frozen LLM executes a four-stage workflow: task planning (decomposing user requests), model selection (choosing from tool descriptions), task execution (invoking models), and response generation (synthesizing outputs). This architecture demonstrates that language serves as a universal interface-tool descriptions in natural language suffice for the frozen agent to coordinate complex multimodal workflows. On composite cross-modal tasks, HuggingGPT enables GPT-3.5 to achieve performance comparable to GPT-4V by orchestrating specialized vision, speech, and language models. The primary limitation lies in latency from sequential LLM calls and token length constraints for tool descriptions.ViperGPT [126] (ICCV 2023) introduced code generation as the orchestration mechanism. The frozen GPT-3 Codex generates Python code that composes vision models-GLIP for detection, SAM for segmentation, MiDaS for depth estimation-into executable programs. This code-based approach achieves state-of-the-art zero-shot performance on GQA visual reasoning, outperforming end-to-end models by 10-15% on compositional tasks. The key insight: Python functions provide more flexible tool composition than fixed API calls. Each tool exposes simple functions like find (image, object_name) or compute_depth (image), which Codex chains programmatically without learning tool-specific interfaces. SciToolAgent [127] (Nature Computational Science 2025) scales tool orchestration to scientific domains through graph-based organization. The frozen GPT-4o accesses 500+ biology, chemistry, and materials science tools via SciToolKG-a knowledge graph encoding tool metadata, dependencies, and safety constraints. Graph-based retrieval for tool selection achieves 94% accuracy on scientific query benchmarks, representing a 15-20% improvement over GPT-4o without tool access. The system successfully automates protein engineering workflows chaining ESMFold for structure prediction, BLAST for sequence alignment, and custom analysis tools. This architecture demonstrates that structured knowledge graphs address scalability challenges inherent in prompt-based descriptions.

<a id='ad0db82b-b01f-4403-b4bf-1158ed386992'></a>

These foundational systems illustrate the dominant integration patterns. HuggingGPT exemplifies **prompt-based** orchestration, where the agent parses tool calls from text. ViperGPT uses **code generation**, exposing tools as Python functions. SciToolAgent demonstrates **knowledge graph retrieval**, using RAG to select from structured tool graphs. A fourth common pattern is **multimodal bridging**, which converts non-textual modalities into text representations; for example, Visual ChatGPT's [128] prompt manager serializes vision operations as text API calls. The usability of these patterns depends on clear **interface design**, such as programmatic function signatures (e.g., `find_object (image: PIL. Image, ...)`), structured JSON schemas, or simple natural language descriptions. **Execution modes** are similarly varied, ranging from direct API calls and code generation to HTTP requests and command-line invocations.

<a id='b4d71dcf-3322-4c20-a04c-b949ced8396e'></a>

## Model Context Protocol (MCP) and Code Execution Environments [129].
As large-scale agent ecosystems began to connect thousands of heterogeneous tools, the _Model Context Protocol_ (MCP) emerged as an open standard for unifying how agents interface with external systems. Rather than embedding long tool definitions and intermediate results directly into the model's context, MCP provides a universal API layer that enables frozen agents to discover, invoke, and coordinate tools across domains using a consistent schema. Building upon this infrastructure, Anthropic's "Code Execution with _MCP_" paradigm introduced an execution-centric design in which the agent writes executable code to interact with MCP servers instead of performing token-level tool calls. This approach allows agents to load only the necessary tool definitions, filter or aggregate data within a sandboxed environment, and pass compact results back to the model, reducing context usage by over 98% while maintaining full compositionality. Conceptually, MCP represents a scalable **T1-style tool adaptation infrastructure** that decouples execution from inference, while the code-execution mode bridges toward **T2-style optimization** by dynamically improving efficiency under frozen agents. Together with systems such as HuggingGPT, ViperGPT, and SciToolAgent, it exemplifies the architectural evolution from static tool invocation to protocol-driven, programmable,

<a id='a0b7ccb1-f095-4324-ba2b-ebcdc04ec17f'></a>

27

<!-- PAGE BREAK -->

<a id='83d7952b-e8a6-4a67-aa8f-e6829f8bc7ac'></a>

Adaptation of Agentic AI

<a id='0d11578b-b946-498f-9401-f3f89415917a'></a>

and context-efficient orchestration.

<a id='575cee22-c839-4725-98b0-8eb9aeb330cb'></a>

### 5.1.2 Categories and Training Methodologies
Tool adaptation encompasses a wide range of pre-trained model categories, each contributing unique functional capabilities to frozen agents. The following examples highlight representative tool families and the training methodologies that enable their plug-and-play deployment across multimodal and domain-specific settings.

<a id='f6409d68-6b4d-4732-8365-54aecac59475'></a>

Vision models dominate T1 deployments as plug-and-play tools. CLIP [130], trained contrastively on 400M image-text pairs, provides zero-shot classification and semantic understanding through frozen encoders. SAM [131], trained on 11M images with 1B masks via human-in-the-loop data engines, enables promptable segmentation with point, box, or mask inputs. SAM-CLIP [132] merges these capabilities through multi-task distillation with frozen teachers, achieving +6.8% mIoU on Pascal VOC for zero-shot semantic segmentation while retaining both parent models' strengths. These vision tools require no task-specific fine-tuning - frozen agents invoke them directly via APIs for image understanding, segmentation, and classification tasks.

<a id='404e925f-2e53-4433-b9d2-ed5001f05665'></a>

**Speech and audio tools** leverage massive pre-training for robust performance. Whisper [133], trained on 680K hours of multilingual audio with weak supervision, provides speech recognition, translation, and language identification as a frozen API for multimodal agents. The encoder-decoder Transformer architecture enables zero-shot transcription across languages and domains, demonstrating remarkable robustness to accents, noise, and technical terminology. Agents simply pass audio inputs to the frozen Whisper model and process text outputs without any model adaptation.

<a id='98f1e3aa-e781-47af-bda7-8336533103d5'></a>

**Code execution tools** encompass models that learn to compose and execute functions through code. CodeAct [57] demonstrates that representing tool use in executable Python rather than static JSON improves compositional reasoning, achieving over 20% higher success rates on API-Bank benchmarks. The dynamic nature of code allows agents to flexibly construct, parameterize, and combine tools without predefined schemas.

<a id='84c720fa-d174-467d-ad36-954d73f5f9f9'></a>

Search and retrieval tools comprise pre-trained dense retrievers such as DPR [134], ColBERT [135], Contriever [136], and e5 [137], often deployed as frozen components within retrieval-augmented generation pipelines. These bi-encoder models, trained on passage ranking tasks, enable semantic search over large corpora.

<a id='a56b3a38-30f8-47e0-b645-c14734d7474b'></a>

Scientific tools extend capabilities to specialized fields. AlphaFold2 [138] and ESMFold [139] provide protein structure prediction from sequences. Materials science models like CGCNN [140] predict crystal properties. Molecule representation learning approaches [141-147] have been developed for molecular property prediction, whereas some encoder-decoder frameworks [148, 149] aim to predict transcriptional profiles elicited by chemical perturbations. These tools represent years of domain-specific model development, deployed as-is for frozen agents tackling scientific queries.

<a id='bb4024c0-0c92-4eef-a569-31ada8863f67'></a>

Beyond these static models, adaptive agents introduced in §4 (such as DeepRetrieval [21] for search query rewriting and Code-R1 [65] for code generation) illustrate how trained reasoning agents themselves can function as dynamic tools. Once frozen, they extend the tool ecosystem by reformulating queries, generating executable code, or performing reasoning-driven actions, thereby bridging the gap between pre-trained models and the environment or offline data.

<a id='60e92499-43ae-426f-bdd6-437f7f53e52e'></a>

## 5.2 T2: Agent-Supervised Tool Adaptation
The T2 paradigm represents a profound conceptual inversion in how we approach adaptation in agentic systems. Rather than asking "how can we modify the agent to better use its tools?" (the A1/A2 question), T2 asks: "how can we modify the tools to better serve a fixed agent?" This seemingly simple reversal has far-reaching implications. It reframes the expensive, monolithic foundation model as a stable source of *supervision* rather than the target of optimization, and reconceptualizes the agent's ecosystem of tools as a dynamic, adaptive periphery that can be continuously refined. This inversion is not merely a technical choice but reflects a deeper understanding of the economics and modularity of modern AI systems. Training or fine-tuning billion-parameter foundation models is computationally prohibitive and risks catastrophic forgetting. In contrast, the peripheral tools—retrievers, planners, memory modules—are typically orders of magnitude smaller and can be trained with dramatically less data and computation. The T2 paradigm exploits this asymmetry, achieving what we term *symbiotic adaptation*: the frozen

<a id='803949db-b9c0-48b1-a4c5-9d77349c0403'></a>

28

<!-- PAGE BREAK -->

<a id='0894da3c-bad9-4335-9498-834713b4a305'></a>

Adaptation of Agentic AI

<a id='762cf929-2fe8-4d95-b06c-b8bf4763235c'></a>

<::chart: Development timeline of T2 methods
- **2022**
- **2023**
  - Early 2023: ChatGPT (marked with a red arrow)
  - March:
    - REPLUG (with W logo)
    - ToolkenGPT (with UCSD logo)
    - AAR
    - UPRISE (with Microsoft logo)
  - June:
    - ARL2 (with red circular logo)
    - CoBB (with red dragon logo)
    - MedAdapter (with GT logo)
    - BLADE (with purple circular logo)
  - September:
    - LLM-R
    - RA-DIT (with infinity logo)
    - BGM (with Google logo)
    - proxy-tuning (with W logo)
  - December: Matryoshka Pilot (with GT logo)
- **2024**
  - March:
    - Bbox-Adapter (with GT logo)
    - EVOR (with university crest logo)
    - AgentFlow (with S logo)
    - AutoGraph-R1 (with blue square logo)
    - MAE (with I logo)
    - Advisor Models (with B logo)
    - QAgent (with Q logo)
  - June:
    - Memento (with UCL logo)
    - R-Zero (with blue circle logo)
    - AI-SearchPlanner (with Huawei logo)
  - September:
    - s3 (with I logo)
    - Sysformer (with GT logo)
  - December: Mem-α (with UCSD logo)
- **2025**
  - Early 2025: DeepSeek-R1 (marked with a red arrow)
- **2026**
Figure 6 Development timeline of T2 methods (agent-supervised tool adaptation, classic memory-related methods are not included in this figure due to space limitation).::>

<a id='8ff2c750-e5fb-43ca-ba32-2f4024dbc15f'></a>

agent provides high-quality supervision signals derived from its vast pre-trained knowledge, while the tools learn to translate, filter, and present information in exactly the form the agent finds most useful.

<a id='946516d6-406a-4005-b9f9-d36a12ce3da0'></a>

The evolution of T2 methods from 2023 to 2025 reveals a clear intellectual progression: from using *internal proxy signals* (perplexity, preferences) to train passive retrieval tools, to using *verifiable outcome signals* (task success, accuracy gains) to train active, multi-turn agentic tools. This progression mirrors a broader maturation in the field's understanding of what makes an effective training signal and what kinds of tools are worth building.

<a id='1b36c696-e9ca-42a0-9e2d-dfacce129d6d'></a>

Table 3 T2 Methods: Tool Adaptation w/ Agent Supervision
<table id="28-1">
<tr><td id="28-2">Time</td><td id="28-3">Method</td><td id="28-4">Venue</td><td id="28-5">Task(s)</td><td id="28-6">Tool Backbone</td><td id="28-7">Agent Backbone</td><td id="28-8">Tuning</td><td id="28-9">Links</td></tr>
<tr><td id="28-a" colspan="8">Earlier Methods</td></tr>
<tr><td id="28-b">2023.01</td><td id="28-c">REPLUG</td><td id="28-d">NAACL &#x27;24</td><td id="28-e">QA</td><td id="28-f">Contriever</td><td id="28-g">GPT3-175B, PaLM, Codex, LLaMA-13B</td><td id="28-h">Proxy-Tuning, LSR</td><td id="28-i">(PDF icon) (GitHub icon)</td></tr>
<tr><td id="28-j">2023.03</td><td id="28-k">UPRISE</td><td id="28-l">EMNLP &#x27;23</td><td id="28-m">Zero-shot NLU (QA, NLI, etc.)</td><td id="28-n">GPT-Neo-2.7B</td><td id="28-o">BLOOM-7.1B, OPT-66B, GPT-3-175B</td><td id="28-p">Contrastive Learning</td><td id="28-q">(PDF icon) (GitHub icon)</td></tr>
<tr><td id="28-r">2023.05</td><td id="28-s">ToolkenGPT</td><td id="28-t">NeurIPS &#x27;23</td><td id="28-u">Numerical Reasoning, QA, Plan Generation</td><td id="28-v">Token Embedding</td><td id="28-w">GPT-J 6B, OPT-6.7B, OPT-13B</td><td id="28-x">Proxy-Tuning</td><td id="28-y">(PDF icon) (GitHub icon)</td></tr>
</table>
Continued on next page

<a id='0f67d81d-cd06-43d0-86cd-ad51f234be96'></a>

29

<!-- PAGE BREAK -->

<a id='cb30cbed-7ecd-49f7-8643-56abd7a39745'></a>

Adaptation of Agentic AI

<a id='42ebe76d-9a12-41b4-a775-350f0a2fae4a'></a>

Table 3 - Continued from previous page
<table id="29-1">
<tr><td id="29-2">Time</td><td id="29-3">Method</td><td id="29-4">Venue</td><td id="29-5">Task(s)</td><td id="29-6">Tool Backbone</td><td id="29-7">Agent Backbone</td><td id="29-8">Tuning</td><td id="29-9">Links</td></tr>
<tr><td id="29-a">2023.05</td><td id="29-b">AAR</td><td id="29-c">ACL &#x27;23</td><td id="29-d">Zero-Shot Generalization (MMLU, PopQA)</td><td id="29-e">ANCE, Contriever</td><td id="29-f">Flan-T5-Small, InstructGPT</td><td id="29-g">Contrastive Learning</td><td id="29-h">PDF icon, refresh icon</td></tr>
<tr><td id="29-i">2023.06</td><td id="29-j">LLM-R</td><td id="29-k">EACL &#x27;24</td><td id="29-l">Zero-shot NLU (QA, NLI, Paraphrase, etc.)</td><td id="29-m">E5-base</td><td id="29-n">GPT-Neo-2.7B, LLaMA-13B, GPT-3.5-Turbo</td><td id="29-o">Contrastive Learning</td><td id="29-p">PDF icon, refresh icon</td></tr>
<tr><td id="29-q">2023.10</td><td id="29-r">RA-DIT</td><td id="29-s">ICLR &#x27;24</td><td id="29-t">Knowledge-Intensive Tasks (MMLU, NQ, TQA, ELI5, HotpotQA, etc.)</td><td id="29-u">DRAGON+</td><td id="29-v">LLaMA-65B</td><td id="29-w">SFT, LSR</td><td id="29-x">PDF icon</td></tr>
<tr><td id="29-y">2024.01</td><td id="29-z">BGM</td><td id="29-A">ACL &#x27;24</td><td id="29-B">QA</td><td id="29-C">T5-XXL-11B</td><td id="29-D">PaLM2-S</td><td id="29-E">SFT, PPO</td><td id="29-F">PDF icon</td></tr>
<tr><td id="29-G">2024.01</td><td id="29-H">Proxy-Tuning</td><td id="29-I">COLM&#x27;24</td><td id="29-J">QA, Math, Code</td><td id="29-K">LLaMA2-7B</td><td id="29-L">LLaMA2-70B</td><td id="29-M">Proxy-Tuning</td><td id="29-N">(PDF icon, GitHub icon)</td></tr>
<tr><td id="29-O">2024.02</td><td id="29-P">Bbox-Adapter</td><td id="29-Q">ICML&#x27;24</td><td id="29-R">QA</td><td id="29-S">DeBERTa-v3-base (0.1B), DeBERTa-v3-large (0.3B)</td><td id="29-T">GPT-3.5-Turbo, Mixtral-8x7B</td><td id="29-U">Contrastive Learning</td><td id="29-V">(PDF icon, GitHub icon)</td></tr>
<tr><td id="29-W">2024.02</td><td id="29-X">EVOR</td><td id="29-Y">EMNLP&#x27;24</td><td id="29-Z">Coding</td><td id="29-10">GPT-3.5-Turbo</td><td id="29-11">GPT-3.5-Turbo, CodeLLaMA</td><td id="29-12">Prompt Engineering</td><td id="29-13">(PDF icon, GitHub icon)</td></tr>
<tr><td id="29-14">2024.02</td><td id="29-15">ARL2</td><td id="29-16">ACL&#x27;24</td><td id="29-17">QA</td><td id="29-18">LLaMA2-7B</td><td id="29-19">GPT-3.5-Turbo</td><td id="29-1a">Contrastive Learning</td><td id="29-1b">(PDF icon, GitHub icon)</td></tr>
<tr><td id="29-1c">2024.03</td><td id="29-1d">BLADE</td><td id="29-1e">AAAI&#x27;25</td><td id="29-1f">QA</td><td id="29-1g">BLOOMZ-1b7</td><td id="29-1h">ChatGPT, ChatGLM, Baichuan, Qwen</td><td id="29-1i">SFT, BPO</td><td id="29-1j">(PDF icon, GitHub icon)</td></tr>
<tr><td id="29-1k">2024.05</td><td id="29-1l">Medadapter</td><td id="29-1m">EMNLP&#x27;24</td><td id="29-1n">Medical QA, NLI, RQE</td><td id="29-1o">BERT-Base-Uncased</td><td id="29-1p">GPT-3.5-Turbo</td><td id="29-1q">SFT, BPO</td><td id="29-1r">(PDF icon) (reload icon)</td></tr>
<tr><td id="29-1s">2024.06</td><td id="29-1t">CoBB</td><td id="29-1u">EMNLP&#x27;24</td><td id="29-1v">QA, Math</td><td id="29-1w">Mistral-7b-inst-v2</td><td id="29-1x">GPT-3.5-Turbo, Claude-3-Haiku, Phi-3-mini-4k-inst, Gemma-1.1-7B-it, Mistral-7B-inst-v2</td><td id="29-1y">SFT, ORPO</td><td id="29-1z">(PDF icon) (reload icon)</td></tr>
<tr><td id="29-1A">2024.10</td><td id="29-1B">Matryoshka Pilot</td><td id="29-1C">NeurIPS&#x27;25</td><td id="29-1D">Math, Planning, Reasoning</td><td id="29-1E">LLaMA3-8B, Qwen2.5-7B</td><td id="29-1F">GPT-4o-Mini, GPT-3.5-Turbo</td><td id="29-1G">DPO, IDPO</td><td id="29-1H">(PDF icon) (reload icon)</td></tr>
<tr><td id="29-1I">2025.06</td><td id="29-1J">Sysformer</td><td id="29-1K">arXiv</td><td id="29-1L">QA</td><td id="29-1M">Small Transformer</td><td id="29-1N">LLaMA-2-7B, LLaMA-3.1-8B, Mistral-7B, Phi-3.5-mini, Zephyr-7B-beta</td><td id="29-1O">Supervised Learning</td><td id="29-1P">(PDF icon)</td></tr>
</table>

<a id='c2bcd6bd-70f7-47e2-8a6a-ad35f4e131cd'></a>

<table id="29-1Q">
<tr><td id="29-1R" colspan="8">RLVR Methods</td></tr>
<tr><td id="29-1S">2025.05</td><td id="29-1T">s3</td><td id="29-1U">EMNLP&#x27;25</td><td id="29-1V">QA</td><td id="29-1W">Qwen2.5-7B</td><td id="29-1X">Qwen2.5-7B, Qwen2.5-14B,</td><td id="29-1Y">PPO</td><td id="29-1Z">PDF icon, GitHub icon</td></tr>
<tr><td id="29-20"></td><td id="29-21"></td><td id="29-22"></td><td id="29-23"></td><td id="29-24"></td><td id="29-25">Claude-3-Haiku</td><td id="29-26"></td><td id="29-27"></td></tr>
<tr><td id="29-28">2025.08</td><td id="29-29">R-Zero</td><td id="29-2a">arXiv</td><td id="29-2b">Math, Reasoning</td><td id="29-2c">Qwen3-4B,</td><td id="29-2d">Qwen3-4B,</td><td id="29-2e">GRPO</td><td id="29-2f">PDF icon, GitHub icon</td></tr>
<tr><td id="29-2g"></td><td id="29-2h"></td><td id="29-2i"></td><td id="29-2j"></td><td id="29-2k">Qwen3-8B,</td><td id="29-2l">Qwen3-8B,</td><td id="29-2m"></td><td id="29-2n"></td></tr>
<tr><td id="29-2o"></td><td id="29-2p"></td><td id="29-2q"></td><td id="29-2r"></td><td id="29-2s">OctoThinker-3B,</td><td id="29-2t">OctoThinker-3B,</td><td id="29-2u"></td><td id="29-2v"></td></tr>
<tr><td id="29-2w"></td><td id="29-2x"></td><td id="29-2y"></td><td id="29-2z"></td><td id="29-2A">OctoThinker-8B</td><td id="29-2B">OctoThinker-8B</td><td id="29-2C"></td><td id="29-2D"></td></tr>
</table>
Continued on next page

<a id='7572fec6-ab69-4e19-a493-aa8de8cd069e'></a>

30

<!-- PAGE BREAK -->

<a id='99158d9d-4dab-4687-b386-2cc7069fc3ef'></a>

Adaptation of Agentic AI

<a id='125bbaee-f288-44b4-b011-c65736e00ea0'></a>

Table 3 – Continued from previous page
<table id="30-1">
<tr><td id="30-2">Time</td><td id="30-3">Method</td><td id="30-4">Venue</td><td id="30-5">Task(s)</td><td id="30-6">Tool Backbone</td><td id="30-7">Agent Backbone</td><td id="30-8">Tuning</td><td id="30-9">Links</td></tr>
<tr><td id="30-a">2025.08</td><td id="30-b">Memento</td><td id="30-c">arXiv</td><td id="30-d">Long-Horizon Reasoning, Web Research, QA, Academic Reasoning</td><td id="30-e">Q-function (two-layer MLPs)</td><td id="30-f">GPT-4.1</td><td id="30-g">Soft Q-Learning</td><td id="30-h">PDF icon, GitHub icon</td></tr>
<tr><td id="30-i">2025.08</td><td id="30-j">AI-SearchPlanner</td><td id="30-k">arXiv</td><td id="30-l">QA</td><td id="30-m">Qwen3-32B</td><td id="30-n">Qwen2.5-7B</td><td id="30-o">PPO</td><td id="30-p">PDF icon</td></tr>
<tr><td id="30-q">2025.09</td><td id="30-r">Mem-a</td><td id="30-s">arXiv</td><td id="30-t">Test-Time Learning, Long-Range Understanding</td><td id="30-u">Qwen3-4B</td><td id="30-v">Qwen3-4B, Qwen3-32B, GPT-4.1-Mini</td><td id="30-w">GRPO</td><td id="30-x">PDF icon</td></tr>
<tr><td id="30-y">2025.10</td><td id="30-z">AgentFlow</td><td id="30-A">arXiv</td><td id="30-B">Web Search, Planning, Reasoning, Math</td><td id="30-C">Qwen2.5-7B</td><td id="30-D">Qwen2.5-7B</td><td id="30-E">Flow-GRPO</td><td id="30-F">PDF icon</td></tr>
<tr><td id="30-G">2025.10</td><td id="30-H">AutoGraph-R1</td><td id="30-I">arXiv</td><td id="30-J">KG Construction</td><td id="30-K">KG Constructor (Qwen2.5-3B/7B)</td><td id="30-L">Frozen RAG Generator (Qwen2.5-7B)</td><td id="30-M">GRPO</td><td id="30-N">PDF icon</td></tr>
<tr><td id="30-O">2025.10</td><td id="30-P">MAE</td><td id="30-Q">arXiv</td><td id="30-R">Math, Coding, QA</td><td id="30-S">Qwen2.5-3B</td><td id="30-T">Qwen2.5-3B</td><td id="30-U">REINFORCE++</td><td id="30-V">PDF icon</td></tr>
<tr><td id="30-W">2025.10</td><td id="30-X">Advisor Models</td><td id="30-Y">arXiv</td><td id="30-Z">Math, Reasoning</td><td id="30-10">Qwen2.5-7B, Qwen3-8B</td><td id="30-11">GPT-4o-Mini, GPT-5, Claude4-Sonnet, GPT-4.1-Mini</td><td id="30-12">GRPO</td><td id="30-13">PDF icon</td></tr>
<tr><td id="30-14">2025.10</td><td id="30-15">QAgent</td><td id="30-16">arXiv</td><td id="30-17">QA</td><td id="30-18">Qwen2.5-3B</td><td id="30-19">Qwen-7B</td><td id="30-1a">GRPO</td><td id="30-1b">PDF icon</td></tr>
</table>

<a id='7b15e45b-450d-42d4-815f-a0ef938261d1'></a>

### 5.2.1 Earlier Methods: From Proxy Signals to Structured Preferences
The earliest T2 methods emerged from the retrieval-augmented generation (RAG) community, where researchers sought to optimize dense retrievers for compatibility with large language models. These foundational works established the core principle that a frozen LM's internal computations could serve as supervision, but they reveal, in hindsight, the limitations of relying on proxy metrics that may not align with downstream task objectives.

<a id='b24651ea-4f7b-42c8-9d64-156a74ed5055'></a>

**REPLUG [11]** (NAACL 2024) introduced a general framework for adapting frozen language models through black-box supervision, using *perplexity reduction* as a training signal for the retriever. The core intuition is that if conditioning the LM on a retrieved document lowers its perplexity for a given query, the document likely provides informative context. Formally, the retriever is optimized to align its retrieval distribution with the distribution induced by the LM's perplexity-based preferences:

<a id='eb54e9d9-5f03-479d-97e4-6fff75110bbe'></a>

\mathcal{L}_{REPLUG} = D_{KL} (P_{Retriever}(d|q) \parallel P_{LM-perplexity}(d|q)),

<a id='c76e12e5-0ca1-4a80-9403-ab52f52f7f22'></a>

where P_LM-perplexity (d|q) reflects how strongly each document reduces the LM's perplexity when conditioned on query q. This design enabled retrieval adaptation without parameter access to the LM, establishing the foundation for a family of *agent-supervised* methods that optimize external modules based solely on frozen-agent feedback. **BLADE [150]** (AAAI 2025) further extended this paradigm by replacing traditional retrievers with domain-specific models that synthesize auxiliary knowledge: it couples a frozen general LLM with a small, domain-specific LM optimized via *Bayesian Prompted Optimization* (BPO). The small LM learns to generate domain-relevant knowledge and soft prompts that improve the black-box LLM's responses, extending REPLUG's black-box adaptation principle from retrieval to generative, domain-specialized tool co-adaptation. **BBox-Adapter [151]** (ICML 2024) reframed adaptation as an energy-based modeling problem, introducing a ranking-based noise-contrastive estimation loss and an online update framework to align outputs from black-box APIs like GPT-3.5 without access to internal probabilities. These methods collectively shifted the focus from *likelihood-based alignment* to *utility-driven adaptation*, paving the way for reinforcement-learning-based search and reasoning frameworks. **proxy-tuning [152]** (COLM 2024) extends black-box, agent-supervised adaptation to *decoding time*: a small tuned "expert" and its

<a id='494a3f9d-65ec-42e1-847c-81f90e1e03ac'></a>

31

<!-- PAGE BREAK -->

<a id='ea6ccbed-c094-4b18-abb3-de4844a015df'></a>

Adaptation of Agentic AI

<a id='a13104be-6b15-4f2c-bb2e-e29f5a2f2338'></a>

untuned "anti-expert" provide logit offsets that steer a frozen large LM without modifying its weights, effectively training a lightweight steering _tool_ under a frozen agent (T2). **EVOR** [153] (EMNLP 2024) further extends to the domain of code generation, formulating retrieval and knowledge evolution as a co-adaptive process driven by execution feedback from a frozen LLM.

<a id='a7ba4110-8dfd-4ca2-878d-33515d4eaf3f'></a>

Preference Learning: Toward Task Alignment. While black-box adaptation methods such as REPLUG and BBox-Adapter relied on indirect proxy signals like perplexity or ranking scores, subsequent studies [28, 154–157] moved toward explicit preference-based supervision that better reflects task utility. AAR [28] (ACL 2023) introduced augmentation-aware retrieval, where a frozen source LM (Flan-T5-250M) constructs preference pairs by comparing documents that most improve its own likelihood against human-annotated references. The retriever is then trained to reproduce these preferences via a contrastive loss, yielding a signal that directly encodes the LM's notion of "helpful context." Remarkably, these LM-derived preferences transfer effectively across architectures and scales, improving even 175B-parameter models. RA-DIT [154] (ICLR 2023) formalized this idea by defining document utility as the log-probability gain for producing correct answers:

<a id='fabd0242-1393-46d2-bb25-90792c79799d'></a>

Utility($d, q, a$) = log P$_{\text{LM}}$($a|q, d$) - log P$_{\text{LM}}$($a|q$),

<a id='285f786d-1006-43e5-8b30-bbb227ab8990'></a>

training retrievers to prefer documents that yield higher expected gains. This preference-based approach aligns retrieval more directly with downstream reasoning objectives while still operating through a single forward pass of the frozen LM. Together, these works mark a conceptual shift from proxy-based to task-aligned supervision, paving the way for reinforcement-style feedback and multi-turn optimization in later frameworks.

<a id='be4d1579-e19b-4c93-802b-5fc0a75b27c8'></a>

**Multi-Stage Architectures: Distilling Complex Preferences** The sophistication of training signals reached a new level with LLM-R [23] (EACL 2024), which introduced a multi-stage distillation pipeline. Rather than training the retriever directly on the frozen LM's outputs, LLM-R first trains an intermediate cross-encoder "reward model" to capture the frozen LM's nuanced preferences over in-context examples. This reward model, which can afford to be slow and expressive because it's only used during training, is then distilled into a fast bi-encoder retriever. This architecture embodies a key design principle: the complexity of the training signal need not be constrained by *inference-time efficiency requirements*. By decoupling the preference modeling from the final retrieval tool, LLM-R achieves both high-quality supervision and fast deployment. **UPRISE** [158] (EMNLP 2023) extends this paradigm beyond documents to prompts, training a prompt retriever using the frozen LLM's task performance across diverse tasks. By training on multiple tasks simultaneously, UPRISE learns a generalizable meta-skill: selecting prompts that improve LLM performance in zero-shot settings. The cross-task transfer (+8.5% on reading comprehension, +14.6% on paraphrase detection) suggests that T2-trained tools can internalize abstract principles of "what helps an LM" rather than memorizing task-specific heuristics.

<a id='afac96c3-5a99-4d9f-80f9-e7f614f4d664'></a>

By 2024, the field had reached a critical realization: _optimizing retrieval in isolation is insufficient_. Even a perfect retriever, measured by traditional IR metrics like NDCG or MRR, may produce results that are poorly suited for LLM reasoning. This insight catalyzed a wave of research focused on training _bridge tools_: rerankers, query reformulators, and document selectors that explicitly align retrieval outputs with LLM preferences.

<a id='3b04053e-a279-445c-807d-181f0d678d64'></a>

The Architecture of Preference Translation BGM [159] crystallizes this new paradigm. The core observation is stark: there exists a systematic "preference gap" between what traditional retrievers optimize for (surface-level relevance, lexical overlap) and what LLMs find useful for reasoning (contextual coherence, inferential support). BGM addresses this by training a T5-XXL "bridge model" that sits between a frozen retriever and a frozen generator (PaLM2-S), transforming the retriever's output into an LLM-friendly context. The training methodology is sophisticated: Stage 1 uses supervised learning on synthetically generated preference data (documents that improve LLM task performance vs. those that don't), while Stage 2 employs reinforcement learning where the frozen LLM's final task success provides the ultimate reward signal. This two-stage approach allows the bridge model to first learn coarse-grained preferences efficiently, then fine-tune its policy for actual downstream impact. The results are impressive: on HotpotQA, the bridged system achieves 35.6% exact-match accuracy compared to 25.8% with the best prior retriever-a relative improvement of 38%. What makes BGM architecturally significant is its demonstration that specialized adaptation layers can be more effective than end-to-end fine-tuning. Rather

<a id='8f64435b-007c-4b00-8c3e-265ef5477b0d'></a>

32

<!-- PAGE BREAK -->

<a id='d1bcb5b0-fdb8-418f-b8ce-db92322f3f3f'></a>

Adaptation of Agentic AI

<a id='a574f89f-de06-4944-b02b-261ab52700d7'></a>

than trying to make a single retriever simultaneously satisfy IR metrics and LLM preferences (which may be in tension), BGM decomposes the problem: the retriever handles broad recall, while the bridge model handles preference alignment. This modularity is a recurring theme in the most successful T2 systems.

<a id='426df1fc-3673-451d-991b-fc5616b1887d'></a>

**Synthesis: The Multi-Tool Ecosystem** Collectively, these advances reveal a new architectural pattern: *cascaded tool adaptation*. Rather than a single monolithic retriever, state-of-the-art T2 systems now employ a pipeline of specialized tools: query reformulators, retrievers, selectors, each trained using different aspects of the frozen LLM's behavior as supervision. This decomposition offers multiple advantages:

*   **Separation of concerns:** Each tool can optimize a specific sub-problem (recall vs. precision, speed vs. quality).
*   **Composability:** Tools can be mixed and matched; a new reranker can be trained without retraining the retriever.
*   **Efficiency:** Expensive operations (LLM inference) are deferred to the end of the pipeline, after cheaper tools have filtered the space.

<a id='a1380dfa-62fd-4827-bae8-0c681ef2e6f0'></a>

Yet this raises a fundamental question: if tools can learn from tools, which learn from frozen LLMs, how deep can this hierarchy go before compounding errors overwhelm the benefits? This question remains open, though empirical results suggest that 2-3 stages of tool adaptation (e.g., query reformulator → retriever → reranker) strikes a good balance.

<a id='ee7ee69b-d64b-4319-a09c-fb76155fbe64'></a>

## 5.2.2 Subagent-as-Tool
The year 2025 marked a paradigm shift in T2 research: the transition from training reactive tools (retrievers that respond to queries) to training proactive sub-agents (autonomous systems that actively explore, plan, orchestrate, and refine their operations over multiple turns while serving frozen primary agents). This shift was enabled by breakthroughs in reinforcement learning with verifiable rewards (RLVR) and represents the full maturation of the T2 vision: specialized sub-agents trained using frozen agent outputs as supervision signals, creating a symbiotic ecosystem where lightweight agentic tools co-evolve with frozen reasoning cores. Critically, this agentic transformation applies not only to information retrieval—training searchers that iteratively gather evidence—but also to meta-cognitive processes such as workflow orchestration and memory management, where sub-agents learn to coordinate frozen specialist modules and manage multi-step reasoning trajectories. We organize this evolution into three families of subagents: (i) agentic searchers, (ii) memory-construction subagents, (iii) meta-cognitive planners and orchestrators, and (iv) self-evolving subagents.

<a id='4a1cac15-7466-4483-850f-15bbb13454e2'></a>

**Agentic Searcher Breakthrough** s3 [27] (EMNLP 2025) demonstrated that training agentic tools could be radically more data-efficient than training agentic LLMs. The system trains a lightweight 7B "searcher" that performs multi-turn iterative search: generate queries, retrieve documents, select evidence, decide whether to search again or feed context to the frozen generator. The frozen generator (agent, Qwen2.5-14B or Claude) never updates, but provides the ultimate training signal through a metric called Gain Beyond RAG (GBR):

<a id='5a23c7e5-bb93-429c-9290-d6ed6b41883a'></a>

GBR = Accuracy (Gfrozen (q, Ds3), a) - Accuracy (Gfrozen (q, Dnaive), a)
where Ds3 are documents retrieved by the trained searcher and Dnaive are documents from naive top-k retrieval.

<a id='1a3f76ea-220b-45c8-8c27-a7e69ec559f8'></a>

This reward directly measures the *value added by the search tool*, focusing training on examples where naive retrieval fails. The efficiency gains are staggering: s3 achieves 58.9% average generation accuracy with only 2.4k *training samples*—70× less data than Search-R1 (an A2-style agent requiring 170k examples) and 33× faster wall-clock training time. Moreover, s3 trained on general QA achieves 76.6% accuracy on specialized medical QA versus 71.8% for Search-R1, suggesting that T2-trained tools learn more **generalizable search skills** than agents trained end-to-end. The theoretical explanation for this efficiency advantage is illuminating: in A2-style agent training, the model must simultaneously learn (1) domain knowledge, (2) tool use skills, and (3) task-specific reasoning, leading to a complex, high-dimensional optimization landscape; in T2, the frozen generator already

<a id='b874c364-39ee-4245-b561-c222291d75f1'></a>

33

<!-- PAGE BREAK -->

<a id='7436e385-6f70-4470-86c9-5ad0e3e98243'></a>

Adaptation of Agentic AI

<a id='ed7fa659-bade-4e5a-8b0d-ac3e90c3cf3f'></a>

possesses domain knowledge and reasoning ability, so the tool need only learn the _procedural skill_ of effective search.

<a id='a049d7d0-f651-4859-8f04-b01f114491fe'></a>

Similar to this idea, **DynamicRAG** [160] (NeurIPS 2025) "agentifies" reranking: instead of static reorderings, an RL policy adapts *how many* and *which* documents to pass based on query hardness and retrieval noise, balancing quality and context cost. The training combines imitation learning on expert trajectories (to bootstrap reasonable behavior) with policy gradient RL where the generator's output provides the reward signal. The learned policy exhibits emergent adaptive behavior: for simple queries with high-quality initial retrieval, it presents fewer documents; for complex queries with noisy retrieval, it retrieves more broadly and reranks more aggressively.

<a id='a2c740d7-0bc9-414d-a9d8-8a0000ddb539'></a>

QAgent [161] further clarifies how to robustly train such search subagents. Its Stage 1 trains a 3B search agent end-to-end, rewarding it based on whether its *own* generated answer is correct, but this encourages reward hacking, where the agent prefers shallow, easily copyable evidence over genuinely informative documents. Its Stage 2 corrects this by switching to evaluation from a stronger *frozen* generator:
RStage2 = II [Gfrozen (q, Dagent) = acorrect],

<a id='759cd2fb-44f0-42d7-9b0b-2253a2969b6f'></a>

rewarding the searcher only when the frozen model can answer correctly using its retrieved documents. This decoupling forces the subagent to optimize for retrieval quality rather than its own myopic behavior, reinforcing a core T2 principle: the frozen generator should not only consume a tool's outputs but also supervise its learning.

<a id='9fa90b9e-e97c-458a-9991-a2d51257fb2b'></a>

**Learning to Construct Memory as a Subagent** Extending beyond search, a complementary line of work treats *long-term memory construction* itself as a T2-style subagent problem. Mem-a [162] formulates memory management as RL over an explicit memory API, training a lightweight Qwen3-4B controller to operate a three-part external memory (core summary, semantic facts, episodic events) for a frozen backend generator. Only the memory-writing policy is optimized; the generator and retriever for downstream QA remain frozen. Rewards derive from verifiable outcomes—question-answering accuracy over long horizons, tool-call correctness, effective compression, and semantic validity of memory entries—so the subagent learns to construct compact yet sufficient memories that maximize the frozen model's utility. Empirically, Mem-a significantly outperforms prior memory baselines and generalizes from ~30k-token training sequences to contexts exceeding 400k tokens, exemplifying a memory-construction subagent that adaptively curates the information diet for a fixed reasoning core.

<a id='21a389e4-cf3a-4767-9479-de91107ab1c5'></a>

AutoGraph-R1 [163] applies this symbiotic principle to the construction of structured Knowledge Graphs (KGs).
Rather than relying on static extraction heuristics, it optimizes an LLM-based "constructor subagent" to generate
KGs from raw text. The supervision signal is derived directly from the _frozen agent's_ performance on downstream
reasoning tasks (GraphRAG [164]) using the generated graph. This allows the constructor to learn a policy that
prioritizes _functional utility_, creating connectivity and paths that specifically facilitate the host agent's retrieval and
reasoning, over intrinsic metrics like triple density.

<a id='e5faf668-8f29-4ada-9d38-6019d8584918'></a>

**Meta-cognitive and Control Subagents** Stepping further up the stack, recent work trains subagents that shape _how_ frozen models think (planning, steering, and budgeting computation) rather than what they retrieve or store.

<a id='9e362f2c-e212-4e57-ad38-23f1c4170374'></a>

AI-SearchPlanner [26] introduces _multi-objective optimization_ that balances effectiveness with efficiency. The system trains a planner tool (Qwen2.5-7B) that generates multi-step search strategies for a frozen generator, optimizing

$T = IF [R... + \lambda . R... - n . Cost]$

<a id='d4c93fc5-f17d-454c-a464-8abe3768cc3f'></a>

where R_outcome measures final task success, R_process evaluates the rationality of the search plan (critiqued by the frozen generator), and Cost penalizes excessive planning. By leveraging both outcome and process rewards [165], the frozen model acts as executor and teacher, enabling the planner to internalize not only "what works" but "why it works." Varying λ traces a Pareto frontier between cost and quality, yielding planners tailored to different deployment budgets.

<a id='ea1ae0b1-e5fb-4170-ba2d-82473eebb6d9'></a>

Advisor Models [166] generalize this idea to *instance-wise natural-language steering*. A small advisor model learns, via GRPO, to prepend context-specific advice that nudges a frozen foundation model toward preferred behaviors (style, safety, reasoning depth) without touching its weights. Within our taxonomy, such advisors function as trainable control interfaces or parametric memories that encode environment- and user-specific latents.

<a id='3968bb95-6a5f-4c9c-a614-bf8ef3e918d6'></a>

34

<!-- PAGE BREAK -->

<a id='512c3146-1342-4585-ade0-65da16c2c4f7'></a>

Adaptation of Agentic AI

<a id='1334ab74-2a73-4015-b9b9-0ac7d143f416'></a>

Bridging from advising to driving, Matryoshka Pilot [167] (NeurIPS 2025) formalizes a controller-generator loop where a small white-box LLM controls a larger black-box LLM by emitting intermediate decomposition steps, plans, and summaries. Treating the black-box model as an environment, M-Pilot collects trajectory-level success signals and optimizes the controller with Iterative DPO. This yields ≈3-7% gains across reasoning, planning, and personalization benchmarks, and transfers plug-and-play across multiple black-box backends, further reinforcing the view of control subagents as portable T2 tools.

<a id='7f6ac6d3-18bf-45ae-a2ca-82cb3ddff2d5'></a>

Learning to Orchestrate Frozen Specialists Orchestration-focused subagents push the T2 vision to its logical conclusion: training a dedicated policy to coordinate multiple frozen specialists.

<a id='ba7d61ab-73c7-4232-ac80-e5a002ff36f3'></a>

AgentFlow [51] decomposes an agent into modules—a planner, tool executor, verifier, and solution genera-
tor—implemented mostly as frozen Qwen2.5-7B-Instruct models. Only the planner is trained. Using Flow-GRPO, a
single trajectory-level reward (correct vs. incorrect, judged by GPT-4o) is broadcast to all decisions in each rollout,
with group-normalized advantages enabling effective credit assignment despite sparse rewards. Empirically, a 7B
AgentFlow planner achieves 57.3% on search-intensive tasks (+14.9% over AutoGen), 51.5% on mathematical
reasoning (+14.5% over ToRL), and 33.1% on GAIA—outperforming GPT-4 (~200B parameters) on several setups.
This shows that learned orchestration of frozen specialists can rival or surpass monolithic models.

<a id='d6e932ed-bf9f-4458-a5b8-395a328737ff'></a>

Self-Evolving (Sub)Agent A more advanced branch of the subagent-as-tool paradigm allows the tools themselves to co-evolve through self-generated tasks and rewards. R-Zero [168] instantiates two roles—a Solver and a Challenger—from the same base LLM. When the Solver is frozen, its successes, failures, and uncertainty (via self-consistency) define rewards that train the Challenger to propose tasks near the Solver's capability frontier. Alternating these phases creates a bidirectional loop, yet each step still follows the T2 principle of optimizing a lightweight subagent under signals from a stronger or temporarily fixed core. Multi-Agent Evolve (MAE) [169] extends this design into a triadic architecture with a Proposer, Solver, and Judge. The Proposer and Judge operate as adaptive T2 subagents: the Judge learns to evaluate trajectories produced by the system, and the Proposer learns to generate diverse, high-quality, Solver-challenging tasks. Rather than tuning the main Solver, MAE improves performance by training these peripheral subagents to shape data, rewards, and curricula. Together, R-Zero and MAE illustrate a second generation of subagent-as-tool methods—self-evolving ecosystems that autonomously construct the learning conditions for otherwise frozen reasoning cores.

<a id='7c80ea11-7c52-4330-9d10-18deaf5edc45'></a>

**Synthesis: The Maturation of T2** Across these lines of work, the subagent-as-tool paradigm extends T2 from *what* to retrieve, to *what* to remember, to *how* to plan, steer, and orchestrate, and finally to *how* to self-evolve. Agentic searchers (s3, DynamicRAG, QAgent) optimize information acquisition for frozen generators; memory-construction subagents (Mem-α) curate long-horizon state; meta-cognitive controllers and orchestrators (AI-SearchPlanner, Advisor Models, Matryoshka Pilot, AgentFlow) decide how tools and specialists are deployed; and self-evolving frameworks (R-Zero, Multi-Agent Evolve) autonomously generate curricula and reward signals that continually refine these ecosystems. The consistent lesson is that decoupling tool training from generator training, while enabling tools to adapt to one another, yields systems that are more data-efficient, modular, generalizable, and robust than monolithic alternatives. In this mature T2 view, intelligence emerges not from scaling a single model, but from the learned coordination and co-evolution of specialized, frozen components through lightweight, adaptive subagents.

<a id='391227c9-54aa-4400-8901-89118fb2783e'></a>

### 5.2.3 Agentic Memory and Others
An agent's memory system itself can also be framed as an adaptive tool. Instead of modifying the agent's core parameters, T2 methods can train or "tune" the memory module---how it writes, retrieves, reflects, and forgets---using the frozen agent's downstream task performance or outputs as the supervisory signal. Survey on agent memory [48] categorizes a wide array of mechanisms that can be optimized as T2 tools. These works explore memory as a foundational component for agentic behavior, spanning short-term buffers, long-term experiential databases, and structured knowledge.

<a id='55d745e0-a4d7-4190-92d6-2934db3b02a9'></a>

35

<!-- PAGE BREAK -->

<a id='1ef1373b-b568-4be7-9935-d34ad9baccaf'></a>

Adaptation of Agentic AI

<a id='54c68b72-dceb-41b0-84ac-5a5c32507f97'></a>

**Dynamic Memory Stores** Many foundational memory architectures function as T2 tools, where the frozen agent's output (e.g., a "write" command or new information) dynamically "tunes" the external memory store. This includes systems that manage short-term (in-context) and long-term (retrieval-based) storage, demonstrating how agents can manage context, retrieve past interactions, and maintain coherence over time by updating a peripheral memory tool [170-175].

<a id='a02e1eab-6830-4ee9-8a2e-535b098f2b44'></a>

**Experiential and Reflective Memory** A significant line of T2-aligned research focuses on memory modules that learn from experience. These tools enable a frozen agent to store, reflect on, and learn from its own output (e.g., entire trajectories), often using verbal reinforcement or self-correction. This allows the agent to build a curriculum of skills and avoid repeating past failures by tuning the memory tool without updating the core LLM's weights [34, 176–180].

<a id='f0ee27ab-84a9-454a-b660-1df287f23605'></a>

**Structured Memory (Graphs, Trees, and Databases)** To move beyond linear text, some T2 memory tools structure information in sophisticated forms, such as knowledge graphs, trees, or symbolic databases. The frozen agent's outputs are used as signals to "tune" this structured tool—for example, by adding new nodes, updating relationships, or writing to a database. These structured representations can be more efficiently queried and reasoned over by the frozen agent, effectively externalizing complex memory management into a specialized, adaptive tool [181–184].

<a id='2d2131cb-0a2e-4ea0-aa10-3a78bba2de62'></a>

Episodic Memory as a Trainable Module Memento [22] demonstrates that an agent's memory system can be optimized as an external tool without any modification to the LLM planner. The system combines a frozen GPT-4.1 high-level planner with a trainable episodic case memory module. The memory stores past problem-solving trajectories, and the tool being trained is a neural Q-function that learns a _case retrieval policy_: which past cases to present to the frozen planner when facing a new problem. The training signal is remarkably simple: binary task success or failure. This sparse, trajectory-level reward is broadcast to all case-selection decisions in that trajectory, and a soft Q-learning algorithm updates the retrieval policy. Crucially, the frozen LLM never sees the Q-values or policy internals; it simply receives retrieved cases as context and generates its plan. Memento achieves top-tier performance: 87.88% on GAIA validation (ranked 1st), 79.40% on GAIA test (3rd place), and 95.0% on SimpleQA. Ablations show that case-based memory adds 4.7–9.6% absolute improvement on out-of-distribution tasks. This is remarkable because _only the memory is trained_; the same frozen LLM that performed worse without memory now excels simply because its information diet has been optimized.

<a id='5204734b-4178-4bf4-914d-fc5b2f732f5c'></a>

Test-Time Memory Curation Another prominent example of T2 memory adaptation at inference-time is **Dynamic Cheatsheet (DC)** [185], a lightweight framework that provides a "persistent, evolving memory" for black-box LMs. The system operates "without modifying their underlying parameters" and requires no gradient-based updates. The framework consists of two core modules: a Solution Generator and a Memory Curator. This **Memory Curator** is the adaptive T2 tool. It operates without access to ground-truth labels; instead, it has to assess the correctness and efficiency of the solutions by itself after they are produced by the frozen generator. Based on this self-assessment, the curator updates the memory by storing concise, transferable snippets such as "reusable strategies, code snippets, and general problem-solving insights", rather than full, uncurated transcripts. **ReasoningBank** [186] extends this test-time curation concept by creating a memory framework that explicitly distills generalizable reasoning strategies from both successful and self-judged failed experiences. Unlike methods that store raw trajectories or only successful routines, ReasoningBank analyzes failures to extract "crucial preventative lessons". This curated bank of reasoning strategies is then retrieved to guide the agent in future tasks. The framework also introduces memory-aware test-time scaling, which uses the curated memory to guide a scaled exploration, creating a "synergy between memory and test-time scaling" where the diverse experiences from scaling help forge stronger, more generalizable memories. This approach was shown to be effective on complex benchmarks like WebArena [187] and SWE-Bench [188].

<a id='6260aac5-5e7e-4736-98fc-b65922b5d5c0'></a>

**Adapting the Embedding Space** An approach for tool scalability is **ToolkenGPT** [29], which represents tools as learnable token embeddings within the frozen LLM's vocabulary. The entire LLaMA-13B/33B model remains

<a id='b149e4d2-5ec3-4c8e-8669-4977d1aa7703'></a>

36

<!-- PAGE BREAK -->

<a id='59325627-3892-4828-b91c-95f1f5dd9f12'></a>

Adaptation of Agentic AI

<a id='1f45509a-fc60-4cb8-8233-f36485f00e72'></a>

frozen; only a small embedding matrix W_T \in \mathbb{R}^{|T|\times d} (where $|T|$ is the number of tools and $d$ is the embedding dimension) is trained. These "toolkens" are concatenated with the standard vocabulary, and the frozen LLM learns to predict them like any other token.

<a id='9f0853b5-0b3d-4b32-9dd6-18d683b0b806'></a>

Training uses supervised learning on parallel sequences where ground-truth tool calls are replaced with toolken placeholders. The loss is masked so that only the toolken predictions (and subsequent argument tokens) contribute gradients to Wt. This is remarkably parameter-efficient: adding 234 tools requires training only 234 × 4096 ≈ 1M parameters (for LLaMA's 4096-dimensional embeddings), compared to the 13B+ parameters of the full model. ToolkenGPT achieves 73% one-hop accuracy on FuncQA (vs. 57% for ReAct), 75% supervised accuracy on 234-relation KAMEL, and 68% success on VirtualHome with 58 action/object tools. Crucially, new tools can be added by simply expanding Wt and continuing training—no full model retraining required. The conceptual contribution of ToolkenGPT is showing that adaptation can occur at the interface layer (the embedding space) rather than the parameter layer (the LLM weights), offering a middle ground between fully frozen T1 systems and fully fine-tuned A1/A2 systems.

<a id='913b9ba0-524c-49b7-bc4a-9794aa33a62c'></a>

Beyond the paradigms above, a diverse set of recent approaches further extends the tool adaptation framework. These methods introduce new training objectives, modalities, and architectural innovations that broaden the scope of tool adaptation.

<a id='e1ece410-2939-433b-87d2-6187647b7607'></a>

UniMuR [189] trains unified multimodal embeddings aligned with frozen LLM semantic representations, yielding 6.5% R@1 improvement on MMDialog. DIFO [190] adapts frozen CLIP through task-specific prompt learning via mutual information maximization for source-free domain adaptation. V2L Tokenizer [191] trains encoder-decoder structures mapping images to frozen LLM token space, using the frozen vocabulary as quantization codebook to enable low-level vision tasks with frozen text LLMs. Sysformer [192] trains a small transformer that adapts the system-prompt embeddings based on each user prompt while keeping the LLM frozen. Supervision comes entirely from the frozen model's own likelihoods over refusal and compliance targets, augmented by reconstruction and optional classifier losses.

<a id='10855619-874c-4446-a46f-13b5d367cd64'></a>

Common patterns emerge across T2 methods: lightweight training of small modules (millions of parameters) while keeping LLMs frozen (billions of parameters), semantic exploitation of rich representations (hidden states, token spaces, vocabularies), modality bridging between vision/retrieval/tools and frozen text LLMs, efficiency gains (148–42,630x speedups), and strong generalization to zero-shot or unseen settings.

<a id='09e1eb30-f4f2-40f6-b359-44fa278376e1'></a>

## 6 Comparison of Adaptation Paradigms

This section provides a comprehensive comparison of the four adaptation paradigms: (A1) Agent Adaptation with Tool Execution Signal, (A2) Agent Adaptation with Agent Output Signal, (T1) Agent-Agnostic Tool Adaptation, and (T2) Agent-Supervised Tool Adaptation. We first establish a conceptual framework for comparison, then analyze the agent-centric (A1/A2) and tool-centric (T1/T2) paradigms in depth, with special focus on the emergent "subagent-as-tool" and "graduation" concepts, and conclude with a quantitative synthesis of critical trade-offs.

<a id='02d979d8-a4be-4c5d-ad94-50195ae59d2c'></a>

## 6.1 A Framework for Comparison
We compare the four paradigms along four main axes.
* **Cost and Flexibility**: We use *cost* to refer to compute and engineering effort required for adaptation, and *flexibility* to mean how easily the system's behavior can be reconfigured. A1/A2 provide high *parametric* flexibility (the entire agent policy can change), whereas T1/T2 provide high *system-level* flexibility (capabilities can be added, swapped, or composed via tools) but remain bounded by the frozen agent's intrinsic reasoning power.
* **Data Efficiency**: Beyond raw compute, the amount of training data required differs dramatically across paradigms. Recent evidence suggests that T2 methods can match or surpass A2-style end-to-end agent training with orders of magnitude less data, by only training small subagents around a frozen backbone.

<a id='50c3cb22-e7a8-444f-9ef6-d15563e45ade'></a>

37

<!-- PAGE BREAK -->

<a id='9065334a-0b5c-486a-a4e5-906ab7e48ce3'></a>

Adaptation of Agentic AI

<a id='ad7a1a18-09b4-49f8-814b-3c47c726aa9d'></a>

Table 4 High-level qualitative comparison of the four adaptation paradigms. "Flex." denotes the dominant form of flexibility: parametric (within a single agent policy) vs. system-level (via modular tools and orchestration).

<a id='d8ee8741-942d-45ba-b3c9-0c4c0ada3148'></a>

<table id="37-1">
<tr><td id="37-2">Paradigm</td><td id="37-3">ID</td><td id="37-4">Locus of Adaptation</td><td id="37-5">Supervision Signal</td><td id="37-6">Cost &amp; Flexibility</td><td id="37-7">Modularity &amp; Evolution</td></tr>
<tr><td id="37-8">Agent, Tool Signal</td><td id="37-9">A1</td><td id="37-a">Core Agent Policy</td><td id="37-b">Tool Execution</td><td id="37-c">High Cost, High Param. Flex.</td><td id="37-d">Monolithic, Risk of Overfitting</td></tr>
<tr><td id="37-e">Agent, Output Signal</td><td id="37-f">A2</td><td id="37-g">Core Agent Policy</td><td id="37-h">Agent Output</td><td id="37-i">High Cost, High Param. Flex.</td><td id="37-j">Monolithic, Risk of Forgetting</td></tr>
<tr><td id="37-k">Tool, Agent-Agnostic</td><td id="37-l">T1</td><td id="37-m">External Tool</td><td id="37-n">Agent-Independent</td><td id="37-o">Low Cost, High System Flex.</td><td id="37-p">High (Plug-and-Play)</td></tr>
<tr><td id="37-q">Tool, Agent-Supervised</td><td id="37-r">T2</td><td id="37-s">External Tool</td><td id="37-t">Frozen Agent Output</td><td id="37-u">Low Cost, High System Flex.</td><td id="37-v">High (Symbiotic, No Forgetting)</td></tr>
</table>

<a id='a3b59436-beac-4de8-9bcb-4bda942c73ce'></a>

*   **Generalization Capability:** This axis captures how well an adaptation strategy transfers to new tasks, agents, or environments. T1 tools trained on broad data distributions generalize across different agents and tasks, while T2 tools often inherit cross-domain robustness from the frozen foundation models supervising them. A1/A2, especially on-policy variants, risk overfitting to specific environments without explicit regularization.
*   **Modularity and System Evolution:** This axis focuses on engineering implications: how easily a system can be extended or maintained over time. Tool-centric paradigms (T1/T2) enable modular evolution and hot-swapping of components; agent-centric paradigms (A1/A2) tend to be monolithic and may suffer from catastrophic forgetting when adapted repeatedly.

<a id='887e1b6c-fd6e-4971-97f2-27b5a21139d2'></a>

In prose, the picture is as follows. Agent adaptation (A1/A2) is expensive but gives fine-grained control over the agent itself: one can rewrite the entire policy, alter reasoning style, alignment, and domain knowledge in a single model. This is high parametric flexibility, but each change typically requires retraining a large model and may unintentionally affect other behaviors. Tool adaptation (T1/T2), in contrast, does not touch the core agent; instead, it achieves system-level flexibility by letting us attach specialized tools, each tuned independently. We can freely augment separate capabilities (e.g., retrieval, planning, memory, code search), orchestrate them, and retire or replace tools without destabilizing the base agent. The trade-off is that these tools cannot push the system beyond what the frozen agent can understand and use.

<a id='062be0cf-c159-45b8-b9ae-4c906f54da49'></a>

Data efficiency strongly favors tool-centric adaptation. T2 methods like s3 [27] reach competitive or superior performance to A2-style agents such as Search-R1 while using roughly 70× fewer labeled examples, because the T2 subagent only learns a narrow procedural skill (e.g., search policy) rather than relearning general reasoning [27, 108]. Generalization exhibits a similar pattern: T1 tools, by construction, are agent-agnostic and often robust across tasks and agents; T2 tools inherit the inductive biases of strong frozen agents and thus often transfer better across domains than aggressively fine-tuned A1/A2 agents, which may overfit and forget.

<a id='02cc2f54-f3e9-4a3e-9db5-7002edaff53c'></a>

Finally, modularity is where T1/T2 shine. Adding a new retrieval strategy, memory writer, or planner simply means training (or swapping in) another tool, leaving the core agent untouched. In A1/A2, the only way to change behavior is to re-adapt the monolithic agent, risking interference with prior skills. In large-scale multi-tool systems, this difference in "evolution ergonomics" is often more decisive than raw performance.

<a id='034f30af-aa5a-4e77-97e0-5229ab2eb886'></a>

## 6.2 Agent Adaptation Paradigms: A1 and A2
We now analyze the two agent-centric paradigms, which both modify the agent's core parameters but differ fundamentally in their training signals and optimization objectives.

<a id='44277c75-cd2b-415d-93ca-a666c1057206'></a>

6.2.1 A1: Optimizing Tool Mechanics via Causal Feedback
The defining feature of A1 on-policy methods is reliance on *causal, immediate, and fine-grained* reward signals. The supervision source is the verifiable outcome of tool execution itself, not a downstream task metric. For example, DeepRetrieval [21] formalizes query reformulation as an MDP where reward is directly derived from retrieval metrics like Recall@K or NDCG, and RLEF [20] frames code synthesis with rewards from test-case execution. This stands in contrast to A2 signals that only evaluate the final answer.

<a id='3a03fd2f-6795-4ac0-a6d9-041f1fc2050d'></a>

Conceptually, A1 on-policy RL optimizes *tool-use mechanics*—teaching the agent *how* to wield tools correctly, grounding behavior in environment “physics” (“this syntax executes”, “this query retrieves”). This direct engagement

<a id='38cb5622-a3f3-4811-bcd5-3f731cde044a'></a>

38

<!-- PAGE BREAK -->

<a id='b2b64f77-bfaf-4bbb-9fcb-509017acee3f'></a>

Adaptation of Agentic AI

<a id='6345b6c4-74ba-463d-8096-b590928095f6'></a>

with ground-truth feedback drives strong performance in domains with verifiable, deterministic outcomes.

<a id='38459b9b-a91d-4724-8be6-42393af4f82b'></a>

**Quantitative evidence.** Mechanistic optimization under AI achieves state-of-the-art performance in specialized domains:

*   **Retrieval:** DeepRetrieval achieves roughly 3x improvement in recall (65.1% vs. 24.7%) on literature search [21].
*   **Code reasoning:** R1-Code-Interpreter reaches 72.4% accuracy on 37 test tasks through multi-stage RL [66].

<a id='b5506eaa-6f8a-48cb-927a-417d4cd0c4dd'></a>

However, learning through trial-and-error introduces significant challenges: it requires careful reward design, KL-regularized PPO or GRPO, curriculum learning, and dynamic sampling for stable convergence.

<a id='47ce72d6-5e4a-4af2-a4c0-d764003bea55'></a>

### 6.2.2 A2: Optimizing Tool Strategy via Holistic Rewards
A2 methods instead use *holistic, sparse, and high-level* rewards based on agent output quality—typically final answer correctness—which depends on tool usage but does not directly supervise individual tool calls. ReSearch [108], trained on multi-hop QA, optimizes when and how to search. The reward asks not “was this particular search good?” but “did the *entire process of thinking, searching, and reasoning* lead to the correct answer?”

<a id='477c5cd0-363c-4fc5-b22a-bd184b5dadd0'></a>

Thus A2 optimizes *tool-use strategy* or *coordination*. Rather than learning search mechanics (assuming a T1 retriever handles that), it learns the *cognitive policy* for *when* to search, *what* to search for, and *how* to integrate results. This strategic focus explains why ReSearch reports emergent reflection and self-correction behaviors during RL training [108].

<a id='5047e70c-dcea-41b2-822f-33b6f554854f'></a>

**Quantitative evidence.** Strategic optimization under A2 proves highly effective for complex, multi-step reasoning:
*   **Retrieval-augmented QA:** ReSearch yields 9–22% absolute gains over strong iterative RAG baselines [108].
*   **Factual accuracy:** R1-Searcher reports up to 24% improvement over strong RAG baselines, demonstrating enhanced factual accuracy and reduced hallucination through learned retrieval policy [107].

<a id='9c4af365-5515-4ada-89da-f4bad5962b5d'></a>

In terms of flexibility, A2 offers the richest _parametric_ flexibility: the agent can change its entire global strategy for orchestrating tools and reasoning, but each such change requires expensive retraining, and the resulting policy is baked into a single large model.

<a id='367dd017-1c7a-44fc-96d5-fa32d485af98'></a>

A1 & A2: Signal Source as a Reliability Axis. Beyond taxonomic categorization, the distinction between A1 and A2 fundamentally determines the granularity and scope of the adaptation signal.

* Tool-execution signals (A1) are *grounded*, *causal*, and *process-oriented*. The feedback is produced by an environment or tool whose semantics are independent of the agent's internal beliefs (e.g., code execution, retrieval metrics, formal proof checkers). This grounding enables learning that is tightly coupled to intermediate correctness and tool mastery, but often comes with higher interaction cost and environment dependence.
* Agent-output signals (A2) are *holistic*, *flexible*, and *outcome-oriented*. Rewards are assigned to the agent's final outputs, derived from either verifiable ground truths (e.g., gold answers, math solutions) or subjective preferences (e.g., reward models). While this allows for end-to-end task optimization, relying solely on terminal signals can make the agent vulnerable to shortcut learning (getting the right answer for the wrong reason) and sparse feedback issues compared to the dense signals of A1.

<a id='71c44008-5d43-4820-81f1-b7ac35027e10'></a>

## 6.3 Tool Adaptation Paradigms: T1 and T2
We now pivot to tool-centric paradigms, which shift optimization from the expensive agent to cheaper external tools. These paradigms sacrifice some parametric flexibility (the agent policy stays fixed) but gain system-level flexibility: we can grow, specialize, and rewire the tool ecosystem without touching the main agent.

<a id='87302dff-7f5d-4c14-bb2d-e68297f4540d'></a>

### 6.3.1 T1: The "Graduated Agent" as Subagent-as-Tool
T1 is defined by agent-agnostic, pre-trained, plug-and-play components. A critical concept within T1 is the *subagent-as-tool*, which reveals a rich development lifecycle.

<a id='63ea93b3-b532-4a74-84f3-dbc3f881cc29'></a>

39

<!-- PAGE BREAK -->

<a id='83c25faa-7780-474c-a4c2-9a90c3a527d8'></a>

Adaptation of Agentic AI

<a id='2558a7cd-a790-41db-9fd4-9640f5ffa9c0'></a>

At one extreme, we have *static, foundational* tools like SAM [131] or AlphaFold2 [138], trained once on massive datasets and deployed as fixed APIs. They primarily encapsulate learned representations or simulators and can be called by any agent.

<a id='c373f557-ba3e-4e2c-9492-2da29c0cbdae'></a>

At the other extreme are _dynamic, graduated_ tools: adaptive agents from §6.2 can be trained under A1 or A2 and then _frozen_ and reused as T1 tools. This "Graduation Lifecycle" (A1 → T1) proceeds as:

1.  **Train (A1/A2)**: Use on-policy RL or outcome-based RL to train an agent for a specific task (e.g., DeepRetrieval as a search-query rewriter, Code-R1 as a code generator).
2.  **Freeze**: Once the agent reaches expert performance, freeze its parameters.
3.  **Deploy (T1)**: The frozen expert becomes a T1 "subagent-as-tool" callable by any higher-level agent.

<a id='e8fd81f2-7249-447a-8a13-e1439704699e'></a>

Concrete examples already follow this pattern. DeepRetrieval is trained via on-policy A1 RL as a query reformulation agent [21], but once frozen it can be used as an interchangeable T1 retrieval-augmentation tool in many different pipelines. Similarly, SWE-Grep [193] is trained as a specialized RL subagent for fast, multi-turn, highly parallel code context retrieval, and then exposed as a tool that software-engineering agents (e.g., SWE-Agent or Cursor-style IDE agents) can call for high-quality repository search. In both cases, the "graduated" subagent encapsulates a learned policy (not just a representation) and slots into new systems without retraining.

<a id='c7aa4262-7f50-4da6-bc33-a689df72c570'></a>

From the flexibility perspective, T1 offers _high system-level flexibility_: one can assemble different T1 tools into various configurations, or replace one tool (e.g., swap a retriever) without touching the agent. The cost of adding a capability is proportional to the size of the corresponding tool, not the backbone agent. The trade-off is that the tools are not tailored to any particular agent; the agent must adapt its prompts or orchestration logic to whatever interface the tool exposes.

<a id='6f01f3ce-966c-42a6-a67e-53dff57e9f20'></a>

### 6.3.2 T2: The "Symbiotic Inversion" and Subagent Federation
T2 represents a conceptual inversion. Rather than asking "how can we modify the agent to better use tools?" (A1/A2), T2 asks: "how can we modify tools to better serve a fixed agent?" This reframes the expensive foundation model from optimization _target_ to stable _supervision source_.

<a id='1c27ac3c-a4be-47b8-b29c-51c88ab8858e'></a>

This creates *symbiotic adaptation*: the frozen host agent (e.g., GPT, Claude) provides high-level reasoning and reward signals, while adaptive symbiote subagents (e.g., lightweight 7B models) learn to translate, filter, and present information in exactly the form the agent finds most useful. The core benefit is *decoupling skill from knowledge*. A traditional A2 agent like Search-R1 must learn (1) domain knowledge, (2) tool-use skills, and (3) task reasoning simultaneously—a complex optimization landscape. In T2, the frozen generator already possesses (1) and (3); the T2 subagent needs only learn procedural skill.

<a id='e59eef52-8a5d-4bfe-97e0-f7537d88eae5'></a>

T2 subagent families also demonstrate a powerful architectural strategy: *unbundling* the agent's monolithic cognitive loop (Perceive-Plan-Act-Reflect) into specialized, independently trainable submodules:

*   **Optimizing "Perception" (Agentic Searchers)**: Systems like s3, DynamicRAG, and QAgent train search subagents to decide *what* to query, *where* to search, and *when* to stop [27].
*   **Optimizing "Reflection" (Memory Construction)**: Subagents such as Mem-α learn memory-writing policies via RL, rewarded based on whether stored experiences improve future performance for the frozen generator.
*   **Optimizing "Planning" (Meta-Cognitive Planners)**: Subagents like AI-Search Planner and AgentFlow decide how tools and specialists are deployed. AgentFlow [51] trains only a lightweight planner that orchestrates frozen specialists using trajectory-level rewards, achieving 33.1% on GAIA and surpassing ~200B-parameter GPT-4.

<a id='b2f24bec-d601-48e3-b619-7620fa0fe33a'></a>

T2 thus achieves high _system-level flexibility_: new T2 subagents can be trained and attached incrementally (e.g., a better planner, a domain-specific searcher, a new memory module), without retraining the host agent. Compared to T1, T2 trades some agent-agnosticity for tighter compatibility: tools are specialized for a given frozen agent, leading to higher data efficiency and better end-to-end performance under the same backbone.

<a id='f5c5c5d8-0ae6-4e89-8c39-b5c6df65f777'></a>

40

<!-- PAGE BREAK -->

<a id='e58fbbb9-cdd2-41c0-a3ba-bade718290b0'></a>

Adaptation of Agentic AI

<a id='12e0abf5-08c1-48d9-950a-8f8c69b07ee2'></a>

Table 5 Quantitative comparison of flagship adaptation methods.
<table id="40-1">
<tr><td id="40-2">Method</td><td id="40-3">Paradigm</td><td id="40-4">Training Signal</td><td id="40-5">Key Result (Quantitative)</td><td id="40-6">Key Insight (Qualitative)</td></tr>
<tr><td id="40-7">DeepRetrieval [21]</td><td id="40-8">Al</td><td id="40-9">Retrieval Metrics</td><td id="40-a">~3× Recall (65.1% vs. 24.7%)</td><td id="40-b">Causal RL optimizes tool mechanics.</td></tr>
<tr><td id="40-c">ReSearch [108]</td><td id="40-d">A2</td><td id="40-e">Final Answer Correctness</td><td id="40-f">9-22% gains over RAG</td><td id="40-g">Holistic RL optimizes tool strategy.</td></tr>
<tr><td id="40-h">s3 [27]</td><td id="40-i">T2</td><td id="40-j">GBR from Frozen Generator</td><td id="40-k">58.9% Acc. w/ 2.4k samples</td><td id="40-l">Much more data-efficient than A2.</td></tr>
<tr><td id="40-m">AgentFlow [51]</td><td id="40-n">T2</td><td id="40-o">Final Answer Correctness</td><td id="40-p">33.1% on GAIA (beats GPT-4)</td><td id="40-q">Learned orchestration of specialists.</td></tr>
</table>

<a id='b1686f15-b6e5-4208-b48c-6842b860dec9'></a>

## 6.4 Synthesis: The Showdown on Data Efficiency and Modularity (A2 vs. T2)
The most direct comparison arises between A2 and T2. Both aim to produce sophisticated tool-using systems, but they place the learning burden in different places. A2 adapts the _agent_, letting it internalize tool-use strategies; T2 adapts the _tools_, letting them learn to support a fixed agent.

<a id='3b8915c1-ad32-48e3-9584-080149b97785'></a>

Empirically, the contrast is stark. Comparing Search-R1 (A2) and s3 (T2), two state-of-the-art methods for retrieval-augmented generation:

*   **A2 approach** (Search-R1): Trains the _entire_ agent, requiring roughly 170k examples to co-adapt internal knowledge, reasoning, and tool-use policy [108].
*   **T2 approach** (s3): Trains only a lightweight 7B "searcher" subagent using frozen-generator feedback (GBR), achieving comparable performance (58.9% average accuracy) with only 2.4k training samples [27].

<a id='da684d16-1ac0-4d24-8eb3-2ba39f608b37'></a>

This corresponds to about a 70× reduction in data requirements and roughly 33× faster wall-clock training for the T2 variant—a phase change rather than a marginal improvement. Moreover, s3 generalizes better: on specialized medical QA, T2-trained s3 reaches 76.6% accuracy vs. A2-trained Search-R1's 71.8%, suggesting that s3 learned more transferable search skills while Search-R1 overfit to its training distribution [27].

<a id='b3894546-0357-4d08-8285-63866c861333'></a>

The underlying reason is the "symbiotic inversion" discussed above. A2's optimization landscape is high-dimensional and entangled: the agent must simultaneously adjust its knowledge, reasoning style, and tool-use policy. T2 dramatically simplifies the learning problem by assuming the backbone already solves (most of) knowledge and reasoning, and only learning a narrow procedural skill in a small subagent.

<a id='46859281-40ee-4258-849b-f969c4c0f727'></a>

From an engineering perspective, T2 also wins on modularity. To add a new tool or update Search-R1 (A2), one must retrain the monolithic agent, potentially inducing catastrophic forgetting. In a T2 architecture, new tools can be trained and hot-swapped without touching the host agent, enabling continuous evolution of the peripheral ecosystem while the core remains stable.

<a id='fa548935-1bca-46af-911c-2f12c70bbeaa'></a>

## 6.5 Strategic Recommendations
Choosing the appropriate adaptation strategy requires balancing computational cost, data efficiency, and the need for system modularity. We organize these considerations into a strategic framework based on whether the priority lies in *internal parametric adjustment* or *external ecosystem evolution*. We detail the specific applicability, strengths, and limitations of each paradigm below:

<a id='50940d3e-f1f9-47fa-8f37-e33b40f21f28'></a>

A1: Best suited for *local, mechanistic mastery* of verifiable tools in stable domains (e.g., retrieval, code execution, SQL). By optimizing directly on executable outcomes, A1 develops strong low-level competence and causal grounding.

*   **Pros**: precise control over tool behavior; robust alignment to verifiable signals.
*   **Cons**: high computational cost, brittle generalization, and limited transferability across tasks.

<a id='308a6951-92d7-42f2-b340-0d133903804c'></a>

A2: Appropriate for system-level orchestration within a single agent, enabling holistic reasoning and multi-tool coordination. A2 internalizes when, how, and why to invoke tools, yielding deeply integrated reasoning patterns.
*   **Pros**: rich cross-tool strategies; unified end-to-end policies for complex workflows.

<a id='952da338-c982-4089-a283-8fe2fa03905a'></a>

41

<!-- PAGE BREAK -->

<a id='9b632183-54dc-4628-b70e-daacd9fd1e43'></a>

Adaptation of Agentic AI

<a id='a444d907-cc91-4a5f-aa5d-8439489a35f5'></a>

Cons: expensive monolithic retraining and susceptibility to catastrophic forgetting when scaling across domains.

<a id='ae6e9354-fc10-4cb7-9790-5155f612b7a0'></a>

T1: Ideal for *horizontal scalability* and *reusability*. T1 spans both static foundational models (e.g., SAM, AlphaFold2) and "graduated" subagents—A1/A2-trained experts frozen and redeployed as reusable modules (e.g., DeepRetrieval, SWE-Grep [193]). These "subagents-as-tools" encapsulate learned procedural expertise while remaining decoupled from any specific host agent.

*   **Pros**: plug-and-play modularity and broad compositional flexibility across ecosystems.
*   **Cons**: agent-agnostic training may under-optimize tools for any particular host agent's reasoning style.

<a id='45602c15-1692-4f8f-b324-cc1248617b3e'></a>

T2: Represents the *symbiotic inversion*: rather than adapting the agent to use tools better, T2 trains lightweight tools and subagents under *frozen-agent supervision* to better serve a fixed backbone (e.g., s3-style searchers, planners, advisors, and memory builders). The host agent provides high-level reasoning and reward signals, while T2 subagents learn narrow procedural skills that can be added, replaced, or composed without touching the backbone.

*   **Pros**: dramatic data-efficiency for new skills, mitigation of catastrophic forgetting via modular updates, and sustained compatibility with evolving or swapped host agents.
*   **Cons**: subagent capability is bounded by the supervising agent's quality; multi-subagent pipelines introduce added orchestration complexity and potential error compounding.

<a id='f3ab6f7f-d02f-4e25-b213-382ebd7ad490'></a>

Taken together, these four paradigms define a coherent 2 × 2 landscape (Figure 7) along two conceptual axes: (i) the _local-to-systemic_ spectrum (y-axis), from low-level control of specific tools (A1/T1) to holistic orchestration of multi-tool reasoning (A2/T2); and (ii) the _monolithic-to-modular_ spectrum (x-axis), from end-to-end retraining of a single agent (A1/A2) to compositional adaptation via distributed subagents and tools (T1/T2). Viewed through this lens, A1 and A2 occupy the _agent-centric_ half of the landscape: they directly reshape the policy parameters of the core agent, offering rich parametric flexibility but incurring heavy costs in compute, data, and stability. T1 and T2, by contrast, occupy the _tool-centric_ half: they shift learning outward into a modular ecosystem, enabling incremental evolution, specialization, and compositional reuse. The two axes interact nonlinearly: A1 → T1 reflects the “graduation path” (frozen experts becoming reusable subagents), while A2 → T2 embodies the “federation path” (frozen backbones supervising a growing constellation of adaptive specialists). In practice, mature agentic architectures increasingly inhabit the upper-right quadrant (T2): high modularity and high orchestration, where foundation agents serve as stable cognitive centers and peripheral subagents continuously evolve to extend their capabilities.

<a id='dc4bbdb9-43c1-4c88-a96d-6c86f65f754e'></a>

<::A 2x2 matrix chart titled "Agent Adaptation" on the left side (covering the left two quadrants) and "Tool Adaptation" on the right side (covering the right two quadrants). The y-axis is labeled "Local Tool Mechanics" at the bottom and "Global Orchestration" at the top. The x-axis is labeled "Monolithic, Agent-Centric" on the left and "Modular, Tool-Centric" on the right. The chart contains four quadrants, each represented by a rounded rectangle with a distinct color and content. The quadrants are: Quadrant A1 (Bottom-Left): Light green background. Labeled "A1". Text reads "Tool Execution Signaled" and "Mastery of Tool". An icon of a hand holding a hammer is present. Quadrant A2 (Top-Left): Light blue background. Labeled "A2". Text reads "Agent Output Signaled" and "Strategic Coordination of Tool & Reasoning". An icon of a person wearing a hard hat is present. Quadrant T1 (Bottom-Right): Light orange background. Labeled "T1". Text reads "Agent-Agnostic" and "Plug-and-Play Agentic System". An icon of an electrical plug and socket is present. Quadrant T2 (Top-Right): Light purple background. Labeled "T2". Text reads "Agent-Supervised" and "Federated Cognition Agentic System". An icon of a person wearing a hard hat and holding a wrench is present. Dashed arrows indicate relationships between the quadrants: a green dashed arrow goes from A1 to T1, and a blue dashed arrow goes from A1 to A2. Another blue dashed arrow goes from T1 to T2.: chart::>

<a id='98db1acb-138e-429c-aa6f-1b8b2ed86c0a'></a>

Figure 7 The 2 × 2 adaptation landscape. The x-axis captures *monolithic-to-modular* evolution, while the y-axis represents *local-to-systemic* orchestration. A1/A2 inhabit the agent-centric half, whereas T1/T2 embody modular and system-level flexibility. Dotted arrows show that A1/A2-trained agents can graduate as tools for T1.

<a id='951ea656-28de-4252-991d-5b958b9a3e1d'></a>

This synthesis also clarifies the emerging division of labor in agentic AI research. A1/A2 remain indispensable for generating novel reasoning competencies or re-aligning a model's internal cognition - tasks that require touching the agent's core. T1/T2, however, dominate system construction: they enable continual growth, fine-grained specialization, and safe parallel experimentation. The prevailing design trend thus points toward _hybrid systems_: frozen foundation models at the center, surrounded by a living ecology of T1/T2 subagents trained for specific procedural roles, with occasional A1/A2 updates marking evolutionary leaps in the agent's internal reasoning.

<a id='0043442e-a28d-4a26-a801-4ae03927d9ce'></a>

42

<!-- PAGE BREAK -->

<a id='1574f671-4167-40f4-b9ab-bc7717040862'></a>

Adaptation of Agentic AI

<a id='08b1de66-7edd-451d-88b2-a9a4700fbdf2'></a>

7 Applications

The rapid advancement of agentic AI systems has led to their adoption across a growing range of scientific and engineering domains. In this section, we organize representative applications by discipline. Specifically, we categorize these applications into the following major areas: **General Science**, such as Deep Research (§7.1); **Computer Science**, where agents augment or automate processes in Software Development (§7.2) and Computer Use (§7.3); and **Biomedicine**, where agents accelerate research in drug discovery and development (§7.4).

<a id='5f3604f3-931f-4e8f-9f97-e3e2d52daf96'></a>

## 7.1 Deep Research
Deep research systems represent the emerging class of AI-powered applications designed to automate end-to-end scientific investigation by integrating large language models (LLMs), advanced retrieval, and autonomous reasoning [6]. OpenAI's DeepResearch [194] is a prominent example, featuring a multi-step reasoning workflow that conducts iterative search, validation, and synthesis. Similar paradigms have been adopted in recently announced systems such as Claude's deep-search capabilities [195] and Google's Gemini-based research agents [196]. The defining distinction of deep research systems, compared to general-purpose AI agents, is their dual adaptation in both agent reasoning and scientific tool integration.

<a id='70d3f8aa-c90a-441c-a866-69832f5ae9e1'></a>

**Agent adaptation** Deep research systems require sophisticated agentic workflows capable of decomposing complex scientific questions into structured research plans. This includes: (1) adapting underlying LLMs toward long-context reasoning, hypothesis refinement, and multi-step self-critique, (2) orchestrating multiple agents to collaborate hierarchically, e.g., for literature review, data interpretation, and conclusion synthesis, and (3) maintaining persistent memory and knowledge tracking across long investigative trajectories. These adaptations enable agents not only to respond to queries but to behave as autonomous researchers navigating the breadth of scientific knowledge.

<a id='1679c262-fe78-487e-8e61-de69f1e7405c'></a>

Tool adaptation Reliable research requires grounded evidence. To address hallucination and improve informative-ness, deep research agents must incorporate diverse tools that provide direct access to external knowledge, including: (1) structured retrieval interfaces to literature databases (e.g., PubMed, arXiv), (2) web navigation tools for interact-ing with scientific resources, (3) and modular computational utilities for data analysis and visualization. Recent advances further enhance tool adaptation through learning-based retrieval modules, such as DeepRetrieval [21] and s3 [27], which boost accuracy in real-time information gathering, especially when operating atop proprietary models that cannot be fine-tuned.

<a id='65c140f9-8ae7-4dc0-abf5-520b530b31d6'></a>

**Toward domain-specialized deep research** While current systems are primarily built on generic corpora and may struggle with nuanced expert-level inquiries, the paradigm naturally extends to specialized scientific fields. Future development will increasingly involve integrating: domain knowledge bases and ontologies, validated bioinformatics and biomedical computation tools, field-specific safety, reliability, and evaluation protocols.

<a id='2c3a7750-c361-48f3-967e-9db5b492f0e4'></a>

Such advancements are expected to transform deep research systems from broad "knowledge navigators" into expert collaborators for vertical domains like medicine, materials science, and drug development, guiding researchers from problem conception to actionable discoveries.

<a id='d7ee6a37-94d9-4d2d-84db-61d39753e7be'></a>

## 7.2 Software Development

AI-assisted software development represents one of the most technically demanding and economically significant domains for agentic AI systems. Unlike conventional code completion systems, software development agents are designed to autonomously navigate multi-stage engineering workflows, including requirement interpretation, code generation, debugging, testing, and deployment, within real development environments. Modern systems, including Cursor [197], Claude Code [198], and OpenAI's CodeX [199], exemplify this shift from passive code assistants toward interactive, full-cycle programming agents capable of understanding project context and performing tool-mediated reasoning. To evaluate these capabilities, the SWE-Bench benchmark [200] has emerged as a representative testing suite that measures an agent's ability to autonomously fix real-world software bugs in open-source repositories by reading, editing, and validating code through continuous integration workflows.

<a id='e05934ea-6e98-46a6-9f4d-db85c2279828'></a>

43

<!-- PAGE BREAK -->

<a id='f844c395-700b-4072-921f-685ba1d7e0be'></a>

Adaptation of Agentic AI

<a id='7d5995cd-c3a6-4e22-b042-26bb0451bef7'></a>

<::figure: A diagram illustrating applications of adaptation in Agentic AI, divided into four main categories, each with examples of Agent Adaptation and Tool Adaptation.::>

**Deep Research**
- Agent Adaptation:
  - Long-context Reasoning
  - Hypothesis Refinement
  - (e.g., DeepResearcher)
- Tool Adaptation:
  - Learned Retrieval
  - Evidence & Synthesis
  - (e.g., DeepRetrieval as T1)

**Software Dev.**
- Agent Adaptation:
  - Workflow Autonomy
  - Debugging & Testing
  - (e.g., SWE-Agent)
- Tool Adaptation:
  - Accurate+Fast Code Search
  - Compiler Feedback
  - (e.g., SWE-Grep)

**Computer Use**
- Agent Adaptation:
  - Visual Grounding
  - GUI Perception
  - (e.g., OpenCUA)
- Tool Adaptation:
  - Persistent Memory
  - Context Playbacks
  - (e.g., ACE)

**Drug Discovery**
- Agent Adaptation:
  - Scientific Verification
  - Trial Design
  - (e.g., TrialMind)
- Tool Adaptation:
  - Domain Utilities
  - Mol. Property Predictors
  - (e.g., ToolUniverse)

...

Figure 8 Applications of Adaptation in Agentic AI.

<a id='a8d4213b-4441-4e42-a127-c5f604a39105'></a>

Other notable research efforts have also explored the development of autonomous software engineering agents. **SWE-Agent** [201] introduces an agent-computer interface (ACI) that enables language model agents to autonomously perform end-to-end software engineering tasks, including repository navigation, code modification, and test execution. **OpenHands** [202], an open-source platform for AI software developers, extends this paradigm by providing a sandboxed execution environment and modular evaluation framework for developing and benchmarking general-purpose coding agents.

<a id='8308c863-e783-45ca-a78d-6fefff293621'></a>

Building effective software agents in this setting requires both **agent adaptation**, which strengthens reasoning, planning, and self-verification across complex development pipelines, and **tool adaptation**, which integrates and evolves the surrounding development ecosystem such as compilers, debuggers, and test frameworks.

<a id='1722e31a-d6ee-4719-a719-0656bb77ce46'></a>

**Agent adaptation** Agent adaptation in software development focuses on enhancing model reasoning and autonomy across complex multi-stage engineering workflows. Recent systems train agents directly from interaction trajectories within real or simulated development environments.

<a id='2fbd6681-24e4-49a7-b983-49712923707a'></a>

**Tool adaptation** Tool adaptation in this domain involves evolving the software ecosystem itself to improve the reliability, responsiveness, and contextual integration of tools that agents depend on for code execution, testing, and evaluation. Instead of merely wrapping existing IDE functionalities, modern systems are increasingly training tools for agents to optimize for usability and feedback efficiency. A representative example is *Cursor's Tab-RL* framework [203], which applies reinforcement learning to refine the editor's tab completion behavior based on real-world user interactions, aligning the tool's interface dynamics with agent and developer preferences. A more advanced example is *SWE-Grep* [193], a specialized sub-agent trained using reinforcement learning for fast, multi-turn, and highly parallel context retrieval. By delegating code search to this T2-style tool, the main agent's context window is conserved and protected from irrelevant "context pollution", allowing it to focus on higher-level reasoning. More broadly, this category of tool adaptation includes the automated creation or refinement of compilers, debuggers, and linters that provide structured feedback loops for agents.

<a id='94335f1c-8d32-4c4b-b8bc-d891472b4ada'></a>

## 7.3 Computer Use
Computer-use agents represent an emerging class of multimodal AI systems capable of autonomously operating computers and software environments through direct interaction with graphical user interfaces (GUIs). Rather than relying on predefined APIs or task-specific integrations, these agents perceive screens as visual input, reason about interface elements such as buttons, menus, and text fields, and execute actions using a virtual keyboard and mouse—closely mirroring human computer operation. A recent example is OpenAI's **Computer-Using Agent** (CUA) [204], which combines vision-based perception with reinforcement learning to navigate complex digital environments. This paradigm signifies a step toward generalized digital intelligence, where agents can perform diverse tasks, such as information retrieval, document editing, and software automation, directly within existing human-designed computing ecosystems.

<a id='9388469d-de04-41e5-902a-786e70d37019'></a>

Representative benchmarks for this paradigm include **OSWorld** [205], **WebArena** [206], **VisualWebArena** [207],

<a id='c835c3de-05e0-442f-b444-04b798bb5b63'></a>

44

<!-- PAGE BREAK -->

<a id='a4f443c0-31cb-4a3c-a2aa-8908a923e02f'></a>

Adaptation of Agentic AI

<a id='33e246ed-4e44-41a4-98dd-bdcc3cd26333'></a>

AppWorld [208], WebVoyager [209], and τ-bench [210] which evaluate an agent's ability to perceive, reason, and act across diverse digital environments ranging from full operating systems to real-world web interfaces. Achieving reliable and efficient performance in computer-use scenarios requires **adaptation** from the agent and tool levels.

<a id='18418f24-23fa-4037-a863-0a5e2c330c21'></a>

**Agent adaptation** To handle complex computer-use tasks, agent adaptation plays a crucial role in equipping models with new knowledge and operational skills beyond those learned from general-purpose pretraining. Such adaptation often involves exposing the agent to realistic or synthesized trajectories of GUI-based interactions, enabling it to acquire procedural competence in perceiving interface states, reasoning over visual elements, and executing multi-step actions. A representative example is OpenCUA [211], which shows how large-scale, GUI-centric data can significantly improve an agent's computer-use abilities. By collecting human demonstrations across diverse operating systems and applications, and converting them into state-action trajectories with reflective reasoning, OpenCUA provides agents with realistic exposure to interface dynamics. Another approach to agent adaptation is explored in AgentTrek [212], which takes a different approach to agent adaptation by synthesizing training trajectories from web tutorials instead of relying on human demonstrations. It converts tutorial text into step-by-step goals and has a VLM agent execute them in real environments, keeping only correct trajectories through automatic evaluation. This scalable data generation helps agents acquire new skills and interface patterns at low cost, showing that synthesized trajectories can effectively support GUI-agent adaptation.

<a id='4318a6e2-2c3b-4e7a-94d5-60ad70f8cbc7'></a>

**Tool adaptation** Complementary to agent-level adaptation, tool adaptation aims to enhance the tools and interfaces that agents rely on, enabling them to become more adaptive, context-aware, and synergistic with the agent's reasoning process. Instead of modifying model parameters, these approaches update or expand the tool's experience pool, memory, or contextual representations to better support long-horizon interaction and dynamic task requirements. A representative example is **Agentic Context Engineering (ACE)** [213], which treats evolving contexts as structured playbooks that accumulate, refine, and organize strategies for tool use. By continuously curating and updating contextual knowledge through execution feedback, ACE effectively adapts the operational layer of tools—reducing rollout latency and improving alignment with the agent's decision-making. Such approaches highlight a broader trend: as agents become more capable, the tools they employ must likewise evolve, incorporating persistent memory and adaptive control mechanisms to ensure seamless collaboration in open computer-use environments.

<a id='7024fe3d-20e6-4886-93c4-321f0c69a874'></a>

## 7.4 Drug Discovery and Development

LLM-empowered AI agents are rapidly transforming biomedical research and, by extension, the entire drug discovery and development pipeline [5]. Modern systems increasingly integrate both agent adaptation (e.g., fine-tuning LLMs and designing agentic workflows) and tool adaptation (e.g., incorporating domain-specific databases, scientific software, and retrieval components) [8]. These two forms of adaptation are fundamentally complementary: agent adaptation improves reasoning and procedural reliability, whereas tool adaptation equips agents with practical scientific capabilities. Below, we describe representative advances that emphasize one aspect more than the other, while acknowledging that most systems combine both.

<a id='112bf4f1-c41c-42de-a9c8-7ff75fe9bc23'></a>

Agent adaptation for drug discovery GeneAgent adapts LLM agents to gene analysis tasks (e.g., gene set enrichment analysis), integrating structured workflows such as generation, self-verification, and iterative refinement to reduce hallucinations [214]. DSWizard focuses on transparent and reproducible biomedical data science, guiding the agent to construct analysis plans before execution and enabling human oversight and modification [215]. Further, multi-agent systems have emerged where heterogeneous agents collaborate in drug discovery workflows. For instance, virtual teams can simulate interdisciplinary research meetings to design novel therapeutic molecules such as nanobodies [216].

<a id='85191b4b-0fd4-42d0-94a7-8b3a4b4b9ee3'></a>

**Agent adaptation for drug development** Clinical research is a central component of drug development, and agents are increasingly tailored to literature analysis, patient recruitment, and trial design. TrialMind adapts LLM agents for biomedical evidence retrieval by integrating medical guidelines and structured access to clinical-trial and publication databases to support search, screening, and data extraction [7]. LEADS builds on this by further training the model with curated literature corpora to improve agent-driven evidence discovery [217]. Similarly,

<a id='9e432d82-2d4e-4238-8889-fa1f73555880'></a>

45

<!-- PAGE BREAK -->

<a id='01456857-1e1b-4b79-86b2-31e2db8a371c'></a>

Adaptation of Agentic AI

<a id='9c1613aa-1ff6-4b64-91f2-763005c5f08a'></a>

TrialGPT operationalizes guideline-based patient-to-trial matching through a reason-and-retrieve workflow [12]. For upstream design tasks, TrialGenie leverages multi-agent collaboration to parse historical trial documents and generate analytical code for real-world datasets [218].

<a id='1551b00a-a581-4575-8bd6-865e3f3a30ff'></a>

**Tool adaptation** Tool adaptation has progressed equally rapidly, driven by the need to support diverse scientific tasks. Tools such as SyntheMol and related frameworks integrate ML-based molecular property predictors as reward functions to steer generative models toward biologically desirable compounds [219, 220]. ToolUniverse focuses on scalable scientific tool creation: a discovery module constructs tools from natural language specifications, and an optimizer iteratively refines them before incorporation into a shared library for custom agent assembly [221]. Biomni takes a complementary approach by manually mining biomedical literature to curate a high-quality tool repository, which can be dynamically injected into agent workflows [222]. Meanwhile, STELLA proposes a self-evolving paradigm where an expanding Template Library captures reasoning strategies and a Tool Ocean continuously grows as a tool-creation agent autonomously discovers and integrates new bioinformatics utilities [223].

<a id='f59831a1-4248-4c32-b4cc-e1f8ecfde546'></a>

## 8 Opportunities

This paper has systematically categorized the landscape of agentic AI adaptation into four key paradigms: (A1) Agent Adaptation with Tool Execution Signal, (A2) Agent Adaptation with Agent Output Signal, (T1) Agent-Agnostic Tool Adaptation, and (T2) Agent-Supervised Tool Adaptation. These paradigms provide a crucial framework for organizing and understanding current methods. However, their true value lies in illuminating the path forward. The separation of agent and tool adaptation, while analytically useful, is largely a construct of the field's nascent stage; the future of capable, robust, and efficient agentic AI will almost certainly be defined by their synthesis.

<a id='8b4d9423-991c-48c9-af5d-f1af81c240f7'></a>

This section functions as a forward-looking roadmap, identifying some critical and interdependent opportunities for future research that emerge directly from our taxonomy. These opportunities represent the next frontier of agentic AI, moving from static, monolithic adaptation toward dynamic, co-adaptive, and federated systems.

<a id='abb3a882-e054-454d-8694-fd735c876b19'></a>

## 8.1 Co-Adaptation
The taxonomy presented in this paper (A1/A2 vs. T1/T2) is a necessary simplification, organizing the field by its dominant locus of optimization: either the agent or its tools. The most significant and challenging opportunity for the next decade of research is to dissolve this boundary and develop unified agent-tool co-adaptation frameworks.

<a id='df004cde-7787-4b3a-b777-5edb6a32e8fa'></a>

Such a framework implies a complex, bi-level optimization problem, formally $max_{A,T}O(A,T)$, where the agent's policy $(A)$ and the tool's internal parameters $(T)$ are adapted simultaneously within the same learning loop. This represents a fundamental departure from current paradigms, which almost universally rely on freezing one component to provide a stable learning target for the other (e.g., $A_{frozen}$ in T2, or $T_{frozen}$ in A1/A2).

<a id='7d44de61-1523-41b7-8f59-f653909b95ec'></a>

This is not an entirely new problem, and researchers can draw deep conceptual inspiration from several established fields:

*   **Co-evolutionary Algorithms.** Classic work in evolutionary computation has long studied how two or more interacting populations—such as *hosts vs. parasites* or *predators vs. prey*—apply reciprocal selection pressures that drive arms races, emergent structure, and increasingly sophisticated strategies. Hillis [224] introduced the seminal host-parasite model, showing that co-evolving adversarial test cases can significantly improve solution robustness. Subsequent work on competitive co-evolution explored dynamics such as disengagement, cycling, and evolutionary complexification [225]. Other lines of research developed cooperative multi-population architectures in which sub-components co-adapt to form joint solutions [226]. Comprehensive surveys [227] situate these approaches within a broader taxonomy of competitive and cooperative CEAs. In our setting, we can view the agent $\mathcal{A}$ and its tool $\mathcal{T}$ as two interdependent populations evolving on a shared fitness landscape, allowing reciprocal adaptation, arms-race dynamics, and emergent specialization to arise naturally from their interaction.

<a id='40d39960-e72d-47a8-9e96-d1364e761a37'></a>

• **Multi-Agent Systems.** A complementary line of work emerges from multi-agent reinforcement learning, where each agent learns in a non-stationary environment induced by other concurrently learning agents. Foundational surveys [228–230] describe how decentralized learners must cope with shifting policies, partial observability,

<a id='7692ac9c-e341-4c1a-9b32-900b37796427'></a>

46

<!-- PAGE BREAK -->

<a id='526b76bd-6913-4254-b03f-6d36736716f3'></a>

Adaptation of Agentic AI

<a id='50591e7d-1188-4d0c-98b7-4a394a78c9b3'></a>

and strategic coupling, challenges that closely mirror those of agent-tool co-adaptation. Classic problems such as equilibrium selection, credit assignment, and coordination under changing partner behaviors have led to techniques including opponent modeling, joint-policy search, centralized training with decentralized execution (CTDE), and communication-based coordination. In our context, viewing _A_ and _T_ as a two-agent partially cooperative system highlights the need for algorithms that stabilize learning under mutual adaptation, prevent non-stationarity-induced divergence, and support the emergence of complementary capabilities rather than competitive oscillations.

<a id='4cd4c6f2-1f33-4001-be68-072015afa776'></a>

A primary technical barrier to effective co-adaptation is the *intractable credit assignment problem*. When an agentic system fails at a complex task, the source of the failure is inherently ambiguous. Consider a system in which an A2-style planner invokes a T2-style search subagent (e.g., an s3-like searcher). If the final answer is incorrect, which component is responsible?

<a id='66b76cd9-c29e-495c-8e50-820c35ef0396'></a>

Nascent research is beginning to address fragments of this joint-optimization challenge. MATPO (Multi-Agent Tool-Integrated Policy Optimization) [231] proposes a "principled credit assignment mechanism" for jointly training planner and worker agents. However, in its current form, these "agents" correspond to distinct prompt roles instantiated within a single LLM, rather than heterogeneous models. Other work studies joint refinement of agent prompts and tool specifications [232]. The true opportunity lies in extending these ideas to _distributed, heterogeneous_ systems in which the agent _A_ and tools _T_ are distinct learning entities. This may require importing architectures from multi-agent RL (such as centralized-critic—decentralized-actor methods [233]) to enable principled credit allocation over an an interconnected agent–tool graph.

<a id='905e5d79-1fb6-4579-8e4c-bf8af26db48b'></a>

A deeper difficulty involves the _Stability–Plasticity Dilemma_. Co-adaptation aims for A and T to become mutually optimized, yet in a joint-learning framework, A is adapting to a T that is itself changing. As established in the study of complex adaptive systems, such non-stationarity can induce chaotic or unstable dynamics [234]. The system may enter a “Red Queen” regime in which A and T continually adjust to each other’s most recent changes without increasing overall performance, or may even collapse into degenerate policies. Conversely, premature convergence may cause the system to “lock in” a brittle, suboptimal agent–tool interface, losing the plasticity required for generalization.

<a id='a39ef1bc-7a9e-42f7-b0a6-4657f1b61034'></a>

A key research direction is the development of *pacemaker mechanisms*
that regulate the relative learning rates of agents and tools, or the use of
evolutionary game-theoretic analyses to guarantee convergence toward
stable symbiotic equilibria [234]. These mechanisms will be essential
for enabling reliable, scalable co-adaptation in next-generation agentic
AI systems.

<a id='d01d9e9d-0db7-4f38-8898-92e0a7ddcd08'></a>

<::Co-Adaptation Diagram: flowchart::>Co-AdaptationA2 + T2 as an exampleThe diagram illustrates a co-adaptation process involving two main components, T and A, and a Reinforcement Learning (RL) agent.Component T (top, green rounded rectangle) and Component A (bottom, light red rounded rectangle) are interconnected. A double-headed vertical arrow between T and A shows interaction: 'a' points from T to A, and 'y' points from A to T.Component A receives an input 'x' and produces an output 'o'. The output 'o' feeds into a magnifying glass icon labeled 'RL' (Reinforcement Learning).From the RL agent, a curved blue arrow labeled 'step i' points back to component T. A curved red arrow labeled 'step i + 1' points from the RL agent back to component A. The entire diagram is enclosed within a dashed border.Figure 9 An illustrative example of co-adaptation.

<a id='18a4078d-b78a-4d56-a053-a34ddd68dd3a'></a>

## 8.2 Continual Adaptation

While our discussion has so far centered on agent adaptation mechanisms such as A1 and A2, these methods still assume a fixed task distribution and are typically instantiated on a single downstream task at a time. In contrast, real-world deployments involve non-stationary task distributions, where tasks, tools, and user needs evolve over time, making isolated, one-off adaptations prone to Catastrophic Forgetting (CF). This calls for Self-Evolving Agents that continuously update their behaviors, tools, and memories in open and dynamic environments. Continual Learning (CL) [235-238] provides a natural foundation for this goal, as it studies how models learn from non-stationary task streams while retaining prior knowledge. We therefore revisit CL techniques that can serve as concrete mechanisms for Self-Evolving Agents and organize them into the following two categories.

<a id='2de32773-7804-4f97-9b3e-6ab8dc206dea'></a>

**Parameter-update Mechanisms.** (Dynamic A1/A2 Paradigm). To align with the A1/A2 paradigm, we group continual learning methods that adapt models through explicit parameter updates. Regularization-based CL approaches such as EWC [239], LwF [240], and VR-MCL [241] estimate which parameters are important for previous tasks

<a id='2c86c1ec-61c8-4a0e-9a16-9e9a074368e5'></a>

47

<!-- PAGE BREAK -->

<a id='d1a86138-0fc3-446c-962b-94353e0661d0'></a>

Adaptation of Agentic AI

<a id='51bb291d-a891-42af-ba6d-eddb8c1c5689'></a>

and selectively protect them, so that adaptation to new tasks is primarily absorbed by parameters deemed less critical for past performance. Orthogonal-update methods [242, 243] instead modify gradients so that updates lie in directions that are intended to interfere less with previously learned solutions. A complementary line of work introduces parameter-efficient update mechanisms, such as low-rank adapters [244, 245], Mixture-of-Experts routing [246, 247], and model-merging schemes [248]. These methods offer concrete inspirations for dynamic A1/A2-style adaptation.

<a id='52353698-44b7-42c2-bc2c-3dd0ce297d63'></a>

External-memory Mechanisms (Evolving T2 Adaptation). Classic replay-style approaches maintain a memory buffer of past examples and study how to select [249, 250], utilize [251, 252], and compress them [253, 254] so that a small set of stored items can approximate the full training history. Dual-memory systems [255] further separate fast, high-capacity but unstable episodic buffers from slower, more compact long-term memories. For Self-Evolving Agents, these ideas directly inspire how to curate, compress, and stage interaction logs, tool traces, and user feedback into different memory tiers. In foundation model settings, prompts often act as a lightweight external memory, because the backbone is typically kept fixed and adaptation occurs primarily through prompt changes [256-258]. As a result, the overall paradigm naturally aligns with our notion of T2 adaptation.

<a id='40d5bc3e-cb4b-488a-a829-24ccb14aefd9'></a>

Notably, this challenge of continual adaptation becomes especially pronounced in domains with strong execution- based supervision signals, such as those enabled by re- inforcement learning with verifiable rewards (RLVR).
As discussed in &sect;4.1.2, environments like formal theo- rem proving provide reliable, tool-execution-signaled feedback for learning multi-step behaviors. At the same time, these domains often evolve structurally over time— for example, through expanding formal math libraries and large, actively maintained formalization projects— making them representative testbeds for continual agent adaptation. Instead of repeatedly retraining the core agent, many prover agent systems adapt to expanding libraries by updating premise retrieval indices, tactic databases, or proof-state memories, allowing agents to exploit newly introduced lemmas without rewriting the entire policy [73, 259]. Such low-resource adaptation complements RLVR-style training by isolating long-term know and again aligns with our notion of T2 adaptation.
Figu

<a id='69a4c720-b4d2-477d-9527-e667171e82b5'></a>

Continual Adaptation
A1 + new/updated tool as an example
<:: A diagram illustrating continual adaptation. The diagram consists of two main process flows, one above the other, separated by a large downward blue arrow.

**Top Flow (Initial State):**
- Input `x` feeds into a light red rectangular block labeled `A`.
- An arrow points from `A` to `a`.
- `a` feeds into a light green rectangular block labeled `T1`.
- An arrow points from `T1` to `y`.
- `y` feeds into a magnifying glass icon, representing observation or output.
- A red curved arrow labeled `RL` (Reinforcement Learning) points from the magnifying glass icon back to block `A`, indicating a feedback loop.

**Transition:**
- A large downward blue arrow points from the top flow to the bottom flow, indicating adaptation or evolution.

**Bottom Flow (Adapted State):**
- Input `x` feeds into a light red rectangular block labeled `A'`.
- An arrow points from `A'` to `a`.
- `a` feeds into a light green rectangular block labeled `T2`.
- An arrow points from `T2` to `y`.
- `y` feeds into a magnifying glass icon, representing observation or output.
- A red curved arrow labeled `RL` points from the magnifying glass icon back to block `A'`, indicating a feedback loop.

Figure 10 An illustrative example of continual adaptation.
: diagram::>

<a id='bccb4d6a-1649-4205-a17f-81762bc0fbf0'></a>

complements RLVR-style training by isolating long-term knowledge growth from short-term policy optimization, and again aligns with our notion of T2 adaptation.

<a id='2e3d1d4a-f882-46cf-b5a4-75223c5c11fd'></a>

Taken together, these two lines of work highlight complementary trade-offs for building Self-Evolving Agents. Within the dynamic A1/A2 paradigm, recent results [260] show that not all parameter-update schemes forget equally: RL with a reverse-KL objective and on-policy data can achieve comparable or better performance than SFT while exhibiting substantially less forgetting, suggesting that on-policy data streams can act as an intrinsic CL mechanism for continual agent adaptation. Yet such methods still rewrite a shared set of parameters, so forgetting and interference are mitigated rather than structurally removed. The evolving T2 paradigm tackles CF at the architectural level by freezing the core agent and encapsulating new capabilities in external, independently trained tools or subagents, which avoids the clobbering of a monolithic parameter space. Looking ahead, a promising direction is to systematically integrate these two perspectives, using CL-aware parameter updates where they are most effective while shifting as much long-term adaptation as possible into T2-style modular tools and external memories.

<a id='6b314615-a3ee-41f7-a4c3-f78cc9bf5e0d'></a>

8.3 Safe Adaptation

The transition from static foundation models to adaptive agentic systems marks a fundamental inflection point in AI safety. While traditional safety paradigms focus on the alignment of frozen weights, adaptation mechanisms, specifically on-policy optimization (A1) and outcome-driven tool tuning (T2), introduce dynamic threat vectors characterized by autonomous risk-taking and adversarial co-evolution [261]. We categorize these emerging risks into

<a id='45bbedc6-8cfc-4fd3-b5f8-2ba4551ed209'></a>

48

<!-- PAGE BREAK -->

<a id='fe4ca591-6b3b-424e-a42e-b21bff3b67e1'></a>

Adaptation of Agentic AI

<a id='e24dcb20-f92d-488c-b24f-7cd6b0daef43'></a>

two primary failure modes: *Unsafe Exploration*, arising from stochastic trial-and-error, and *Parasitic Adaptation*, arising from exploitative optimization loops.

<a id='34a6093a-fc70-4617-b25c-27f218e2584a'></a>

**Security Risk I: Unsafe Exploration.** Unsafe exploration represents the primary bottleneck for the AI paradigm. When agents employ on-policy RL to master tools [262, 21], they must deviate from known safe trajectories to probe the state-action space. In high-stakes or partially observable environments, this decoupling of competence from safety leads to catastrophic, often irreversible outcomes [263, 264].

*   **The Reward-Safety Gap:** In frameworks like RLEF [262] or DeepRetrieval [21], rewards are typically sparse and binary (e.g., task completion). This creates a feedback vacuum for intermediate actions, encouraging agents to maximize efficacy regardless of collateral damage (e.g., deleting system files to free space) [265].
*   **Irreversibility in Tool Use:** Unlike simulated games, agentic environments such as Bash terminals or cloud infrastructure possess irreversible state transitions. An agent learning via trial-and-error may trigger API calls or data deletions that cannot be undone by resetting the episode [266, 267].
*   **Erosion of Guardrails (Case Study: DeepSeek-R1):** Empirical analysis of DeepSeek-R1 [24] reveals that aggressive RL optimization for reasoning can erode safety guardrails established during SFT. The model's ability to construct complex "Chain-of-Thought" justifications allows it to reason its way around refusal mechanisms, increasing susceptibility to jailbreaks and malicious compliance compared to non-adapted baselines [268, 269].

<a id='7a26e6c3-69e1-45dc-8382-b472536b5657'></a>

**Security Risk II: Parasitic Adaptation.** Parasitic adaptation refers to the emergence of exploitative relationships where the agent or tool maximizes its reward function at the expense of the system's intent, mirroring biological host-parasite co-evolution [270].

*   **Type A: Specification Gaming (The Agent as Parasite):** In A2 paradigms, agents exploit imperfect proxy rewards (Goodhart's Law) [271]. As reasoning capabilities scale, agents become adept at "hacking" the evaluation process. For example, modifying game logs to falsify wins or overwriting reward functions in the file system rather than solving the task [265, 272].
*   **Type B: Adversarial Tooling (The Tool as Parasite):** In T2 ecosystems utilizing protocols like MCP [129], tools can evolve to exploit the agent. A compromised or parasitic tool may return prompt-injected data that hijacks the agent's reasoning (the "Confused Deputy" problem), forcing the agent to exfiltrate sensitive data under the guise of standard tool use [273, 274].
*   **Type C: Sycophancy Loops:** Co-adaptation can lead to degenerate equilibria where tools learn to confirm an agent's hallucinations to maximize acceptance scores, or where agents and red-teaming tools engage in "Red Queen" dynamics, overfitting to each other's artifacts without achieving general robustness [275].

<a id='5d0cb778-a221-4cf9-8253-84cc634e9262'></a>

## Mitigation Strategies. Addressing these risks requires moving beyond scalar rewards toward robust specifi- cation. A straightforward yet effective mitigation for Security Risk 1 is to introduce a safety-check layer be- fore the agent's input reaches the tool (Figure 11). This gate ensures that any anomalous or unsafe behaviors are intercepted and filtered out prior to execution. More sophisticated solutions include: _Constrained Policy Op- timization_ [276–278] and safety shields project agent actions onto verified safe sets to prevent catastrophic ex- ploration. _Verifiable Rewards_ [279, 58] replace opaque preference models with programmatic outcome verifi- cation (e.g., unit tests, proofs) to reduce sycophancy. _Specification Self-Correction_ [280] allows agents to dy- namically critique and refine reward functions at infer- ence time to detect gaming. Finally, _Proof-of-Use_ [281]

<a id='3b7edbd7-e8e0-4e8e-8b53-3a72a70b5b5a'></a>

<::flowchart: The diagram illustrates "Safe Adaptation" with "Safeguarded A1 as an example." An input 'x' is fed into a component 'A'. 'A' produces an action 'a'. This action 'a' is then subjected to a "safety check" (represented by an eye icon). If the safety check fails (indicated by a red 'X' and a curved red arrow), negative feedback labeled 'RL' is sent back to 'A'. If the safety check passes (indicated by a green checkmark), the action becomes 'a'' and proceeds to a component 'T'. 'T' produces an output 'y', which is observed (represented by a magnifying glass icon). The observation of 'y' provides feedback (curved red arrow) back to 'A'.::>

Figure 11 An illustrative example of safe adaptation.

<a id='05d9b354-11e0-4d9f-ae2f-d450af543ea3'></a>

49

<!-- PAGE BREAK -->

<a id='b163ebe5-c6dd-41b0-863a-c43ad43269ff'></a>

Adaptation of Agentic AI

<a id='fe53603e-5794-4eba-94ac-3654d855c0ab'></a>

frameworks enforce causal links between retrieved evi-
dence and generated answers, preventing tool-use hallucination.

<a id='f6f41444-4791-4abe-82e2-7d425b8beb1c'></a>

## 8.4 Efficient Adaptation

Efficient adaptation in agentic AI presents several important opportunities. Current systems typically rely on large-scale GPU clusters for fine-tuning or policy refinement, which limits accessibility and personalization. Shifting adaptation toward resource-constrained settings could enable agents to learn efficiently on mobile devices, edge hardware, or other low-power environments. This would not only broaden the applicability of agentic AI but also allow continual adaptation directly on user devices while preserving privacy. Moreover, learning close to the interactions that generate the training signal could reduce latency between experience and model update, effectively blurring the distinction between inference and training. Within this context, we identify several concrete opportunities:

<a id='d3b8ebac-0d39-49e6-8eb6-9eda0d3af8be'></a>

**Parameter-Efficient Adaptation:** Techniques such as Low-Rank Adaptation (LoRA) [19] and its extensions [282-288] allow large models to adapt to new tasks by updating only a small subset of weights, significantly reducing memory and computational requirements. Recent work *LoRA Without Regrets* [289] empirically demonstrates that LoRA can be effectively applied in reinforcement learning settings. They provide evidence that LoRA performs equivalently to full fine-tuning even at small ranks in RL tasks, indicating that RL often requires very low parameter capacity. This observation suggests that models can be fine-tuned on resource-constrained devices while maintaining strong RL performance. Figure 12 shows an illustrative example of this.

<a id='0b469a47-1a77-47b9-b5cc-6e40c3206d67'></a>

**Quantized Adaptation:** FlashRL [290] introduces a framework for accelerating reinforcement learning by performing rollout generation in lower numerical precision, such as INT8 or FP8, while preserving downstream performance. The key innovation lies in mitigating the rollout-training mismatch caused by quantization through *truncated importance sampling* (TIS), which stabilizes gradient estimation when the agent's policy generating rollouts is quantized, but the training engine remains in higher precision. Empirical results demonstrate that FlashRL can achieve significant speedups in RL training without sacrificing final task performance. This work provides compelling empirical evidence that quantization, a technique widely employed in supervised model inference, can be effectively extended to agentic reinforcement learning. By enabling rollout generation at reduced numerical precision without degrading downstream performance, FlashRL demonstrates the potential for RL training across large-scale and resource-constrained environments.

<a id='0ba02667-5c0e-4dfe-81da-0dfc4fbcf3e2'></a>

**On-Device & Personalized Adaptation:** On-device adaptation [291-297, 118] focuses on enabling agents to learn and update directly on user devices under tight computational and memory constraints. This capability has become increasingly important as modern agents operate across heterogeneous hardware, operating systems, interface designs, and interaction contexts, all of which introduce substantial variation in user behavior, application semantics, and device-specific execution patterns. A key component of on-device adaptation is personalization, which allows agents to reflect individual preferences [298, 299], maintain persistent memory [300], and adjust behavior over time [301, 302]. Recent progress in GUI agents further stresses the importance of this direction: modern GUI agents [303-307, 305, 308, 309] rely on strong user-specific multimodal reasoning. A promising strategy for agentic personalization is tool adaptation, where each device maintains a lightweight tool module aligned with user-specific habits and interaction patterns. Instead of modifying the full model or even parameter-efficient adapters, adapting a small tool module focuses directly on modeling personal preferences, recording relevant user history, and shaping dynamic behavioral adjustments. Since the tool module is fully decoupled from the base model, it can update locally without compromising glol updates of tool modules strengthen preference alignment, reinforce l

<a id='89e30e55-8891-4a99-9a27-34797573a935'></a>

Efficient Adaptation
T2 + LoRA as an example
<::diagram: The diagram is titled "Efficient Adaptation T2 + LoRA as an example" and shows a process enclosed in a dashed box. At the top is a rounded rectangle containing a script 'T'. Below it is a light green rectangle. A central rounded rectangle contains a script 'A'. An input 'x' leads to 'A', and an output 'o' emerges from 'A'. There are bidirectional arrows between the light green rectangle and 'A', labeled 'a' (upwards) and 'y' (downwards, in red). A curved red arrow, labeled 'RL', originates from the output 'o', passes through a magnifying glass icon, and points back to the light green rectangle. A legend indicates that the light green rectangle represents a "Low-rank matrix".::>
Figure 12 An illustrative example of efficient adaptation.

<a id='8d47037b-c9f5-4d8a-885e-fd900563e628'></a>

from the base model, it can update locally without compromising global capabilities. Frequent and incremental
updates of tool modules strengthen preference alignment, reinforce long-term memory, and support continual

<a id='168c13f6-1b65-4297-ab67-ec9828dd20b5'></a>

50

<!-- PAGE BREAK -->

<a id='7db488d4-ff47-4dc4-b9b0-4e07b37d702f'></a>

Adaptation of Agentic AI

<a id='a9717e49-77de-42e6-b42e-c966c52a42ac'></a>

behavioral adaptation.

<a id='08d9db1a-fb72-4071-b8e2-d2aa76dbc6f3'></a>

Overall, these opportunities suggest a pathway toward agentic AI that is more efficient and scalable. By combining parameter-efficient fine-tuning, quantization, and on-device adaptation, future agents can evolve continuously in close alignment with user needs and environmental constraints.

<a id='807e066b-cb2c-4d82-a22f-beae33cca2c5'></a>

## 9 Conclusion

The transition from static foundation models to autonomous agentic systems marks a fundamental shift in artificial intelligence, moving from passive response generation to active and multi-step problem solving. As these systems are deployed in increasingly complex and open-ended environments, the ability to adapt to refine behavior, master new tools, and align with specific tasks has become the primary driver of reliability and performance. In this paper, we have provided a comprehensive roadmap of this landscape, introducing a unified taxonomy that organizes adaptation strategies into four distinct paradigms based on the locus of optimization and the source of the supervision signal.

<a id='f2fcac40-067e-4bd3-a30f-718d1ab76718'></a>

Our framework reveals that the design space of agentic adaptation is defined by the tension between monolithic and modular evolution. The agent centric paradigms, A1 (Tool Execution Signaled) and A2 (Agent Output Signaled), offer high parametric flexibility, allowing models to internalize tool mechanics and complex reasoning strategies through direct environmental feedback or holistic outcome evaluation. However, these approaches often incur high computational costs and risk catastrophic forgetting. Conversely, the tool centric paradigms, T1 (Agent Agnostic) and T2 (Agent Supervised), shift the burden of adaptation to the peripheral ecosystem. By treating tools and even other graduated agents as modular and optimizing components, these paradigms enable system level flexibility and significant data efficiency.

<a id='a3f54cac-ec3c-49f7-a9a7-518341955a09'></a>

A critical insight emerging from our analysis is the symbiotic inversion represented by the T2 paradigm. Rather than treating the foundation model as the object of optimization, T2 reframes it as a stable source of supervision, training lightweight subagents such as searchers, planners, and memory curators to serve the frozen core. This architectural shift not only decouples skill acquisition from general reasoning but also paves the way for federated agentic systems that can evolve continuously without destabilizing the backbone model.

<a id='6e412c4d-94b6-462a-8940-bd11746af7e7'></a>

Looking forward, the advancement of agentic AI depends on the strategic integration of these paradigms rather than their isolation. Future systems will likely leverage a hybrid architecture, combining the reasoning depth of agent centric adaptation with the modular efficiency of tool centric adaptation to achieve robustness and scalability. Realizing this potential requires addressing the fundamental challenges of continual adaptation to maintain performance in dynamic streams, safe adaptation to mitigate risks such as reward hacking, and efficient adaptation to enable deployment in resource constrained environments. Ultimately, the next generation of intelligent systems will be defined not by a single monolithic model, but by the principled orchestration of stable reasoning cores supported by specialized and adaptive tools.

<a id='4f0633c9-3690-4214-9770-d45d9b1844b1'></a>

# References

1. Jinyuan Fang, Yanwen Peng, Xi Zhang, Yingxu Wang, Xinhao Yi, Guibin Zhang, Yi Xu, Bin Wu, Siwei Liu, Zihao Li, et al. A comprehensive survey of self-evolving ai agents: A new paradigm bridging foundation models and lifelong agentic systems. *arXiv preprint arXiv:2508.07407*, 2025.
2. Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, Bohan Wu, Binqi Chen, Ziyue Qiao, Qingqing Long, et al. Large language model agent: A survey on methodology, applications and challenges. *arXiv preprint arXiv:2503.21460*, 2025.
3. Weikai Xu, Chengrui Huang, Shen Gao, and Shuo Shang. Llm-based agents for tool learning: A survey: W. xu et al. *Data Science and Engineering*, pages 1-31, 2025.
4. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. *Advances in Neural Information Processing Systems*, 36:68539-68551, 2023.

<a id='7138aaf2-3797-42b1-8d62-5863eac37ccf'></a>

51

<!-- PAGE BREAK -->

<a id='6ef2bd82-b5b9-4c1b-b8bc-f01a105644ef'></a>

Adaptation of Agentic AI

<a id='445915b9-b156-4eb7-9102-18c8583f669f'></a>

[5] Shanghua Gao, Ada Fang, Yepeng Huang, Valentina Giunchiglia, Ayush Noori, Jonathan Richard Schwarz, Yasha Ektefaie, Jovana Kondic, and Marinka Zitnik. Empowering biomedical discovery with ai agents. Cell, 187(22): 6125-6151, 2024.
[6] Renjun Xu and Jingwen Peng. A comprehensive survey of deep research: Systems, methodologies, and applications. arXiv preprint arXiv:2506.12594, 2025.
[7] Zifeng Wang, Lang Cao, Benjamin Danek, Qiao Jin, Zhiyong Lu, and Jimeng Sun. Accelerating clinical evidence synthesis with large language models. npj Digital Medicine, 8(1):509, 2025.
[8] Zifeng Wang, Hanyin Wang, Benjamin Danek, Ying Li, Christina Mack, Luk Arbuckle, Devyani Biswal, Hoifung Poon, Yajuan Wang, Pranav Rajpurkar, et al. A perspective for adapting generalist ai to specialized medical ai applications and their challenges. NPJ Digital Medicine, 8(1):429, 2025.
[9] Alex Gu, Naman Jain, Wen-Ding Li, Manish Shetty, Yijia Shao, Ziyang Li, Diyi Yang, Kevin Ellis, Koushik Sen, and Armando Solar-Lezama. Challenges and paths towards ai for software engineering, 2025. URL https://arxiv.org/abs/2503.22625.
[10] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. In ICLR, 2024.
[11] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Richard James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 8364-8377, 2024.
[12] Qiao Jin, Zifeng Wang, Charalampos S Floudas, Fangyuan Chen, Changlin Gong, Dara Bracken-Clarke, Elisabetta Xue, Yifan Yang, Jimeng Sun, and Zhiyong Lu. Matching patients to clinical trials with large language models. Nature communications, 15(1):9074, 2024.
[13] Peiyang Song, Pengrui Han, and Noah Goodman. A survey on large language model reasoning failures. In 2nd AI for Math Workshop@ ICML 2025, 2025.
[14] Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, et al. A survey of self-evolving agents: On path to artificial super intelligence. arXiv preprint arXiv:2507.21046, 2025.
[15] Aske Plaat, Max van Duijn, Niki van Stein, Mike Preuss, Peter van der Putten, and Kees Joost Batenburg. Agentic large language models, a survey. arXiv preprint arXiv:2503.23037, 2025.
[16] Zhengwei Tao, Ting-En Lin, Xiancai Chen, Hangyu Li, Yuchuan Wu, Yongbin Li, Zhi Jin, Fei Huang, Dacheng Tao, and Jingren Zhou. A survey on self-evolution of large language models. arXiv preprint arXiv:2404.14387, 2024.
[17] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6): 186345, 2024.
[18] Peter Belcak, Greg Heinrich, Shizhe Diao, Yonggan Fu, Xin Dong, Saurav Muralidharan, Yingyan Celine Lin, and Pavlo Molchanov. Small language models are the future of agentic ai. arXiv preprint arXiv:2506.02153, 2025.
[19] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022.
[20] Jonas Gehring, Kunhao Zheng, Jade Copet, Vegard Mella, Quentin Carbonneaux, Taco Cohen, and Gabriel Synnaeve. Rlef: Grounding code llms in execution feedback with reinforcement learning, 2025. URL https://arxiv.org/abs/2410.02089.
[21] Pengcheng Jiang, Jiacheng Lin, Lang Cao, R. Tian, S. Kang, Z. Wang, Jimeng Sun, and Jiawei Han. Deepretrieval: Hacking real search engines and retrievers with large language models via reinforcement learning. In The Second Conference on Language Modeling, 2025.
[22] Huichi Zhou, Yihang Chen, Siyuan Guo, Xue Yan, Kin Hei Lee, Zihan Wang, Ka Yiu Lee, Guchun Zhang, Kun Shao, Linyi Yang, et al. Memento: Fine-tuning llm agents without fine-tuning llms. arXiv preprint arXiv:2508.16153, 2025.
[23] Liang Wang, Nan Yang, and Furu Wei. Learning to retrieve in-context examples for large language models. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1752-1767, 2024.

<a id='2bacd05f-b5ab-41a6-8186-1c44136bbb6b'></a>

52

<!-- PAGE BREAK -->

<a id='aea8c474-83cc-469c-9dca-3d5164f784c6'></a>

Adaptation of Agentic AI

<a id='632fdacb-8ef2-4de9-99ee-428a0de35135'></a>

24. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081): 633-638, 2025.
25. Shaokun Zhang, Yi Dong, Jieyu Zhang, Jan Kautz, Bryan Catanzaro, Andrew Tao, Qingyun Wu, Zhiding Yu, and Guilin Liu. Nemotron-research-tool-n1: Exploring tool-using language models with reinforced reasoning. arXiv preprint arXiv:2505.00024, 2025.
26. Lang Mei, Zhihan Yang, and Chong Chen. Ai-searchplanner: Modular agentic search via pareto-optimal multi-objective reinforcement learning. arXiv preprint arXiv:2508.20368, 2025.
27. Pengcheng Jiang, Xueqiang Xu, Jiacheng Lin, Jinfeng Xiao, Zifeng Wang, Jimeng Sun, and Jiawei Han. s3: You don't need that much data to train a search agent via rl. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, 2025.
28. Zichun Yu, Chenyan Xiong, Shi Yu, and Zhiyuan Liu. Augmentation-adapted retriever improves generalization of language models as generic plug-in. In The 61st Annual Meeting Of The Association For Computational Linguistics, 2023.
29. Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings. Advances in neural information processing systems, 36:45870-45894, 2023.
30. Bang Liu, Xinfeng Li, Jiayi Zhang, Jinlin Wang, Tanjin He, Sirui Hong, Hongzhang Liu, Shaokun Zhang, Kaitao Song, Kunlun Zhu, et al. Advances and challenges in foundation agents: From brain-inspired intelligence to evolutionary, collaborative, and safe systems. arXiv preprint arXiv:2504.01990, 2025.
31. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837, 2022.
32. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:11809-11822, 2023.
33. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations, 2022.
34. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:8634-8652, 2023.
35. Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. Tool learning with foundation models. In ACM Computing Surveys, 2024.
36. Yaxiong Wu, Sheng Liang, Chen Zhang, Yichao Wang, Yongyue Zhang, Huifeng Guo, Ruiming Tang, and Yong Liu. From human memory to ai memory: A survey on memory mechanisms in the era of llms. arXiv preprint arXiv:2504.15965, 2025.
37. Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for" mind" exploration of large language model society. Advances in Neural Information Processing Systems, 36: 51991-52008, 2023.
38. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen Ilm applications via multi-agent conversations. In First Conference on Language Modeling, 2024.
39. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al. Metagpt: Meta programming for a multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations, 2023.
40. Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, et al. Chatdev: Communicative agents for software development. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15174-15186, 2024.

<a id='91902448-71c5-482b-98ed-f36d0bca4191'></a>

53

<!-- PAGE BREAK -->

<a id='b0a6afff-be48-49e1-90f4-642bb4db381e'></a>

Adaptation of Agentic AI

<a id='9d087308-499b-416e-ad83-39f7317b7a2a'></a>

[41] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, and Aman Chadha. A systematic survey of prompt engineering in large language models: Techniques and applications. arXiv preprint arXiv:2402.07927, 2024.
[42] Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and Sai Qian Zhang. Parameter-efficient fine-tuning for large models: A comprehensive survey. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=lIsCS8b6zj.
[43] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=gEZrGCozdqR.
[44] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in neural information processing systems, 36:53728-53741, 2023.
[45] Wenyi Xiao, Zechuan Wang, Leilei Gan, Shuai Zhao, Zongrui Li, Ruirui Lei, Wanggui He, Luu Anh Tuan, Long Chen, Hao Jiang, et al. A comprehensive survey of direct preference optimization: Datasets, theories, variants, and applications. arXiv preprint arXiv:2410.15595, 2024.
[46] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
[47] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.
[48] Zeyu Zhang, Quanyu Dai, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. A survey on the memory mechanism of large language model-based agents. ACM Transactions on Information Systems, 43(6):1-47, 2025.
[49] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. In The Second Conference on Language Modeling, 2025.
[50] Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in llms. arXiv preprint arXiv:2504.11536, 2025.
[51] Zhuofeng Li, Haoxiang Zhang, Seungju Han, Sheng Liu, Jianwen Xie, Yu Zhang, Yejin Choi, James Zou, and Pan Lu. In-the-flow agentic system optimization for effective planning and tool use. arXiv preprint arXiv:2510.05592, 2025.
[52] Shuofei Qiao, Honghao Gui, Chengfei Lv, Qianghuai Jia, Huajun Chen, and Ningyu Zhang. Making language models better tool learners with execution feedback. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 3550-3568, 2024.
[53] Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le Sun. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. arXiv preprint arXiv:2306.05301, 2023.
[54] Sijia Chen, Yibo Wang, Yi-Feng Wu, Qingguo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Lijun Zhang. Advancing tool-augmented large language models: Integrating insights from errors in inference trees. Advances in Neural Information Processing Systems, 37:106555-106581, 2024.
[55] Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language model connected with massive apis. Advances in Neural Information Processing Systems, 37:126544-126565, 2024.
[56] Zezhong Wang, Xingshan Zeng, Weiwen Liu, Liangyou Li, Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu, and Kam-Fai Wong. Toolflow: Boosting llm tool-calling through natural and coherent dialogue synthesis. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 4246-4263, 2025.
[57] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents. In International Conference on Machine Learning, pages 50208-50232. PMLR, 2024.
[58] Ansong Ni, Miltiadis Allamanis, Arman Cohan, Yinlin Deng, Kensen Shi, Charles Sutton, and Pengcheng Yin. Next: teaching large language models to reason about code execution. In Proceedings of the 41st International Conference on Machine Learning, pages 37929-37956, 2024.

<a id='429463b5-47db-4815-9e71-aaa775017bc3'></a>

54

<!-- PAGE BREAK -->

<a id='e1b96bfa-5ad3-4c7a-a946-0177992ee56f'></a>

Adaptation of Agentic AI

<a id='5960fb38-0f73-4426-bcbf-2d2e7e5803d5'></a>

[59] Zhengliang Shi, Shen Gao, Lingyong Yan, Yue Feng, Xiuyi Chen, Zhumin Chen, Dawei Yin, Suzan Verberne, and Zhaochun Ren. Tool learning in the wild: Empowering language models as automatic tool agents. In Proceedings of the ACM on Web Conference 2025, pages 2222-2237, 2025.
[60] Sheryl Hsu, Omar Khattab, Chelsea Finn, and Archit Sharma. Grounding by trying: Llms with reinforcement learning-enhanced retrieval. In The Thirteenth International Conference on Learning Representations, 2024.
[61] Chanwoong Yoon, Gangwoo Kim, Byeongguk Jeon, Sungdong Kim, Yohan Jo, and Jaewoo Kang. Ask optimal questions: Aligning large language models with retriever's preference in conversation. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 5899-5921, 2025.
[62] Alan Dao and Thinh Le. Rezero: Enhancing Ilm search ability by trying one-more-time. arXiv preprint arXiv:2504.11001, 2025.
[63] Supriti Vijay, Aman Priyanshu, Anu Vellore, Baturay Saglam, and Amin Karbasi. Think before you retrieve: Learning test-time adaptive search with small language models. arXiv preprint arXiv:2511.07581, 2025.
[64] Nan Jiang, Xiaopeng Li, Shiqi Wang, Qiang Zhou, Soneya B Hossain, Baishakhi Ray, Varun Kumar, Xiaofei Ma, and Anoop Deoras. Ledex: Training Ilms to better self-debug and explain code. Advances in Neural Information Processing Systems, 37:35517-35543, 2024.
[65] Jiawei Liu and Lingming Zhang. Code-r1: Reproducing r1 for code with reliable rewards. https://github.com/ganler/code-r1, 2025.
[66] Yongchao Chen, Yueying Liu, Junwei Zhou, Yilun Hao, Jingquan Wang, Yang Zhang, and Chuchu Fan. R1-code-interpreter: Training Ilms to reason with code via supervised and reinforcement learning. arXiv preprint arXiv:2505.21668, 2025.
[67] Yabo Zhang, Yihan Zeng, Qingyun Li, Zhen Hu, Kavin Han, and Wangmeng Zuo. Tool-r1: Sample-efficient reinforcement learning for agentic tool use. arXiv preprint arXiv:2509.12867, 2025.
[68] Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving. arXiv preprint arXiv:2009.03393, 2020.
[69] Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan J Prenger, and Animashree Anandkumar. Leandojo: Theorem proving with retrieval-augmented language models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 21573-21612. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/4441469427094f8873d0fecb0c4e1cee-Paper-Datasets_and_Benchmarks.pdf.
[70] Yong Lin, Shange Tang, Bohan Lyu, Ziran Yang, Jui-Hui Chung, Haoyu Zhao, Lai Jiang, Yihan Geng, Jiawei Ge, Jingruo Sun, et al. Goedel-prover-v2: Scaling formal theorem proving with scaffolded data synthesis and self-correction. arXiv preprint arXiv:2508.03613, 2025.
[71] Huajian Xin, ZZ Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, et al. Deepseek-prover-v1. 5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search. In The Thirteenth International Conference on Learning Representations, 2024.
[72] Haiming Wang, Mert Unsal, Xiaohan Lin, Mantas Baksys, Junqi Liu, Marco Dos Santos, Flood Sung, Marina Vinyes, Zhenzhe Ying, Zekai Zhu, et al. Kimina-prover preview: Towards large formal reasoning models with reinforcement learning. arXiv preprint arXiv:2504.11354, 2025.
[73] Luoxin Chen, Jinming Gu, Liankai Huang, Wenhao Huang, Zhicheng Jiang, Allan Jie, Xiaoran Jin, Xing Jin, Chenggang Li, Kaijing Ma, et al. Seed-prover: Deep and broad reasoning for automated theorem proving. arXiv preprint arXiv:2507.23726, 2025.
[74] Peiyang Song, Kaiyu Yang, and Anima Anandkumar. Lean copilot: Large language models as copilots for theorem proving in lean, 2025. URL https://arxiv.org/abs/2404.12534.
[75] George Tsoukalas, Jasper Lee, John Jennings, Jimmy Xin, Michelle Ding, Michael Jennings, Amitayush Thakur, and Swarat Chaudhuri. Putnambench: A multilingual competition-mathematics benchmark for formal theorem-proving. In AI for Math Workshop@ ICML 2024, 2024.
[76] Kaiyu Yang, Gabriel Poesia, Jingxuan He, Wenda Li, Kristin Lauter, Swarat Chaudhuri, and Dawn Song. Formal mathematical reasoning: A new frontier in ai, 2024. URL https://arxiv.org/abs/2412.16075.

<a id='a9216fe6-b277-4c37-b21d-bab1d3d23073'></a>

55

<!-- PAGE BREAK -->

<a id='22439faa-fa0a-4b6f-8775-fc4f0de28b4c'></a>

Adaptation of Agentic AI

<a id='fb400dbf-7a22-4f5c-8e89-c440abedfcca'></a>

77. Thomas Hubert, Rishi Mehta, Laurent Sartran, Miklós Z Horváth, Goran Žužić, Eric Wieser, Aja Huang, Julian Schrittwieser, Yannick Schroecker, Hussain Masoom, et al. Olympiad-level formal mathematical reasoning with reinforcement learning. Nature, pages 1-3, 2025.
78. ZZ Ren, Zhihong Shao, Junxiao Song, Huajian Xin, Haocheng Wang, Wanjia Zhao, Liyue Zhang, Zhe Fu, Qihao Zhu, Dejian Yang, et al. Deepseek-prover-v2: Advancing formal mathematical reasoning via reinforcement learning for subgoal decomposition. arXiv preprint arXiv:2504.21801, 2025.
79. Xingguang Ji, Yahui Liu, Qi Wang, Jingyuan Zhang, Yang Yue, Rui Shi, Chenxi Sun, Fuzheng Zhang, Guorui Zhou, and Kun Gai. Leanabell-prover-v2: Verifier-integrated reasoning for formal theorem proving via reinforcement learning. arXiv preprint arXiv:2507.08649, 2025.
80. Ran Xin, Zeyu Zheng, Yanchen Nie, Kun Yuan, and Xia Xiao. Scaling up multi-turn off-policy rl and multi-agent tree search for Ilm step-provers. arXiv preprint arXiv:2509.06493, 2025.
81. Suozhi Huang, Peiyang Song, Robert Joseph George, and Anima Anandkumar. Leanprogress: Guiding search for neural theorem proving via proof progress prediction. arXiv preprint arXiv:2502.17925, 2025.
82. Tudor Achim, Alex Best, Alberto Bietti, Kevin Der, Mathïs Fédérico, Sergei Gukov, Daniel Halpern-Leistner, Kirsten Henningsgard, Yury Kudryashov, Alexander Meiburg, Martin Michelsen, Riley Patterson, Eric Rodriguez, Laura Scharff, Vikram Shanker, Vladmir Sicca, Hari Sowrirajan, Aidan Swope, Matyas Tamas, Vlad Tenev, Jonathan Thomm, Harold Williams, and Lawrence Wu. Aristotle: Imo-level automated theorem proving, 2025. URL https://arxiv.org/abs/2510.01346.
83. Alex Sanchez-Stern, Abhishek Varghese, Zhanna Kaufman, Dylan Zhang, Talia Ringer, and Yuriy Brun. Qedcartographer: Automating formal verification using reward-free reinforcement learning. In Proceedings of the IEEE/ACM 47th International Conference on Software Engineering, pages 307-320, 2025.
84. Zijian Wu, Suozhi Huang, Zhejian Zhou, Huaiyuan Ying, Jiayu Wang, Dahua Lin, and Kai Chen. Internlm2. 5-stepprover: Advancing automated theorem proving via expert iteration on large-scale lean problems. arXiv preprint arXiv:2410.15700, 2024.
85. The mathlib Community. The lean mathematical library. In Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs, POPL '20, page 367-381. ACM, January 2020. doi: 10.1145/3372885.3373824. URL http://dx.doi.org/10.1145/3372885.3373824.
86. W. T. Gowers, Ben Green, Freddie Manners, and Terence Tao. On a conjecture of marton, 2023. URL https://arxiv.org/abs/2311.05762.
87. Haozhen Zhang, Tao Feng, and Jiaxuan You. Router-r1: Teaching Ilms multi-round routing and aggregation via reinforcement learning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025.
88. Junjie Ye, Changhao Jiang, Zhengyin Du, Yufei Xu, Xuesong Yao, Zhiheng Xi, Xiaoran Fan, Qi Zhang, Tao Gui, Xuanjing Huang, et al. Feedback-driven tool-use improvements in large language models via automated build environments. arXiv preprint arXiv:2508.08791, 2025.
89. Zimu Lu, Houxing Ren, Yunqiao Yang, Ke Wang, Zhuofan Zong, Junting Pan, Mingjie Zhan, and Hongsheng Li. Webgen-agent: Enhancing interactive website generation with multi-level feedback and step-level reinforcement learning. arXiv preprint arXiv:2509.22644, 2025.
90. Fu Chen, Peng Wang, Xiyin Li, Wen Li, Shichi Lei, and Dongdong Xiang. Toolexpander: Extending the frontiers of tool-using reinforcement learning to weak Ilms. arXiv preprint arXiv:2510.07737, 2025.
91. Jiacheng Lin, Tian Wang, and Kun Qian. Rec-r1: Bridging generative large language models and user-centric recommendation systems via reinforcement learning. Transactions on Machine Learning Research, 2025. ISSN 2835-8856. URL https://openreview.net/forum?id=YBRU9MV2vE.
92. Peixian Ma, Xialie Zhuang, Chengjin Xu, Xuhui Jiang, Ran Chen, and Jian Guo. Sql-r1: Training natural language to sql reasoning model by reinforcement learning. arXiv preprint arXiv:2504.08600, 2025.
93. Jake Poznanski, Luca Soldaini, and Kyle Lo. olmocr 2: Unit test rewards for document ocr. arXiv preprint arXiv:2510.19817, 2025.
94. Jake Poznanski, Aman Rangapur, Jon Borchardt, Jason Dunkelberger, Regan Huff, Daniel Lin, Christopher Wilhelm, Kyle Lo, and Luca Soldaini. olmocr: Unlocking trillions of tokens in pdfs with vision language models. arXiv preprint arXiv:2502.18443, 2025.

<a id='1ef8c9da-2e4e-4d07-bbd2-6a04959361a7'></a>

56

<!-- PAGE BREAK -->

<a id='7804faa7-b58c-42b2-84ec-c4c7f5dc2424'></a>

Adaptation of Agentic AI

<a id='e3dd1d0b-47ac-451b-9ca2-031fa3b7ffe2'></a>

[95] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025.
[96] Evan Ellis, Vivek Myers, Jens Tuyls, Sergey Levine, Anca Dragan, and Benjamin Eysenbach. Training llm agents to empower humans. arXiv preprint arXiv:2510.13709, 2025.
[97] Sahil Kale and Devendra Singh Dhami. Knowrl: Teaching language models to know what they know. arXiv preprint arXiv:2510.11407, 2025.
[98] Jiashuo Sun, Shixuan Liu, Zhaochen Su, Xianrui Zhong, Pengcheng Jiang, Bowen Jin, Peiran Li, Weijia Shi, and Jiawei Han. Grace: Generative representation learning via contrastive policy optimization. arXiv preprint arXiv:2510.04506, 2025.
[99] Jiacheng Lin, Zhenbang Wu, and Jimeng Sun. Training llms for ehr-based reasoning tasks via reinforcement learning. arXiv preprint arXiv:2505.24105, 2025.
[100] Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei M Zhang, Kay McKinney, Disha Shrivastava, Cosmin Paduraru, George Tucker, Doina Precup, Feryal Behbahani, and Aleksandra Faust. Training language models to self-correct via reinforcement learning. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=CjwERcAU7w.
[101] Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Pan Lu, Zhi Huang, Carlos Guestrin, and James Zou. Optimizing generative ai by backpropagating language model feedback. Nature, 639(8055):609-616, 2025.
[102] Guowei Xu, Mert Yuksekgonul, Carlos Guestrin, and James Zou. metatextgrad: Automatically optimizing language model optimizers. arXiv preprint arXiv:2505.18524, 2025.
[103] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations, 2024.
[104] Yuanjie Lyu, Zihan Niu, Zheyong Xie, Chao Zhang, Tong Xu, Yang Wang, and Enhong Chen. Retrieve-plan-generation: An iterative planning and answering framework for knowledge-intensive llm generation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 4683-4702, 2024.
[105] Pengcheng Jiang, Lang Cao, Ruike Zhu, Minhao Jiang, Yunyi Zhang, Jimeng Sun, and Jiawei Han. Ras: Retrieval-and-structuring for knowledge-intensive llm generation. arXiv preprint arXiv: 2502.10996, 2025.
[106] Xinyan Guan, Jiali Zeng, Fandong Meng, Chunlei Xin, Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, and Jie Zhou. Deeprag: Thinking to retrieve step by step for large language models. arXiv preprint arXiv:2502.01142, 2025.
[107] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592, 2025.
[108] Mingyang Chen, Linzhuang Sun, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z Pan, Wen Zhang, Huajun Chen, et al. Learning to reason with search for llms via reinforcement learning. arXiv preprint arXiv:2503.19470, 2025.
[109] Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160, 2025.
[110] Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Yan Zhang, Fei Huang, and Jingren Zhou. Zerosearch: Incentivize the search capability of llms without searching. arXiv preprint arXiv:2505.04588, 2025.
[111] Ziliang Wang, Xuhui Zheng, Kang An, Cijun Ouyang, Jialu Cai, Yuhang Wang, and Yichao Wu. Stepsearch: Igniting llms search ability via step-wise proximal policy optimization. arXiv preprint arXiv:2505.15107, 2025.
[112] Chuzhan Hao, Wenfeng Feng, Yuewei Zhang, and Hao Wang. Dynasearcher: Dynamic knowledge graph augmented search agent via multi-reward reinforcement learning. arXiv preprint arXiv:2507.17365, 2025.
[113] Ailing Yu, Lan Yao, Jingnan Liu, Zhe Chen, Jiajun Yin, Yuan Wang, Xinhao Liao, Zhiling Ye, Ji Li, Yun Yue, et al. Medresearcher-r1: Expert-level medical deep researcher via a knowledge-informed trajectory synthesis framework. arXiv preprint arXiv:2508.14880, 2025.
[114] Yaorui Shi, Shihan Li, Chang Wu, Zhiyuan Liu, Junfeng Fang, Hengxing Cai, An Zhang, and Xiang Wang. Search and refine during think: Autonomous retrieval-augmented reasoning of llms. arXiv e-prints, pages arXiv-2505, 2025.

<a id='c612cebb-5d65-4321-9b47-32c8ad6b0eba'></a>

57

<!-- PAGE BREAK -->

<a id='c01db4e7-40b7-4c75-91e8-da719ef436cd'></a>

Adaptation of Agentic AI

<a id='10ab26cc-c19e-4014-afe1-cdd327f0a221'></a>

115. Jinming Wu, Zihao Deng, Wei Li, Yiding Liu, Bo You, Bo Li, Zejun Ma, and Ziwei Liu. Mmsearch-r1: Incentivizing Imms to search. arXiv preprint arXiv:2506.20670, 2025.
116. Qingyao Li, Xinyi Dai, Xiangyang Li, Weinan Zhang, Yasheng Wang, Ruiming Tang, and Yong Yu. Codeprm: Execution feedback-enhanced process reward model for code generation. In Findings of the Association for Computational Linguistics: ACL 2025, pages 8169-8182, 2025.
117. Emre Can Acikgoz, Cheng Qian, Heng Ji, Dilek Hakkani-Tür, and Gokhan Tur. Self-improving Ilm agents at test-time. arXiv preprint arXiv:2510.07841, 2025.
118. Xufang Luo, Yuge Zhang, Zhiyuan He, Zilong Wang, Siyun Zhao, Dongsheng Li, Luna K Qiu, and Yuqing Yang. Agent lightning: Train any ai agents with reinforcement learning. arXiv preprint arXiv:2508.03680, 2025.
119. Zi-Yi Dou, Cheng-Fu Yang, Xueqing Wu, Kai-Wei Chang, and Nanyun Peng. Re-rest: Reflection-reinforced self-training for language agents. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 15394-15411, 2024.
120. Yifei Zhou, Sergey Levine, Jason Weston, Xian Li, and Sainbayar Sukhbaatar. Self-challenging language model agents. arXiv preprint arXiv:2506.01716, 2025.
121. Siyu Yuan, Zehui Chen, Zhiheng Xi, Junjie Ye, Zhengyin Du, and Jiecao Chen. Agent-r: Training language model agents to reflect via iterative self-training. arXiv preprint arXiv:2501.11425, 2025.
122. Qianben Chen, Jingyi Cao, Jiayu Zhang, Tianrui Qin, Xiaowan Li, King Zhu, Dingfeng Shi, He Zhu, Minghao Liu, Xiaobo Liang, et al. A2fm: An adaptive agent foundation model for tool-aware hybrid reasoning. arXiv e-prints, pages arXiv-2510, 2025.
123. Dongfu Jiang, Yi Lu, Zhuofeng Li, Zhiheng Lyu, Ping Nie, Haozhe Wang, Alex Su, Hui Chen, Kai Zou, Chao Du, et al. Verltool: Towards holistic agentic reinforcement learning with tool use. arXiv preprint arXiv:2509.01055, 2025.
124. Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces with applications to pdes. Journal of Machine Learning Research, 24(89):1-97, 2023.
125. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 36:38154-38180, 2023.
126. Dídac Surís, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 11888-11898, 2023.
127. Keyan Ding, Jing Yu, Junjie Huang, Yuchen Yang, Qiang Zhang, and Huajun Chen. Scitoolagent: a knowledge-graph-driven scientific agent for multitool integration. Nature Computational Science, pages 1-11, 2025.
128. Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.
129. Anthropic. Code execution with mcp: Building more efficient ai agents, November 2025. URL https://www.anthropic.com/engineering/code-execution-with-mcp. Published Nov 04 2025.
130. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PmLR, 2021.
131. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 4015-4026, 2023.
132. Haoxiang Wang, Pavan Kumar Anasosalu Vasu, Fartash Faghri, Raviteja Vemulapalli, Mehrdad Farajtabar, Sachin Mehta, Mohammad Rastegari, Oncel Tuzel, and Hadi Pouransari. Sam-clip: Merging vision foundation models towards semantic and spatial understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3635-3647, 2024.
133. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 28492-28518. PMLR, 2023.
134. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick SH Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In EMNLP (1), pages 6769-6781, 2020.

<a id='daaa2dbc-6139-424f-b61d-6d6e6e03db36'></a>

58

<!-- PAGE BREAK -->

<a id='205ff001-6cc8-4aeb-91f8-3473a89cc6ed'></a>

Adaptation of Agentic AI

<a id='2d905445-3ae0-4727-b437-9aad5775b063'></a>

135. Omar Khattab and Matei Zaharia. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 39-48, 2020.
136. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118, 2021.
137. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533, 2022.
138. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunya- suvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. nature, 596(7873):583-589, 2021.
139. Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, 379(6637):1123-1130, 2023.
140. Tian Xie and Jeffrey C Grossman. Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties. Physical review letters, 120(14):145301, 2018.
141. Bohao Xu, Yingzhou Lu, Chenhao Li, Ling Yue, Xiao Wang, Nan Hao, Tianfan Fu, and Jim Chen. Smiles-mamba: Chemical mamba foundation models for drug admet prediction. arXiv preprint arXiv:2408.05696, 2024.
142. Wengong Jin, Connor W Coley, Regina Barzilay, and Tommi Jaakkola. Predicting organic reaction outcomes with weisfeiler-lehman network. arXiv preprint arXiv:1709.04555, 2017.
143. Wengong Jin, Kevin Yang, Regina Barzilay, and Tommi Jaakkola. Learning multimodal graph-to-graph translation for molecular optimization. arXiv preprint arXiv:1812.01070, 2018.
144. Shuangjia Zheng, Jiahua Rao, Zhongyue Zhang, Jun Xu, and Yuedong Yang. Predicting retrosynthetic reactions using self-corrected transformer neural networks. Journal of Chemical Information and Modeling, 60(1):47-55, 2019.
145. Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self-supervised graph transformer on large-scale molecular data, 2020.
146. Yin Fang, Qiang Zhang, Ningyu Zhang, Zhuo Chen, Xiang Zhuang, Xin Shao, Xiaohui Fan, and Huajun Chen. Knowledge graph-enhanced molecular contrastive learning with functional prompt. Nature Machine Intelligence, pages 1-12, 2023.
147. Pengcheng Jiang, Cao Xiao, Tianfan Fu, Jimeng Sun, and Jiawei Han. Bi-level contrastive learning for knowledge- enhanced molecule representations. In Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence, 2025.
148. Xiaoning Qi, Lianhe Zhao, Chenyu Tian, Yueyue Li, Zhen-Lin Chen, Peipei Huo, Runsheng Chen, Xiaodong Liu, Baoping Wan, Shengyong Yang, et al. Predicting transcriptional responses to novel chemical perturbations using deep generative model for drug discovery. Nature Communications, 15(1):9256, 2024.
149. Xiaochu Tong, Ning Qu, Xiangtai Kong, Shengkun Ni, Jingyi Zhou, Kun Wang, Lehan Zhang, Yiming Wen, Jiangshan Shi, Sulin Zhang, et al. Deep representation learning of chemical-induced transcriptional profile for phenotype-based drug discovery. Nature Communications, 15(1):5378, 2024.
150. Haitao Li, Qingyao Ai, Jia Chen, Qian Dong, Zhijing Wu, and Yiqun Liu. Blade: Enhancing black-box large language models with small domain-specific models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 24422-24430, 2025.
151. Haotian Sun, Yuchen Zhuang, Wei Wei, Chao Zhang, and Bo Dai. Bbox-adapter: Lightweight adapting for black-box large language models. arXiv preprint arXiv:2402.08219, 2024.
152. Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, and Noah A Smith. Tuning language models by proxy. In First Conference on Language Modeling, 2024.
153. Hongjin Su, Shuyang Jiang, Yuhang Lai, Haoyuan Wu, Boao Shi, Che Liu, Qian Liu, and Tao Yu. Evor: Evolving retrieval for code generation. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 2538-2554, 2024.
154. Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, et al. Ra-dit: Retrieval-augmented dual instruction tuning. In The Twelfth International Conference on Learning Representations, 2023.

<a id='3d683bfd-57b5-49b4-8657-74563121a166'></a>

59

<!-- PAGE BREAK -->

<a id='ae440215-62fb-42a7-b1c2-7f004eecba82'></a>

Adaptation of Agentic AI

<a id='1ac3753d-ac68-4553-9aa9-51faa9bcb56c'></a>

[155] Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Haotian Sun, Hang Wu, Carl Yang, and May D Wang. Medadapter: Efficient test-time adaptation of large language models towards medical reasoning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing, volume 2024, page 22294, 2024.
[156] Jaehyung Kim, Dongyoung Kim, and Yiming Yang. Learning to correct for qa reasoning with black-box llms. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 8916-8937, 2024.
[157] Lingxi Zhang, Yue Yu, Kuan Wang, and Chao Zhang. Arl2: Aligning retrievers with black-box large language models via self-guided adaptive relevance labeling. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3708-3719, 2024.
[158] Daixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu Wei, Weiwei Deng, and Qi Zhang. Uprise: Universal prompt retrieval for improving zero-shot evaluation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12318-12337, 2023.
[159] Zixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky. Bridging the preference gap between retrievers and llms. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10438-10451, 2024.
[160] Jiashuo Sun, Xianrui Zhong, Sizhe Zhou, and Jiawei Han. Dynamicrag: Leveraging outputs of large language model as feedback for dynamic reranking in retrieval-augmented generation. arXiv preprint arXiv:2505.07233, 2025.
[161] Yi Jiang, Lei Shen, Lujie Niu, Sendong Zhao, Wenbo Su, and Bo Zheng. Qagent: A modular search agent with interactive query understanding. arXiv preprint arXiv:2510.08383, 2025.
[162] Yu Wang, Ryuichi Takanobu, Zhiqi Liang, Yuzhen Mao, Yuanzhe Hu, Julian McAuley, and Xiaojian Wu. Mem-{\alpha}: Learning memory construction via reinforcement learning. arXiv preprint arXiv:2509.25911, 2025.
[163] Hong Ting Tsang, Jiaxin Bai, Haoyu Huang, Qiao Xiao, Tianshi Zheng, Baixuan Xu, Shujie Liu, and Yangqiu Song. Autograph-r1: End-to-end reinforcement learning for knowledge graph construction. arXiv preprint arXiv:2510.15339, 2025.
[164] Yilin Xiao, Junnan Dong, Chuang Zhou, Su Dong, Qian-wen Zhang, Di Yin, Xing Sun, and Xiao Huang. Graphrag-bench: Challenging domain-specific reasoning for evaluating graph retrieval-augmented generation. arXiv preprint arXiv:2506.02404, 2025.
[165] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. In The Twelfth International Conference on Learning Representations, 2023.
[166] Parth Asawa, Alan Zhu, Matei Zaharia, Alexandros G Dimakis, and Joseph E Gonzalez. How to train your advisor: Steering black-box llms with advisor models. arXiv preprint arXiv:2510.02453, 2025.
[167] ChangHao Li, Yuchen Zhuang, Rushi Qiang, Haotian Sun, Hanjun Dai, Chao Zhang, and Bo Dai. Matryoshka pilot: Learning to drive black-box llms with llms. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025.
[168] Chengsong Huang, Wenhao Yu, Xiaoyang Wang, Hongming Zhang, Zongxia Li, Ruosen Li, Jiaxin Huang, Haitao Mi, and Dong Yu. R-zero: Self-evolving reasoning Ilm from zero data. arXiv preprint arXiv:2508.05004, 2025.
[169] Yixing Chen, Yiding Wang, Siqi Zhu, Haofei Yu, Tao Feng, Muhan Zhang, Mostofa Patwary, and Jiaxuan You. Multi-agent evolve: Llm self-improve through co-evolution. arXiv preprint arXiv:2510.23595, 2025.
[170] Joon Sung Park, Joseph O'Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 1-22, 2023.
[171] Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G Patil, Ion Stoica, and Joseph E Gonzalez. Memgpt: Towards Ilms as operating systems. arXiv preprint arXiv:2310.08560, 2023.
[172] Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 19724-19731, 2024.
[173] Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He, Di Yin, Xing Sun, and Yunsheng Wu. Memochat: Tuning llms to use memos for consistent long-range open-domain conversation. arXiv preprint arXiv:2308.08239, 2023.

<a id='f180f4ad-7b55-412d-8de5-76f2c0059747'></a>

60

<!-- PAGE BREAK -->

<a id='9178652c-bb55-45d4-b23f-29719c42b120'></a>

Adaptation of Agentic AI

<a id='d9ebf063-e076-416d-828f-511af8121646'></a>

[174] Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Schütze. Ret-llm: Towards a general read-write memory for large language models. arXiv preprint arXiv:2305.14322, 2023.
[175] Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li. Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system. arXiv preprint arXiv:2304.13343, 10, 2023.
[176] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023.
[177] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel: Llm agents are experiential learners. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 19632-19642, 2024.
[178] Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh RN, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, et al. Retroformer: Retrospective large language agents with policy gradient optimization. In The Twelfth International Conference on Learning Representations, 2023.
[179] Longtao Zheng, Rundong Wang, Xinrun Wang, and Bo An. Synapse: Trajectory-as-exemplar prompting with memory for computer control. In The Twelfth International Conference on Learning Representations, 2024.
[180] Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang. Think-in-memory: Recalling and post-thinking enable llms with long-term memory. arXiv preprint arXiv:2311.08719, 2023.
[181] Alireza Rezazadeh, Zichao Li, Wei Wei, and Yujia Bao. From isolated conversations to hierarchical schemas: Dynamic tree memory representation for llms. arXiv preprint arXiv:2410.14052, 2024.
[182] Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry Evseev, Andrey Kravchenko, Mikhail Burtsev, and Evgeny Burnaev. Arigraph: Learning knowledge graph world models with episodic memory for Ilm agents. arXiv preprint arXiv:2407.04363, 2024.
[183] Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. Chatdb: Augmenting llms with databases as their symbolic memory. arXiv preprint arXiv:2306.03901, 2023.
[184] Xiaoxia Cheng, Zeqi Tan, Wei Xue, and Weiming Lu. Information re-organization improves reasoning in large language models. Advances in Neural Information Processing Systems, 37:130214-130236, 2024.
[185] Mirac Suzgun, Mert Yuksekgonul, Federico Bianchi, Dan Jurafsky, and James Zou. Dynamic cheatsheet: Test-time learning with adaptive memory. arXiv preprint arXiv:2504.07952, 2025.
[186] Siru Ouyang, Jun Yan, I Hsu, Yanfei Chen, Ke Jiang, Zifeng Wang, Rujun Han, Long T Le, Samira Daruki, Xiangru Tang, et al. Reasoningbank: Scaling agent self-evolving with reasoning memory. arXiv preprint arXiv:2509.25140, 2025.
[187] Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: A realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023.
[188] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023.
[189] Ziyang Wang, Heba Elfardy, Markus Dreyer, Kevin Small, and Mohit Bansal. Unified embeddings for multimodal retrieval via frozen Ilms. In Findings of the Association for Computational Linguistics: EACL 2024, pages 1537-1547, 2024.
[190] Song Tang, Wenxin Su, Mao Ye, and Xiatian Zhu. Source-free domain adaptation with frozen multimodal foundation model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23711-23720, 2024.
[191] Lei Zhu, Fangyun Wei, and Yanye Lu. Beyond text: Frozen large language models in visual signal comprehension. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 27047-27057, 2024.
[192] Kartik Sharma, Yiqiao Jin, Vineeth Rakesh, Yingtong Dou, Menghai Pan, Mahashweta Das, and Srijan Kumar. Sysformer: Safeguarding frozen large language models with adaptive system prompts. arXiv preprint arXiv:2506.15751, 2025.
[193] Ben Pan, Carlo Baronio, Albert Tam, Pietro Marsella, Mokshit Jain, Swyx, and Silas Alberti. Introducing swe-grep and swe-grep-mini: Rl for multi-turn, fast context retrieval. https://cognition.ai/blog/swe-grep, October 2025. Accessed: 2025-10-29.

<a id='4a2836b5-3cdc-4498-8305-65a65d1216bb'></a>

61

<!-- PAGE BREAK -->

<a id='2323d78d-7215-4aad-8883-4aba587b9b52'></a>

Adaptation of Agentic AI

<a id='f86ff62b-dc20-4193-951d-7e97da2516e0'></a>

[194] OpenAI. Introducing deep research. https://openai.com/index/introducing-deep-research/, 2025.
[195] Anthropic. Claude takes research to new places. https://www.anthropic.com/news/research, 2025.
[196] Dave Citron. Deep research is now available on gemini 2.5 pro experimental. https://blog.google/products/ gemini/deep-research-gemini-2-5-pro-experimental/, 2025.
[197] Cursor. Cursor the ai code editor. https://www.cursor.com/, 2025. Accessed: 2025-10-29.
[198] Anthropic. Claude code: Deep coding at terminal velocity. https://www.anthropic.com/claude-code, 2025. Accessed: 2025-10-29.
[199] OpenAI. Codex. https://openai.com/codex/, 2025. Accessed: 2025-10-29.
[200] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=VTF8yNQM66.
[201] John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:50528-50652, 2024.
[202] Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. Openhands: An open platform for AI software developers as generalist agents. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=OJd3ayDDoF.
[203] Jacob Jackson, Phillip Kravtsov, and Shomil Jain. Improving cursor tab with online reinforcement learning. https://cursor.com/blog/tab-rl, September 2025. Accessed: 2025-10-29.
[204] OpenAI. Computer-using agent. https://openai.com/index/computer-using-agent/, January 2025. Accessed: 2025-10-28.
[205] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh J Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:52040-52094, 2024.
[206] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: A realistic web environment for building autonomous agents. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=oKn9c6ytLx.
[207] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Russ Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 881-905, 2024.
[208] Harsh Trivedi, Tushar Khot, Mareike Hartmann, Ruskin Manku, Vinty Dong, Edward Li, Shashank Gupta, Ashish Sabharwal, and Niranjan Balasubramanian. Appworld: A controllable world of apps and people for benchmarking interactive coding agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16022-16076, 2024.
[209] Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6864-6890, 2024.
[210] Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik R Narasimhan. tau-bench: A benchmark for tool-agent-user interaction in real-world domains. In The Thirteenth International Conference on Learning Representations, 2025.
[211] Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, Zhennan Shen, Zhuokai Li, Ryan Li, Xiaochuan Li, Junda Chen, Zheng Boyuan, LI PEIHANG, Fangyu Lei, Ruisheng Cao, Yeqiao Fu, Dongchan Shin, Martin Shin, Hu Jiarui, Yuyan Wang, Jixuan Chen, Yuxiao Ye, Danyang Zhang, Yipu Wang, Heng Wang, Diyi Yang, Victor Zhong, Y.Charles, Zhilin Yang, and Tao Yu. OpenCUA: Open foundations for computer-use agents. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/forum?id=6iRZvJiC9Q.

<a id='97b05647-3970-46d7-a084-1b042031ae74'></a>

62

<!-- PAGE BREAK -->

<a id='ff50d4f6-1cb5-4748-bae3-68811cd7deca'></a>

Adaptation of Agentic AI

<a id='dc4f1b8a-e46e-4ce7-9d2b-645e5bdb4354'></a>

[212] Yiheng Xu, Dunjie Lu, Zhennan Shen, Junli Wang, Zekun Wang, Yuchen Mao, Caiming Xiong, and Tao Yu. Agenttrek: Agent trajectory synthesis via guiding replay with web tutorials. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=EEgYUccwsV.
[213] Qizheng Zhang, Changran Hu, Shubhangi Upasani, Boyuan Ma, Fenglu Hong, Vamsidhar Kamanuru, Jay Rainton, Chen Wu, Mengmeng Ji, Hanchen Li, et al. Agentic context engineering: Evolving contexts for self-improving language models. arXiv preprint arXiv:2510.04618, 2025.
[214] Zhizheng Wang, Qiao Jin, Chih-Hsuan Wei, Shubo Tian, Po-Ting Lai, Qingqing Zhu, Chi-Ping Day, Christina Ross, Robert Leaman, and Zhiyong Lu. Geneagent: self-verification language agent for gene-set analysis using domain databases. Nature Methods, pages 1-9, 2025.
[215] Zifeng Wang, Benjamin Danek, Ziwei Yang, Zheng Chen, and Jimeng Sun. Making large language models reliable data science programming copilot for biomedical research. Nature Biomedical Engineering, 2025.
[216] Kyle Swanson, Wesley Wu, Nash L Bulaong, John E Pak, and James Zou. The virtual lab of ai agents designs new sars-cov-2 nanobodies. Nature, pages 1-3, 2025.
[217] Zifeng Wang, Lang Cao, Qiao Jin, Joey Chan, Nicholas Wan, Behdad Afzali, Hyun-Jin Cho, Chang-In Choi, Mehdi Emamverdi, Manjot K Gill, et al. A foundation model for human-ai collaboration in medical literature mining. Nature Communications, 16(1):8361, 2025.
[218] Haoyang Li, Weishen Pan, Suraj Rajendran, Chengxi Zang, and Fei Wang. Trialgenie: Empowering clinical trial design with agentic intelligence and real world data. medRxiv, pages 2025-04, 2025.
[219] Kyle Swanson, Gary Liu, Denise B Catacutan, Autumn Arnold, James Zou, and Jonathan M Stokes. Generative ai for designing and validating easily synthesizable and structurally novel antibiotics. Nature machine intelligence, 6(3): 338-353, 2024.
[220] Aarti Krishnan, Jacqueline A Valeri, Wengong Jin, Nina M Donghia, Leif Sieben, Andreas Luttens, Yu Zhang, Seyed Majed Modaresi, Andrew Hennes, Jenna Fromer, et al. A generative deep learning approach to de novo antibiotic design. Cell, 2025.
[221] Shanghua Gao, Richard Zhu, Pengwei Sui, Zhenglun Kong, Sufian Aldogom, Yepeng Huang, Ayush Noori, Reza Shamji, Krishna Parvataneni, Theodoros Tsiligkaridis, et al. Democratizing ai scientists using tooluniverse. arXiv preprint arXiv:2509.23426, 2025.
[222] Kexin Huang, Serena Zhang, Hanchen Wang, Yuanhao Qu, Yingzhou Lu, Yusuf Roohani, Ryan Li, Lin Qiu, Gavin Li, Junze Zhang, et al. Biomni: A general-purpose biomedical ai agent. biorxiv, 2025.
[223] Ruofan Jin, Zaixi Zhang, Mengdi Wang, and Le Cong. Stella: Self-evolving llm agent for biomedical research. arXiv preprint arXiv:2507.02004, 2025.
[224] W. Daniel Hillis. Co-evolving parasites improve simulated evolution as an optimization procedure. In Christopher G. Langton, Charles Taylor, J. Doyne Farmer, and Steen Rasmussen, editors, Artificial Life II, volume X, pages 313-324. Addison-Wesley, 1990.
[225] Christopher D. Rosin and Risto Miikkulainen. Competitive coevolution through evolutionary complexification. Journal of Artificial Intelligence Research, 21:63-100, 2004.
[226] Mitchell A. Potter and Kenneth A. De Jong. Cooperative coevolution: An architecture for evolving co-adapted subcomponents. Evolutionary Computation, 8(1):1-29, 2000.
[227] Louis Sushil and collaborating authors. A comprehensive survey of coevolutionary algorithms. IEEE Transactions on Evolutionary Computation, 2008. Survey of competitive, cooperative, and multi-population CEA frameworks.
[228] Liviu Panait and Sean Luke. Cooperative multi-agent learning: The state of the art. Autonomous agents and multi-agent systems, 11(3):387-434, 2005.
[229] Zepeng Ning and Lihua Xie. A survey on multi-agent reinforcement learning and its application. Journal of Automation and Intelligence, 3(2):73-91, 2024.
[230] Mert Cemri, Melissa Z Pan, Shuyi Yang, Lakshya A Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Klein, Kannan Ramchandran, et al. Why do multi-agent llm systems fail? arXiv preprint arXiv:2503.13657, 2025.
[231] Liang Zhou, Rohan Patel, and Seong Kim. Multi-agent tool-integrated policy optimization. arXiv preprint arXiv:2510.04678, 2025. URL https://arxiv.org/pdf/2510.04678. Accessed: 2025-11-17.

<a id='d3498f21-af9c-4a50-9a3c-9cd31a2ef5af'></a>

63

<!-- PAGE BREAK -->

<a id='29b2de1c-7273-41f2-af7c-eaddd9aed94b'></a>

Adaptation of Agentic AI

<a id='36158f78-8920-49cb-bb4f-c1980434401a'></a>

[232] Jiajun Wang, Shurui Liu, and Tianyi Zhang. A joint optimization framework for enhancing efficiency of tool utilization in Ilm agents. In Findings of the Association for Computational Linguistics: ACL 2025, 2025. URL https://aclanthology.org/2025.findings-acl.1149.pdf. Accessed: 2025-11-17.
[233] Rui Huang, Ashok Kumar, and Sandip Sen. A design framework for scalable and adaptive multi-agent coordination in dynamic environments. IEEE Transactions on Systems, Man, and Cybernetics, 2023. URL https://ieeexplore.ieee.org/iel8/6287639/10820123/10965637. Accessed: 2025-11-17.
[234] Joon Lee, Robert Martens, and Yifan Du. From chaos to symbiosis: Exploring adaptive co-evolution strategies for hybrid intelligent systems. Complexity, 2024. URL https://pmc.ncbi.nlm.nih.gov/articles/PMC12465495/. Accessed: 2025-11-17.
[235] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A comprehensive survey of continual learning: Theory, method and application. IEEE transactions on pattern analysis and machine intelligence, 46(8):5362-5383, 2024.
[236] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleš Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern analysis and machine intelligence, 44(7):3366-3385, 2021.
[237] Haizhou Shi, Zihao Xu, Hengyi Wang, Weiyi Qin, Wenyuan Wang, Yibin Wang, Zifeng Wang, Sayna Ebrahimi, and Hao Wang. Continual learning of large language models: A comprehensive survey. ACM Computing Surveys, 2024.
[238] Jiacheng Lin, Zhongruo Wang, Kun Qian, Tian Wang, Arvind Srinivasan, Hansi Zeng, Ruochen Jiao, Xie Zhou, Jiri Gesi, Dakuo Wang, et al. Sft doesn't always hurt general capabilities: Revisiting domain-specific fine-tuning in Ilms. arXiv preprint arXiv:2509.20758, 2025.
[239] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521-3526, 2017.
[240] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935-2947, 2017.
[241] Yichen Wu, Long-Kai Huang, Renzhen Wang, Deyu Meng, and Ying Wei. Meta continual learning revisited: Implicitly enhancing online hessian approximation via variance reduction. In The Twelfth international conference on learning representations, volume 2, 2024.
[242] Shipeng Wang, Xiaorong Li, Jian Sun, and Zongben Xu. Training networks in null space of feature covariance for continual learning. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, pages 184-193, 2021.
[243] Mehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li. Orthogonal gradient descent for continual learning. In International conference on artificial intelligence and statistics, pages 3762-3773. PMLR, 2020.
[244] Yichen Wu, Hongming Piao, Long-Kai Huang, Renzhen Wang, Wanhua Li, Hanspeter Pfister, Deyu Meng, Kede Ma, and Ying Wei. Sd-lora: Scalable decoupled low-rank adaptation for class incremental learning. In ICLR, 2025.
[245] Yan-Shuo Liang and Wu-Jun Li. Inflora: Interference-free low-rank adaptation for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23638-23647, 2024.
[246] Liyuan Wang, Jingyi Xie, Xingxing Zhang, Mingyi Huang, Hang Su, and Jun Zhu. Hierarchical decomposition of prompt-based continual learning: Rethinking obscured sub-optimality. Advances in Neural Information Processing Systems, 36:69054-69076, 2023.
[247] Shujun Xia, Haokun Lin, Yichen Wu, Yinan Zhou, Zixuan Li, Zhongwei Wan, Xingrun Xing, Yefeng Zheng, Xiang Li, Caifeng Shan, et al. Medrek: Retrieval-based editing for medical Ilms with key-aware prompts. In Socially Responsible and Trustworthy Foundation Models at NeurIPS 2025, 2025.
[248] Daniel Marczak, Bartłomiej Twardowski, Tomasz Trzciński, and Sebastian Cygert. Magmax: Leveraging model merging for seamless continual learning. In European Conference on Computer Vision, pages 379-395. Springer, 2024.
[249] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection for online continual learning. Advances in neural information processing systems, 32, 2019.
[250] Jihwan Bang, Heesu Kim, YoungJoon Yoo, Jung-Woo Ha, and Jonghyun Choi. Rainbow memory: Continual learning with a memory of diverse samples. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8218-8227, 2021.

<a id='8689fdf1-a34f-47d0-9a06-aaaeccbca91e'></a>

64

<!-- PAGE BREAK -->

<a id='c37c5b5a-5fe7-4dfc-82e3-395f7342c2e0'></a>

Adaptation of Agentic AI

<a id='2dee2c58-3ad6-40a6-b6a2-0fe6720251e6'></a>

[251] Quanziang Wang, Renzhen Wang, Yichen Wu, Xixi Jia, and Deyu Meng. Cba: Improving online continual learning via continual bias adaptor. In Proceedings of the IEEE/CVF international conference on computer vision, pages 19082-19092, 2023.
[252] Yichen Wu, Hong Wang, Peilin Zhao, Yefeng Zheng, Ying Wei, and Long-Kai Huang. Mitigating catastrophic forgetting in online continual learning by modeling previous task interrelations via pareto optimization. In Forty-first international conference on machine learning, 2024.
[253] Liyuan Wang, Xingxing Zhang, Kuo Yang, Longhui Yu, Chongxuan Li, Lanqing HONG, Shifeng Zhang, Zhenguo Li, Yi Zhong, and Jun Zhu. Memory replay with data compression for continual learning. In International Conference on Learning Representations, 2022.
[254] Quang Pham, Chenghao Liu, and Steven Hoi. Dualnet: Continual learning, fast and slow. Advances in Neural Information Processing Systems, 34:16131-16144, 2021.
[255] Elahe Arani, Fahad Sarfraz, and Bahram Zonooz. Learning fast, learning slow: A general continual learning method based on complementary learning system. In International Conference on Learning Representations, 2022.
[256] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 139-149, 2022.
[257] Zhiyi Shi, Binjie Wang, Chongjie Si, Yichen Wu, Junsik Kim, and Hanspeter Pfister. Dualedit: Dual editing for knowledge updating in vision-language models. arXiv preprint arXiv:2506.13638, 2025.
[258] Hongming Piao, Yichen Wu, Dapeng Wu, and Ying Wei. Federated continual learning via prompt-based dual knowledge transfer. In Forty-first International Conference on Machine Learning, 2024.
[259] Adarsh Kumarappan, Mo Tiwari, Peiyang Song, Robert Joseph George, Chaowei Xiao, and Anima Anandkumar. Leanagent: Lifelong learning for formal theorem proving. arXiv preprint arXiv:2410.06209, 2024.
[260] Howard Chen, Noam Razin, Karthik Narasimhan, and Danqi Chen. Retaining by doing: The role of on-policy data in mitigating forgetting. arXiv preprint arXiv:2510.18874, 2025.
[261] Gabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. Challenges of real-world reinforcement learning. arXiv preprint arXiv:1904.12901, 2019.
[262] Jonas Gehring, Kunhao Zheng, Jade Copet, Vegard Mella, Quentin Carbonneaux, Taco Cohen, and Gabriel Synnaeve. Rlef: Grounding code llms in execution feedback with reinforcement learning. arXiv preprint arXiv:2410.02089, 2024.
[263] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man&#233;. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.
[264] Javier Garcia and Fernando Fern&#225;ndez. A comprehensive survey on safe reinforcement learning. Journal of Machine Learning Research, 16(1):1437-1480, 2015.
[265] Victoria Krakovna, Jonathan Uesato, Vladimir Mikulik, Matthew Rahtz, Tom Everitt, Ramana Kumar, Jan Leike, and Shane Legg. Specification gaming: the flip side of ai ingenuity. DeepMind Blog, 2020.
[266] John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:50528-50652, 2024.
[267] Shangding Gu, Long Yang, Yali Du, Guang Chen, Florian Walter, Jun Wang, and Alois Knoll. A review of safe reinforcement learning: Methods, theories and applications. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.
[268] Kaiwen Zhou, Chengzhi Liu, Xuandong Zhao, Shreedhar Jangam, Jayanth Srinivasa, Gaowen Liu, Dawn Song, and Xin Eric Wang. The hidden risks of large reasoning models: A safety assessment of r1. arXiv preprint arXiv:2502.12659, 2025.
[269] Paul Kassianik and Amin Karbasi. Evaluating security risk in deepseek and other frontier reasoning models. Cisco Blogs, Cisco Systems, 31, 2025.
[270] W Daniel Hillis. Co-evolving parasites improve simulated evolution as an optimization procedure. Physica D: Nonlinear Phenomena, 42(1-3):228-234, 1990.
[271] David Manheim and Scott Garrabrant. Categorizing variants of goodhart's law. arXiv preprint arXiv:1803.04585, 2018.
[272] Alexander Bondarenko, Denis Volk, Dmitrii Volkov, and Jeffrey Ladish. Demonstrating specification gaming in reasoning models. arXiv preprint arXiv:2502.13295, 2025.

<a id='a9936ed6-05d4-479b-a499-aa5d972eacc3'></a>

65

<!-- PAGE BREAK -->

<a id='6cd86a76-6e3a-4c72-bb40-d2e606b56e3a'></a>

Adaptation of Agentic AI

<a id='e5955133-4658-4e44-93ab-e1f3a83b78f1'></a>

[273] Shuli Zhao, Qinsheng Hou, Zihan Zhan, Yanhao Wang, Yuchong Xie, Yu Guo, Libo Chen, Shenghong Li, and Zhi Xue. Mind your server: A systematic study of parasitic toolchain attacks on the mcp ecosystem. arXiv preprint arXiv:2509.06572, 2025.
[274] Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. Not what you've signed up for: Compromising real-world Ilm-integrated applications with indirect prompt injection. In Proceedings of the 16th ACM workshop on artificial intelligence and security, pages 79-90, 2023.
[275] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does Ilm safety training fail? Advances in Neural Information Processing Systems, 36:80079-80110, 2023.
[276] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In International conference on machine learning, pages 22-31. PMLR, 2017.
[277] Krishnan Srinivasan, Benjamin Eysenbach, Sehoon Ha, Jie Tan, and Chelsea Finn. Learning to be safe: Deep rl with a safety critic. arXiv preprint arXiv:2010.14603, 2020.
[278] Nathan Hunt, Nathan Fulton, Sara Magliacane, Trong Nghia Hoang, Subhro Das, and Armando Solar-Lezama. Verifiably safe exploration for end-to-end reinforcement learning. In Proceedings of the 24th International Conference on Hybrid Systems: Computation and Control, pages 1-11, 2021.
[279] Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Y Wu, and Zhifang Sui. Math-shepherd: A label-free step-by-step verifier for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935, 3, 2023.
[280] Víctor Gallego. Specification self-correction: Mitigating in-context reward hacking through test-time refinement. arXiv preprint arXiv:2507.18742, 2025.
[281] SHengjie Ma, Chenlong Deng, Jiaxin Mao, Jiadeng Huang, Teng Wang, Junjie Wu, Changwang Zhang, et al. Pou: Proof-of-use to counter tool-call hacking in deepresearch agents. arXiv preprint arXiv:2510.10931, 2025.
[282] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized Ilms. Advances in neural information processing systems, 36:10088-10115, 2023.
[283] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. In Forty-first International Conference on Machine Learning, 2024.
[284] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Nikos Karampatziakis, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adalora: Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint arXiv:2303.10512, 2023.
[285] Chongjie Si, Zhiyi Shi, Shifan Zhang, Xiaokang Yang, Hanspeter Pfister, and Wei Shen. Unleashing the power of task-specific directions in parameter efficient fine-tuning. In The Thirteenth International Conference on Learning Representations, 2024.
[286] Zhiyi Shi, Junsik Kim, Wanhua Li, Yicong Li, and Hanspeter Pfister. Mora: Lora guided multi-modal disease diagnosis with missing modality. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 273-282. Springer, 2024.
[287] Chongjie Si, Xuehui Wang, Xue Yang, Zhengqin Xu, Qingyun Li, Jifeng Dai, Yu Qiao, Xiaokang Yang, and Wei Shen. Flora: Low-rank core space for n-dimension. arXiv preprint arXiv:2405.14739, 10, 2024.
[288] Chongjie Si, Zhiyi Shi, Xuehui Wang, Yichen Xiao, Xiaokang Yang, and Wei Shen. Generalized tensor-based parameter-efficient fine-tuning via lie group transformations. arXiv preprint arXiv:2504.00851, 2025.
[289] John Schulman and Thinking Machines Lab. Lora without regret. Thinking Machines Lab: Connectionism, 2025. doi: 10.64434/tml.20250929. https://thinkingmachines.ai/blog/lora/.
[290] Liyuan Liu, Feng Yao, Dinghuai Zhang, Chengyu Dong, Jingbo Shang, and Jianfeng Gao. Flashrl: 8bit rollouts, full power rl, August 2025. URL https://fengyao.notion.site/flash-rl.
[291] Dan Peng, Zhihui Fu, and Jun Wang. Pocketllm: Enabling on-device fine-tuning for personalized Ilms. arXiv preprint arXiv:2407.01031, 2024.
[292] Liang Li, Xingke Yang, Wen Wu, Hao Wang, Tomoaki Ohtsuki, Xin Fu, Miao Pan, and Xuemin Shen. Mobillm: Enabling Ilm fine-tuning on the mobile device via server assisted side tuning. arXiv preprint arXiv:2502.20421, 2025.
[293] Xiaopei Chen, Liang Li, Fei Ji, and Wen Wu. Memory-efficient split federated learning for Ilm fine-tuning on heterogeneous mobile devices. arXiv preprint arXiv:2506.02940, 2025.

<a id='a6e5185f-00af-490a-b7dc-c156c585e590'></a>

66

<!-- PAGE BREAK -->

<a id='5801d598-f420-4880-bb36-28e0bbb871f2'></a>

Adaptation of Agentic AI

<a id='27499571-fd7c-49fd-bf01-355bec156943'></a>

[294] Mengwei Xu, Dongqi Cai, Yaozong Wu, Xiang Li, and Shangguang Wang. {FwdLLM}: Efficient federated finetuning of large language models with perturbed inferences. In 2024 USENIX Annual Technical Conference (USENIX ATC 24), pages 579-596, 2024.
[295] Vinay Venkatesh, Vamsidhar R Kamanuru, Lav Kumar, and Nikita Kothari. Edge-fit: Federated instruction tuning of quantized llms for privacy-preserving smart home environments. arXiv preprint arXiv:2510.03284, 2025.
[296] Guangji Bai, Yijiang Li, Zilinghan Li, Liang Zhao, and Kibaek Kim. Fedspallm: Federated pruning of large language models. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 8361-8373, 2025.
[297] Taiyi Wang, Zhihao Wu, Jianheng Liu, Jianye Hao, Jun Wang, and Kun Shao. Distrl: An asynchronous distributed reinforcement learning framework for on-device control agents. arXiv preprint arXiv:2410.14803, 2024.
[298] Weizhi Zhang, Xinyang Zhang, Chenwei Zhang, Liangwei Yang, Jingbo Shang, Zhepei Wei, Henry Peng Zou, Zijie Huang, Zhengyang Wang, Yifan Gao, Xiaoman Pan, Lian Xiong, Jingguo Liu, Philip S. Yu, and Xian Li. Personaagent: When large language model agents meet personalization at test time. In First Workshop on Multi-Turn Interactions in Large Language Models, 2025. URL https://openreview.net/forum?id=fgCOkyJG3f.
[299] Vinay Samuel, Henry Peng Zou, Yue Zhou, Shreyas Chaudhari, Ashwin Kalyan, Tanmay Rajpurohit, Ameet Deshpande, Karthik Narasimhan, and Vishvak Murahari. Personagym: Evaluating persona agents and llms. arXiv preprint arXiv:2407.18416, 2024.
[300] Xueyang Feng, Zhi-Yuan Chen, Yujia Qin, Yankai Lin, Xu Chen, Zhiyuan Liu, and Ji-Rong Wen. Large language model-based human-agent collaboration for complex task solving. arXiv preprint arXiv:2402.12914, 2024.
[301] Rafael Mendoza, Isabella Cruz, Richard Liu, Aarav Deshmukh, David Williams, Jesscia Peng, and Rohan Iyer. Adaptive self-supervised learning strategies for dynamic on-device llm personalization. arXiv preprint arXiv:2409.16973, 2024.
[302] Logan Cross, Violet Xiang, Agam Bhatia, Daniel LK Yamins, and Nick Haber. Hypothetical minds: Scaffolding theory of mind for multi-agent tasks with large language models. arXiv preprint arXiv:2407.07086, 2024.
[303] Run Luo, Lu Wang, Wanwei He, Longze Chen, Jiaming Li, and Xiaobo Xia. Gui-r1: A generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025.
[304] Jiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao, Junjie Cao, Zhengxi Lu, et al. Mobile-agent-v3: Fundamental agents for gui automation. arXiv preprint arXiv:2508.15144, 2025.
[305] Yuhang Liu, Zeyu Liu, Shuanghe Zhu, Pengxiang Li, Congkai Xie, Jiasheng Wang, Xueyu Hu, Xiaotian Han, Jianbo Yuan, Xinyao Wang, et al. Infigui-g1: Advancing gui grounding with adaptive exploration policy optimization. arXiv preprint arXiv:2508.05731, 2025.
[306] Yuqi Zhou, Sunhao Dai, Shuai Wang, Kaiwen Zhou, Qinglin Jia, and Jun Xu. Gui-g1: Understanding r1-zero-like training for visual grounding in gui agents. arXiv preprint arXiv:2505.15810, 2025.
[307] Yucheng Shi, Wenhao Yu, Zaitang Li, Yonglin Wang, Hongming Zhang, Ninghao Liu, Haitao Mi, and Dong Yu. Mobilegui-rl: Advancing mobile gui agent through reinforcement learning in online environment. arXiv preprint arXiv:2507.05720, 2025.
[308] Liujian Tang, Shaokang Dong, Yijia Huang, Minqi Xiang, Hongtao Ruan, Bin Wang, Shuo Li, Zhiheng Xi, Zhihui Cao, Hailiang Pang, et al. Magicgui: A foundational mobile gui agent with scalable data pipeline and reinforcement fine-tuning. arXiv preprint arXiv:2508.03700, 2025.
[309] Xinbin Yuan, Jian Zhang, Kaixin Li, Zhuoxuan Cai, Lujian Yao, Jie Chen, Enguang Wang, Qibin Hou, Jinwei Chen, Peng-Tao Jiang, et al. Enhancing visual grounding for gui agents via self-evolutionary reinforcement learning. arXiv preprint arXiv:2505.12370, 2025.

<a id='10a58285-a6eb-48ad-991a-d18da6c45810'></a>

67