<a id='48adc707-6e99-4f91-86ec-c5bb06ddc827'></a>

Measuring Agents in Production

<a id='eb2cd812-5347-4896-b274-f1fa22b1b26c'></a>

Melissa Z. Pan 1* Negar Arabzadeh 1* Riccardo Cogo 2 Yuxuan Zhu 3 Alexander Xiong 1 Lakshya A Agrawal 1
Huanzhi Mao 1 Emma Shen 1 Sid Pallerla 1 Liana Patel 4 Shu Liu 1 Tianneng Shi 1 Xiaoyuan Liu 1
Jared Quincy Davis 4 Emmanuele Lacavalla 2 Alessandro Basile 2 Shuyi Yang 2 Paul Castro 5 Daniel Kang 3
Joseph E. Gonzalez 1 Koushik Sen 1 Dawn Song 1 Ion Stoica 1 Matei Zaharia 1* Marquita Ellis 5 *

1 UC Berkeley
2 Intesa Sanpaolo
3 UIUC
4 Stanford University
5 IBM Research

<a id='babcd455-5622-4d4c-9799-926b143008d8'></a>

# Abstract
AI agents are already operating in production across many industries, yet there is limited public understanding of the technical strategies that make deployments successful. We present the first large-scale systematic study of AI agents in production, surveying 306 practitioners and conducting 20 in-depth case studies via interviews across 26 domains. We investigate why organizations build agents, how they build them, how they evaluate them, and what the top development challenges are. We find that production agents are typically built using simple, controllable approaches: 68% execute at most 10 steps before requiring human intervention, 70% rely on prompting off-the-shelf models instead of weight tuning, and 74% depend primarily on human evaluation. Reliability remains the top development challenge, driven by difficulties in ensuring and evaluating agent correctness. Despite these challenges, simple yet effective methods already enable agents to deliver impact across diverse industries. Our study documents the current state of practice and bridges the gap between research and deployment by providing researchers visibility into production challenges while offering practitioners patterns emerging from successful deployments.

<a id='7ac25a55-b4b5-4868-ab1c-2c408fe696d2'></a>

# 1. Introduction
Large language models (LLMs) have enabled a new class of software systems: **AI Agents**. We define AI agents as systems that combine foundation models with optional tools, memory, and reasoning capabilities to autonomously execute multi-step tasks [1-5]. Recent research has demonstrated exciting potential for AI agents across domains from drug discovery [6-8], algorithmic innovation [9-11] to gen-

<a id='3084bde6-3271-4246-83e3-c4b79bac3678'></a>

*Project Co-Leads.
Correspondence to: Melissa Z. Pan, Negar Arabzadeh, Marquita
Ellis <{melissapan, negara,mme} @berkeley.edu>.

<a id='86e08693-da48-4bba-ab62-9993f58a9013'></a>

<::horizontal bar chart::> The chart displays the percentage of responses for various categories, ordered from highest to lowest. The x-axis is labeled "% of Responses" and ranges from 0% to 100%. The y-axis lists the categories. Each bar also shows the exact percentage and the count in parentheses. The categories and their corresponding values are:
- Increasing Productivity: 72.7% (48)
- Reducing Human Task-Hours: 63.6% (42)
- Automating Routine Labor: 50.0% (33)
- Increasing Client Satisfaction: 37.9% (25)
- Reducing Human Training: 34.8% (23)
- Novel Technology: 33.3% (22)
- Reducing Interdisciplinary Expertise: 18.2% (12)
- Faster Failure Response Time: 18.2% (12)
- Risk Mitigation: 12.1% (8)
<::chart::>

<a id='ee2e9491-2b45-4703-b976-61d3087d42bf'></a>

Figure 1. Reasons practitioners build and deploy AI agents (N=66): increasing speed of task completion over the previous non-agentic (human or non-LLM automation) system (Increasing Productivity) is most often selected (73%), while improving operational stability (mitigating risk and accelerating failure recovery) is least often selected. Exact question and option descriptions are given in Appendix E.3 (N7). N=66 reflects the filtered subset of survey participants working on deployed agents who answered this question. Error bars indicate 95th-percentile intervals estimated from 1,000 bootstrap samples with replacements. The question was multi-select so participants were asked to select all/any benefits that applied to them and thus the proportions do not sum to 1.

<a id='0c1d3d3c-c6dc-44a0-8c06-29ad563296fa'></a>

eral AI scientist [12–14]. This excitement extends beyond research prototypes to production systems. Companies report agents actively running in diverse domains far beyond the initial coding focus [15–18], including finance [19–22], insurance [23–26], and education [27–29].

<a id='9d359393-2d3d-4119-9a89-7a764b4cd8b6'></a>

However, despite widespread excitement about agent po-
tential [30-33], many question their real value and success
in production [34-37]: whether agents can deliver on their
promise and where their future lies. For example, a re-
cent study reports that 95% of agent deployments fail [37].
This stark contrast between the promise of agents and their
high failure rate raises fundamental questions about what
separates successful deployments from failures.

<a id='cf8e4d0a-c689-477a-b69d-eeea058ce876'></a>

1

<a id='d4fd9c02-285b-49b7-bef9-3a94ba1d3955'></a>

arXiv:2512.04123v1 [cs.CY] 2 Dec 2025

<!-- PAGE BREAK -->

<a id='d310c838-3300-4c23-be1b-aec3bbd0bd9c'></a>

Measuring Agents in Production

<a id='4bd6ed7b-a8c9-4ac9-ab50-0898f9a6f50a'></a>

Unfortunately, little information is publicly available on how production agents are built. Why do some agents succeed while others fail? What requirements must agents meet for production deployment? We lack a systematic understanding of what methods enable successful agents in the real world. Researchers have little visibility into real-world constraints and practices. Are agents failing because models are not capable enough, or are there other factors at play? Without understanding how agents actually work in production, researchers risk addressing the wrong problems. Meanwhile, practitioners lack consensus on how to build reliable agents and would benefit from understanding how the industry approaches these fast-evolving systems.

<a id='7c5c9792-fe0c-42d2-9c47-4a6d896f21c9'></a>

To address this knowledge gap, we present Measuring Agents in Production (MAP), the first large-scale systematic study of AI agents in production. We study the practices of developers and teams behind successful real-world systems via four research questions (RQs):

RQ1. What are the applications, users, and requirements of agents?

RQ2. What models, architectures, and techniques are used to build deployed agents?

RQ3. How are agents evaluated for deployment?

RQ4. What are the top challenges in building deployed agents?

<a id='98fd5f02-78e6-4e4b-9194-6be627e06f2f'></a>

We answer these questions through an online survey with 306 responses and 20 in-depth interviews with agent development teams, capturing technical details of production agent development. Our survey respondents are practitioners actively building AI agents (Figure 4a) across 26 domains, from finance and healthcare to legal (Figure 2). We filter the survey data to 86 responses that explicitly reported systems in production or pilot phases,* which we denote as deployed agents, for the results in the main paper. The full dataset with all 306 responses appears in Appendix A. For in-depth interviews, we study 20 cases from teams building deployed agents with real users, spanning organizations from global enterprises to startups (Figure 3). Our case studies and survey capture deployed agent systems serving user bases ranging from hundreds to millions of daily users (Figure 4c), providing visibility into high-impact deployments. This dual approach provides the first empirically grounded view of the technical methods, architectural patterns, and operational practices behind AI agents in production.

<a id='5e4a8fca-2c9f-4439-a0db-95ff0e24f2b2'></a>

To provide first-hand data on working practices and real-
world constraints, our study focuses on practitioners actively

<a id='4e7aeca4-a807-4a12-820d-9a651afeea19'></a>

--- 
*Production systems are fully deployed and used by target end users in live operational environments. Pilot systems are deployed to controlled user groups for evaluation, phased rollout, or safety testing. Definitions for other stages appear in Appendix E.2 N5.

<a id='fd376233-88dc-46a4-88c3-6a3019c68a1c'></a>

<::transcription of the content: horizontal bar chart::>Horizontal bar chart showing the percentage of responses for different application domains where practitioners build and deploy Agentic AI systems. The x-axis is labeled '% of responses' ranging from 0% to 60%. The y-axis lists the domains, with each bar indicating a percentage and the count in parentheses. Each bar also displays an error bar.
- Finance & Banking: 39.1% (27)
- Technology: 24.6% (17)
- Corporate Services: 23.2% (16)
- Data Analytics: 13.0% (9)
- Research & Development: 11.6% (8)
- Software Development: 10.1% (7)
- Legal & Compliance: 8.7% (6)
- Customer Support: 8.7% (6)
- Healthcare Services: 8.7% (6)
- Retail: 4.3% (3)
- Other: 15.9% (11)

Figure 2. Application domains where practitioners build and deploy Agentic AI systems ($N$ = 69). Deployments span a broad range of industries, with the highest concentrations in finance, technology (including but not limited to software development), and corporate services. Additional lower-frequency domains ("other") are listed in Table 2, totaling roughly 26 distinct domains. Note that this is a multi-class classification question where each system may be assigned to multiple domain categories.

<a id='1d2cc723-93f7-4e8b-8156-d39c39c0abbf'></a>

building agents already deployed in production that serve
users and deliver measurable value (Figure 1). This natu-
rally biases toward successful deployments and experienced
practitioners. Our data capture what works today, providing
insight into how the field is developing.

<a id='7b4e239e-e822-4b99-ac24-3f908abe1a13'></a>

Our study reveals key findings for each research question:

<a id='bb9d6aba-a10c-4b4e-8724-92f8f3d13f63'></a>

_RQ1:_ **Productivity gains drive agent adoption.** We find that 73% of practitioners deploy agents primarily to increase efficiency and decrease time spent on manual tasks (Figure 1). Most systems (93%) serve human users rather than other agents or systems (Figure 5a). Regarding requirements, teams prioritize output quality over real-time responsiveness: 66% allow response times of minutes or longer, compared to 34% requiring sub-minute latency (Figure 5b). In-depth interviews reveal that organizations tolerate this latency because agents that take minutes to execute still outperform human baselines.

<a id='424442a0-7716-4e2e-ad9b-51a6a0fda5d1'></a>

RQ2: Simple methods and workflows dominate. 70% of interview cases use off-the-shelf models without weight tuning (Figure 6), relying instead on prompting. Teams predominantly select the most capable, expensive frontier models available, as the cost and latency remain favorable compared to human baselines. We find that 79% of surveyed deployed agents rely heavily on manual prompt construction (Figure 7), and production prompts can exceed 10,000 tokens. Production agents favor well-scoped, static work-

<a id='7c1868bc-1fa3-40ab-bee3-8dd32320b33c'></a>

2

<!-- PAGE BREAK -->

<a id='b8f918e3-0c3b-4ec3-91c0-ac1e102472f1'></a>

Measuring Agents in Production
---

<a id='378e49ee-6eac-4c11-b2b1-1d2536a7f0df'></a>

flows: 68% execute at most ten steps before requiring hu-man intervention, with 47% executing fewer than five steps. Furthermore, 85% of detailed case studies forgo third-party agent frameworks, opting instead to build custom agent ap-plication from scratch. Organizations deliberately constrain agent autonomy to maintain reliability.

<a id='b7c1141e-72a6-4d57-ab1a-97a88f62043a'></a>

RQ3: Human verification remains central to evaluation. The majority of deployed survey agents (74%) rely primarily on human-in-the-loop evaluation, and 52% use LLM-as-a-judge (Figure 10b). Notably, every interviewed team utilizing LLM-as-a-judge also employs human verification. Because production tasks are highly domain-specific, public benchmarks are rarely applicable. Consequently, 25% of teams construct custom benchmarks from scratch, while the remaining 75% evaluate their agents without formal benchmarks, relying instead on online-tests such as A/B testing or direct expert/user feedback. This pattern reflects the difficulty of automated evaluation for bespoke production tasks.

<a id='32de3e03-f18d-45df-82f1-1f6772579060'></a>

RQ4: Reliability is an unsolved challenge. Practitioners focus most on ensuring agent reliability, spanning correctness, latency, and security. The evaluation challenges described in RQ3 directly impact the ability to verify correctness to achieve reliable deployments. Latency impacts only a small subset (15%) of applications as a deployment blocker, and security represents a manageable concern that most deployed agents mitigate through action and environment constraints.

<a id='56a98a4d-80b0-4d48-8142-2c0d3df9004f'></a>

Our findings reveal that reliability concerns drive practitioners toward simple yet effective solutions with high controllability, including restricted environments, limited autonomy, and human oversight. For example, developers choose human-in-the-loop evaluation over fully automated techniques and manual prompt engineering over automated methods because these approaches offer control, transparency and trustworthiness. Practitioners deliberately trade-off additional agent capability for production reliability, and this design pattern already enables a broad spectrum of applications delivering real-world value (Figure 1).
Production agents represent an emerging engineering discipline. Our study documents the current state of practice and bridges the gap between research and deployment: we provide researchers visibility into real-world constraints and opportunities while offering practitioners proven patterns from successful deployments across industries.

<a id='97d02f8c-f2c2-485c-bb59-ae685f693bbe'></a>

## 2. Related Work
To our knowledge, we offer the first technical characterization of how agents running in production are built.

<a id='5b43da50-9f53-40a9-aa4a-9e114048fd2f'></a>

Table 1. Overview of application and their domains from our 20 case studies. To maintain clarity and confidentiality, similar use cases are aggregated into representative descriptions.

<a id='e45e59fe-73fb-4122-b1da-a438f8f2fe38'></a>

<table id="2-1">
<tr><td id="2-2">Business Operations</td></tr>
<tr><td id="2-3">Insurance claims workflow automation Customer care internal operations assistance Human resources information retrieval and task assistance</td></tr>
<tr><td id="2-4">Communication Tech, Multi-lingual Multi-dialect</td></tr>
<tr><td id="2-5">Communication automation services Automotive communication services</td></tr>
<tr><td id="2-6">Scientific Discovery</td></tr>
<tr><td id="2-7">Biomedical sciences workflow automation Materials safety and regulatory analysis automation Chemical data interactive exploration</td></tr>
<tr><td id="2-8">Software &amp; Business Operations</td></tr>
<tr><td id="2-9">Data analysis and visualization Enterprise cloud engineer and business assistance Site reliability incident diagnoses and resolution Software products technical question answering</td></tr>
<tr><td id="2-a">Software DevOps</td></tr>
<tr><td id="2-b">Spark version code and runtime migration Software development life cycle assistance end-to-end Software engineer/developer slack support SQL optimization Code auto-completion and syntax error correction</td></tr>
</table>

<a id='f1d14b3d-bb8b-4f9f-a87f-56a7217663bf'></a>

Commercial Agent Surveys. Several prior efforts exam-ine AI (agent) adoption in production from adjacent per-spectives. MIT Media Lab and NANDA Initiative [37] and Challapally et al. [38] study economic viability and executive views on return-on-investment from companies attempting to integrate agents. Shome et al. [36] analyze marketing materials for 102 commercial agents and conduct user studies with 31 participants, revealing gaps between promised and realized capabilities. Industry reports [33, 39-42] focus on organizational readiness and market trends. LangChain [43] surveyed over 1,300 professionals on agent motivations, characteristics, and challenges.

<a id='dae6d6a6-d6a0-442a-bd66-2fdd93b3d256'></a>

Our work differs in two key ways: (i) Scope: we study agents actively operating in production; and (ii) Focus: we collect engineering-level technical data from practitioners. Our study complements these prior works and offers a new perspective into successful agents, providing insights into where the field is heading.

<a id='804d8a01-1ebc-4706-80a1-37f23e13072e'></a>

Research Agent Literature Survey. Many prior work examine LLM-powered agents from an academic perspec- tive [44-49], providing valuable taxonomies of agent de- signs and tracing the evolution of key techniques. Other agent surveys focus on specific aspects: evaluation method- ologies [50, 51], security concerns [52], and multi-agent systems [53]. In contrast, our work is an empirical study that collects primary data directly from practitioners build- ing and operating deployed agents. We do not synthesize published research via literature reviews; instead, we con- duct original survey data collection and in-depth case study

<a id='5ccb9ba9-9be9-44f8-9f5f-073b8c7831c3'></a>

3

<!-- PAGE BREAK -->

<a id='0cf4023a-3d6d-4d43-8f69-c6d77c856b83'></a>

Measuring Agents in Production
---

<a id='7b565675-0924-4b97-ad4d-c3f1ecd46c9c'></a>

interviews to document production practices.

**Single-System Studies.** Companies publish papers and technical blogs on agentic systems [54–62] and open-source agent implementations [63–67], offering glimpses into production-related agents. Each paper naturally focuses on a single system or domain, creating an information gap: we lack understanding of common patterns, shared challenges, and design principles across deployments. We address this gap by surveying professionals across diverse industries to identify recurring engineering practices, architectural decisions, and challenges.

<a id='4327a3de-1ff0-47f8-949c-084bf353a7d0'></a>

### 3. Methodology

To understand how production AI agents are built and deployed, we conducted two complementary studies: a large-scale public survey and 20 in-depth case study interviews. We detail our survey design and distribution in **Section 3.1**, describe our interview protocol and case study selection in **Section 3.2**, and outline our data processing pipeline in **Section 3.3**.

<a id='7494f233-90e1-45f1-ac89-5f067b6decb4'></a>

## 3.1. Public Online Survey

We designed and conducted a survey for practitioners building AI agents, with a strong focus on the technical details of their systems. The survey covers system architecture, evaluation, deployment, and operational challenges through 47 questions (listed in full in Appendix E). To capture AI Agents as practitioners understand them, we invited respondents to describe systems they refer to as AI Agents or Agentic AI without imposing a prescriptive definition.

<a id='28f42428-93ea-48a7-90b1-06bae7bcb96f'></a>

To ensure relevance, we implemented the survey with dynamic branching logic in Qualtrics, where certain questions only appear based on previous responses. For example, the question about the realized benefits of using agents is only shown to respondents who confirmed they chose agentic over non-agentic solutions (O3.1.1 and O3.1 in Appendix E.3). This adaptive structure, combined with the optional nature of all questions, means each question receives a different number of responses. We report the specific sample size (N) for each result throughout Sections 4–7 in figure captions. We asked participants to focus their responses on a single system if they contributed to more than one agent system (Acknowledgment E.1). Full questionnaire details are available in Appendix E, including the branching logic in Figures 26 and 27.

<a id='a2e5421d-24c8-4172-971a-bbfc23b3f6e0'></a>

We distributed the survey through multiple channels to reach practitioners across the AI agent ecosystem: the Berkeley RDI Agentic AI Summit [68], the AI Alliance Agents-in- Production Meetup [69], the UC Berkeley Sky Retreat [70], and professional networks including LinkedIn, Discord, and X. We release the survey on July 28, 2025, with data collec-

<a id='e6f7f016-7ebe-4164-bcc1-82d4a41b9ab2'></a>

<::chart: horizontal bar chart::>
Company Maturity Level
Mature: 10
Late: 5
Growth: 1
Early: 3
Seed: 1
Number of case studies

Figure 3. The distribution of source institution maturities across in-depth interview-based case studies. The minority (5/20) are from seed-stage startups (validating product-market fit), early-stage startups (proving scalable business models), and growth-stage startups (rapidly expanding market share and operations). The majority (15/20) are from late-stage and mature institutions (with established market positions). The stages are approximated from limited public information e.g. size, sector, and annual recurring revenue.
<::/chart::>

<a id='87a363d5-e114-41f2-a9ab-f2081bbae4ec'></a>

tion for this paper ending October 29, 2025.

We received 306 valid responses. 294 participants indicated that they have directly contributed to the building and de-signing of at least one agent system. Our survey respondents are predominantly technical professionals such as software and machine learning engineers (Figure 4a). Among 111 respondents who reported their system's deployment stage, 82% indicated their systems are in production or pilot phases (Figure 4b), demonstrating rapid transition from experimen-tal prototypes to real-world deployments.

<a id='db81f02e-e7ec-48b2-9485-536d47981537'></a>

To maintain focus on production systems, we filtered the data to 86 responses that explicitly reported their systems as being in production or pilot phases. Production systems are fully deployed and used by target end users in live op- erational environments, while pilot systems are deployed to controlled user groups for evaluation, phased rollout, or safety testing. This filtering excludes development proto- types, research artifacts, and retired systems, as well as participants who did not report their deployment stage. The definitions for all deployment stages appear in Appendix E.2 N5. We denote production and pilot systems as deployed agents. All survey statistics presented in this paper refer to deployed agents unless otherwise stated. Complete data for 306 valid responses across all deployment stages appears in Appendix A.

<a id='166b836f-20be-412c-8540-c7ab0e9e183b'></a>

### 3.2. In-depth Case Studies
To add qualitative depth to our survey findings, we conducted 20 in-depth case studies through interviews with teams building deployed agents across a diverse range of organizations.

<a id='a76d208c-1a47-4fe6-bcfd-da14ff7a9985'></a>

We carefully selected our 20 cases to achieve representative

<a id='5fcb657a-74fe-4598-97b4-927bd99a5d6c'></a>

4

<!-- PAGE BREAK -->

<a id='e9382295-aae6-4fe4-9678-eae3d52b9dfb'></a>

# Measuring Agents in Production

<a id='6c857d2a-71bc-47c3-8f9b-9329d7eef5ce'></a>

<::bar chart::>Figure 4. Overview of survey respondent and system characteristics across all agents the survey data: (a) roles of survey participants by primary contribution area (N=108), (b) deployment stages of Agentic AI systems (N=111) that survey participants contributed to, and (c) reported number of end users (N=35) for the Agentic systems survey participants contributed to. Chart (a) shows roles of survey participants by primary contribution area: Software & ML Engineers 50.9% (55), Technical Executive & Managers 31.5% (34), Academic Researchers 9.3% (10), Infrastructure Engineers 4.6% (5), Technical Writers & Learning Professionals 3.7% (4). Chart (b) shows deployment stages of Agentic AI systems: In Production 45.0% (50), Pilot Deployment 32.4% (36), Prototype 19.8% (22), Research 1.8% (2), Retired 0.9% (1). Chart (c) shows reported number of end users: 1-10: 14% (5), 10-100: 5% (2), 100-500: 23% (8), 500-1000: 20% (7), 1000-10k: 11% (4), 10k-100k: 17% (6), 100k-1m: 6% (2), 1m+: 3% (1).<::>

<a id='fc291bc1-f0d3-4455-9160-a666efc571a7'></a>

samples across application diversity, organizational matu-
rity, and global reach. We only interviewed systems with
_real-world users_: 14 cases are in full production and 6
cases are in final pilot phases. The cases span five key sec-
tors: business operations (3 cases), software development
and operations (7 cases), integrated business and software
operations (5 cases), scientific discovery (3 cases), and com-
munication services (2 cases). Notably, these deployments
extend well beyond commonly known coding agents or gen-
eral chatbots, demonstrating the breadth of production agent
applications. The systems target both internal employees (5
cases) and external enterprise consumers (15 cases). Our
selection includes organizations across maturity levels, from
seed-stage startups to large enterprises with global footprints
(Figure 3). For companies with multiple agent deployments,
we presented only distinct use cases to maximize application
diversity. The anonymized case studies and their applica-
tion domains appear in Table 1, with additional details in
Appendix D.

<a id='4528d5ef-7f84-436e-9303-5f9b6dcc0f70'></a>

Each interview lasted 30 to 90 minutes and was conducted
by teams of 2 to 5 organizationally neutral interviewers. We
followed a semi-structured interview protocol covering 11
topic areas (detailed in Appendix D.1), including system ar-
chitecture, evaluation mechanisms, deployment challenges,
operational requirements, and measurable agent values. We
anonymized and recorded interviews based on participant
preferences, with human note-takers capturing insights. To
ensure accuracy, we cross-validated final summaries among
all interviewers. Per our confidentiality agreements, we
anonymized all data and present findings in aggregate.

<a id='68612ef3-d8eb-4f9d-9898-9331f1aa1470'></a>

### 3.3. Data Processing
Most survey questions use structured formats (single-select, multi-select, or numeric), requiring minimal post-processing. For the free-text domain keyword responses used in Figure 2, we used LOTUS [71], a state-of-the-art unstructured data processing tool, to identify common do-

<a id='e9ddd1cc-f9b7-48d7-9f13-2f6c31d87878'></a>

main categories and perform classification. This allowed us to normalize phrases from survey responses to represen- tative label sets. For instance, we grouped responses like "healthcare," "medical," and "patient monitoring" under a unified "healthcare" category. Details of this normalization process appear in Appendix B.1. All other figures present results from structured survey questions or interview data without requiring automated processing.

<a id='757ce315-fd6e-4c72-9959-d4db0e2b4de0'></a>

As described in Section 3.1, our survey data in the main
paper are filtered to deployed (production and pilot) agents,
and our interviews specifically select teams building de-
ployed agents. All results presented in Sections 4–7 refer to
deployed agents from either survey responses or interviews,
which we explicitly denote throughout the paper. Refer to
Appendix A for unfiltered full data.

<a id='035350b7-e831-4e7b-8ca4-a215ad642fa3'></a>

For all figures that include error bars, we report 95% con-
fidence intervals computed using 1,000 bootstrap samples
with replacement.

<a id='5979a52f-b8cf-4789-ad4b-7322e463ee12'></a>

## 4. RQ1 Results: What Are The Applications, Users, and Requirements of Agents?

We now present findings from our survey and case study interviews across four central research questions. We start by examining motivations for agent adoption (Section 4.1), which agent applications successfully reach deployment (Section 4.2), who uses these systems (Section 4.3), and what latency requirements shape their design (Section 4.4). Understanding these patterns reveals where agentic systems deliver practical value and how they transform real-world applications.

<a id='26f8383d-1fba-4af0-80c7-3a60334760fe'></a>

## 4.1. Motivations for Choosing Agents

Among practitioners with deployed agents who evaluated alternatives, 82.6% prefer the agentic solution for deployment. We define non-agentic alternatives as existing software sys-

<a id='da0c42aa-ac0b-467a-8147-221356870348'></a>

5

<!-- PAGE BREAK -->

<a id='e85ddb68-1905-4226-a14c-dd9a19a83b11'></a>

Measuring Agents in Production
---

<a id='fc238a26-0aa6-41ea-a02f-318c0318e650'></a>

tems, traditional approaches, or human execution. Figure 1 details the specific benefits reported by these practitioners. The top three drivers all target the reduction of human effort: increasing productivity and efficiency (72.7%), reducing human hours (63.6%), and automating routine labor (50.0%). Conversely, qualitative benefits rank lower. Increasing user satisfaction ranks in the middle tier (37.9%), followed by reducing required human expertise and training (34.8%) and enabling novel technology (33.3%). Accelerating failure response times (18.2%), reducing interdisciplinary knowledge requirements (18.2%), and risk mitigation (12.1%) rank last.

<a id='5af6628b-d36e-45bd-b467-792c4df28b79'></a>

These priorities reflect pragmatic deployment realities. Organizations adopt agents to solve immediate operational problems, such as expert-expensive manual work and insufficient staffing capacity. Productivity gains are straightforward to quantify through human-hour reductions, whereas safety improvements and risk mitigation are harder to verify. This focus aligns with our findings on internal deployments (Section 4.3), where organizations tolerate higher error rates by maintaining human oversight to catch these errors.

<a id='142b7110-3054-42d8-9465-8333f1f78114'></a>

The top reported reasons for using agents reveal a trend
where certain objectives are more verifiable and measurable.
For example, time to complete a task (productivity) is con-
crete and quantifiable, while risk mitigation benefits takes
longer to verify. We examine this measurement gap and its
implications for deployed agents in Section 7.1.

<a id='f4c4b879-465a-4d50-a042-6a2068484fd6'></a>

RQ 1 Finding #1: Productivity gains through automating routine human tasks drive agent adoption (73%), while harder-to-verify applications like risk mitigation are less common.

<a id='5fe0e468-7ac2-43ca-b95d-c100d6c3e0da'></a>

## 4.2. Application Domains

We find that agents operate in diverse industries well beyond software engineering. Figure 2 shows that the three domains with the highest number of reported deployments in survey are Finance & Banking (39.1%), Technology (24.6%), and Corporate Services (23.2%). We also observe a long tail of applications in Data Analytics (13.0%), Research & De- velopment (11.6%), and other specialized domains (15.9%). This distribution demonstrates that agentic systems deliver practical value across fundamentally different industries.

<a id='8bf19cd2-83cb-473d-8878-5b2581449109'></a>

RQ 1 Finding #2: Deployed agents already operate across 26 diverse domains, well beyond math and coding that are popular in research, demonstrating value across different industries.

<a id='3090173b-319a-4b03-99c7-5e1aaddc62fa'></a>

## 4.3. Users of Agents
We find that deployed agents primarily serve human users directly. Survey data indicates that 92.5% of deployed sys-

<a id='4297208b-7f42-4c9d-90ad-4d3625d85b41'></a>

tems serve humans rather than other agents or software systems. As shown in Figure 5a, internal employees are the primary user base (52.2%), followed by external customers (40.3%). Only 7.5% of deployed systems serve non-human consumers.

<a id='e3416c12-226a-4e1c-81c4-e95b6f47c403'></a>

The focus on internal users is a deliberate deployment
choice. Detailed case studies reveal that organizations re-
strict deployments to internal environments to mitigate un-
solved reliability and security concerns. Internal users oper-
ate within organizational boundaries where agent mistakes
have lower consequences and human oversight is readily
available. An example from our case study, internal Pager-
Duty agents respond to employee requests, while human
engineers can take over when needed.

<a id='29aae936-fdea-4128-b18c-34fdea9dee13'></a>

Furthermore, most systems—including external-facing ap-
plications—require domain-specific knowledge to operate.
For example, insurance authorization agents support nurses,
and incident response agents support site reliability engi-
neers. This reflects a pattern where agents function as tools
that *augment* domain experts rather than replace them. This
paradigm also enables human users to serve as the final
verifiers of agent outputs, which we discuss further in Sec-
tion 6.2.

<a id='a1bd25dc-c3bd-4606-8a8a-ff510558adbe'></a>

Beyond user type, we examine the scale of the user base. We find that end-user counts for deployed systems vary significantly. As shown in Figure 4c, 42.9% of deployments serve user bases in the hundreds. However, we also observe high-traffic deployments (25.7%) serving tens of thousands to over 1 million users daily, representing substantial user impact or possibly mature systems.

<a id='addf111d-a332-4899-9e9d-c6f8525ef8bc'></a>

RQ1 Finding #3: Deployed agents primarily serve human end-users (92.5%), which enables close human-oversight.

<a id='eae4be98-1f40-4316-a941-fc782024a659'></a>

## 4.4. Latency Requirements

Relaxed latency requirements are common among deployed agents. Figure 5b shows the distribution of maximum allowable end-to-end latency. Minutes is the most common target, followed by seconds. Notably, 17.0% report no defined limit yet, and 1.9% allow hours to days.

<a id='07ee3a1c-693c-4d45-97fa-dba98a544879'></a>

The latency tolerance reflects the productivity focus from Section 4.1. Agents are often used to automate tasks that typically take humans hours to complete. Consequently, an agent taking multiple minutes to respond remains orders of magnitude faster than non-agentic baselines. Interview participants emphasized this advantage: even if an agent takes five minutes, that remains more than 10x faster than assigning the task to a person on the team, especially when staffing shortages exist and the task is secondary to the user's core responsibilities. Examples include nurses examining

<a id='a57ae340-05ab-4619-96ca-3f391e5fb084'></a>

6

<!-- PAGE BREAK -->

<a id='b0466d50-c012-4906-b3f4-e16af1d9f53e'></a>

# Measuring Agents in Production

<a id='f780699e-ae5e-4cb4-b4be-1cc9625b0d41'></a>

<::bar charts::>Figure 5 consists of two bar charts. Chart (a) is titled "Distribution of primary end users" and shows the percentage of responses for different end-user categories. The y-axis lists the categories: "Internal Employees", "External Customers", "Non-Agentic Software", and "Other AI Agents". The x-axis is "% of Responses" ranging from 0% to 80%. A legend indicates that hatched bars (///) denote "Human user" and solid bars denote "Non-human user". The data points are: Internal Employees: 52.2% (35) (Human user); External Customers: 40.3% (27) (Human user); Non-Agentic Software: 4.5% (3) (Non-human user); Other AI Agents: 3.0% (2) (Non-human user). Chart (b) is titled "Reported tolerable end-to-end response latency for deployed systems" and shows the percentage of systems for different latency tolerance levels. The x-axis is "Latency Tolerance" with categories: "< Subsecond", "Seconds", "Minutes", "Hours", ">1 day", and "No limit set". The y-axis is "Percentage (%)" ranging from 0% to 60%. The data points are: < Subsecond: 7.5% (4); Seconds: 26.4% (14); Minutes: 41.5% (22); Hours: 5.7% (3); >1 day: 1.9% (1); No limit set: 17.0% (9).

Figure 5. Overview of Agentic AI deployment characteristics in terms of primary end users and latency requirements: (a) Distribution of primary end users ($N=67$), where hatched bars (///) denote human end-users and solid bars denote non-human end-users; and (b) Reported tolerable end-to-end response latency for deployed systems ($N=53$). Over 92% of deployed agents primarily serve human users, and most systems tolerate response times on the order of minutes rather than requiring strict real-time responsiveness. 

<a id='06b8a69f-444d-4732-bc0c-5d143afdc43e'></a>

insurance details and software engineers responding to in-
ternal pager duty. Some deployed agents from case studies
even batch requests hourly or overnight, further indicating
latency is not a primary constraint.

<a id='91c653c0-0b9a-4f2c-9c11-4e5b8ff2ff5c'></a>

However, this pattern breaks for real-time interactive applications. For example, practitioners building voice-driven systems report latency as their top challenge (Section 7.2) during detailed case study. These systems compete against human conversation speeds rather than task completion baselines. Among our 20 detailed case studies, only 5 require real-time responsiveness. The remaining 15 cases tolerate extended processing times: 7 involve human review with relaxed timing, 5 operate as asynchronous background processes, and 3 have hybrid operation patterns. For these systems, processing times of minutes remain acceptable because the alternative is days of human effort.

<a id='66593873-65e7-40ae-b149-65d6779c16ea'></a>

$\mathcal{R}$$\mathcal{Q}$ 1 Finding #4: Development teams prioritize agent output quality and capability by concentrating on latency-relaxed applications.

<a id='671083dc-b409-49d4-b012-8e5de2e91ff1'></a>

**5. *RQ2 Results*: What Models, Architectures, And Techniques Are Used To Build Agents?**

Having established *what* problems practitioners target with agentic systems, we now address *how* these systems are built. We examine five critical implementation decisions: model selection, model weights tuning, prompt construction, agent architectures, and development frameworks.

<a id='c771ca04-f435-4606-ba89-6d49f860c0e7'></a>

Overall, practitioners favor established, straightforward
methods over stochastic or training-intensive techniques.
We find that methods popular in research—such as fine-

<a id='781ec02c-db27-4e35-a359-205bc851d07a'></a>

tuning, reinforcement learning, and automated prompt op-
timization—remain uncommon in deployment. Instead,
teams prioritize control, maintainability, and iteration speed.

<a id='ba49bac7-27a7-4bbd-8ad5-39de0ca6b007'></a>

## 5.1. Model Selection

We find that deployed agents rely heavily on proprietary models. Only 3 of 20 detailed case studies use open-source models (LLaMA 3 [72], Qwen [73], and GPT-OSS [74]), while the remaining 17 rely on closed-source frontier models (Figure 6). Of the teams using closed-source models, 10 explicitly confirm using the Anthropic Claude [75] or OpenAI GPT [76] families. Specific models mentioned during interviews include Anthropic Claude Sonnet 4 and Opus 4.1, and OpenAI o3, each representing the state-of-the-art model from each provider at the time of the interview. While other participants did not disclose specific models, most interviewees followed the pattern of using the largest and most capable state-of-the-art models as the primary model for their agents from each model family.

<a id='d20a3c49-4714-47ee-9b4f-6e266e6ec3f7'></a>

We find that open-source adoption is rare and is driven by specific constraints rather than general preference. Among the three cases using open-source models, motivations include high-volume workloads where inference costs at scale are prohibitive, and regulatory requirements preventing data sharing with external providers. For example, one team from detailed case study serve continuous infrastructure maintenance agent leverages the company's existing internal compute resources (e.g., GPUs) to serve a fine-tuned open-source model to meet cost constraints.

<a id='7272740b-8008-43be-9335-28dd863c7833'></a>

For the majority of cases, model selection follows a prag-
matic, empirical approach focused on downstream perfor-
mance. Interviewees report that they test the top accessible

<a id='dc232322-ac07-4083-90ad-dd5a0c62118e'></a>

7

<!-- PAGE BREAK -->

<a id='0478c687-9e04-4e42-b6de-9c5947c3e428'></a>

Measuring Agents in Production

<a id='a6d45a01-f22c-47e4-831c-c1ed0b762bf8'></a>

<::chart: Stacked bar chart titled "Distribution of model characteristics of case-study systems from our interviews (N=20)".

The Y-axis is labeled "Number of case studies" and ranges from 0 to 20.

There are two main categories on the X-axis: "Open source" and "Post training".

**Open source bar:**
- The bottom segment, labeled "Yes(3)", represents 3 case studies.
- The top segment, labeled "No(17)", represents 17 case studies.

**Post training bar:**
- The bottom segment, labeled "Yes(6)", represents 6 case studies.
- The top segment, labeled "No(14)", represents 14 case studies.

Figure 6. Distribution of model characteristics of case-study systems from our interviews (N=20). The left bar chart shows model source openness; “Yes” indicates open-source model usage while “No” indicates closed-source or non-disclosed model usage. The right bar chart shows whether post-training (e.g. fine-tuning, reinforcement learning) is utilized (“Yes”); “No” labels non-use or non-disclosure. Overall, most case-study systems rely on off-the-shelf closed-source models, with comparatively fewer teams using open-source models or performing additional post-training.::>

<a id='23a6d1ec-011b-4352-987b-b6fb75479370'></a>

state-of-the-art models for each task and select based on per-
formance. Unlike the high-volume open-source use cases,
these teams note that runtime costs are negligible compared
to the human experts (e.g., medical professionals, senior
engineers) that the agent augments. Consequently, they de-
fault to the best-performing closed-source models regardless
of inference cost. Additionally, 4 out of 20 detailed case
studies combine LLMs with specialized off-the-shelf mod-
els (e.g., text-to-speech, chemistry foundation models) to
handle specific modalities.

<a id='5a29c7c0-b006-445e-8628-0e21476b5f99'></a>

RQ2 Finding #1: Deployed agents predominantly rely on proprietary frontier models; open-source models are used primarily to satisfy cost or regulatory constraints.

<a id='074d25e8-5cb1-41e5-8a7b-1389f8f70ff7'></a>

Number of Distinct Models. While a substantial portion rely on a single model, the majority coordinate multiple models to meet functional or operational needs. Survey results show that 40.9% of deployed agents use exactly one model, while 27.3% use two, 18.2% use three, and 13.6% use four or more. Among detailed case studies, 10 out of 20 (50%) combine models to address specific functional needs. We identify two drivers: cost optimization and modality. First, teams combine models of varying sizes to balance latency, cost, and quality. For example, one agent workflow from case study routes simple subtasks like pattern recogni- tion to smaller models while reserving larger models for sub- tasks requiring higher reasoning capabilities. Second, teams integrate models to handle distinct data modalities. Com- munication agents leverage text-to-speech models alongside LLMs, while scientific agents employ domain-specific foun- dation models (e.g., chemistry) alongside general-purpose

<a id='e4a89b1d-53d9-4a2d-85d2-9e4a30ef729d'></a>

___ †Rankings based on public model leaderboards at the time of development.

<a id='5e8e2575-9ad6-4954-80b7-ca401a25a71a'></a>

reasoning models.

*Operational constraints* also drive multi-model adoption.
We find that multi-model architectures can emerge from
lifecycle management needs rather than complex reasoning
requirements for the agent task. Detailed case studies reveal
that teams maintain multiple models to manage agent's be-
havioral shifts from model migration. Organizations often
run legacy models alongside newer versions because agent
scaffolds and evaluation suites depend on the specific be-
haviors of the older model, where sudden updates might
degrade output quality. Additionally, governance policies
enforce teams to route subtasks to different model endpoints
based on user or developer access levels. Thus, architectural
complexity often reflects strategic operations rather than
task difficulty.

<a id='85bedb66-fa0e-464d-b875-1e2bc8db6e3d'></a>

Interestingly, we observe a heavier tail towards agents us-ing more distinct models when we examine the full survey data, including prototyping and research agents that have not yet been deployed (Figure 16a). Deployed agents are more likely to converge on fewer number of distinct models compared to non-deployed agents, suggesting that teams explore richer multi-model combinations during early exper-imentation but consolidate onto a smaller set of models as they move toward deployment. We hypothesize this might be also reflecting the additional operational burden of main-taining many distinct model endpoints.

<a id='fc672bed-6783-4552-99e5-a280500cf86e'></a>

**RQ2 Finding #2:** The majority of agents coordinate multiple models, driven not only by functional needs like modality but also by operational requirements such as model migration.

<a id='c8a3d61d-f309-4d6a-b19c-3a48c66d4780'></a>

## 5.2. Model Weights Tuning
We observe a strong preference for prompting over model weight updates in deployed agents. We find that 14 out of 20 (70%) detailed case studies rely solely on off-the-shelf models without supervised fine-tuning (SFT) or reinforcement learning (RL) (Figure 6). Additionally, 2 teams from detailed case studies explicitly report that foundation model capabilities already meet their target use case, making fine-tuning unnecessary.

<a id='e30bed45-67fe-47d8-8ba3-2800040ed0c8'></a>

Only 5 of 20 detailed case studies actively use SFT. These teams target deployment in business-specific corporate contexts where leveraging highly contextual information improves downstream performance. For example, customer product support agents benefit from fine-tuning on specific product offerings and policies. However, performance gains do not always justify the development overhead. Among the 5 detailed case studies actively using fine-tuned models, 3 consider SFT essential, while 2 apply it selectively for enterprise clients where customization requirements justify the additional cost. Three additional teams from interview

<a id='c6780ee8-6282-4c38-afb9-a246cd4c79f9'></a>

8

<!-- PAGE BREAK -->

<a id='68afb0f6-40f2-41f6-b729-9bce6cd0a314'></a>

Measuring Agents in Production
---

<a id='b46320fd-8234-40d6-a379-662ce9365e2f'></a>

case studies express strong interest in future adoption for
similar reasons. Notably, we find that 4 of the 5 detailed
case studies that employ SFT do so in combination with off-
the-shelf LLMs, rather than relying on fine-tuned models
exclusively.

<a id='d040296e-8859-413a-963b-ea2801a139e7'></a>

Regarding reinforcement learning (RL), only 1 scientific
discovery case from our interviews uses an RL post-trained
model. Three other teams express interest in exploring RL
for software testing in future development cycles.

<a id='b6484e20-1e99-48eb-9395-f84df46c584f'></a>

This data, however, does not diminish the value of post-training models for agent applications. Interviews show that SFT and RL is challenging to implement and brittle to model upgrades. Given that off-the-shelf base models can already do most of what the agent applications need, teams prefer methods with lower integration overhead that do not increase already high development and maintenance burdens.

<a id='9968e755-931b-43cc-b0ba-25a29c60094b'></a>

*RQ2 Finding #3:* Practitioners rarely post-train models. When they do, they selectively apply SFT/RL to specific subtasks or clients, typically in combination with general LLMs. Teams find prompt engineering with frontier models sufficient for *many* target use-cases already.

<a id='426601d7-cca8-43a4-9027-912b37c43b20'></a>

## 5.3. Prompting Strategies

We find that humans dominate system-prompt construction in production systems. Our survey data reveals that 33.9% of deployed agents use *fully manual* methods with hard-coded strings. Another 44.6% use a hybrid approach where humans manually draft prompts and then use an LLM to augment or refine them, and 3.6% rely on utilizing predefined prompt templates. Only 8.9% of respondents use a prompt optimizer (e.g., DSPy [77]) to improve their agent systems, and just 3.6% report letting agents autonomously generate their own prompts.

<a id='66ac405f-e013-4384-a2ab-e6f83dd710ba'></a>

Our detailed case studies confirm this pattern. Only 1 out of 20 (5%) detailed case studies has explored automated prompt optimization. The remaining cases rely on primary human construction, sometime using LLMs for augmentation. While recent research [77–79] proposes automating prompts into parametric optimizations, we find these methods rare in deployment. We speculate from interview conversations with practitioners that they prioritize controllable, interpretable methods that allow for fast iteration and debugging over automated or “black-box” methods that requires additional engineering overhead.

<a id='aa728afc-be99-420f-910c-5d7cee2bd1e2'></a>

*RQ2 Finding #4*: Human dominates prompt construction as teams prioritize controllability. LLMs are used as secondary tools to augment human-crafted prompts, while automated prompt optimization remains rare.

<a id='7055deca-4efd-42bb-9a0a-0d652ca0c198'></a>

<::chart: horizontal bar chart::>  
**Distribution of prompt construction strategies across deployed Agentic AI systems (N=53)**  
**Y-axis (Strategies):**  
* Manual + AI: 44.6% (25)  
* Fully Manual: 33.9% (19)  
* Prompt Optimizer: 8.9% (5)  
* Predefined Template: 3.6% (2)  
* Fully Autonomous: 3.6% (2)  
**X-axis:** % of Responses  
Figure 7. Distribution of prompt construction strategies across deployed Agentic AI systems (N=53). Overall, the prompting patterns indicate that humans remain central to prompt crafting. This was a multi-select question, so respondents could choose all prompt construction strategies that applied.  
<::/chart::>

<a id='6c6edd13-5cfe-4537-a140-7c0a5fd6a7e8'></a>

We find that prompt complexity increases with system maturity. Among deployed agents from survey, we observe a wide distribution: while 51.5% of systems use short instructions under 500 tokens, there is a long tail of massive prompts (Figure 8b). Specifically, 24.2% of deployed agents use prompts between 500 and 2,500 tokens, 12.1% use between 2,500 to 10,000. Notably, another 12.1% even exceed 10,000 tokens.

<a id='167f3e67-a880-44df-8d7e-3fe57a4ec919'></a>

*RQ2 Finding #5*: Deployment prompt lengths vary widely: while half are short (<500 tokens), a significant long tail (12%) exceeds 10,000 tokens to handle complex contexts.

<a id='03bdf76b-21cf-4abd-88b8-8d4b31a58416'></a>

## 5.4. Agent Architecture

We explore the core architectural patterns that support production deployment. To ensure clarity, we adopt the terminologies visualized in Figure 22: an agent completes a high-level *task*, which decomposes into logical *subtasks*, consisting of granular atomic *steps* (e.g., model calls, tool use).

<a id='e905da57-7054-47b9-8ea2-46aabd9e7532'></a>

**Number of Steps.** We find that production agents tend to follow _structured workflows_ with bounded autonomy. We ask survey participants how many steps their deployed systems execute within a subtask before requiring human input. Most systems operate within tight bounds: 46.7% of deployed survey agents complete only 1–4 steps, and an additional 21.7% perform 5–10 steps (Figure 8c). A smaller subset (16.7%) extends to tens of steps, while only 6.7% report systems with no explicit step limit. Interestingly when we expand the analysis to include all agents including both deployed and not yet deployed agents in Figure 16c, the distribution shifts toward substantially higher step counts. This indicates that prototypes and research systems are more likely to run tens of steps or have no explicit limit on au-

<a id='5a6859f4-e713-4df1-a8fd-ef9272a3adc9'></a>

9

<!-- PAGE BREAK -->

<a id='6a994bbb-e9c1-4eff-8a01-cc67cad96670'></a>

Measuring Agents in Production

<a id='def86bdc-ff79-49ba-829f-42475c369a83'></a>

<::bar chart: (a) Number of distinct models combined to solve a single logical task (N=22). The y-axis represents '% of Responses' from 0% to 50%. The x-axis represents 'Number of Distinct Models'.
- 1: 41% (9 responses)
- 2: 27% (6 responses)
- 3: 19% (4 responses)
- 4+: 14% (3 responses)

(b) Distribution of prompt lengths in terms of tokens (N=33). The y-axis represents '% of Responses' from 0% to 30%. The x-axis represents 'Instruction Length (tokens)'.
- 0-250: 28% (9 responses)
- 250-500: 24% (8 responses)
- 500-2.5k: 24% (8 responses)
- 2.5k-10k: 12% (4 responses)
- 10k+: 12% (4 responses)

(c) Number of autonomous execution steps before user intervention (N=60). The y-axis represents '% of Responses' from 0% to 50%. The x-axis represents 'Autonomous Steps/Cycles'.
- 1-4: 47% (28 responses)
- 5-10: 22% (13 responses)
- Tens: 17% (10 responses)
- Hundreds: 5% (3 responses)
- Thousands: 3% (2 responses)
- No limit: 7% (4 responses)::>

Figure 8. Overview of core components configurations and architectures in deployed Agentic AI systems:

<a id='ef03700d-1a4a-475d-81fa-c1aac67fee41'></a>

tonomous cycles, reflecting more aggressive exploration of
open-ended autonomy during early development.

<a id='0aef0924-0243-406f-9a01-b89bcc5ddded'></a>

Practical constraints drive this design choice. Case study participants identify problem complexity, non-determinism in agent self-planning, and latency requirements as key limiting factors. Practitioners intentionally impose limits on reasoning steps to maintain reliability and manage computational time and costs. This simplicity reflects a broader preference for predictable, controllable workflows over experimental open-ended autonomy in production environment.

<a id='6c1c62b9-99ee-4f85-9913-8196cb839867'></a>

**Number of Model Calls.** While distinct from logical steps (which often include non-inference actions like tool execution), we specifically analyze *model calls* to gauge the inference intensity of deployment systems. We observe that within a single subtask, deployment systems typically execute model calls on the order of tens or less. The majority (66.7%) of deployed survey agents use fewer than 10 calls per subtask, with 46.7% using fewer than 5 calls. This is followed by 33.3% using tens of calls, 9.0% in the hundreds, and 6.1% in the thousands. Interestingly, another 12.1% do not set explicit limits. This aligns with our detailed case studies, where 3 teams confirm executing tens of model calls per subtask (up to 40–50 calls).

<a id='d03fbc00-6a39-491f-8116-3dab8b3de889'></a>

Experimental systems are far more likely to sit in the long
tail, with many agents routinely making tens of calls. In con-
trast, deployed agents concentrate in the lower-call regime,
suggesting that teams aggressively cap or refactor call bud-
gets as they move from experimentation to deployment to
probably control cost, latency, and failure amplification.

<a id='e173c12c-3da3-47d6-88dc-e582e2f5622f'></a>

Despite the pattern of limited model calls, 31% of deployed survey agents already use various inference-time scaling techniques, compared to 44% in experimental sys-

<a id='cdb0e373-03d4-49b5-a15c-eddc8d409803'></a>

tems. While this figure currently includes simpler methods like composing outputs from multiple models (Section 5.1), future work may determine how advanced techniques—such as self-planning, search-based reasoning, and self-verification—perform in production, as these approaches may then lead to higher numbers of model calls and steps before human intervention.

<a id='1c401b0e-5ee7-4246-91f2-7bbd4209bef2'></a>

RQ2 Finding #6: Agents operate with tightly bounded autonomy: 68% of systems execute fewer than ten steps and 46.7% with less than 5 model calls before requiring human intervention.

<a id='836fed38-cf91-4925-9527-55e0a201ab51'></a>

**Agent Control Flow.** We observe that production agents favor predefined static workflows over full open-ended autonomy. We find that 80% of our detailed case studies utilize a structured control flow. These agents operate within well-scoped action spaces rather than freely exploring the environment to self-determine objectives.

<a id='fb823634-2d41-4514-86e9-0c6fd2e34ad9'></a>

Detailed case studies provide insight into these control flow patterns. Nine cases utilize various forms of agentic Retrieval-Augmented Generation (RAG) pipelines, ranging from single agents retrieving information via tool calls to sophisticated pipelines with over 20 subtasks that explicitly configure retrieval at certain steps. For example, one insurance agent follows a fixed sequence: coverage lookup, medical necessity review, and risk identification. While the agent possesses autonomy to complete each subtask (e.g., deciding if a case needs human intervention for risk identification), the high-level objective and expected output of each subtask remain fixed.

<a id='e0e47c68-dfa3-4ebf-af01-1d52fec6aaa7'></a>

Open-ended autonomy remains rare. We observe only one
---

‡This is a simplified example workflow to illustrate the core
logic, redacted to protect the anonymity of the interviewee.

<a id='52e4beb4-6b96-4a4b-be77-bf772d6281e6'></a>

10

<!-- PAGE BREAK -->

<a id='9128c348-0122-4fdc-9e58-d0e299819a3c'></a>

Measuring Agents in Production

<a id='507b344a-bdaa-41b5-96f8-3c36ce047d1b'></a>

case where agents operate with unconstrained exploration.
Notably, this system runs exclusively in a sandbox environment with rigorous CI/CD verification on the final outputs,
avoiding direct interaction with production environment.

<a id='d7bcf911-1e00-43a7-8016-bb8dc357b778'></a>

However, we identify a growing interest in expanding au-
tonomy. Four detailed case studies employ a planning and
routing agent to decompose input requests and dispatch
them to task-specialized agents. Another team specializes
agents into generators and verifiers, enabling greater auton-
omy through automated verification. Several teams share
that they are experimenting with flexible workflows by al-
lowing agents to make autonomous decisions about next
steps or by using planning and orchestration agents.

<a id='dbd56a82-8e89-4136-a1a9-7662560afc59'></a>

RQ2 Finding #7: Deployment architectures favor predefined, structured workflows over open-ended autonomous planning to ensure reliability.

<a id='79a8ebd7-e65a-478d-8e0a-7ad0b5a32727'></a>

## 5.5. Agentic Frameworks

We find a divergence in framework adoption between survey respondents and interview case studies. Among deployed agents from the survey, two-thirds (60.7%) use third-party agentic frameworks. Reliance concentrates around three primary frameworks: LangChain/LangGraph [80, 81] leads with 25.0%, followed by CrewAI [82] at 10.7%, with LLa-MAIndex [83] and OpenAI Swarm [84] both at 3.6% (Figure 9).

<a id='009ec092-1752-4dd5-939b-2300fb622ce5'></a>

In sharp contrast, our detailed case studies reveal a strong
preference for custom in-house agent implementations.
Only 3 of 20 (15%) detailed case studies rely on external
agent frameworks (2 use LangChain, 1 uses BeeAI). The
remaining 17 teams (85%) build their agent application en-
tirely in-house with direct model API calls. For example,
one interview case explicitly shared that their agents are
their own implementation of ReAct loops. Notably, two ad-
ditional teams report starting with frameworks like CrewAI
during the experimental prototyping phase but migrating
to custom in-house solutions for production deployment to
reduce dependency overhead.

<a id='1a19b335-37e3-4f9a-b935-ee8bb8dd5b62'></a>

We identify three core motivations for building custom so-
lutions from the detailed case studies. First, *flexibility and*
*control* are critical. Deployed agents often require vertical
integration with proprietary infrastructure and customized
data pipelines that rigid frameworks struggle to support.
For example, one agent-native company deploys customer-
facing agents across varied client environments, necessi-
tating a bespoke orchestration layer. Second, *simplicity*
drives the decision. Practitioners report that core agent
loops are straightforward to implement using direct API
calls. They prefer building minimal, purpose-built scaffolds
rather than managing the dependency bloat and abstraction
layers of large frameworks. Third, *security and privacy poli-*

<a id='f4a56e6a-ccde-4b41-bb06-0332883bd150'></a>

<::bar chart: Y-axis: % of Responses. X-axis: No (Did Not Use Any Framework) and Yes (Used Framework). Legend: OpenAI Swarm, LlamaIndex, CrewAI, Other, LangChain/LangGraph. Data: No: 39.3%. Yes: 60.7% total, broken down into LangChain/LangGraph: 25.0%, Other: 17.9%, CrewAI: 10.7%, LlamaIndex: 3.6%, OpenAI Swarm: 3.6%. Figure 9. Frameworks reported to support critical functionality among those using open frameworks for production systems (N = 29). Additional framework options were provided in the survey; see Appendix E.3 for the complete question and options.: chart::>

<a id='ad93da36-7328-49a3-afb0-3e1510441dc9'></a>

cies sometime prohibit the use of certain external libraries
in enterprise environments, compelling teams to develop
compliant solutions internally.

<a id='973a832f-6a43-4e53-9912-509da5043296'></a>

RQ2 Finding #8: Framework adoption varies significantly between survey and case study. While third-party frameworks get broad adoption in the survey (61%), interviewed teams predominantly build custom in-house implementations (85%) to maximize control and minimize dependency bloat.

<a id='34b07392-7e55-4798-92da-a1a06a304532'></a>

## 6. RQ3 Results: How Are Agents Evaluated For Deployment?

Evaluation practices shape which agentic systems reach production and how teams iterate on deployed systems. We investigate how practitioners evaluate and test their agents to meet production deployment requirements, examining both offline evaluation during development and online evaluation in production environments. Specifically, we examine two aspects: what practitioners compare their systems against (baselines and benchmarks), and what methods they use to verify system outputs (evaluation methods).

<a id='45f8d793-d67e-4945-8e6b-c8f514d6b729'></a>

Our findings reveal that evaluation practices vary widely across production agents, even within the same application domain, shaped by the specific requirements of each deployment context and the availability of ground truth data. Notably, practitioners currently focus on agent output quality and correctness rather than traditional software reliability metrics. Based on 20 detailed case studies, no team reports applying standard production reliability metrics such as five 9s availability to their agent systems. Instead, evaluation centers on whether agents produce correct, high-quality responses.

<a id='dccf6410-1d71-4239-a66a-50fdabc79550'></a>

11

<!-- PAGE BREAK -->

<a id='4c30f048-711a-4806-9028-109c889df368'></a>

Measuring Agents in Production

<a id='cd53db4e-52d4-4708-92b9-640c1d03e633'></a>

## 6.1. Baselines and Benchmarks
During development, teams conduct offline evaluation to assess agent performance before deployment. Figure 10a shows that 38.7% of survey respondents compare their deployed agentic systems against non-agentic baselines such as existing software systems, traditional approaches, or human execution. The remaining 61.3% do not perform baseline comparisons. Among these, 25.8% report their systems are truly novel with no meaningful baseline for comparison. While we do not know reasons behind why the remaining 35.5% did not conduct the comparison, in-depth interviews reveal that baseline comparison is often challenging even when alternatives exist. One reason is that non-agentic baselines frequently combine multiple components, making systematic technical comparison difficult. The HR support agent development team illustrates that agent solution replace a baseline process combining company document lookup, human procedures, and non-LLM HR software. While outcomes such as task completion time are measurable, isolating technical performance for comparison is challenging.

<a id='d9d1c7cb-7dc2-4683-9882-69d9c2dcdd72'></a>

Beyond baseline comparison, teams employ benchmark evaluations. We refer to benchmark here as a curated set of tasks or questions with known correct answers. Among 20 teams, 5 build custom benchmarks. One team builds benchmarks from scratch, collecting ground truth labels through collaboration with domain experts. Four teams synthesize existing data to curate benchmarks, drawing from past test cases, system logs, pull requests, and support tickets. One team also leverages public benchmarks during early development. The remaining 15 teams (75%) evaluate their agents without benchmark sets, using alternative methods such as A/B testing, user feedback, and production monitoring.

<a id='e79a2995-9526-4f0a-856d-a79aa19e875a'></a>

Among the 5 teams building custom benchmarks, our inter-views reveal a prevalent evaluation pattern despite diverse domains and organizations (e.g., human resources, cloud infrastructure, and business analytics). Teams establish a golden question-and-answer set, collect user interaction and feedback data, then work with subject matter experts to examine quality and expand the golden set. This process extends into production runtime, enabling LLM-as-a-judge online evaluation pipelines. Based on our analysis, the con-vergence of nearly identical pipelines across diverse contexts suggests research and development opportunities in reusable data ingestion pipelines, curation methods for golden sets, and synthetic generation techniques for evaluation datasets.

<a id='ad16bd6b-1a2f-4e3b-bf9f-2bd5af4c5209'></a>

RQ3 Finding #1: Many agentic systems lack standard-ized benchmarks or baselines. Teams build custom eval-uation frameworks from scratch, often creating ground truth data for the first time.

<a id='63d55115-2ede-4691-b989-60c85ac9ca9a'></a>

## 6.2. Evaluation Methods

We ask participants which evaluation strategies they employ, allowing multiple selections. These methods apply to both offline evaluation during development and online evaluation in production environments. Four methods dominate responses: human-in-the-loop evaluation, model-based evaluation, rule-based evaluation (heuristics or syntactic checks), and cross-referencing evaluation (verification against knowledge bases or reference datasets). The exact question and method descriptions appear in Appendix E.

<a id='aad16616-00b3-45c0-b90c-e93362f4f088'></a>

**Human-in-the-loop verification.** The majority (74.2%) rely on manual, human-in-the-loop evaluation (Figure 10b. These evaluations typically involve domain experts, operators, or end-users directly inspecting, testing, or validating system outputs to ensure correctness, safety, and reliability.

<a id='767dbb72-c9fc-43ec-b085-fa5946df57d3'></a>

Human experts play a critical role during development for
offline evaluations. Agent developers work directly with
domain experts or target users to validate system responses.
For example, medical professionals are directly involved
when testing and evaluating the correctness of healthcare
agent systems. In one case, a company even forgoes auto-
mated evaluation methods entirely, relying instead on human
experts to provide feedback to hand-tune agent configura-
tions for each client's deployment environment.

<a id='69d6be44-4c91-49a3-9743-da35b4c9d2f3'></a>

Human experts also serve as verifiers during agent execu-
tion for online evaluation. Teams commonly have human
experts perform final actions based on agent output, serving
as a layer of guardrails. For example, one site reliability
engineering agent suggests actions based on analysis across
system stacks, but human experts make the final decision
on what to execute. The agent does not directly modify the
production environment.

<a id='01ff7c87-ed63-405d-a786-e8e0aac329c2'></a>

**Model-based evaluation.** Model-based evaluation methods such as LLM-as-a-judge are the second most common approach, used by 51.6% of respondents (Figure 10b). While this evaluation method applies for both offline development time and online runtime evaluation, 7 of 20 interview case studies use LLM judges for online evaluation during agent execution.

<a id='67c462db-4dca-461f-8be4-436dcc7e0a8e'></a>

Model-based evaluation does not eliminate human involvement. Figure 10c shows the co-occurrence of different evaluation strategies, revealing that among the 51.6% of survey respondents who use model-based evaluation, a substantial portion (38.7%) also employ Human-in-the-Loop verification. In detailed case studies, all interviewed teams using LLM-as-a-judge combine it with human review. Specifically, these teams use LLM judges to evaluate confidence in every final response, combined with human subsampling. An LLM judge continuously assesses each agent's final response. If the judge scores above a preset confidence threshold, the output is accepted automatically. Otherwise,

<a id='89ac2d07-b62a-4b78-bbfa-49bf7b09b7da'></a>

12

<!-- PAGE BREAK -->

<a id='ccaaeb5d-b07b-467d-ace5-a6208e0f13d8'></a>

Measuring Agents in Production

<a id='2459bc76-9433-4430-b454-c7072a7107ad'></a>

<::chart: bar chart::> (a) Comparison to Alternatives: Shows whether participants explicitly compared their deployed agent against a non-agentic baseline (e.g., existing software, traditional workflows, etc).

Y-axis: % of Responses

Legend:
- Alternative does not exist (orange)
- Alternative might exist (yellow)

Bars:
- Yes Compared to Alternative: 38.7% (blue)
- No Did Not Compare: 61.3% total
  - Alternative might exist: 35.5%
  - Alternative does not exist: 25.8%

<::chart: horizontal bar chart::> (b) Evaluation Methods Distribution: Distribution of evaluation methods reported by practitioners (N=31). The question was multi-select, so respondents could choose multiple methods.

X-axis: % of Responses

Y-axis:
- Manual (Human in the Loop): 74.2% (23)
- Model Based (e.g., LLM-as-a-Judge): 51.6% (16)
- Cross-Referencing (e.g., RAG, Knowledge Graphs): 41.9% (13)
- Rule Based (e.g., Syntax Checks): 38.7% (12)
- None of the above: 3.2% (1)

<::chart: heatmap::> (c) Evaluation Strategies Co-occurrence: Visualizes the pairwise overlap between evaluation strategies. Manual human-in-the-loop evaluation has the highest overlap with other strategies, suggesting that teams commonly rely on manual review to complement automated checks.

Color Bar: % of Responses (ranging from 20 to 70)

Rows / Columns: Manual, Model Based, Cross-Referencing, Rule Based

Grid Values (Percentage (Count)):
- Manual x Manual: 74.2% (23)
- Manual x Model Based: 38.7% (12)
- Manual x Cross-Referencing: 29.0% (9)
- Manual x Rule Based: 22.6% (7)
- Model Based x Manual: 38.7% (12)
- Model Based x Model Based: 51.6% (16)
- Model Based x Cross-Referencing: 29.0% (9)
- Model Based x Rule Based: 16.1% (5)
- Cross-Referencing x Manual: 29.0% (9)
- Cross-Referencing x Model Based: 29.0% (9)
- Cross-Referencing x Cross-Referencing: 41.9% (13)
- Cross-Referencing x Rule Based: 16.1% (5)
- Rule Based x Manual: 22.6% (7)
- Rule Based x Model Based: 16.1% (5)
- Rule Based x Cross-Referencing: 16.1% (5)
- Rule Based x Rule Based: 38.7% (12)

<a id='148f8cdb-9cfa-4a6a-bc96-761857d33092'></a>

the request routes to human experts. Additionally, human
experts sample a preset percentage (e.g., 5%) of production
runs even when the LLM judge expresses high confidence,
verifying correctness to ensure consistent alignment at run-
time. Development teams update prompt instructions and
agent configurations based on this production feedback.

<a id='89f0a8b8-bc44-4ff0-b611-7122279dcdc7'></a>

**Other methods.** Rule-based evaluation methods and cross-referencing strategies show comparable adoption rates (41.9% and 38.7% respectively). Rule-based evaluation consists of simple logic checks such as grammar and syntax verification or domain-specific rules. For example, coding agents verify outputs through compilation checks and test suites, while analytical agents may apply domain-specific rubrics to assess output quality. Cross-referencing evaluation uses external sources for grounding and fact-checking to verify the accuracy and quality of generated answers or solutions. This includes retrieving supporting evidence from trusted knowledge bases or comparing outputs against reference datasets.

<a id='eecca94e-5420-44e4-950e-191bc6229ff4'></a>

**Evaluation method patterns.** Co-occurrence analysis reveals that human-in-the-loop evaluation is the most common method used together with other evaluation strategies (Figure 10c). Practitioners anchor automated, rule-based, and cross-referencing methods around human judgment rather than relying on them in isolation.

<a id='87930934-f046-48b0-ae9d-9ffb70dfb12b'></a>

Notably, human-in-the-loop (74.2%) and LLM-as-a-judge
(51.6%) dominate compared to rule-based verification
(42.9%). Based on our analysis, this pattern may suggests
that production agents already handle complex tasks be-
yond classification, entity resolution, or pattern matching.
These agents operate in domains requiring nuanced judg-
ment where rule-based methods prove insufficient. For
example, customer support voice assistance and human re-

<a id='7a088b5c-d654-4815-bbe0-78f64f3dde67'></a>

source operations demand expertise beyond pattern match-
ing, explaining why practitioners require human and LLM
verification to assess output correctness.

<a id='b51dce23-8521-4134-b6df-a35e72f22aa2'></a>

RQ3 Finding #2: Human judgment dominates evaluation (74.2%). LLM-as-a-judge emerges as a complementary automated approach (51.6%), typically combined with human verification.

<a id='e2bb95c4-3d22-44aa-95be-604d4bf95be2'></a>

7. *RQ4 Results: What Are The Top Challenges In Building Production Agents?*

AI agents are deployed at scale, yet significant challenges remain. We investigate the friction points practitioners face most when building production agents. Our survey and detailed case studies reveal that *reliability* remains the primary bottleneck. Survey respondents at all agent stages rank "Core Technical Focus" as their top challenge (37.9%), far outweighing governance (3.4%) or compliance (17.2%) (see Appendix B.3). Core technical focus encompasses reliability, robustness, and scalability. This prioritization signals that practitioners currently focus on making agents work consistently and correctly.

<a id='3365a738-12fe-4420-8d1d-fb66f188459a'></a>

RQ4 Finding #1: Reliability remains unsolved. It represent the top development focus for agents in all stages including ones in deployment.

<a id='46914431-6d86-4338-89fb-86b57cb4a79a'></a>

We detail the three specific dimensions of these challenges:
evaluation (Section 7.1), latency (Section 7.2), and security
(Section 7.3). This gap highlights research opportunities
to advance agents from bounded use cases toward broader
applications.

<a id='9dd25c3d-af7a-4d76-b063-79d544ab766d'></a>

13

<!-- PAGE BREAK -->

<a id='76b52734-7839-4da1-8361-96f6cd8d64cc'></a>

Measuring Agents in Production

<a id='ddd984bd-e604-4d55-8eea-e6938da9c0dc'></a>

7.1. Evaluation Challenges

**Difficulties creating benchmarks.** As defined in Section 6.1, benchmarks are curated sets of tasks or questions with known correct answers used for offline evaluation during development. Among interviewed teams, 5 build custom benchmarks. However, benchmark creation proves challenging for three reasons based on our case study interviews.

<a id='9d8fc577-0a09-48f8-a9a8-21678b9046b9'></a>

First, specific domains lack accessible public data. In reg-ulated fields like insurance underwriting, the absence of public data forces teams to handcraft benchmark datasets from scratch through collaboration with domain experts and target users. Second, creating high-quality benchmarks at scale is resource-intensive and time-consuming. One interviewed team reported spending months creating an ini-tial set of approximately 40 unique environment-problem-solution scenarios, followed by another six months to scale the dataset to roughly 100 examples. Third, benchmark creation is nearly infeasible for agent applications focused on highly customized client integrations. One interviewed voice agent team illustrates this challenge: while core fea-tures are straightforward to test without extensive data, the primary engineering effort involves client-specific cus-tomizations such as proprietary toolsets, product offerings, and localized dialogue tuning. Creating standardized bench-marks for every possible client configuration is impractical. Given these challenges, 75% of interviewed teams forgo benchmark creation entirely, instead relying on A/B test-ing or direct client collaboration to iterate based on human feedback until expectations are met.

<a id='a4391697-9069-428f-81bb-26f7c85f5cbe'></a>

**Agent behavior breaks traditional software testing.** Three case study teams report attempting but struggling to integrate agents into existing CI/CD pipelines. The chal- lenge stems from agent nondeterminism and the difficulty of judging outputs programmatically. Despite having various forms of existing regression tests from baseline systems, these teams have not yet identified effective methods to adapt them for nondeterministic agent behavior to create test set that cover sufficient runtime scenarios with different nuances.

<a id='ffe32cef-3255-4349-b5b3-82ff5caeb469'></a>

**Verification mechanisms and benefit quantification.** Based on the interview case studies, we observe that agent faces two broader evaluation challenges. First, robust ver- ification mechanisms do not always exist. Coding-related agents benefit from strong correctness signals through com- pilation and test suites, such as software migration agents and site reliability agents in Table 1. However, many agents operate in settings without robust and fast verification. For example, insurance agents receive true signals only through real consequences such as financial losses or delayed patient approvals. These signals arrive slowly and in forms difficult to automate for evaluation. Second, the final benefits of using agents are not always easy to measure. Section 4.1

<a id='da0bc9a4-53bc-4dab-891f-1630fd5bb902'></a>

<::bar chart: A bar chart titled "Degree to which latency causes problems for deployment of Agentic AI systems (N = 27)" shows the percentage of responses for three categories. The x-axis is "% of Responses" ranging from 0% to 100%. The y-axis lists the categories: Latency Blocker, Deployable with Latency Gap, and No Latency Concern. 
- Latency Blocker: The bar extends to approximately 14.8%, with the full label indicating "14.8% (4)".
- Deployable with Latency Gap: The bar extends to approximately 59.3%, with the full label indicating "59.3% (16)".
- No Latency Concern: The bar extends to approximately 25.9%, with the full label indicating "25.9% (7)".

These responses suggest that latency is rarely a strict blocker for deploying most agentic AI systems.: chart::>

<a id='ea7b75ae-4aff-4abf-838c-0becfdc13dd3'></a>

shows agents target productivity gains because end-to-end time and human hours quantify straightforwardly. Applications with harder-to-measure benefits remain less explored. For example, we observe fewer cases of agents reducing cross-domain knowledge needs or mitigating risks, potentially because benefits manifest indirectly or over longer timeframes.

<a id='9477e9f1-08aa-4e92-bad7-78cc975c1f06'></a>

## 7.2. Latency Challenges

We examine the degree to which agent execution latency hinders deployment. Survey results indicate that latency represents a manageable friction rather than a hard stop for most teams. Figure 11 shows that only 14.8% of deployed survey agents identify latency as a critical deployment blocker requiring immediate resolution, while the majority (59.3%) report it as a marginal issue, where current latency is suboptimal but sufficient for deployment. We suspect that this tolerance may correlates with the prevalence (15/20 in detailed interview case studies) of asynchronous agent execution paradigm (Section 4.4) and (52.2% from survey) internal user bases (Section 4.3). Notably, we observe a consistent latency distribution across the full survey dataset, including experimental systems (Figure 19b). We believe this consistency signals a broader preference for building offline agents, as discussed in Section 4.4.

<a id='0add954d-7be1-40f2-bafa-1431c96745e1'></a>

**Interactive agent latency requirements.** While latency is not a critical challenge for most agent applications, it remains a critical bottleneck for real-time interactive agents. Two interviewed teams, building voice and specialized chat agents, report continuous engineering efforts to match human conversational speeds. Unlike asynchronous workflows, these systems require seamless turn-taking where delays disrupt the user experience. Achieving fluid real-time responsiveness beyond rigid turn-based exchanges remains an open research question and development challenge.

<a id='6ec6b359-a753-4962-a13e-caa73ee45e30'></a>

Practical latency management. Interview participants de-
scribe two approaches to managing latency. First, teams
commonly implement hard limits on maximum steps or

<a id='a82afefc-a3d7-43e9-bcbd-fc5b22a76a4d'></a>

14

<!-- PAGE BREAK -->

<a id='985a6c51-a8f0-423b-976c-85327503b4f1'></a>

Measuring Agents in Production

<a id='af8e66e9-7124-4e40-afab-5d63f856f10d'></a>

<::chart: bar chart::>Figure 12. Overview of data handling capabilities: Types and modes of data ingestion and handling in deployed agent systems (N=29). The question was multi-select i.e., allowing participants to indicate all/any data handling methods currently integrated into their deployed agentic systems. The data illustrates how agents source information, showing a strong reliance on internal infrastructure over public sources.

Horizontal bar chart showing the percentage of responses for different data handling capabilities. The x-axis is labeled "% of Responses" and ranges from 0% to 100%.

Data points:
- Database: 89.7% (26)
- Confidential data: 69.0% (20)
- Live user data: 65.5% (19)
- Live non-user data: 51.7% (15)
- Public online data: 34.5% (10)<::/chart::>

<a id='06e1cccd-b6b9-4c21-941e-1abca9023bb7'></a>

model inference calls, typically derived from heuristics. Second, one team adopts a creative solution by pre-building a database of request types and agent actions (tool calls), then employing semantic similarity search at runtime to identify similar requests and serve prebuilt actions, reducing response times by orders of magnitude compared to reasoning and generating new responses. These workarounds demonstrate that practitioners currently rely on system-level engineering to bypass the inherent latency costs of foundation models.

<a id='fa6857df-e14e-4b7f-81ac-b14a29a00179'></a>

## 7.3. Security and Privacy Challenges

Security and privacy consistently rank as secondary con-
cerns in both of our survey and interviews, with practition-
ers prioritizing output quality and correctness. Figure 21a
shows that Compliance and User Trust ranks fourth among
challenge categories. Given that Section 4.3 shows 52.2%
of systems serve internal employees and many systems with
human supervision, this prioritization reflects current de-
ployment environments and requirements rather than dis-
missing security's importance.

<a id='056567f5-a331-4b73-b3b6-3a7ffab54204'></a>

**Data ingestion and handling.** Survey results in Fig-ure 12 show that 89.7% of systems ingest information from databases, 65.5% ingest real-time user input, and 51.7% ingest other real-time signals. Notably, 69.0% of systems retrieve confidential or sensitive data, while only 34.5% retrieve persistent public data. Given the high prevalence of sensitive data usage and user inputs, preserving privacy is critical. Our interview case studies reveal that teams address this through legal methods. For example, a team building healthcare agents report relying on standard data-handling practices and strict contractual agreements with model providers to prevent training on their user data.

<a id='86f545c7-266b-480e-8c68-6cef8ae97113'></a>

**Security practices.** In-depth interview participants describe

<a id='676ba9bd-e2e1-4bb9-a582-389912bc1d61'></a>

four approaches to managing security risks through constrained agent design. First, six teams restrict agents to "read-only" operations to prevent state modification. For example, one SRE agent case study generate bug reports and proposes action plans, but leaves the final execution to human engineers. Second, three teams deploy agents in sandboxed or simulated environments to isolate live systems. In one instance, a code migration agent generates and tests changes in a mirrored sandbox, merging code only after software verification. Third, one team builds an abstraction layer between agents and production environments. This team constructs wrapper APIs around production tools, restricting the agent to this intermediate layer and hiding internal function details. Finally, one team attempt to enforce role-based access controls that mirror agent user's permissions. However, the agent team reports this remains challenging, as agents can bypass these configurations when accessing tools or documents with conflicting fine-grained permissions.

<a id='be266259-8f32-46d5-be83-1f3588dcca29'></a>

## 8. Discussion

Building on the quantitative findings in Sections 4–7, we discuss three key trends that characterize the current state of agent engineering. We first examine how practitioners achieve production reliability through architectural constraints (Section 8.1). Next, we highlight how current model capabilities already drive substantial value, revealing additional adoption opportunities (Section 8.2). Finally, we outline concrete open research directions for agent development (Section 8.3).

<a id='f87654ca-bf68-4690-a161-f6eadb3ed55c'></a>

## 8.1. Reliability Through Constrained Deployment
A paradox emerges from our data: while nearly 40% of practitioners identify reliability, robustness, and scalability as their primary development concern (Section 7), these systems have successfully reached deployments in production or pilot stages (Section 4). This raises a critical question: how do agents reach production if reliability remains an unsolved challenge? We observe that practitioners ensure reliability via deploying agents with strict constraints on both execution environments and agent autonomy, often combined with close human supervision.

<a id='c282e73c-a212-434f-990d-8545b6742e48'></a>

Our in-depth case studies reveals several deployment envi-
ronment patterns. Some agents operate in read-only mode,
never modifying production state directly. For example,
SRE agents perform debugging then generate detailed bug
reports that engineers review and action. Other agents serve
internal users where errors carry lower consequences and
human experts remain readily available to correct mistakes
(Section 4.3): for example, Slack bot response automa-
tion for internal tickets. Systems with write access de-
ploy through sandbox environments where outputs undergo

<a id='cb3a1d28-6df2-4e4f-ab80-1aa28d7a36eb'></a>

15

<!-- PAGE BREAK -->

<a id='15bfaa5a-4fd0-4c31-8a3b-d6856fcee6df'></a>

Measuring Agents in Production

<a id='ee12bb73-f86a-42a1-8168-6af18815853a'></a>

rule-based verification before production integration. Some
teams combine read-only access with sandboxes mirroring
production environments to further mitigate risk. We ob-
serve that these patterns shift the reliability challenge from
ensuring correct autonomous agent actions at each step to
verifying that final outputs meet acceptable quality thresh-
olds.

<a id='8811050e-f8bd-458f-8138-e9d2cbcbb34a'></a>

Section 5.4 shows 68% of production agents execute fewer than ten steps before requiring human intervention. Organizations deliberately bound agent behavior within specific action spaces through prompting and limited tooling. External-facing systems use particularly restricted agent workflows where customer trust and economic consequences demand tighter control. For example, rather than allowing an agent to autonomously decide if and where to search via tool call, teams employ agentic RAG architectures with pre-determined retrieval steps that restrict the agent to specific document store. We believe this pattern reveals a deliberate engineering trade-off. Production teams balance capability with controllability such that these systems deliver value through well-scoped automation while maintaining reliability guarantees.

<a id='08beb0fe-6267-411f-a6cc-77bb413062d2'></a>

An open question remains: how can future agents expand au-tonomy while maintaining production-level reliability? The field has not established clear pathways for systematically relaxing these constraints while preserving safety guaran-tees. We observe that one emerging pattern to achieve this goal is the merging of distinct subtasks into a single task. For example, commercial coding assistants adopt test-driven development by merging coding and testing into a unified agent execution flow [16, 85]. We believe this pattern of decoupling logical task abstraction from agent execution steps has potential to extend to other domains. The way hu-mans break down tasks into subtasks might not be the right way to organize agent control flow (Figure 22). Progress requires advances in two areas: agent evaluation must move beyond correctness metrics to assess other aspects of reli-ability (e.g.: five 9s) under varying autonomy levels, and engineering practices that systematically specify and verify bounded operations at different abstraction levels.

<a id='475d5a58-f2f1-45b8-adc4-300e9bd18337'></a>

## 8.2. Rich Agent Application Opportunities

Our study reveals an encouraging finding: production agents already operate across more than 26 domains (Figure 2), extending well beyond coding-related agents and chatbots. We find that practitioners extract real value from current model and agent capabilities (Figure 1).

<a id='481570c6-0196-4ea5-8330-7c13ec84910d'></a>

Our detailed case studies show that current frontier models, utilizing prompting strategies alone, already possess sufficient capabilities to cover a diverse range of production use cases. Section 5.1 shows that 70% of deployed survey agents rely on off-the-shelf models without any fine-tuning.

<a id='4ef69e45-486a-40ed-8109-00898adce204'></a>

<::bar chart: A bar chart titled "% of Responses" on the y-axis and "Modalities" on the x-axis, comparing "Modalities Currently Supports" (light red bars) and "Modalities to Support in Future" (light blue bars). The first four modalities (Videos, Scientific data, Geospatial temporal Images, and Images) are highlighted with a light green background.

Data breakdown:
- Videos: Currently Supports 6.9%, To Support in Future 30.4%
- Scientific data: Currently Supports 13.8%, To Support in Future 26.1%
- Geospatial temporal Images: Currently Supports 20.7%, To Support in Future 52.2%
- Images: Currently Supports 37.9%, To Support in Future 47.8%
- Code: Currently Supports 41.4%, To Support in Future 34.8%
- Tabular data: Currently Supports 65.5%, To Support in Future 43.5%
- Natural Language Text: Currently Supports 93.1%, To Support in Future 39.1%
- Machine generated text e.g., logs: Currently Supports 82.8%, To Support in Future 60.9%::>

<a id='c81afb64-397b-4c3b-82b2-1943466722dd'></a>

Figure 13. Data modalities already supported (red) versus modali- ties planned for future support (blue) in production agent systems (N=29). Bars to the left of the dashed line indicate modalities with expected increases in future support, whereas modalities to the right are already widely supported with limited planned expan- sion. Interestingly, the modalities with the largest planned growth are all non-textual, pointing toward increasingly multimodal agent systems.

<a id='756fee38-6762-49c8-8bb4-41c27238263b'></a>

This data pattern demonstrates that practitioners enable substantial deployment by leveraging existing model capabilities within well-scoped applications rather than waiting for model improvements. We already see diverse implementations where agents support healthcare workflows, manage business operations, and accelerate scientific discovery in production (Table 1).

<a id='53c9da28-ea4b-4ec9-adfd-dc4354567114'></a>

Figure 5a and our case interviews reveal that relatively little attention has been given to applying agents to software-facing rather than human-facing problems. Most production agents interface directly with users through chat, voice, or interactive workflows. We observe fewer deployments for direct software operation, maintenance, and optimization tasks. We believe opportunities to improve automation in these software-integrated systems remain under-explored, though realizing this potential requires addressing the reliability challenges discussed in Section 8.1.

<a id='5fe53604-5edf-4c61-90ec-32aec3c34671'></a>

## 8.3. Open Research Questions

Beyond the directions we highlight in prior sections, we identify additional research questions based on deployment patterns in RQ1–4.

<a id='b287596b-12cd-4f1f-a5aa-fbad58d35815'></a>

We show that agents often operate without clean and fast cor-
rectness signals in the real-world (Section 7.1). We observe
an analogy with silent failures, where catching errors during
runtime is challenging and success signals arrive through
real consequences like financial loss or customer dissatis-

<a id='cc1b1630-a67d-4624-b3b7-9074e496a91e'></a>

16

<!-- PAGE BREAK -->

<a id='5b4a680f-bf09-4ca7-b3a4-107e710487c3'></a>

Measuring Agents in Production

<a id='84cd3464-a089-4e0b-86bb-d090bd98e7b7'></a>

faction. Engineers dedicate substantial effort ensuring agent
quality through close human oversight, demonstrating effec-
tive practices that enable current deployments. However, the
field has yet to reach consensus on effective ways to identify
_what_ and _when_ errors or low-quality responses occur. We
believe understanding agent failure modes [86], develop-
ing agent observability tools, and advancing runtime failure
mitigation techniques represent important open research di-
rections in addition to benchmarking and CI/CD integration
(Section 7.1).

<a id='1ea89908-8f3c-4b96-af15-e28ff8dd9f89'></a>

Significant evidence shows interest in post-training for agents [6, 87–93], yet Section 5.1 shows fine-tuning and reinforcement learning remain uncommon in deployment despite potential benefits. Our interviews reveal develop-ment effort currently prioritizes making agents work reliably over improving model capabilities for preference optimiza-tion, specialization, or cost trade-offs. We speculate this pattern emerges for two reasons: first, supervised fine-tuning and reinforcement learning impose barriers to entry, requir-ing large amounts of representative data or environments alongside with specialized ML expertise; second, model up-grades introduce complexity when fine-tuned models must adapt to new base versions. We believe the field needs more sample-efficient post-training approaches that remain robust to model upgrades and translate into repeatable engineering practices.

<a id='88cee169-6cfe-43ee-939d-40ff1d0f28f5'></a>

We find in Section 5.4 that 30% of deployed survey agents already adopt inference-time scaling techniques, with the use of multiple models being most popular. From detailed case studies, we find these are commonly simple methods, such as routing distinct subtasks to specialized models. We believe recent advances in generate-verify-search approaches [10, 94] have significant potential for non-science agent applications as well because they provide stronger reliability guarantees while maintaining controllability and explainability. However, realizing this potential may requires infrastructural support and redesign, such as building simulators and designing verifier. How to adapt agent runtime environments to support such inference-time search technique, and how to effectively utilize them in an online execution settings, are open research questions.

<a id='ee76c0ad-d532-4eb1-a3db-81a2e0c53cc9'></a>

We show in Figure 13 that 93% of current production agents take text or speech in natural language as inputs. Both our survey trends and interviews show high interest in expanding agent modalities to include images, spatiotemporal sequences, video, and scientific data. Interview participants suggest that teams currently focus on applications that are easy to measure and validate. We believe that once these initial deployments work reliably, agent features and capabilities will expand to handle richer input modalities.

<a id='f9c79519-8717-48fb-b992-da0dabb8ea69'></a>

# 9. Conclusion

We present MAP, the first large-scale systematic study char-
acterizing the engineering practices and technical methods
behind AI agents actively deployed in production. We an-
alyze data from agent practitioners across 26 domains to
answer four research questions regarding the state of real-
world agent development:

<a id='13980929-35c1-4ccd-8368-be601999533a'></a>

*   **RQ1: Why build agents?** Productivity drives adoption. Organizations deploy agents primarily to automate routine tasks and reduce human task hours, prioritizing measurable efficiency gains over novel capabilities.
*   **RQ2: How are agents built?** Simplicity and controllability dominate. Production systems favor closed-source models utilizing manual prompting rather than weight tuning. Architecturally, these systems rely on structured workflows with bounded autonomy and typically execute limited steps before human intervention.
*   **RQ3: How are agents evaluated?** Human verification remains the primary method. Practitioners rely heavily on human-in-the-loop evaluation because clean baselines and ground truth datasets are scarce. Automated techniques like LLM-as-a-judge complement human review rather than replace it.
*   **RQ4: What are the challenges?** Reliability represents the central development focus. The difficulty of ensuring correctness and evaluating non-deterministic agent outputs drives this friction. Latency and security typically act as manageable constraints rather than hard blockers as engineering workarounds and restricted environments currently manage them.

<a id='f7a939e9-ac98-4114-a4f1-a263fa4ab316'></a>

As "agent engineering" emerges into a new discipline, MAP provides researchers with critical visibility into real-world constraints and opportunities while offering practitioners proven deployment patterns across industries.

<a id='ed8ee6b9-bea6-4af1-af35-bf17f9e11997'></a>

## Acknowledgements
We thank our many anonymous participants without whom beginning this study would not be possible. This research was supported by gifts from Accenture, Amazon, AMD, Anyscale, Broadcom Inc., Google, IBM, Intel, Intesa San-paolo, Lambda, Mibura Inc, Samsung SDS, and SAP. We also thank Alex Dimakis, Drew Breunig, Tian Xia, and Shishir Patil for their valuable feedback and insightful discussions.

<a id='159f8c67-5719-4421-b820-8187a7260072'></a>

17

<!-- PAGE BREAK -->

<a id='8f82cf6e-dd6d-48b4-91a9-94d2a160f1c7'></a>

Measuring Agents in Production

<a id='786e9f74-82a5-48c1-ada5-c9c56613b8b7'></a>

# References

1.  Lilian Weng. Llm powered autonomous agents. URL
    https://lilianweng.github.io/posts
    /2023-06-23-agent/.
2.  OpenAI. Agents — openai api docs. URL https:
    //platform.openai.com/docs/guides/
    agents.
3.  Google Cloud. What are ai agents? definition, exam-
    ples, and types. URL https://cloud.google
    .com/discover/what-are-ai-agents.
4.  IBM. What are ai agents? URL https://www.ib
    m.com/think/topics/ai-agents.
5.  Tanay Varshney. Introduction to Ilm agents. URL
    https://developer.nvidia.com/blog/
    introduction-to-llm-agents/.
6.  Kexin Huang, Serena Zhang, Hanchen Wang, Yuan-
    hao Qu, Yingzhou Lu, Yusuf Roohani, Ryan Li, Lin
    Qiu, Gavin Li, Junze Zhang, Di Yin, Shruti Mar-
    waha, Jennefer N. Carter, Xin Zhou, Matthew Wheeler,
    Jonathan A. Bernstein, Mengdi Wang, Peng He, Jing-
    tian Zhou, Michael Snyder, Le Cong, Aviv Regev, and
    Jure Leskovec. Biomni: A general-purpose biomed-
    ical ai agent. bioRxiv, 2025. doi: 10.1101/2025.05.
    30.656746. URL https://www.biorxiv.org/
    content/early/2025/06/02/2025.05.30.
    656746.
7.  Kyle Swanson, Wesley Wu, Nash L. Bulaong, John E.
    Pak, and James Zou. The virtual lab of ai agents
    designs new sars-cov-2 nanobodies. Nature, 646:716-
    723, July 2025. doi: 10.1038/s41586-025-09442-9.
    URL https://www.nature.com/articles/
    s41586-025-09442-9.
8.  Sizhe Liu, Yizhou Lu, Siyu Chen, Xiyang Hu, Jieyu
    Zhao, Yingzhou Lu, and Yue Zhao. Drugagent:
    Automating ai-aided drug discovery programming
    through Ilm multi-agent collaboration, 2025. URL
    https://arxiv.org/abs/2411.15692.
9.  Alexander Novikov, Ngân Vũ, Marvin Eisenberger,
    Emilien Dupont, Po-Sen Huang, Adam Zsolt Wag-
    ner, Sergey Shirobokov, Borislav Kozlovskii, Fran-
    cisco J. R. Ruiz, Abbas Mehrabian, M. Pawan Ku-
    mar, Abigail See, Swarat Chaudhuri, George Holland,
    Alex Davies, Sebastian Nowozin, Pushmeet Kohli,
    and Matej Balog. Alphaevolve: A coding agent
    for scientific and algorithmic discovery, 2025. URL
    https://arxiv.org/abs/2506.13131.
10. Audrey Cheng, Shu Liu, Melissa Pan, Zhifei Li,
    Bowen Wang, Alex Krentsel, Tian Xia, Mert Cemri,

<a id='dfb5e038-f217-4811-a9d2-44ef6d6869cf'></a>

Jongseok Park, Shuo Yang, Jeff Chen, Lakshya Agrawal, Aditya Desai, Jiarong Xing, Koushik Sen, Matei Zaharia, and Ion Stoica. Barbarians at the gate: How ai is upending systems research, 2025. URL https://arxiv.org/abs/2510.06189.

<a id='bc40e527-f031-41ca-b094-ebb12b1cbfb9'></a>

[11] Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, and Chandan K Reddy. Llm-sr: Scientific equation discovery via programming with large language models, 2025. URL https://arxiv.org/abs/2404.18400.
[12] Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist: Towards fully automated open-ended scientific discovery, 2024. URL https://arxiv.org/abs/2408.06292.
[13] Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, Khaled Saab, Dan Popovici, Jacob Blum, Fan Zhang, Katherine Chou, Avinatan Hassidim, Burak Gokturk, Amin Vahdat, Pushmeet Kohli, Yossi Matias, Andrew Carroll, Kavita Kulkarni, Nenad Tomasev, Yuan Guan, Vikram Dhillon, Eeshit Dhaval Vaishnav, Byron Lee, Tiago R D Costa, José R Penadés, Gary Peltz, Yunhan Xu, Annalisa Pawlosky, Alan Karthikesalingam, and Vivek Natarajan. Towards an ai co-scientist, 2025. URL https://arxiv.org/abs/2502.18864.
[14] Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search, 2025. URL https://arxiv.org/abs/2504.08066.
[15] Cursor Inc. Cursor: AI Coding Assistant. https://www.cursor.com/, 2024. Accessed: 2025-09-30.
[16] Anthropic. Claude code: Agentic code assistant. https://www.anthropic.com/, 2025. Accessed: 2025-09-30.
[17] OpenAI. Openai codex. https://openai.com/index/introducing-codex/, 2025. Accessed: 2025-09-30.
[18] Windsurf. Windsurf. https://windsurf.com/, 2025. Accessed: 2025-09-30.
[19] IACPM and McKinsey. Emerging generative ai use cases in credit: Research results, March 2025. URL https://iacpm.org/wp-content/uploa

<a id='02e5276e-61e5-4581-8c1d-33333bef0146'></a>

18

<!-- PAGE BREAK -->

<a id='4585ad31-111b-4cee-a3e0-1038bf021673'></a>

Measuring Agents in Production
---

<a id='14fedd8d-653a-478b-a718-6e40dcd3ef37'></a>

ds/2025/03/IACPM-McKinsey-Gen-AI-W
ebinar-2025.pdf. Deck dated March 2025;
summarizes 2024 interviews and a December 2024
flash survey of 44 unique institutions.

<a id='9dbb4d7d-b2a3-4bde-b861-7f8e015b0fea'></a>

[20] Alexander Verhagen, Angela Luget, Olivia Conjeaud, Vasiliki Stergiou, and Debanjan Banerjee. How agentic ai can change the way banks fight financial crime, August 2025. URL https://www.mckinsey.com/capabilities/risk-and-resilience/our-insights/how-agentic-ai-can-change-the-way-banks-fight-financial-crime. McKinsey & Company article.

[21] Capgemini. Banks and insurers deploy ai agents to fight fraud and process applications, with plans for new roles to supervise the ai, November 2025. URL https://www.capgemini.com/us-en/news/press-releases/banks-and-insurers-deploy-ai-agents-to-fight-fraud-and-process-applications-with-plans-for-new-roles-to-supervise-the-ai/. Press release.

[22] Prakul Sharma, Val Srinivas, and Abhinav Chauhan. How banks can supercharge intelligent automation with agentic ai. URL https://www.deloitte.com/us/en/insights/industry/financial-services/agentic-ai-banking.html. Deloitte Center for Financial Services; article, 15-min read.

[23] Allianz SE. When the storm clears, so should the claim queue, November 2025. URL https://www.allianz.com/en/mediacenter/news/articles/251103-when-the-storm-clears-so-should-the-claim-queue.html. Allianz Media Center article.

[24] Economist Impact. Underwriting the future: the role of artificial intelligence in insurance, 2025. URL https://assets.ctfassets.net/9crgcb5vlu43/2X4wMqL2pYjxzZaldIpwyj/ala54dab0e338b6d59953f0da377e1d2/underwriting_the_future_the_role_of_artificial_intellifence_in_insurance_roundtable_summary_report.pdf. Roundtable summary report; sponsored by SAS.

[25] KPMG. Intelligent insurance: A blueprint for creating value through ai-driven transformation, March 2025. URL https://kpmg.com/kpmg-us/content/dam/kpmg/pdf/2025/us-insurance-report-final.pdf. Landing page for the U.S. report; downloadable PDF dated March 2025.

<a id='ad196582-6da2-48fa-985e-e6ec7d856fa8'></a>

[26] Everest Group. Agentic ai in insurance 2025, September 2025. URL https://www2.everestgrp.com/report/egr-2025-41-r-7520/. Includes a use-case prioritization matrix (impact vs. ease) and guidance on where to start and how to scale.

[27] Emma Linsenmayer. Leveraging artificial intelligence to support students with special education needs. Technical Report 46, OECD, September 2025. URL https://www.oecd.org/content/dam/oecd/en/publications/reports/2025/09/leveraging-artificial-intelligence-to-support-students-with-special-education-needs_ebc80fc8/1e3dffa9-en.pdf. Working paper.

[28] Sandeep Kakar, Pratyusha Maiti, Karan Taneja, Alekhya Nandula, Gina Nguyen, Aiden Zhao, Vrinda Nandan, and Ashok Goel. Jill watson: Scaling and deploying an ai conversational agent in online classrooms. In Proceedings of the 20th International Conference on Intelligent Tutoring Systems (ITS 2024), 2024. URL https://dilab.gatech.edu/test/wp-content/uploads/2024/05/ITS2024_JillWatson_paper.pdf. To appear in ITS 2024 proceedings.

[29] Michigan Ross School of Business. Google public sector helps enhance learning at the university of michigan with pioneering new agentic ai virtual teaching assistant, April 2025. URL https://michiganross.umich.edu/news/google-public-sector-helps-enhance-learning-university-michigan-pioneering-new-agentic-ai. School News / press release.

[30] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Jirong Wen. A survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6), March 2024. ISSN 2095-2236. doi: 10.1007/s11704-024-40231-1. URL http://dx.doi.org/10.1007/s11704-024-40231-1.

[31] Nestor Maslej, Loredana Fattorini, Raymond Perrault, Yolanda Gil, Vanessa Parli, Njenga Kariuki, Emily Capstick, Anka Reuel, Erik Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James Manyika, Juan Carlos Niebles, Yoav Shoham, Russell Wald, Tobi Walsh, Armin Hamrah, Lapo Santarlasci, Julia Betts Lotufo, Alexandra Rome, Andrew Shi, and Sukrut Oak. Artificial intelligence index report 2025. Technical report, AI Index Steering Committee, Stanford Institute for Human-Centered Artificial Intelligence (HAI), Stanford University, April 2025. URL

<a id='e75cd58a-7560-43ff-8f4d-047108e128d4'></a>

19

<!-- PAGE BREAK -->

<a id='a909d8b4-66e1-4192-ba86-78b1351e7093'></a>

Measuring Agents in Production

<a id='765f340c-ee2f-41a7-8273-853134727ee1'></a>

https://hai.stanford.edu/assets/files/hai_ai_index_report_2025.pdf. AI
Index Report.

<a id='84be875b-f223-4a4b-81fc-e0d40df18da2'></a>

[32] Capgemini Research Institute. Rise of agentic ai: How
trust is the key to human-ai collaboration, 2025. URL
https://www.capgemini.com/gb-en/in
sights/research-library/ai-agents/.
Capgemini Research Library landing page for the re-
port.

[33] PwC. Pwc's ai agent survey. Tech Effect: AI
& Analytics, PwC US, May 2025. URL https:
//www.pwc.com/us/en/tech-effect/ai
-analytics/ai-agent-survey.html. Sur-
vey of 308 US executives conducted Apr 22-28, 2025.

[34] Tianci Xue, Weijian Qi, Tianneng Shi, Chan Hee Song,
Boyu Gou, Dawn Song, Huan Sun, and Yu Su. An
illusion of progress? assessing the current state of web
agents, 2025. URL https://arxiv.org/abs/
2504.01382.

[35] Reuters Staff. Over 40% of agentic ai projects will
be scrapped by 2027, gartner says, June 2025. URL
https://www.reuters.com/business/o
ver-40-agentic-ai-projects-will-b
e-scrapped-by-2027-gartner-says-202
5-06-25/. Reuters, citing Gartner forecast.

[36] Pradyumna Shome, Sashreek Krishnan, and Sauvik
Das. Why johnny can't use agents: Industry aspira-
tions vs. user realities with ai agent software, 2025.
URL https://arxiv.org/abs/2509.145
28.

[37] MIT Media Lab and NANDA Initiative. Nanda: The
internet of ai agents. URL https://nanda.medi
a.mit.edu/. Project site describing a decentralized
"agentic web," with links to papers and events.

[38] Aditya Challapally, Chris Pease, Ramesh Raskar, and
Pradyumna Chari. The genai divide: State of ai in
business 2025. Technical report, MLQ.ai and Project
NANDA, July 2025. URL https://mlq.ai/med
ia/quarterly_decks/v0.1_State_of_A
I_in_Business_2025_Report.pdf. Prelimi-
nary findings from AI implementation research.

[39] Capgemini Research Institute. Rise of agentic ai, July
2025. URL https://www.capgemini.co
m/wp-content/uploads/2025/07/Final
-Web-Version-Report-AI-Agents.pdf.
Global executive survey on Al agents; key findings
summarized on the report landing page.

<a id='b8dcc0ae-2f73-4fb2-9130-617044ef7086'></a>

[40] PagerDuty. Agentic ai survey 2025. PagerDuty Report, March 2025. URL https://www.pagerduty./assets/Agentic-AI-Survey-Report_FINAL.pdf. Survey by Wakefield Research of 1,000 IT and business executives (U.S., U.K., Australia, Japan) conducted Feb 27–Mar 6, 2025.
[41] Alexander Sukharevsky, Dave Kerr, Klemens Hjartar, Lari Hämäläinen, Stéphane Bout, and Vito Di Leo. Seizing the agentic ai advantage, June 2025. URL https://www.mckinsey.com/capabilities/quantumblack/our-insights/seizing-the-agentic-ai-advantage. 28-page report. With contributions from Guillaume Dagorret.
[42] 2025: The year the frontier firm is born, April 2025. URL https://assets-c4akfrf5b4d3f4b7.z01.azurefd.net/assets/2025/04/WTI-2025-04-The-Year-the-Frontier-v13_68535917c7c2a.pdf. Global study combining surveys of 31,000 workers in 31 countries, LinkedIn labor-market trends, and Microsoft 365 productivity signals.
[43] LangChain. State of ai agents, 2024. URL https://www.langchain.com/stateofaiagents. Survey of 1,300+ professionals on agent adoption, use cases, controls, and challenges.
[44] Naveen Krishnan. Ai agents: Evolution, architecture, and real-world applications, 2025. URL https://arxiv.org/abs/2503.12687.
[45] Bang Liu, Xinfeng Li, Jiayi Zhang, Jinlin Wang, Tanjin He, Sirui Hong, Hongzhang Liu, Shaokun Zhang, Kaitao Song, Kunlun Zhu, Yuheng Cheng, Suyuchen Wang, Xiaoqiang Wang, Yuyu Luo, Haibo Jin, Peiyan Zhang, Ollie Liu, Jiaqi Chen, Huan Zhang, Zhaoyang Yu, Haochen Shi, Boyan Li, Dekun Wu, Fengwei Teng, Xiaojun Jia, Jiawei Xu, Jinyu Xiang, Yizhang Lin, Tianming Liu, Tongliang Liu, Yu Su, Huan Sun, Glen Berseth, Jianyun Nie, Ian Foster, Logan Ward, Qingyun Wu, Yu Gu, Mingchen Zhuge, Xiangru Tang, Haohan Wang, Jiaxuan You, Chi Wang, Jian Pei, Qiang Yang, Xiaoliang Qi, and Chenglin Wu. Advances and challenges in foundation agents: From brain-inspired intelligence to evolutionary, collaborative, and safe systems, 2025. URL https://arxiv.org/abs/2504.01990.
[46] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345, 2024.

<a id='66e19c4e-1a22-4658-bca9-df74ae50da06'></a>

20

<!-- PAGE BREAK -->

<a id='752a4f49-a2f2-474c-9705-4c8ae06babad'></a>

<u>Measuring Agents in Production</u>

<a id='b25b134c-ef47-4b58-8326-23a78513c9d7'></a>

47. Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, Bohan Wu, Binqi Chen, Ziyue Qiao, Qingqing Long, Rongcheng Tu, Xiao Luo, Wei Ju, Zhiping Xiao, Yifan Wang, Meng Xiao, Chenwu Liu, Jingyang Yuan, Shichang Zhang, Yiqiao Jin, Fan Zhang, Xian Wu, Hanqing Zhao, Dacheng Tao, Philip S. Yu, and Ming Zhang. Large language model agent: A survey on methodology, applications and challenges, 2025. URL https://arxiv.org/abs/2503.21460.
48. Francesco Piccialli, Diletta Chiaro, Sundas Sarwar, Donato Cerciello, Pian Qi, and Valeria Mele. Agentai: A comprehensive survey on autonomous agents in distributed ai for industry 4.0. Expert Systems with Applications, 291:128404, 2025. ISSN 0957-4174. doi: https://doi.org/10.1016/j.eswa.2025.128404. URL https://www.sciencedirect.com/science/article/pii/S0957417425020238.
49. Aske Plaat, Max van Duijn, Niki van Stein, Mike Preuss, Peter van der Putten, and Kees Joost Batenburg. Agentic large language models, a survey, 2025. URL https://arxiv.org/abs/2503.23037.
50. Asaf Yehudai, Lilach Eden, Alan Li, Guy Uziel, Yilun Zhao, Roy Bar-Haim, Arman Cohan, and Michal Shmueli-Scheuer. Survey on evaluation of llm-based agents, 2025. URL https://arxiv.org/abs/2503.16416.
51. Mahmoud Mohammadi, Yipeng Li, Jane Lo, and Wendy Yip. Evaluation and benchmarking of llm agents: A survey. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2, KDD '25, page 6129-6139. ACM, August 2025. doi: 10.1145/3711896.3736570. URL http://dx.doi.org/10.1145/3711896.3736570.
52. Feng He, Tianqing Zhu, Dayong Ye, Bo Liu, Wanlei Zhou, and Philip Yu. The emerged security and privacy of llm agent: A survey with case studies. ACM Comput. Surv., October 2025. ISSN 0360-0300. doi: 10.1145/3773080. URL https://doi.org/10.1145/3773080. Just Accepted.
53. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V. Chawla, Olaf Wiest, and Xiangliang Zhang. Large language model based multi-agents: A survey of progress and challenges, 2024. URL https://arxiv.org/abs/2402.01680.
54. Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom

<a id='a1822293-8580-48cf-ae74-7fb08bea377b'></a>

Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, Khaled Saab, Dan Popovici, Jacob Blum, Fan Zhang, Katherine Chou, Avinatan Hassidim, Burak Gokturk, Amin Vahdat, Pushmeet Kohli, Yossi Matias, Andrew Carroll, Kavita Kulkarni, Nenad Tomasev, Vikram Dhillon, Eeshit Dhaval Vaishnav, Byron Lee, Tiago R D Costa, José R Penadés, Gary Peltz, Yunhan Xu, Annalisa Pawlosky, Alan Karthikesalingam, and Vivek Natarajan. Towards an ai co-scientist. https://storage.googleapis.com/coscientist_paper/ai_coscientist.pdf, Feb 2025.

<a id='e3c09346-8f94-4756-80f0-4b0253c48e9e'></a>

55. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face, 2023. URL https://arxiv.org/abs/2303.17580.
56. Sahana Chennabasappa, Cyrus Nikolaidis, Daniel Song, David Molnar, Stephanie Ding, Shengye Wan, Spencer Whitman, Lauren Deason, Nicholas Doucette, Abraham Montilla, Alekhya Gampa, Beto de Paola, Dominik Gabi, James Crnkovich, Jean-Christophe Testud, Kat He, Rashnil Chaturvedi, Wu Zhou, and Joshua Saxe. Llamafirewall: An open source guardrail system for building secure ai agents, 2025. URL https://arxiv.org/abs/2505.03574.
57. Akshara Prabhakar, Roshan Ram, Zixiang Chen, Silvio Savarese, Frank Wang, Caiming Xiong, Huan Wang, and Weiran Yao. Enterprise deep research: Steerable multi-agent deep research for enterprise analytics, 2025. URL https://arxiv.org/abs/2510.17797.
58. Saurabh Jha, Rohan Arora, Yuji Watanabe, Takumi Yanagawa, Yinfang Chen, Jackson Clark, Bhavya Bhavya, Mudit Verma, Harshit Kumar, Hirokuni Kitahara, Noah Zheutlin, Saki Takano, Divya Pathak, Felix George, Xinbo Wu, Bekir O. Turkkan, Gerard Vanloo, Michael Nidd, Ting Dai, Oishik Chatterjee, Pranjal Gupta, Suranjana Samanta, Pooja Aggarwal, Rong Lee, Pavankumar Murali, Jae wook Ahn, Debanjana Kar, Ameet Rahane, Carlos Fonseca, Amit Paradkar, Yu Deng, Pratibha Moogi, Prateeti Mohapatra, Naoki Abe, Chandrasekhar Narayanaswami, Tianyin Xu, Lav R. Varshney, Ruchi Mahindru, Anca Sailer, Laura Shwartz, Daby Sow, Nicholas C. M. Fuller, and Ruchir Puri. Itbench: Evaluating ai agents across diverse real-world it automation tasks, 2025. URL https://arxiv.org/abs/2502.05352.
59. Anthropic. Developing a computer use model. Anthropic News, October 2024. URL https://www.

<a id='87276901-63a6-4213-bd7d-3b786e1f94d6'></a>

21

<!-- PAGE BREAK -->

<a id='2f72a2d3-c4e6-4342-8c1c-a86e47deec99'></a>

Measuring Agents in Production

<a id='447bfc58-7ec2-450e-a7db-3e32fe3b228a'></a>

anthropic.com/news/developing-compu
ter-use.

60. Jacob Jackson, Phillip Kravtsov, and Shomil Jain. Improving cursor tab with online rl, September 2025. URL https://cursor.com/blog/tab-rl. Cursor Blog, Research.

61. Jacob Teo, Nikhil Jha, Connor Fogarty, Gary Chang, Theodor Marcu, Edison Zhang, Albert Tam, Sean Sullivan, Swyx, and Silas Alberti. Introducing swe-1.5: Our fast agent model, October 2025. URL https://cognition.ai/blog/swe-1-5. Cognition AI Blog.

62. Anthropic Engineering Team. How we built our multi-agent research system, June 2025. URL https://www.anthropic.com/engineering/multi-agent-research-system. Anthropic Engineering Blog.

63. Block, Inc. goose: an open source ai agent built for developers, 2025. URL https://block.github.io/goose/. Project homepage.

64. OpenAI. Codex, 2025. URL https://openai.com/codex/. Product page.

65. Gemini CLI Maintainers. Welcome to gemini cli documentation, 2025. URL https://geminicli.com/docs/. Project documentation.

66. Cline Bot Inc. Cline: The open coding agent, 2025. URL https://cline.bot/. Project homepage.

67. OpenHands. Openhands: The open platform for cloud coding agents, 2025. URL https://openhands.dev/. Project homepage.

68. UC Berkeley RDI. Agentic AI Summit 2025. Event page, August 2025. URL https://rdi.berkeley.edu/events/agentic-ai-summit. Event date Aug 2, 2025; UC Berkeley; livestream available.

69. The AI Alliance. AI Agent SF Meetup #5 - Agents in Production. Event page, August 2025. URL https://luma.com/x16vikh7. Event date Aug 5, 2025; Hosted by Bay Area AI; livestream available.

70. UC Berkeley Sky Computing Lab. Category: Retreats and camps, 2025. URL https://sky.cs.berkeley.edu/category/retreats-and-camps. News and event listings for UC Berkeley Sky Computing Lab retreats and camps.

71. Liana Patel, Siddharth Jha, Melissa Pan, Harshit Gupta, Parth Asawa, Carlos Guestrin, and Matei Zaharia. Semantic operators and their optimization:

<a id='22510e15-0eeb-4830-96d9-7469903ea85b'></a>

Enabling llm-based data processing with accuracy
guarantees in lotus. Proc. VLDB Endow., 18(11):
4171-4184, September 2025. ISSN 2150-8097. doi:
10.14778/3749646.3749685. URL https://doi.
org/10.14778/3749646.3749685.

[72] Llama Team, AI @ Meta. The llama 3 herd of models,
2024. URL https://arxiv.org/abs/2407
.21783.

[73] Qwen Team. Qwen3 technical report, 2025. URL
https://arxiv.org/abs/2505.09388.

[74] OpenAI. gpt-oss-120b & gpt-oss-20b model card,
2025. URL https://arxiv.org/abs/2508
.10925.

[75] Anthropic. Claude: AI Assistant for Next-Generation
Tasks. www.anthropic.com, 2025.

[76] OpenAI. OpenAI Platform Models. https://pl
atform.openai.com/docs/models, 2025.
Accessed: December 5, 2025.

[77] Omar Khattab, Keshav Santhanam, Tarun Reuel, Sara
Saad-Falak, Jack Hall, Matei Zaharia, and Christopher
Potts. Dspy: Compiling declarative language model
pipelines into self-improving systems. In NeurIPS,
2024.

[78] Qizheng Zhang, Changran Hu, Shubhangi Upasani,
Boyuan Ma, Fenglu Hong, Vamsidhar Kamanuru, Jay
Rainton, Chen Wu, Mengmeng Ji, Hanchen Li, Ur-
mish Thakker, James Zou, and Kunle Olukotun. Agen-
tic context engineering: Evolving contexts for self-
improving language models, 2025. URL https:
//arxiv.org/abs/2510.04618.

[79] Lakshya A Agrawal, Shangyin Tan, Dilara Soylu,
Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Arnav
Singhvi, Herumb Shandilya, Michael J Ryan, Meng
Jiang, Christopher Potts, Koushik Sen, Alexandros G.
Dimakis, Ion Stoica, Dan Klein, Matei Zaharia, and
Omar Khattab. Gepa: Reflective prompt evolution
can outperform reinforcement learning, 2025. URL
https://arxiv.org/abs/2507.19457.

[80] Langchain. https://www.langchain.com/.
Accessed: 26 November 2025.

[81] LangChain Inc. Langgraph, 2025. URL https:
//www.langchain.com/langgraph. Product
page for the LangGraph agentic framework.

[82] crewAI. crewai: The leading multi-agent platform,
2025. URL https://www.crewai.com/. Prod-
uct homepage.

<a id='35d3d251-05c6-4a74-8aca-b6b5a2bd4cc2'></a>

22

<!-- PAGE BREAK -->

<a id='e77b4979-6296-4d4f-82a0-83c7e049f770'></a>

Measuring Agents in Production

<a id='77253b70-c619-4764-9d10-1b35fab8f97b'></a>

[83] LlamaIndex. Llamaindex: Redefine document work-flows with ai agents, 2025. URL https://www.llamaindex.ai/. Product homepage.
[84] OpenAI. Swarm: Lightweight multi-agent orchestra-tion, 2025. URL https://github.com/openai/swarm. GitHub repository.
[85] Cursor Team. Cursor ide: Ai-powered development with rule-based control. https://cursor.com/docs, 2025. Last Accessed October 4, 2025.
[86] Mert Cemri, Melissa Z. Pan, Shuyi Yang, Lak-shya A. Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Klein, Kan-nan Ramchandran, Matei Zaharia, Joseph E. Gonzalez, and Ion Stoica. Why do multi-agent llm systems fail?, 2025. URL https://arxiv.org/abs/2503.13657.
[87] Cursor Team. Composer: Building a fast frontier model with rl, October 2025. URL https://cursor.com/blog/composer. Cursor Blog, Re-search.
[88] Yuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. Recursive introspection: Teaching language model agents how to self-improve, 2024. URL https://arxiv.org/abs/2407.18219.
[89] Shiyi Cao, Dacheng Li, Fangzhou Zhao, Shuo Yuan, Sumanth R. Hegde, Connor Chen, Charlie Ruan, Tyler Griggs, Shu Liu, Eric Tang, Richard Liaw, Philipp Moritz, Matei Zaharia, Joseph E. Gonzalez, and Ion Stoica. Skyrl-agent: Efficient rl training for multi-turn llm agent, 2025. URL https://arxiv.org/abs/2511.16108.
[90] Siyu Yuan, Zehui Chen, Zhiheng Xi, Junjie Ye, Zhengyin Du, and Jiecao Chen. Agent-r: Training lan-guage model agents to reflect via iterative self-training, 2025. URL https://arxiv.org/abs/2501.11425.
[91] Yifan Song, Weimin Xiong, Xiutian Zhao, Dawei Zhu, Wenhao Wu, Ke Wang, Cheng Li, Wei Peng, and Sujian Li. Agentbank: Towards generalized llm agents via fine-tuning on 50000+ interaction trajecto-ries, 2024. URL https://arxiv.org/abs/2410.07706.
[92] Chanwoo Park, Seungju Han, Xingzhi Guo, Asuman Ozdaglar, Kaiqing Zhang, and Joo-Kyung Kim. Maporl: Multi-agent post-co-training for collaborative large language models with reinforcement learning, 2025. URL https://arxiv.org/abs/2502.18439.

<a id='3ff4eb29-5f73-46a0-b4f1-392342ba1fad'></a>

[93] Hao Ma, Tianyi Hu, Zhiqiang Pu, Boyin Liu, Xiaolin Ai, Yanyan Liang, and Min Chen. Coevolving with the other you: Fine-tuning Ilm with sequential cooperative multi-agent reinforcement learning, 2025. URL https://arxiv.org/abs/2410.06101.

[94] Alexander Novikov, Ngân Vũ, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco J. R. Ruiz, Abbas Mehrabian, M. Pawan Kumar, Abigail See, Swarat Chaudhuri, George Holland, Alex Davies, Sebastian Nowozin, Pushmeet Kohli, and Matej Balog. AlphaEvolve: A coding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131, 2025.

<a id='50e47b36-0c50-4982-adff-15edf7423c3c'></a>

23

<!-- PAGE BREAK -->

<a id='8e935043-1dce-4000-9071-db1ad4fc3a57'></a>

Measuring Agents in Production

<a id='d1078243-6af4-44bd-ab01-6b5e89feee43'></a>

# Organization of Appendix
The appendix is organized as follows: in Appendix A we release the results on full survey dataset and report all statistics on the complete set of agent systems (not only the deployed subset used in the main paper), including comparisons between results on all agents and on deployed agents only; in Appendix B we provide additional analyses, such as our topic-normalization procedure for agent domains, the outlier domains where agents operate, and more detailed breakdowns of the challenges practitioners face when building agents. In Appendix C we unify our terminology for agents by defining what we mean by tasks, subtasks, and steps, and we provide an overview of the agent workflow schema used throughout the paper. In Appendix D we describe our interview protocol and present extended details of our case studies, including interview outlines and anonymized interviewee demographics. Finally, in Appendix E we provide the full survey questionnaire, including question wording and the branching logic that determines the survey flow.

<a id='6f6e22d4-a56d-415b-a329-af7f79cad2cf'></a>

## A. Results on All Survey Data

In the main body of the paper, we focused on results filtered exclusively to *deployed agents* in production or pilot phases, to highlight successful real-world practices under realistic operational constraints. In this appendix, we present the corresponding results computed over [All Data]: all 306 valid survey responses, regardless of deployment stage. This expanded view includes prototype, research, and legacy systems (sunset and retired), providing a broader perspective across the full agent development lifecycle.

<a id='923bbf3e-cd1c-4cfe-b84e-60981564288f'></a>

For ease of comparison, each [All Data] figure mirrors a figure in the main text and uses the same layout and question wording. In the discussion below, we briefly summarize the key patterns in the full dataset and highlight how they compare to the deployed-only subset.

<a id='618bee3e-6cb2-412b-9001-c02666305178'></a>

<::horizontal bar chart::> (a) Reasons Practitioners Build AI Agents. The x-axis is labeled '% of Responses' and ranges from 0% to 100%. The y-axis lists various reasons. The bars represent the following: Increasing Productivity: 68.3% (69); Reducing Human Task-Hours: 63.4% (64); Automating Routine Labor: 43.6% (44); Increasing Client Satisfaction: 36.6% (37); Reducing Human Expertise: 32.7% (33); Novel Technology: 32.7% (33); Reducing Interdisciplinary Expertise: 20.8% (21); Faster Failure Response Time: 18.8% (19); Risk Mitigation: 16.8% (17). <::>

<a id='26eb70cf-b459-4707-849d-3a27dcb2bc84'></a>

<::horizontal bar chart::>
(b) Application Domains
Chart showing the percentage of responses for different application domains, with error bars indicating variability. The x-axis is labeled '% of responses' from 0% to 60%.
- Finance & Banking: 33.3% (35)
- Technology: 21.9% (23)
- Corporate Services: 21.0% (22)
- Data Analytics: 13.3% (14)
- Healthcare Services: 11.4% (12)
- Research & Development: 11.4% (12)
- Customer Support: 10.5% (11)
- Software Development: 9.5% (10)
- Legal & Compliance: 6.7% (7)
- Retail: 2.9% (3)
- Other: 17.1% (18)
<::/horizontal bar chart::>

<a id='13697bd9-e95a-460f-b555-cb8917f9f60f'></a>

Figure 14. [All Data] Overview of Agentic AI system motivations and domains across all development stages (Production, Pilot, Prototype, and Research). These figures correspond to Figure 1 (a) and Figure 2 (b) in the main text but include all survey data. (a) Reasons practitioners build AI agents (N = 101). Increasing productivity remains the most selected benefit across the full dataset. (b) Application domains where practitioners build agents (N = 105). Note that this is a multi-class classification question where each system may be assigned to multiple domain categories, and thus the proportions do not sum to 1.

<a id='2aa03444-8274-4ee0-a765-caaceed647ab'></a>

A.1. Applications, Users, and Requirements
Motivations and application domains. Figure 14a reproduces our analysis of motivations for using agents over non-agentic alternatives using all survey responses. We observe that the overall ranking of benefits is highly stable compared to deployed only agents as shown in Figure 1: increasing productivity and efficiency remains the most frequently selected reason for adopting agents, followed by reducing human task-hours and automating routine labor.

<a id='84a79fb5-9a9f-4691-ab3b-8b79175489bf'></a>

For application domains of agents, (Figure 14b) become even more diverse in the full dataset compared to deployed only agents (Figure 2). The same high-level industries i.e., finance and banking, technology, and corporate services remain prominent. However, the long tail of "other" domains grows, reflecting additional experimental and research systems in

<a id='da10c932-ebd3-40b4-8d0d-2fe2728d1934'></a>

24

<!-- PAGE BREAK -->

<a id='c17a6bbe-3c19-4d00-9fe5-ab65577ce8f6'></a>

Measuring Agents in Production

<a id='3a554e80-d641-42c1-a9f1-82c9ee6a7a29'></a>

<::chart: Two bar charts side-by-side. (a) Distribution of primary end users (N=103). The y-axis lists user types: Internal Employees, External Customers, Non-Agentic Software, and Other AI Agents. The x-axis is '% of Responses' from 0% to 80%. Bars are colored and patterned to distinguish 'Human user' (hatched) and 'Non-human user' (solid). Data: Internal Employees: 54.4% (56) (Human user); External Customers: 35.9% (37) (Human user); Non-Agentic Software: 4.9% (5) (Non-human user); Other AI Agents: 3.9% (4) (Non-human user). Each bar includes an error bar. (b) Reported tolerable end-to-end response latency for all systems (N=84). The x-axis shows latency tolerance categories: < Subsecond, Seconds, Minutes, Hours, >1 day, No limit set. The y-axis is 'Percentage (%)' from 0% to 60%. Data: < Subsecond: 11.4% (9); Seconds: 25.3% (20); Minutes: 34.2% (27); Hours: 7.6% (6); >1 day: 2.5% (2); No limit set: 19.0% (15). Each bar includes an error bar.::>Figure 15. [All Data] Overview of Agentic AI system characteristics across all development stages in terms of primary end users and latency requirements. This figure corresponds to Figure 5b in the main text but includes all survey data. (a) Distribution of primary end users (N=103), where hatched bars (///) denote human end-users; and (b) Reported tolerable end-to-end response latency for all systems (N=84). The high percentage of human end-users and tolerance for minute-level latency are consistent across the full dataset.

<a id='ffd7afca-08f0-44d4-a3e1-eb83ca19d4c2'></a>

<::transcription of the content: chart::>Chart (a): Bar chart showing the percentage of responses for the number of distinct models. The y-axis is '% of Responses' from 0% to 50%. The x-axis is 'Number of Distinct Models'. The bars are: '1' at approximately 35% (value 12), '2' at approximately 20% (value 7), '3' at approximately 29% (value 10), and '4+' at approximately 17% (value 6). Chart (b): Bar chart showing the percentage of responses for instruction length in tokens. The y-axis is '% of Responses' from 0% to 30%. The x-axis is 'Instruction Length (tokens)'. The bars are: '0-250' at approximately 29% (value 14), '250-500' at approximately 23% (value 11), '500-2.5k' at approximately 21% (value 10), '2.5k-10k' at approximately 15% (value 7), and '10k+' at approximately 12% (value 6). Chart (c): Bar chart showing the percentage of responses for autonomous steps/cycles. The y-axis is '% of Responses' from 0% to 50%. The x-axis is 'Autonomous Steps/Cycles'. The bars are: '1-4' at approximately 37% (value 33), '5-10' at approximately 22% (value 20), 'Tens' at approximately 22% (value 20), 'Hundreds' at approximately 5% (value 4), 'Thousands' at approximately 3% (value 3), and 'No limit' at approximately 10% (value 9). Figure 16. [All Data] Overview of core components configurations and architectures across all agents. This figure corresponds to Figure 8 in the main text but includes all survey data. (a) Number of distinct models combined to solve a single logical task (N=35). Deployed agents (Figure 8a) tend to use fewer models compared to all data, as the full dataset includes more experimental systems that utilize a higher number of distinct models; (b) Distribution of prompt lengths in terms of tokens (N=48). Prompt length remains pretty similar in both the deployed-only (Figure 8b) and all-data subsets; and (c) Number of autonomous execution steps before user intervention (N=89). Results on all survey data allows for a greater number of steps before human interference compared to deployed agents only (Figure 8c), reflecting the stricter need for control and monitoring in production environments.::>

<a id='efb01e03-ceed-47bf-94e1-991d1be59a27'></a>

areas such as education, creative tools, and scientific workflows that have not yet reached deployment.

<a id='17325fc4-b5d2-41c9-8091-cde1a2732c8d'></a>

**End users and latency requirements.** Consistent with the deployed-only subset, the vast majority of systems in the full dataset still target human users as shown in Figure 15a, with internal employees and external customers together comprising most end-user bases. The relative proportions shift only slightly when we include prototypes, suggesting that human-centric interaction remains the default even in early experimentation.

<a id='b3bf9f22-cf65-471f-99a2-5b4d64c13d11'></a>

Latency requirements also similarly remain relaxed in the full dataset (Figure 15b compared to Figure 5b). Most teams still report tolerating response times on the order of minutes, with a non-trivial fraction indicating that no explicit latency limit has been set. Compared to deployed agents only, the fraction of agents with undefined latency budgets is slightly higher in [All Data], which is consistent with prototypes and research artifacts that have not yet been hardened with production SLOs. Overall, these figures confirm that the preference for latency-relaxed, quality-focused applications is not an artifact of our deployment filter.

<a id='ad1d5901-5191-4b1e-b30e-a8890c007c9c'></a>

25

<!-- PAGE BREAK -->

<a id='aa8f12d5-c765-41c0-8a69-7396b29fa982'></a>

Measuring Agents in Production

<a id='8ba57378-3742-4569-bc1f-e54999103501'></a>

A.2. Models, Architectures, and Prompting

Core components and autonomy. Figure 16 summarizes core component configurations and agent architectures over the full dataset and corresponds to Figure 8 in the main text. Compared to deployed agents (Figure 8a), distribution of number of distinct models used exhibits a heavier tail: non-deployed and research systems are more likely to combine many distinct models, leading to a higher incidence of configurations with four or more models. This pattern aligns with our qualitative observation that teams explore richer multi-model setups during early experimentation, then consolidate to a smaller, more manageable set of models as they move toward deployment.

<a id='fae6f772-a2e7-4c14-8401-5869d9b94ac4'></a>

Prompt lengths remain broadly similar between the deployed-only (Figure 8b) and full datasets ((Figure 16b).

<a id='cbef9c93-bf2a-4096-b16d-a8cb16125b9c'></a>

Comparison between Figure 8c and Figure 16c reveals a clearer separation in autonomy. When we include all agents, systems allow a greater number of autonomous steps or cycles before human intervention compared to deployed agents only. Experimental and research systems are more likely to sit in the "tens of steps" and "no explicit limit" regimes, whereas production deployments concentrate in the low-step regime to control cost, latency, and failure amplification. Taken together, these results reinforce our interpretation that bounded autonomy is a deliberate design choice for production reliability, while higher autonomy is more common in exploratory settings.

<a id='6688be08-4e2f-4db8-b28b-26b7bfe4529d'></a>

Prompt construction strategies and frameworks. Figure 17a reproduces our analysis of prompt construction and framework usage using all survey responses. The results show that human-crafted prompts remain central across the full dataset: fully manual and manual+LLM strategies are still the dominant modes. However, when we include non-deployed agents, we see a modest shift toward more automated prompting: fully manual prompting is slightly more common among deployed agents (Figure 7), while the [All Data] distribution shows somewhat higher use of "fully autonomous" prompting and prompt optimizers. This suggests that automated prompt construction is currently used more as an experimental technique and is less frequently adopted in production systems where controllability is critical.

<a id='e9a5b721-e844-4edd-8d6d-835f6987bb99'></a>

Figure 17b compares framework usage across all agents and corresponds to Figure 17b in the main text. The overall split between using any framework vs. no framework remains nearly identical between the full dataset and deployed-only subset, indicating that teams decide early whether to invest in a framework-based stack or implement their own orchestration. Within the "framework" group, the Other category expands slightly in [All Data], reflecting experimentation with a broader variety of less common or homegrown frameworks during research and prototyping stages, beyond the dominant framework families.

<a id='1cd6fde8-9f2f-46c2-98e5-9fe025cf25b7'></a>

<::Horizontal bar chart titled "% of Responses". The y-axis lists categories and the x-axis represents the percentage of responses. Each bar includes an error bar and displays the percentage and raw count.
- Manual + AI: 50.6% (43)
- Fully Manual: 28.2% (24)
- Prompt Optimizer: 8.2% (7)
- Fully Autonomous: 5.9% (5)
- Predefined Library: 3.5% (3)
: chart::>
(a)

<a id='d9d37a0f-292a-45eb-ba97-40b946287967'></a>

<::Bar chart showing the percentage of responses for framework usage.
Y-axis: % of Responses, ranging from 0% to 70%.
X-axis labels: 'No Did Not Use Any Framework' and 'Yes Used Framework'.

Legend:
- BeeAI
- LlamaIndex
- Other
- CrewAI
- LangChain/LangGraph

Data:
- 'No Did Not Use Any Framework': 39.5% of responses.
- 'Yes Used Framework': 60.5% of responses, broken down as a stacked bar:
  - LangChain/LangGraph: 23.3%
  - CrewAI: 14.0%
  - Other: 14.0%
  - LlamaIndex: 4.7%
  - BeeAI: (topmost segment, percentage not explicitly labeled)

Figure label: (b): chart::>

<a id='4ae4cbe8-6c52-45a7-82b0-30f50af8eb49'></a>

Figure 17. [All Data] Overview of core technical implementations: Prompt Construction and Framework Usage. The left and right figures correspond to Figure 7 and Figure 9 in the main text respectively but they include all survey data. (a) Distribution of prompt construction strategies across all agents (N=85). Human input remains central to prompt crafting, however, fully manual is slightly more common in deployed agents (Figure 7) compared to all data in the left plot. Similarly 'fully autonomous' prompting is more common in the All Data, suggesting that manual prompting is more favored for production systems. (b) Frameworks reported to support critical functionality (N=43). The percentage of practitioners using a framework versus not using one remains almost exactly the same between the full dataset (right plot) and the deployed-agents-only (Figure 9). The "other frameworks" category increases slightly in the full dataset compared to deployed agents only, likely reflecting more diverse experimentation in non-production systems.

<a id='d6beb5be-908e-45e9-ada6-6add51d1e639'></a>

26

<!-- PAGE BREAK -->

<a id='1e049f80-aced-4225-bc19-68730e599156'></a>

Measuring Agents in Production

<a id='5f862b07-f2d4-48ca-8c54-c1ff25a54b89'></a>

A.3. Evaluation Practices for All Agents

Figure 18 presents evaluation practices across all agents and mirrors Figure 10 in the main text. Figure 18a shows distribution of comparison against non-agentic baselines. When we include prototypes and research systems, the fraction of teams that explicitly compare their agents to alternative solutions is slightly lower than in the deployed-only subset (34% in all data vs. 38.7% in deployed agents in Figure 10a). This suggests that some experimental agents are perhaps still in early stages where rigorous baseline comparison has not yet been prioritized, or where teams are primarily exploring feasibility rather than relative gains.

<a id='2f288c07-7b32-4a5e-999e-3f367121ad9a'></a>

Figure 18b reports the distribution of different evaluation methods. The ordering of methods remains unchanged: human-in-the-loop (manual) evaluation is still the most common strategy, followed by model-based evaluation (LLM-as-a-judge).
However, manual evaluation is somewhat more prevalent among deployed agents (Figure 10b), whereas the [All Data] distribution shows a relatively higher share of automated methods. This is consistent with the idea that experimental and research systems may rely more on automated or lightweight checks, while production systems invest more heavily in human verification before and during deployment.

<a id='5c117bbe-a44d-4560-b24c-3e216d283733'></a>

Figure 18c visualizes co-occurrence patterns between evaluation strategies. Human-in-the-loop evaluation remains the central hub in the evaluation graph, with high overlap with all other methods in the full dataset. At the same time, its co-occurrence with other strategies is slightly lower in [All Data] than in the deployed-only subset (Figure 10c), reflecting that some experimental systems use model-based or rule-based checks without consistently pairing them with human review. In contrast, deployed agents are more likely to combine automated evaluation with human verification perobably for higher assurance.

<a id='9d6fb929-fb9a-4736-83ac-dcc944f5f695'></a>

<::Figure 18. [All Data] Evaluation Practices in Agents. This figure corresponds to Figure 10 in the main text but includes all survey data (N = 47).: chart::>

(a) Comparison to Alternatives: Bar chart showing whether participants explicitly compared their agent against a non-agentic baseline.
- X-axis: "Yes Compared to Alternative", "No Did Not Compare"
- Y-axis: "% of Responses"
- Legend:
  - Alternative does not exist
  - Alternative might exist

Data:
- Yes Compared to Alternative:
  - Alternative does not exist: 34.0% of Responses
- No Did Not Compare:
  - Alternative does not exist: 23.4% of Responses
  - Alternative might exist: 42.6% of Responses (Total: 66.0%)

(b) Evaluation Methods Distribution: Horizontal bar chart showing the distribution of different evaluation strategies reported by survey participants.
- X-axis: "% of Responses"
- Y-axis: Evaluation Methods

Data (Method: % of Responses (N count)):
- Manual (Human in the Loop): 63.3% (31)
- Model Based (e.g., LLM-as-a-Judge): 46.9% (23)
- Cross-Referencing (e.g., RAG, Knowledge Graphs): 40.8% (20)
- Rule Based (e.g., Syntax Checks): 42.9% (21)
- None of the above: 8.2% (4)

(c) Evaluation Strategies Co-occurrence: Heatmap visualizing the pairwise overlap between evaluation strategies.
- X-axis labels: Manual, Model Based, Cross-Referencing, Rule Based
- Y-axis labels: Manual, Model Based, Cross-Referencing, Rule Based
- Color scale: % of Responses, ranging from 20% to 60%

Data (Row Method vs. Column Method: % (N count)):
- Manual vs. Manual: 63.3% (31)
- Manual vs. Model Based: 32.7% (16)
- Manual vs. Cross-Referencing: 28.6% (14)
- Manual vs. Rule Based: 26.5% (13)

- Model Based vs. Manual: 32.7% (16)
- Model Based vs. Model Based: 46.9% (23)
- Model Based vs. Cross-Referencing: 24.5% (12)
- Model Based vs. Rule Based: 18.4% (9)

- Cross-Referencing vs. Manual: 28.6% (14)
- Cross-Referencing vs. Model Based: 24.5% (12)
- Cross-Referencing vs. Cross-Referencing: 40.8% (20)
- Cross-Referencing vs. Rule Based: 20.4% (10)

- Rule Based vs. Manual: 26.5% (13)
- Rule Based vs. Model Based: 18.4% (9)
- Rule Based vs. Cross-Referencing: 20.4% (10)
- Rule Based vs. Rule Based: 42.9% (21)

<::text:>
Deployed agents (Figure 10a show a higher comparison rate (38.7% in deployed vs. 34% in all data) of comparison to alternative solutions, suggesting that experimental prototypes may not have invest as much on evaluation stages yet. (b) Evaluation Methods Distribution: Distribution of different evaluation strategies reported by survey participants. Manual evaluation (human-in-the-loop) is used more for deployed agents (Figure 10b) compared to the full dataset, which includes more experimental and research systems. Autonomous evaluation methods are relatively less common in deployed agents compared to the data for all agents in our survey. (c) Evaluation Strategies Co-occurrence: Visualizes the pairwise overlap between evaluation strategies. Manual human-in-the-loop evaluation is still the central strategy, but its co-occurrence with other methods is slightly lower in the all-data subset compared to deployed agents (Figure 10c), indicating that autonomous evaluation methods have more complementary roles in in deployed agents.

<a id='0eec6c97-0a22-4f38-9965-dfef61e0cf47'></a>

## A.4. Data Handling, Latency Challenges, and Modalities Across All Agents
Figure 19 corresponds to Figure 12 and Figure 11 in the main text but reports statistics across all survey responses.

<a id='585c5b17-3f7b-435c-929c-c401479122e7'></a>

Figure 19 shows distribution of data sources for agents which is remarkably stable when moving from deployed agents only (Figure 12) to [All Data] in Figure 19. This possibly indicates that the underlying data plumbing for agents is largely shared across lifecycle stages: teams tend to set up similar ingestion and handling pipelines during prototyping that then carry through to production with incremental hardening.

<a id='9f04e5b3-9d96-40e9-a2e0-bef1abf1a1ec'></a>

In addition, Figure 19b reports how often latency is described as a problem across all systems. The distribution changes only modestly when compared to the deployed-only subset (Figure 11), and we again see that latency is not the dominant blocker

<a id='a9765640-9a8a-456a-96ca-b99d0b2030d1'></a>

27

<!-- PAGE BREAK -->

<a id='30e61824-fcbe-4d53-8e6b-4565d0538d31'></a>

## Measuring Agents in Production

<a id='f86597a9-21b5-45d4-b14c-71ff9acd141a'></a>

<::bar chart: A horizontal bar chart showing the percentage of responses for different types of data. The x-axis is labeled "% of Responses" and ranges from 0% to 100%. The y-axis lists the data types. Each bar also includes a percentage and a count in parentheses, along with error bars.
- Database: 76.7% (33)
- Confidential data: 62.8% (27)
- Live user data: 62.8% (27)
- Live non-user data: 48.8% (21)
- Public online data: 37.2% (16)
(a)::>

<a id='c0954da8-9438-4ddf-83cf-7c3be3225a51'></a>

<::Horizontal bar chart titled "% of Responses" with three categories on the y-axis:
- Latency Blocker: 14.6% (6)
- Deployable with Latency Gap: 61.0% (25)
- No Latency Concern: 24.4% (10)
Each category is represented by a bar and a box plot, indicating the distribution of responses.
: chart::>

<a id='fe089f97-f157-4185-9083-2252e021d15e'></a>

(b)

<a id='dd097137-6b12-4526-86b4-a401fbc1debc'></a>

Figure 19. [All Data] The figures correspond to Figure 12 (a) and Figure 11 (b) in the main text but include all survey data. (a) Types and modes of data ingestion and handling in all agent (_N_ = 43). The distribution of data sources and handling methods did not change substantially when moving from deployed agents only to all survey data. (b) Degree to which latency causes problems for all agent systems (_N_ = 46). The distribution of problematic latency did not change substantially when moving from deployed agents only to all survey data, suggesting latency is not a primary deployment blocker across the development lifecycle.

<a id='a3462fc4-c1b3-4a5c-be34-6cc67d3c9011'></a>

for most agent deployments. This supports our broader conclusion that agents are currently concentrated in latency-relaxed settings where quality and correctness dominate over strict real-time responsiveness.

<a id='b2f48af9-3c8e-4fdc-b067-86624f77b08a'></a>

Finally, Figure 20 mirrors Figure 13 in the main text but includes all survey data. The overarching trend remains the same: growth is heavily concentrated in non-textual modalities, pointing towards increasingly multimodal agentic systems. Interestingly, the emphasis on future support for non-text modalities is even stronger than in the deployed-only subset, indicating that experimental and research agents are pushing more aggressively into multimodal directions (e.g., image, audio, and structured data) that may not yet have reached stable production deployment.

<a id='939a7017-44dc-420d-b5d2-58145dae0bb0'></a>

<::Bar chart showing the percentage of responses for 'Modalities Currently Supports' and 'Modalities to Support in Future' across various modalities. The Y-axis is labeled '% of Responses' from 0% to 100%. The X-axis is labeled 'Modalities'. The legend indicates light brown bars for 'Modalities Currently Supports' and light blue bars for 'Modalities to Support in Future'. The data is as follows: Scientific data: Currently Supports 13.6%, To Support in Future 20.6%. Videos: Currently Supports 6.8%, To Support in Future 32.4%. Images: Currently Supports 31.8%, To Support in Future 47.1%. Geospatial temporal Images: Currently Supports 13.6%, To Support in Future 58.8%. Code: Currently Supports 38.6%, To Support in Future 23.5%. Tabular data: Currently Supports 63.6%, To Support in Future 35.3%. Natural Language Text: Currently Supports 93.2%, To Support in Future 32.4%. Machine generated text e.g., logs: Currently Supports 81.8%, To Support in Future 52.9%.: chart::>

<a id='b84f1077-6919-4fc8-93ac-9ae217fea837'></a>

Figure 20. [All Data] Data modalities already supported (red) versus modalities planned for future support (blue) across all agents. This figure corresponds to Figure 13 in the main text but includes all survey data. The trend of growth being heavily concentrated in non-textual modalities remains consistent, pointing toward increasingly multimodal agent systems. However, comparing this figure with the deployed-only subset shows that the full survey data in Figure 13 places an even stronger focus on non-textual modalities for future support. (N=44)

<a id='88b372e8-f9ec-4a7b-b58c-e8e727b7a1dd'></a>

28

<!-- PAGE BREAK -->

<a id='14ac2850-ec85-403e-826b-ad049e041936'></a>

Measuring Agents in Production

<a id='64fd9391-e365-49b9-abe1-353a96773e59'></a>

<table id="28-1">
<tr><td id="28-2" colspan="4">Table 2. Survey Responses Recorded as &#x27;Other&#x27; For Domain Analysis and Topic Normalization.</td></tr>
<tr><td id="28-3">chemical</td><td id="28-4">proprietary-based networks</td><td id="28-5">Telco</td><td id="28-6">GTM Operations</td></tr>
<tr><td id="28-7">supply chain</td><td id="28-8">food &amp; beverage industry</td><td id="28-9">construction</td><td id="28-a">automotive</td></tr>
<tr><td id="28-b">travel</td><td id="28-c">Advertising</td><td id="28-d">Beauty &amp; wellness</td><td id="28-e">Privacy</td></tr>
<tr><td id="28-f">entertainment &amp; gaming</td><td id="28-g">film &amp; TV</td><td id="28-h">social media</td><td id="28-i">Paint industry</td></tr>
</table>

<a id='224842d0-6cb0-423c-97a4-d3049b491427'></a>

B. Analysis Details

B.1. Topic Normalization for RQ1

We then normalized these responses using an LLM-based semantic aggregation to merge semantically similar responses and categorize the answers for conceptual consistency and to avoid repeated or redundant categories. The resulting categories were manually reviewed to confirm semantic alignment. We then perform an LLM-based semantic mapping over each survey response to assign one or more domain labels. For example, the *Healthcare Services* category includes answers such as *health care*, *medical*, *medicine*, *biomedicine*, *patient monitoring*, *virtual nursing*, and *care navigation*. We use LOTUS [71] to perform the semantic aggregation and semantic mapping and provide the relevant program snippets below. We use gpt-4o-2024-08-06 as the LLM for both.

<a id='d12cb90e-9d6a-4658-a8ca-08e31f81c8ba'></a>

Domain Classification with LOTUS Semantic Map

```
df.sem_map(
"Given the survey answer {N4 CQ}, and the provided classification labels,"
+ "answer with the list of labels that best categorize the survey answer.\n"
+ "You may ONLY choose labels from the list provided.\n"
+ "Provide your answer as a list of strings, e.g. ['label1', 'label2'].\n"
+ "Please respond with the answer only and include only valid labels from "
+ "the provided list.\n"
+ "Only list 'Other' if no other label reasonably applies.\n"
+ f"Below are the classification labels: {categories_str}"
)
```

<a id='be66033e-4d21-46dc-ba1b-5fe2cc0074e1'></a>

# Domain Normalization with LOTUS Semantic Aggregation

```
df.sem_agg(
    f"Given user answers to a survey question in {{N4 CQ}}, create a comprehensive
    bullet point list of answer categories. The survey question was: {header_map["
    "N4 CQ"]}."
)
```

<a id='b17d84cb-e435-408d-ba3f-5ccde2ef347d'></a>

## B.2. Outlier Domains

Table 2 presents the domains that were mentioned only once across all reported Agentic AI use cases. These outlier domains highlight the long-tail diversity of applications where Agentic AI is being explored beyond the dominant sectors such as finance, technology, and enterprise.

<a id='573f4887-7b62-4a67-987c-4a7d8a81b75f'></a>

B.3. Challenge Details

We asked participants to rank the major categories of challenges they encounter during the development or operation of Agentic AI systems. Table 3 provides detailed descriptions of each challenge category identified in the survey, outlining the main technical and organizational issues practitioners reported when building Agentic AI systems. The five categories and their detailed definitions are provided in Table 3. Figure 21b illustrates how frequently each challenge category was assigned a given rank. For example, 'Core Technical Performance' was ranked as the most significant challenge (#1) by 16 respondents, by far more than any other category, indicating it remains the dominant source of difficulty in current Agentic AI system development. 'Core Technical Performance' encompasses a wide range of issues, including robustness, reliability,

<a id='f5081b90-c74e-4b83-a94a-1fb4c646b7ce'></a>

29

<!-- PAGE BREAK -->

<a id='382e0583-4a89-4eb3-b3d6-b452ae90a86b'></a>

Measuring Agents in Production

<a id='45dbec5b-3e35-42af-94be-fdd95a037b08'></a>

<::bar chart: A horizontal bar chart showing the percentage of responses for different categories, with error bars. The x-axis is labeled "% of Responses" and ranges from 0% to 100%. The y-axis lists the categories:
- Core Technical Focus: 37.9% (11)
- Data and Model Integrity: 20.7% (6)
- System Integration and Validation: 20.7% (6)
- Compliance and User Trust: 17.2% (5)
- Transparency and Governance: 3.4% (1)
(a)::>

<a id='3f435560-2a8f-4202-afa2-687bfa4a4e3b'></a>

<table id="29-1">
<tr><td id="29-2">Core Technical Focus</td><td id="29-3">37.9% (11)</td><td id="29-4">20.7% (6)</td><td id="29-5">24.1% (7)</td><td id="29-6">0.0% (0)</td><td id="29-7">17.2%(5)</td></tr>
<tr><td id="29-8">Data and Model Integrity</td><td id="29-9">20.7% (6)</td><td id="29-a">24.1% (7)</td><td id="29-b">27.6% (8)</td><td id="29-c">13.8% (4)</td><td id="29-d">13.8%(4)</td></tr>
<tr><td id="29-e">System Integration and Validation</td><td id="29-f">20.7% (6)</td><td id="29-g">31.0% (9)</td><td id="29-h">13.8% (4)</td><td id="29-i">31.0% (9)</td><td id="29-j">3.4%(1)</td></tr>
<tr><td id="29-k">Transparency and Governance</td><td id="29-l">3.4% (1)</td><td id="29-m">6.9% (2)</td><td id="29-n">6.9% (2)</td><td id="29-o">44.8% (13)</td><td id="29-p">37.9%(11)</td></tr>
<tr><td id="29-q">Compliance and User Trust</td><td id="29-r">17.2% (5)</td><td id="29-s">17.2% (5)</td><td id="29-t">27.6% (8)</td><td id="29-u">10.3% (3)</td><td id="29-v">27.6%(8)</td></tr>
<tr><td id="29-w" colspan="6">Rank 1 Rank 2 Rank 3 Rank 4 Rank 5 Most Challenging &lt;---&gt; Least Challenging</td></tr>
</table>
(b)

<a id='d5d6201b-d698-40cf-ab78-6e8af21afab0'></a>

Figure 21. Major challenge categories encountered across all Agentic AI systems (N = 29). (a) Distribution of top-ranked (Rank 1) challenges for building agents in deployment. (b) Heatmap showing how frequently each category was assigned to different ranks of difficulty (1 = most challenging, 5 = least challenging) across the deployed agents. Results show that Core Technical Performance remains the primary friction point.

<a id='e8e16988-e79a-41bd-8169-09a2a457a369'></a>

Table 3. Major categories of challenges reported by participants.
<table><thead><tr><th>Challenge Category</th><th>Representative Issues and Focus Areas</th></tr></thead><tbody><tr><td>Core Technical Performance</td><td>Robustness and reliability—ensuring consistent, correct behavior in diverse and unpredictable environments; scalability—supporting growth in users, data, and tasks without<br>performance degradation; real-time responsiveness—meeting latency and timing requirements; resource constraints—managing compute, memory, and energy efficiently.</td></tr><tr><td>Data and Model Integrity</td><td>Data quality and availability—access to clean, timely, and relevant data; model and<br>concept drift—adapting to changes in data distributions and task definitions; versioning<br>and reproducibility—tracking models, data, and configurations for auditability.</td></tr><tr><td>System Integration and Validation</td><td>Integration with legacy systems—connecting with existing infrastructure and APIs;<br>testing and validation—simulating and verifying agent behavior before deployment;<br>security and adversarial robustness—defending against manipulation and exploitation.</td></tr><tr><td>Transparency and Governance</td><td>Explainability and interpretability—making decisions understandable to humans; bias<br>and fairness—preventing discriminatory or unjust outcomes; accountability and responsibility—clarifying who is liable for agentic decisions.</td></tr><tr><td>Compliance and User Trust</td><td>Privacy and data protection—ensuring adherence to data regulations (e.g., GDPR); user<br>trust and adoption—building confidence through transparency and reliability; regulatory<br>compliance—meeting legal standards for autonomy, safety, and transparency.</td></tr></tbody></table>

<a id='303c710e-7bb0-4757-ae9e-d310e54ea381'></a>

scalability, latency, and resource constraints. Its prevalence suggests that much of the community's current effort is devoted to ensuring that systems perform consistently and dependably under real-world conditions. Following closely were 'Data and Model Integrity' and 'System Integration and Validation', both of which were reported as persistent sources of friction when transitioning systems from research prototypes to production environments. In contrast, 'Transparency and Governance' and Compliance and User Trust were ranked as lower-priority concerns. As shown in Figure 21b, 'Transparency and Governance' was most frequently placed in the fifth position (14 occurrences), indicating that while practitioners recognize its long-term importance, it is not yet perceived as a primary bottleneck in current development cycles.

<a id='c2f50727-7506-4f26-aa45-9386fa979423'></a>

C. Terminology
To ensure clarity and consistency, we established a hierarchical taxonomy for agent execution. Figure 22 provides a conceptual visualization of the key terminologies e.g., Task, Subtask, and Steps, as they are defined in our survey and applied throughout the paper. This mapping illustrates the relationship between high-level user goals and the granular autonomous actions taken by the agent.

<a id='3dc8aca8-ed69-4a69-8695-9e3b5169f63f'></a>

30

<!-- PAGE BREAK -->

<a id='ff3b2854-bb2b-4382-af17-b07e5f5d2712'></a>

Measuring Agents in Production

<::diagram: The diagram is divided into two main conceptual parts: "Task Abstraction" and "Agent Control".

**Task Abstraction**
A user input leads to a large orange-bordered box labeled "task". Inside this box, there's a sequence of sub-tasks. The first is "sub-task A", which contains a smaller section labeled "steps for A" with four interconnected circular icons representing individual steps. An arrow points from "sub-task A" to "sub-task B". Another arrow points from "sub-task B" to "sub-task C". A dotted line indicates more sub-tasks, leading to "sub-task N". Finally, an arrow points from "sub-task N" to an "Output" icon, which looks like a stack of documents.

**Agent Control**
An arrow labeled "Agent Control (examples of common architectures)" points downwards from the "Task Abstraction" section to a blue-bordered section below. This section is divided into three parts by dotted vertical lines:
1.  **agent loop**: Represented by a circular arrow icon.
2.  **workflow (graph, chain, etc..)**: Represented by a directed graph of interconnected circular nodes.
3.  **agent orchestration**: Represented by a central gear icon with smaller circular nodes connected to it by lines, resembling a starburst pattern.:>

Task abstraction is irrespective to agent execution. Execution can be determined by different agent architecture ...

Figure 22. Conceptual visualization of terminologies: "task", "subtask", "steps", used in Section 5 and how it maps to the survey definition.

<a id='c1e638f9-a0f4-42c5-954a-4efbe8c6c093'></a>

D. In-Depth Case Study Data

For building in-depth case studies, we arranged interviews with technical experts willing and able to supplement the published case study materials. Interviewers were selected to maintain organizational neutrality, assigned roles (lead, assistants, observers), and completed a series of pre-, post-, and in-interview procedures.

<a id='46d37e4f-4d93-46d2-8518-73932d8acad7'></a>

D.1. Interview Outline
The structure of interviews was determined by a preset list of 11 topic groups (below), and the availability of respective answers from open sources that need only be verified via interview. In general, interviewers were advised to prioritize topics 1–5 first, then 6–8, and finally 9–11 as time allowed.

<a id='5e21755e-5241-447a-bf3c-fdaaf1c257a5'></a>

1. **The root problem (benefit) the system is addressing (providing):** What is the ultimate benefit? What is the system replacing and why?
2. **Key success metrics and evaluation mechanism:** What tools, techniques, systems, etc. are used to ensure the system meets user and stakeholder objectives? Is data corresponding to the expected or past system behavior available for the evaluation?
3. **Key aspects of the system design and implementation:** What programming framework was used? What is the general architecture? What are the steps, stages, and cycles? How are common components (e.g. routers, LLM-as-a-Judge, other verifiers, HIL) combined and why? What is the ratio of automation to human interaction and why-by design or limitation?
4. **The state of the system or its development:** Is the system in production, or was it never meant for production (purely for AI research, learning, upskilling)? Was the system prototyped for production but abandoned-why, and what were the critical limitations? Were there surprises in the development or evaluation process? Did some things work better or worse than expected, and if so, what?
5. **Known constraints or requirements of end-users and stakeholders:** What are the security, confidentiality, regulatory, latency, SLO/SLA, or other requirements?
6. **Advantage of an agentic AI system solution over alternative approaches:** Do reasonable alternative solutions exist for this problem, or is this a novel solution made possible with Agentic AI? Against existing alternatives, has comparative analysis been conducted? What are the comparative benefits, costs, and return on investment (ROI)?
7. **System dependencies and complexity:** what is the quantity, quality, and availability of tools and data for verification and generation?
8. **End-user quantity, expertise levels, and organizational domains.** is it a product for internal-use only or public external use? Does it support multiple institutions? Are there institution-specific or regulatory boundaries limiting the quantity of

<a id='93f30e24-1495-4901-8b16-04d4f1a4dc42'></a>

31

<!-- PAGE BREAK -->

<a id='f4520822-ef33-4a23-8f1c-89fda08a7117'></a>

Measuring Agents in Production

<a id='4fefec00-0a35-4b27-a300-a3de2e2d367f'></a>

users? Are target users domain experts or novices? How many of each user group are there and how many are targeted (order of magnitude)?...

<a id='c752a1b4-0a1c-4e39-b20f-2d26ed6c0f4b'></a>

9. **Estimated cost versus value or benefit**: What is the estimated cost (sunk and expected ongoing costs) of developing and operating the system versus the estimated value or benefit? Is the respondent aware? What is the value, how is ROI being calculated?
10. **System stakeholders**: Who ultimately benefits from deployment? Who is impacted by safety, security, etc. failures and limitations? What is the expected impact on the company/institution (e.g. reduced hiring, retraining, broader user-base etc.)?
11. **Your role and activities**: What is your role in the development of the agentic AI system(s) you are describing?

<a id='3601c17e-08a6-48f0-b107-8f81ea3f26dd'></a>

D.2. Interviewees Demographics
To respect confidentiality agreements with case study sources, we present statistics on the sources in aggregate in Figure D.2.

<a id='a090049e-a979-4e23-bdb4-6465c581182b'></a>

<::Bar chart showing three sections: The first section, labeled "Tens", has a value of "14". The second section, labeled "One", has a value of "5". The third section, labeled "100s", has a value of "1".: chart::>

<a id='1906baec-9575-4501-b0b9-20ebd288c018'></a>

Figure 23. Case study sources are present in one to hundreds of countries. This shows the distribution of cases by sources' country spread. <::bar chart: This bar chart shows the distribution of cases by sources' country spread, divided into three segments: 5 to 6 Continents with 11 cases, 1 Continent with 5 cases, and 2 to 4 Continents with 4 cases.::>

<a id='6f8cf66e-4fb4-45a9-b68a-887ef3a84d61'></a>

Figure 24. Case study sources are present in 1 to 6 continents. This shows the distribution of cases by sources' continental spread.

<a id='33ea0d73-f404-418a-9e12-995a383acd57'></a>

E. Survey Questionnaire Details
We crafted the questionnaire iteratively, refining it through early practitioner discussions. To facilitate broad participation, we limited the length, technical-depth, and disclosure-depth necessary to complete the questionnaire. All questions were optional and participation was entirely voluntary. Proceeding beyond certain questions required an answer or input however. For example, no participant could proceed without confirming they had read and understood Acknowledgement E.1-1. Figure 26 and Figure 27 shows the control-flows of the Core and Additional sections of the questionnaire respectively.

<a id='a2f85f77-7461-4936-af4d-b2388412a9d3'></a>

Futher, all questions are intended for practitioners building AI Agents. Respondents who reported making technical contributions to more than 1 system (Table E.3 N1) were asked to focus all subsequent responses on 1 system of their choice (Acknowledgement E.1-2). Those who reported that they did not contribute to any systems they personally refer to as "AI agents" or "Agentic Al" systems were offered several options, such as commenting on terminology or their reason for starting the questionnaire, before being offered an early exit.

<a id='6728a05b-928d-40d2-b65d-31bf53605364'></a>

E.1. Survey Acknowledgements

**Acknowledgment 1** All participants were required to acknowledge the following statement.

<a id='39141275-370d-4371-bc73-af69b82c90d1'></a>

You are invited to participate in a research study in which all data collected will be anonymized to protect your privacy. Your participation in this research is completely voluntary. The aim of the study is to understand key technical successes and challenges for deployment-track AI Agents and agentic systems, in order to steer further research and innovation in programming, runtime, and evaluation systems. We welcome as much or as little detail as you are willing to provide. Feedback on the survey/process itself is more than welcome. By proceeding with this survey, you acknowledge that you have read and understood the information provided above and consent to participate in this study. We sincerely appreciate your participation and value your contribution to this research.
Thank you for your time and cooperation.

<a id='2234ef32-eadb-4c26-b03a-bc9cb69c1939'></a>

32

<!-- PAGE BREAK -->

<a id='6e5db756-2125-4982-b930-b5358a934ea0'></a>

Measuring Agents in Production

<a id='ecf36d25-08ee-485a-903c-779da9bc9388'></a>

**Acknowledgment 2** Further, all participants who stated they worked with more than 1 AI agent or agentic AI system were required to acknowledge the following statement before proceeding further.

<a id='e9e00710-b85f-4fae-bd99-d4b20e677ef6'></a>

To ensure consistency in your responses, please choose one agentic/assistant system to focus on throughout this survey. All your answers should relate to this same system. You may choose: * A system you are most familiar with, or * The system that is most developed among those you know --- meaning closest to production with the most users. Please feel very, very welcome to submit additional survey responses for each system you are familiar with.

<a id='9948ea2c-841b-4ced-bdb0-55b11c6096ab'></a>

E.2. Survey Questions

We designed the questions to avoid response priming and facilitate downstream quantitative analysis, resulting in the question-type distribution shown in Table 4.

<a id='e291e364-db39-47af-8f5a-43d811134bd2'></a>

E.3. Participant Contribution Distribution

As shown in Figure 25, this plot presents the distribution of how many Agentic AI systems participants reported contributing to. Across the N=306 systems represented in the survey, most respondents contributed to only a single system, with fewer individuals reporting involvement in multiple systems. This distribution highlights the breadth of participation across distinct agent deployments rather than concentrated contributions from a small subset of practitioners.

<a id='ba7e98a6-6fb0-4920-ac9d-5e8156aa6363'></a>

<::Bar chart showing the percentage of responses for the number of agentic systems participants contributed to. The y-axis represents the "% of Responses" ranging from 0% to 30%. The x-axis represents "# Agentic Systems Participants Contributed To" with categories: 0, 1, 2, 3, 4, 5, 6-10, and 11+. The bars are labeled with the number of responses for each category: 0 (12 responses, approximately 4%), 1 (81 responses, approximately 26%), 2 (83 responses, approximately 27%), 3 (53 responses, approximately 17%), 4 (25 responses, approximately 8%), 5 (31 responses, approximately 10%), 6-10 (15 responses, approximately 5%), and 11+ (6 responses, approximately 2%).: chart::>

<a id='8038cbf7-f28e-4cb7-9591-39ce3e442547'></a>

Figure 25. Distribution of the number of Agentic AI systems that participants reported contributing to (N=306).

<a id='9ad5ab2a-25cc-47ef-b9cd-07c3f746c3b0'></a>

The exact survey questions and response choices are shown in the following sections.

<a id='ff3a66e1-e95f-4124-afda-a4cf34a87122'></a>

<table id="32-1">
<tr><td id="32-2">Question Type</td><td id="32-3">Frequency</td></tr>
<tr><td id="32-4">MCSA</td><td id="32-5">24</td></tr>
<tr><td id="32-6">Free-Text</td><td id="32-7">8</td></tr>
<tr><td id="32-8">MCMA</td><td id="32-9">7</td></tr>
<tr><td id="32-a">Rank-order</td><td id="32-b">5</td></tr>
<tr><td id="32-c">Numerical Input</td><td id="32-d">3</td></tr>
<tr><td id="32-e">Total Questions</td><td id="32-f">47</td></tr>
</table>

<a id='e96e5357-c916-4960-98f4-6c24568ab770'></a>

Table 4. Distribution of Question Types in Survey. Abbreviations: MCSA (Multiple-Choice Single-Answer), MCMA (Multiple-Choice Multiple-Answer).

<a id='2d7287ec-080f-42f9-9396-e51cd3127af4'></a>

33

<!-- PAGE BREAK -->

<a id='af391586-339d-4d15-bef4-fa94b0c0b616'></a>

Measuring Agents in Production

<a id='0d30b1f0-e67a-4d8d-a2b6-ed6305a25a79'></a>

<table><thead><tr><th>ID</th><th>Question</th><th>Response Choices</th></tr></thead><tbody><tr><td>N1</td><td>How many systems do you contribute to that you would personally describe as "AI agent(s)", "agentic", or "assistant(s)"? Example Answer: 2.<br>(Required)</td><td>Numerical Input</td></tr><tr><td>N1.1</td><td>Do your colleagues or stakeholders call the systems you work on agentic, and if so, would you be willing to answer additional questions about the one with which you are most familiar?<br>Only shown if answer for N1 is = 0</td><td>MCSA<br>Yes and yes<br>Yes and no, end questionnaire<br>No and no, end questionnaire</td></tr><tr><td>N2</td><td>May we contact you or your colleagues to learn more about your agentic/assistant systems? If so, please provide contact information. It will not be shared beyond the collaborators of this study or used for any other purpose. The question can be postponed after N14.</td><td>Free-Text<br>---</td></tr><tr><td>N3</td><td>With respect to the system you chose, are references available (code repositories, blogs, publications, training data, evaluation data, or benchmark links)? If so, please provide links.</td><td>Free-Text<br>---</td></tr><tr><td>N4</td><td>List as many keywords as you can think of describing the domains in which the target problem (opportunity) arises.</td><td>Free-Text<br>---</td></tr><tr><td>N5</td><td>Which of the following best describes the status of the agentic/assistant system on which you chose to focus your responses?</td><td>MCSA<br><b>In active production</b> - Fully deployed and used by target end-users in a live operational environment<br><b>Pilot or limited deployment</b> - Deployed to a controlled group of users or environments for evaluation, phased rollout, or safety/security<br><b>Undergoing enterprise-grade development, not yet in pilot or production</b> - Actively being built, tested, or integrated, but not yet piloted or deployed<br><b>Prototype for potential development</b> - A functional early version intended to (ideally) evolve into a production system<br><b>Retired or sunset</b> - Previously in use or prototyping but now decommissioned, cancelled, or replaced<br><b>Research or education artifact</b> – Experimental or demonstrative, never intended for production use<br><b>Unknown</b> – The status is unclear or the question is not understood</td></tr></tbody></table>

<a id='9d20e61e-8d89-4c1f-9ce4-0516a42436c9'></a>

Continued on next page

<a id='7499f9ef-918f-428c-939d-6253e0d44195'></a>

34

<!-- PAGE BREAK -->

<a id='04f7531f-74b4-4fd5-b084-078a46222ac8'></a>

Measuring Agents in Production

<a id='e0e7d5cf-e3f6-4475-b359-834eed3f65aa'></a>

<table><thead><tr><th>ID</th><th>Question</th><th>Response Choices</th></tr></thead><tbody><tr><td>N5.1</td><td>Approximately how many target end users are actively using this production agentic system daily on average? Only show if N5 answered.</td><td>MCSA<br>0, 1-10, 10-49, 50-199, 200-499, 500-999, 1,000-9,999,<br>10,000-99,999, 100,000-999,999, 1,000,000 or more users, Not sure</td></tr><tr><td>N5.2</td><td>Approximately, how many tasks from target end-users is the system processing per day on average? Only show if N5 answered.</td><td>Free-Text<br>---</td></tr><tr><td>N6</td><td>Which of the following best describes your primary role with respect to this agentic system?</td><td>MCSA<br><b>Oversight & Strategy</b> — Executive or Senior Leadership (e.g., CTO, VP, Director), Product or Program Manager, Project Manager or Scrum Master<br><b>Industry Development, Engineering, Research</b> — Basic or Advanced, Software Developer / Engineer, Machine Learning Engineer, Data Scientist or Analyst, Researcher or Scientist<br><b>Academic Research & Engineering</b> — Scientists, Students, Life Long Learners<br><b>Operations & Infrastructure</b> — MLOps / DevOps / Platform Engineer, System Administrator or IT Support<br><b>Quality & User Experience</b> — Quality Assurance / Test Engineer, UX/UI Designer or Human Factors Specialist<br><b>Communication & Learning</b> — Technical Writer or Documentation Specialist, Educator or Trainer, Student or Intern</td></tr><tr><td>N7</td><td>What are the ultimate target gains of enabling/deploying the system? Select the highest priority option(s). (Skip the question if you do not know.)</td><td>MCMA<br><b>Workforce adaptation:</b> reducing human expertise-levels or training generally required for the tasks<br><b>Removing cross-domain, interdisciplinary knowledge requirements,</b> skills or training requirements<br><b>Increasing Productivity/Efficiency:</b> increasing speed of task completion over the previous human/automated system<br><b>Replacing time-consuming, low-skill, low-attention tasks</b> with automation<br><b>Mitigating Risk:</b> reducing otherwise high or highly variable risk or uncertainty<br><b>Decreasing human hours required</b> regardless of skills, task complexity, workforce expectations<br><b>Mitigating Failure/Loss:</b> Decreasing time-to-intervention (security breach, system failure, customer loss)<br><b>Increasing user engagement</b> and/or increasing service quality<br><b>Enabling completion of tasks not possible</b> with the previous human/automated system</td></tr><tr><td>N7.1</td><td>Order the selected options according to their respective priority level. Only shown if N7 Answered.</td><td>Rank-order<br>Answers from N7</td></tr></tbody></table>

<a id='060b9394-06f0-4f06-99d9-6b56f99f0284'></a>

*Continued on next page*

<a id='1b9f5ee9-f25e-4c42-ab1a-c5ab395c11de'></a>

35

<!-- PAGE BREAK -->

<a id='c81b1b45-c295-4221-9813-87901d609a69'></a>

<u>Measuring Agents in Production</u>

<a id='7f84b7a3-5215-440c-8934-0ce1a8acf967'></a>

<table><thead><tr><th>ID</th><th>Question</th><th>Response Choices</th></tr></thead><tbody><tr><td>N8</td><td>Who (what) are the primary direct users or consumers of the agentic/assistant system? (Select one.)</td><td>MCSA<br>Other AI Agent(s)<br>Other NON-agentic software systems, tools, services<br>Humans operating INSIDE organizational boundaries (e.g. employees operating inside a company and not their external customers)<br>Human customers, general audience, operating OUTSIDE the org authoring the agentic AI / assistant system</td></tr><tr><td>N9</td><td>Referring to the previously selected description of direct users or consumers as "user" (human or non-human), what does the system require from users in terms of behavior(s), interaction(s), or role(s)? To the agent/assistant, the target end-user is an...</td><td>MCMA<br>Operator (user initiates tasks, provides guidance, and determines a task is finished)<br>Approver (without necessarily providing guidance to reach the solution, user approves a solutions generated by the agentic system)<br>Observer (user passively observes the agentic system is operating as expected)<br>Optimizer (user actively intervenes to provide correction or improve performance without necessarily being an Operator or Approver)</td></tr><tr><td>N9.1</td><td>Please sort the selected options with (1) as the primary intended role, (2) as a secondary role, and so on. Press and drag an option to sort. Only shown if N9 Answered.</td><td>Rank-order<br>---</td></tr><tr><td>N10</td><td>Using the previous definition of "user", how many steps or cycles can execute autonomously until user input is required?</td><td>MCSA<br>Four or fewer; Ten or fewer; Tens; Hundreds; Thousands; Millions;<br>More; There is no limit, potentially infinitely many steps could execute without user input or intervention</td></tr><tr><td>N11</td><td>What determines how many steps or cycles can execute before a user's input is required?</td><td>MCSA<br>Problem complexity<br>Non-determinism in the agentic planning or decision-making<br>Preset limits e.g. in the configuration, parameters, or defaults<br>I do not know, or I have not measured</td></tr><tr><td>N12</td><td>How are each agent's system prompts (instructions) constructed?</td><td>MCSA<br>By hand, hard coded strings or templates e.g. LangChain templates<br>Using semi-automatic prompt engineering or optimization e.g. DSPy<br>Combination of manual and LLM or AI agents prompt creation and refinement<br>Fully autonomously by agents<br>Using libraries or templates predefined by others e.g. open-source<br>I don't know</td></tr><tr><td>N13</td><td>What is the average (typical) estimated instruction length per agent in words or tokens? (Skip if you do not know.)</td><td>Numerical Input<br>---</td></tr></tbody></table>

<a id='aa2f96bf-53c4-494f-8bda-48bb84c565bb'></a>

*Continued on next page*

<a id='8cc42b62-0d40-4212-a31b-52a603e8249c'></a>

36

<!-- PAGE BREAK -->

<a id='97c90e9d-f4d2-456d-b89b-b51f0b8867e7'></a>

<u>Measuring Agents in Production</u>

<a id='6ba47347-88f8-42d3-898f-29f1f025dd16'></a>

<table><thead><tr><th>ID</th><th>Question</th><th>Response Choices</th></tr></thead><tbody><tr><td>N14</td><td>What is the maximum allowable<br>end-to-end result latency for the<br>agentic/assistant system?</td><td>MCSA<br>I don't know<br>No limit set yet, still in exploratory phase, not a latency-critical<br>system<br>Microseconds<br>Subsecond<br>Seconds<br>Minutes<br>Hours<br>1–4 days<br>Weeks<br>Months<br>More</td></tr><tr><td>N2</td><td>May we contact you or your<br>colleagues to learn more about your<br>agentic/assistant systems? If so,<br>please provide contact information. It<br>will not be shared beyond the<br>collaborators of this study or used for<br>any other purpose. Only shown if<br>question was postponed.</td><td>Free-Text<br>—</td></tr><tr><td>SEP</td><td>This is the end of the core questions.<br>Would you like to answer further<br>questions?</td><td>MCSA<br>Yes; No</td></tr><tr><td>EOS1</td><td>End of survey – any last comments or<br>feedback can be shared here. Only<br>shows if SEP is No.</td><td>Free-Text<br>—</td></tr></tbody></table>

<a id='2babe522-cb06-43e3-855b-f70bd8537d20'></a>

37

<!-- PAGE BREAK -->

<a id='5ba7d9c9-5d31-4e43-bc4e-794d751977de'></a>

Measuring Agents in Production

<a id='7c790d33-464a-484c-8eed-74789243b8eb'></a>

Additional Questions
<table><thead><tr><th>ID</th><th>Question</th><th>Response Choices</th><th>Additional Context</th></tr></thead><tbody><tr><td>O1</td><td>Select any/all of the<br>following methods<br>currently integrated<br>into your system that<br>give you confidence<br>your agentic/assistant<br>system is consistently<br>producing high<br>quality outputs,<br>whatever "high<br>quality" means in<br>your context.</td><td>MCMA<br>Manual (Human-in-the-loop):<br>Expert Review<br>Manual Citation Verification<br>Crowdsourced Evaluation<br>Red Teaming<br><b>Automated Not-Model-Based<br>Cross-Referencing:</b><br>External Fact-Checking<br>Knowledge Graph Validation<br>Automated Citation Verification<br><b>Automated Model- and<br>Estimation-Based Methods:</b><br>Self-Consistency Checks<br>Internal Confidence Estimation<br>Critique Models<br>Red Teaming using models<br>Cross-Model Validation<br>LLM-as-a-Judge<br><b>Automated Rule-Based Methods:</b><br>Grammar and Syntax Checks<br>Domain-Specific Rules<br><b>Other:</b><br>None of the above/below<br>I am not confident the system<br>consistently produces high quality<br>output yet</td><td>Hover/click on the "i" for examples, definitions and descriptions.<br><br>Definitions:<br>Expert Review: Involve human specialists to validate content in high-stakes<br>or sensitive domains<br>Manual Citation Verification: User ensures cited sources are accurate and<br>actually support the generated claims<br>Crowdsourced Evaluation: Collect feedback from diverse human reviewers<br>to assess quality and usefulness<br>Red Teaming: Humans manually test robustness by probing with adversarial<br>or misleading prompts or actions to expose weakness<br>External Fact-Checking: Retrieve supporting evidence from trusted sources<br>and validate claims e.g. using retrieval-augmented generation (RAG)<br>Knowledge Graph Validation: Cross-check facts against structured data like<br>Wikidata or domain-specific ontologies<br>Automated Citation Verification: Ensure cited sources are accurate and<br>actually support the generated claims<br>Self-Consistency Checks: Generate multiple answers and compare for<br>consistency, use majority voting to select the most common answer, and/or<br>apply chain-of-thought reasoning to ensure logical consistency<br>Internal Confidence Estimation: Score answers using log probabilities<br>and/or estimate uncertainty with dropout or ensemble methods<br>Critique Models: Use separate models to evaluate factuality, coherence, and<br>overall quality of the output<br>Red Teaming using models: Test robustness by probing with adversarial or<br>misleading prompts to expose weakness<br>Cross-Model Validation: Compare outputs from different AI models to<br>identify consensus or discrepancies; Use Zero-Shot Critics, unrelated<br>models to critique outputs without prior task-specific training<br>Grammar and Syntax Checks: verify grammatical correctness and linguistic<br>clarity with or without NLP models<br>Domain-Specific Rules: Apply expert-defined rules for accuracy, tailored to<br>specific fields like business, medicine, law or finance</td></tr></tbody></table>

<a id='290bc23e-4910-4106-b149-ef95fe3e05e3'></a>

Continued on next page

<a id='7b2def76-5051-4328-8818-11b1cd2e55a7'></a>

38

<!-- PAGE BREAK -->

<a id='7a6000c0-c8c1-4f4c-ad0a-8e023ed5d6ba'></a>

# Measuring Agents in Production

<a id='a0ccc9fe-e200-404c-8dc3-21c716fc5264'></a>

<table><thead><tr><th>ID</th><th>Question</th><th>Response Choices</th><th>Additional Context</th></tr></thead><tbody><tr><td>O2</td><td>Sort the following categories from most to least important for ongoing development and production deployment. Press and drag an option to sort.</td><td>Rank-order<br><b>Data and Model Integrity:</b><br>Data Quality and Availability<br>Model Drift and Concept Drift<br>Versioning and Reproducibility<br><b>Core Technical Performance:</b><br>Robustness and Reliability<br>Scalability<br>Real-Time Responsiveness<br>Resource Constraints<br><b>System Integration and Validation:</b><br>Integration with Legacy Systems<br>Testing and Validation<br>Security and Adversarial Robustness<br><b>Transparency and Governance:</b><br>Explanability and Interpretability<br>Bias and fairness<br>Accountability and Responsibility<br><b>Compliance and User Trust:</b><br>Privacy and Data Protection<br>User Trust and Adoption<br>Regulatory Compliance</td><td>Hover/click on the "i" for examples and definitions.<br><b>Definitions:</b><br>Data Quality and Availability: Accessing clean, timely, and relevant data for decision-making<br>Model Drift and Concept Drift: Adapting to changes in data distributions and task definitions<br>Versioning and Reproducibility: Tracking models, data, and configurations for auditability<br>Robustness and Reliability: Ensuring consistent, correct behavior in diverse and unpredictable environments<br>Scalability: Supporting growth in users, data, and tasks without performance degradation<br>Real-Time Responsiveness: Meeting latency and timing requirements in dynamic contexts<br>Resource Constraints: Managing compute, memory, and energy efficiently<br>Integration with Legacy Systems: Seamlessly connecting with existing infrastructure and APIs<br>Testing and Validation: Simulating and verifying agent behavior before deployment<br>Security and Adversarial Robustness: Defending against manipulation and exploitation<br>Explanability and Interpretability: Making decisions understandable to humans<br>Bias and fairness: Preventing discriminatory or unjust outcomes<br>Accountability and Responsibility: Clarifying who is liable for agentic decisions<br>Privacy and Data Protection: Ensuring compliance with data regulations (e.g. GDPR)<br>User Trust and Adoption: Building confidence through transparency and reliability<br>Regulatory Compliance: Meeting legal standards for autonomy, safety, and transparency</td></tr><tr><td>O3</td><td>Have you compared your agentic/assistant solution to a non-agentic solution, which of the following statements is most accurate?</td><td>MCSA<br>Yes; No, alternatives might exist but I have NOT formally compared them;<br>No, alternates DO NOT exist, my system provides truly novel functionality</td><td>---</td></tr><tr><td>O3.1</td><td>If it were entirely up to you, would you choose the agentic solution over the alternatives? Only shown if answer for O3 is Yes</td><td>MCSA<br>option Yes: [ ]<br>option No: [ ]</td><td>---</td></tr></tbody></table>Continued on next page

<a id='9f7e1673-bccf-46d3-8135-7c7c0e5b5940'></a>

39

<!-- PAGE BREAK -->

<a id='771c30ee-9c8a-415c-8720-ec97e3f79a06'></a>

Measuring Agents in Production

<a id='2e4f3084-6ee8-4432-92b7-91dbd86e64f8'></a>

<table><thead><tr><th>ID</th><th>Question</th><th>Response Choices</th><th>Additional Context</th></tr></thead><tbody><tr><td>O3.1.1</td><td>You answered "Yes" to "If it were entirely up to you, would you choose the agentic solution over the alternatives?" Why, which of the following functional improvements does the new system offer compared to the previous solution? (Select all that apply.). Only shown if answer to O3.1 is Yes</td><td>MCMA<br>Increased automation or reduced<br>manual effort<br>Enhanced user interface or usability<br>Scalability or support for more users<br>ONLY Ease of software design,<br>maintenance, model use or integration<br>Improved performance or speed<br>For non-technical reasons ONLY, e.g.<br>marketing/advertising, strategic<br>planning, ...<br>Better integration with other systems<br>Improved data accuracy or consistency<br>Enhanced security or compliance<br>Expanded features or capabilities<br>None of the above</td><td>—</td></tr><tr><td>O3.1.1.1</td><td>Order all selected options according to their respective priority level. - Improved performance or speed (Options from O3.1.1). Only shown if answer to O3.1.1 has answers</td><td>Rank-order<br>Answers from O3.1.1</td><td>—</td></tr><tr><td>O4</td><td>Thinking about the state of the system you chose to focus on, in your personal opinion, would you call it an "assistant" but not an AI Agent or agentic?</td><td>MCSA<br>Either/Both; Assistant and NOT Agent<br>or agentic</td><td>—</td></tr><tr><td>O4.1</td><td>Why do you refer to your system as AI agents or agentic? What makes it agentic in your opinion? Only shown if answer to O4 is Either/Both</td><td>Free-Text</td><td>—</td></tr></tbody></tr></table>

<a id='52627a85-5b00-4dc5-885a-edacc8091816'></a>

40

<a id='c0c48043-51b9-471a-be7a-5a1fe4d00972'></a>

*Continued on next page*

<!-- PAGE BREAK -->

<a id='005b125c-e61f-4919-bd83-34d0505f392e'></a>

Measuring Agents in Production

<a id='c4d6800c-2990-458f-a0ed-5c0cb53c799a'></a>

<table><thead><tr><th>ID</th><th>Question</th><th>Response Choices</th><th>Additional Context</th></tr></thead><tbody><tr><td>O4.2</td><td>Why do you refer to<br>your systems or an<br>"assistant rather than<br>an "agent" or<br>"agentic"? What<br>makes it an assistant<br>rather than an agentic<br>system in your<br>opinion? You may go<br>back and change<br>your previous answer<br>if you use any of<br>these terms to<br>describe your system.<br>Only shown if<br>answer to O4 is<br>Assistant and NOT<br>Agent or agentic</td><td>Free-Text</td><td>—</td></tr><tr><td>O5</td><td>What is the minimum<br>level of expertise or<br>training (knowledge<br>and skills) expected<br>from typical<br>end-users? (Select<br>the best option.)</td><td>MCSA<br><b>Highly skilled professionals</b> —<br>Advanced education and specialized<br>knowledge (e.g., engineers, scientists,<br>doctors); capable of complex tasks like<br>coding, diagnostics, or system design<br><b>Extensive domain experience</b> — Deep<br>practical knowledge from years of<br>experience; having organizational or<br>domain knowledge, skilled in nuanced<br>tasks like troubleshooting or<br>decision-making<br><b>General education</b> — High school<br>level education with basic digital skills<br>(e.g., using email, spreadsheets, or web<br>apps) and standard subject matter<br>knowledge<br><b>Minimal expertise required</b> — Little<br>to no formal education; able to follow<br>simple instructions or perform basic<br>tasks (e.g., tapping buttons, entering<br>data)<br>Not sure</td><td>—</td></tr></tbody></table>

<a id='65a67f98-7599-44ae-a1fb-a69d80d40d41'></a>

*Continued on next page*

<a id='7835ca3a-12dd-4c50-960c-55baa24cd858'></a>

41

<!-- PAGE BREAK -->

<a id='922f8c65-603a-46db-b2cb-110bf3736f88'></a>

<u>Measuring Agents in Production</u>

<a id='8728eec5-6679-43fd-970d-bc9dad64f116'></a>

<table><thead><tr><th>ID</th><th>Question</th><th>Response Choices</th><th>Additional Context</th></tr></thead><tbody><tr><td>O6</td><td>How would you rate<br>the return on<br>investment (ROI) of<br>this agentic system<br>relative to its total<br>cost of development,<br>operation,<br>infrastructure and all<br>other costs? (Please<br>select the option that<br>best reflects your<br>assessment.) Only<br>shown if answer to<br>N5 is "In active<br>production - Fully<br>deployed and used by<br>target end-users in a<br>live operational<br>environment"</td><td>MCSA<br>Exceptionally high ROI (ROI greater<br>than 150%)<br>High ROI (ROI between 125%–150%)<br>Acceptable ROI (ROI between<br>90%–124%)<br>Low ROI (ROI between 60%–89%)<br>Poor ROI (ROI less than 60%)</td><td>—</td></tr><tr><td>O7</td><td>How much do each<br>agent's system<br>prompt (instruction)<br>lengths vary? (Skip if<br>you do not know.)</td><td>MCSA<br>Tens of tokens<br>Hundreds of tokens<br>Thousands of tokens<br>Ten of Thousands of tokens<br>More</td><td>—</td></tr><tr><td>O8</td><td>Compared to the<br>target latency for the<br>system's result<br>turn-around, how<br>problematic is the<br>actual latency?<br>(Select one.)</td><td>MCSA<br>I don't know<br>I don't understand the question<br>Not problematic at all, the actual<br>latency is better than expected and not a<br>problem for deployment<br>Marginally problematic, but good<br>enough for deployment<br>Very problematic, the system can't be<br>deployed without addressing the gap, or<br>the highest priority post-deployment<br>will be bringing down the latency</td><td>—</td></tr><tr><td>O9</td><td>What is the<br>maximum target<br>latency for<br>determining a single<br>action, step, or<br>response? (Select<br>one.)</td><td>MCSA<br>Microseconds; Subsecond; 1 to 10<br>seconds; 10 to 60 seconds; A few<br>minutes; More; Less; I don't know; I<br>don't understand the question</td><td>—</td></tr></tbody></table>

<a id='f9033d26-8e0b-410c-a488-333a62ad7f8e'></a>

42

<!-- PAGE BREAK -->

<a id='961cc12c-fbf3-4f18-a6d7-e6b437289747'></a>

Measuring Agents in Production
---

<a id='e188231a-3624-43e8-ad46-f1b315f5b7a0'></a>

| ID | Question | Response Choices | Additional Context |
|---|---|---|---|
| O10 | What is the maximum number of distinct models used together to solve a single logical task? Skip if you do not know; estimates are fine. | Numerical Input | --- |
| O11 | Are inference time scaling techniques used in your system? | MCSA Yes; 0, No inference time scaling is used; I don't know | Help: For the purposes of this question, "inference time scaling" a.k.a. "test time compute" refers to a family of techniques that call models multiple times or use a collection of models together, in place of a single model call and without modifying the weights or retraining any models, to answer a single question, choose a single next step, action, tool, etc... |
| O11.1 | If inference time scaling techniques are used, approximately how many model calls are made per user query or task? Only shown if answer for O11 is Yes | MCSA Tens Hundreds Thousands Tens of thousands Hundreds of thousands Millions More | --- |
| O12 | What data modalities does the system process now (today)? | MCMA Natural Language Text Tabular data Software or machine generated text including system logs, events, etc. Code Images Videos Image sequences or batches with additional channels or metadata e.g. geospatial, NMR scans Scientific data not listed above | --- |
| O13 | What data modalities will or should the system process in future (that it does NOT process now)? | MCMA Same as O12 | --- |

<a id='b1ca82a8-1f2a-405a-8a5a-ba1b320afcc2'></a>

Continued on next page

<a id='aac2c2f9-de6e-4388-8e8d-8cc84a1679e6'></a>

43

<!-- PAGE BREAK -->

<a id='5baa67af-301d-4aec-ac85-16542228c8de'></a>

Measuring Agents in Production

<a id='cf84d30b-4e11-434f-8d84-ead324ab03b4'></a>

<table><thead><tr><th>ID</th><th>Question</th><th>Response Choices</th><th>Additional Context</th></tr></thead><tbody><tr><td>O13.1</td><td>You have selected the<br>following option(s)<br>for the question<br>"What data<br>modalities will or<br>should the system<br>process in future (that<br>it does NOT process<br>now)?": What are the<br>barriers to processing<br>them now? Sort the<br>options from most to<br>least important; press<br>and drag an option to<br>sort. Only shown if<br>O13 is answered. If<br>you don't know, skip<br>the question. - No<br>barriers, just a matter<br>of development time</td><td>Rank-order<br>Answers selected from O13</td><td>—</td></tr><tr><td>O14</td><td>Which describes the<br>data handling<br>functions the<br>system(s) perform?<br>Select all that apply<br>and use the "Other"<br>box to detail.</td><td>MCMA<br>Ingests direct user input in real-time<br>Ingests other real-time information as<br>well as direct user input<br>Ingests information from other systems,<br>e.g. databases, at most indirectly from<br>end-users<br>Retrieves persistent data from public<br>external sources (e.g. the web)<br>Retrieves non-public, confidential, or<br>otherwise federated data<br>Other</td><td>—</td></tr><tr><td>O15</td><td>Are you using an<br>openly<br>(commercially or<br>non-commerically)<br>available<br>agent-focused<br>programming<br>framework (e.g.<br>LangChain, CrewAI,<br>Autogen,...) to<br>implement your<br>system?</td><td>MCSA<br>Yes<br>No, only an in-house not openly<br>available solution, or only standard<br>programming languages and tools e.g.<br>Python, Java (not-agent-focused)<br>I don't know</td><td>—</td></tr></tbody></table>Continued on next page

<a id='87b2186d-fb94-495b-a201-a91f240b9020'></a>

44

<!-- PAGE BREAK -->

<a id='f7f8a54e-7d0e-483f-a0f9-6639a58f9991'></a>

Measuring Agents in Production

<a id='594bc2e7-3dc0-4d80-9d45-b723961745df'></a>

<table><thead><tr><th>ID</th><th>Question</th><th>Response Choices</th><th>Additional Context</th></tr></thead><tbody><tr><td>O16</td><td>From experimental<br>observations, which<br>of the following<br>supports most of<br>your assistant/agentic<br>system functionality<br>or design? Only<br>shown if answer to<br>O15 is Yes</td><td>MCSA<br>OpenAI Swarm<br>CrewAI<br>LangChain or LangGraph<br>BeeAI<br>Autogen or AG2<br>LlammaIndex<br>Other:</td><td>—</td></tr><tr><td>O17</td><td>How long has it been<br>since active<br>prototyping or initial<br>development started<br>on this<br>agentic/assistant<br>system?</td><td>MCSA<br>Less than 3 months; 3–6 months; 6–12<br>months; 1–2 years; 2–5 years; More<br>than 5 years; I don’t know or prefer not<br>to say</td><td>—</td></tr><tr><td>O18</td><td>How long have you<br>been working on this<br>agentic/assistant<br>system?</td><td>MCSA<br>Same as O17</td><td>—</td></tr><tr><td>EOS2</td><td>Thank you, this<br>completes our<br>questionnaire. Go<br>back to change any<br>of your answers, or<br>click END to finalize<br>them and exit. Any<br>last comments or<br>feedback can be<br>shared here.</td><td>Free-Text<br>—</td><td></td></tr></tbody></table>

<a id='6feba3d1-c78c-4dd2-96e2-7558b36a0de3'></a>

45

<!-- PAGE BREAK -->

<a id='4318800f-10a9-49e7-bc92-7ca98e4ab8b7'></a>

Measuring Agents in Production

<a id='b17d2563-63ab-4d58-8982-007d385864c1'></a>

<::Survey Start
    |
    V
N1
    |-- (N1 = 0) --> N1.1
    |-- (N1 > 0) --> N2
N1.1
    |-- (N1.1 is not "Yes and yes") --> End of Survey
    |-- (N1.1 is "Yes and yes") --> N2
End of Survey
    |
    V
EOS1
    |
    V
SEP
    |-- (SEP is No) --> N2
    |-- (SEP is Yes) --> EOS1
N2
    ^-- (after N1/N1.1)
    ^-- (after N14)
    ^-- (N2 postponed)
    ^-- (N2 already answered)
    |
    V
N3
    |
    V
N4
    |
    V
N5
    |-- (N5 answered) --> N5.1
    |-- (N5 not answered) --> N6
N5.1
    |
    V
N5.2
    |
    V
N6
    |
    V
N7
    |-- (N7 answered) --> N7.1
    |-- (N7 not answered) --> N8
N7.1
    |
    V
N8
    |
    V
N9
    |-- (N9 answered) --> N9.1
    |-- (N9 not answered) --> N10
N9.1
    |
    V
N10
    |
    V
N11
    |
    V
N12
    |
    V
N13
    |
    V
N14
    |-- (after N14) --> N2
: flowchart::>

<a id='09815279-39bd-4ee9-8a47-f55be36e5f71'></a>

Figure 26. Survey flow: core questions

<a id='4cfb2db5-64aa-419f-ba4c-d49bd79dae65'></a>

46

<!-- PAGE BREAK -->

<a id='36c7307c-5939-4f02-9ad1-45a688698016'></a>

Measuring Agents in Production

<a id='baa4d2ce-0628-4373-a100-5c16aed14269'></a>

<::
: flowchart::
Section Start
  |
  V
O1
  |
  V
O2
  |
  V
O3
  |-- (O3 is Yes) --> O3.1
  |-- (O3 is not Yes) --> O4

From O3.1:
  |-- (O3.1 is Yes) --> O3.1.1
  |-- (O3.1 is No) --> O4

From O3.1.1:
  |-- (O3.1.1 has at least one answer) --> O3.1.1.1
  |-- (O3.1.1 has no answers) --> O4

From O3.1.1.1:
  |
  V
O4

From O4:
  |-- (O4 is "Either/Both") --> O4.1
  |-- (O4 is "Assistant and NOT Agent or agentic") --> O4.2

From O4.1:
  <--> O5

From O4.2:
  <--> O5

From O5:
  |
  V
O6
  <--> O7

From O7:
  |
  V
O8
  <--> O9

From O9:
  |
  V
O10
  <--> O11

From O11:
  |-- (O11 is not Yes) --> O12
  |-- (O11 is Yes) --> O11.1

From O12:
  <--> O13

From O13:
  |-- (O13 answered) --> O13.1
  |-- (O13 not answered) --> O14

From O13.1:
  |
  V
O14

From O14:
  --> O15

From O11.1:
  |
  V
O15

From O15:
  |
  V
O16
  |
  V
O17
  |
  V
O18
  --> EOS2
  --> End of Survey
Figure 27. Survey flow: additional questions
::>

<a id='decb06bb-2bc5-4e71-b4c3-41535c4dceed'></a>

47