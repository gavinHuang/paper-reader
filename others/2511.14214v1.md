<a id='684a72ce-f69f-490c-8f37-0bfadf7bae1a'></a>

Do Large Language Models (LLMs) Understand
Chronology?

<a id='28f476db-7bdf-443b-8ed0-353565985545'></a>

**Pattaraphon Kenny Wongchamcharoen**
University of California, Berkeley
Berkeley, CA 94720
pattaraphon.kenny@berkeley.edu

<a id='4a03bae0-725d-4cff-b610-060cb1468f56'></a>

Paul Glasserman
Columbia Business School
New York, NY 10027
pg20@gsb.columbia.edu

<a id='fa49ec1b-8f65-4006-b68f-0898678f513b'></a>

# Abstract
Large language models (LLMs) are increasingly used in finance and economics, where prompt-based attempts against look-ahead bias implicitly assume that models understand chronology. We test this fundamental question with a series of chronological ordering tasks with increasing complexities over facts the model already knows from pre-training. Our tasks cover (1) chronological ordering, (2) conditional sorting (filter, then order), and (3) anachronism detection. We evaluate GPT-4.1 and Claude-3.7 Sonnet, with and without Extended Thinking (ET), and the newly released GPT-5 across multiple reasoning-effort settings. Across models, Exact match rate drops sharply as sequences lengthen even while rank correlations stay high as LLMs largely preserve local order but struggle to maintain a single globally consistent timeline. In conditional sorting, most failures stem from the filtering step rather than the ordering step, but GPT-5 and Claude-3.7 with Extended Thinking outshine normal models significantly. Lastly, anachronism detection is found to be the easiest task for the LLMs but performance still declines with increasingly overlapping timelines or entities. Overall, our main significant contribution is showing that allocating explicit reasoning budget helps with chronological ordering with GPT-5 at medium/high reasoning effort achieving flawless ordering at all lengths and perfect conditional sorting (both self-filtered and given-subset), whereas low/minimal effort degrades with longer lists, mirroring earlier models. We release all code and evaluation templates to support full reproducibility.1"

<a id='8de9dd6a-eb22-42e8-a6b5-65cd223ad2d1'></a>

# 1 Introduction

Large language models (LLMs) have shown great potential as forecasting tools for finance and economics. Typical applications ask LLMs to predict the direction of stock prices based on news reports, company earnings calls, or analysts' research. Yet it has also been recognized that the backtesting of LLM forecasts is subject to look-ahead bias ((Glasserman & Lin, 2024), Sarkar & Vafa (2024)) when the backtesting period overlaps with the LLMs training window: the LLM may be asked to predict an outcome it has already seen. Leakage of post-event information embedded in the LLM's pre-training corpus can inflate estimated forecast performance and lead to disappointing out-of-sample results.

<a id='98bf2576-4267-47d6-bfca-ec24a6b538ff'></a>

Various methods have been proposed to try to measure and mitigate this problem. The simplest approach is to limit testing to an LLM's post-training period (as in, e.g., Halawi et al. (2024), Lopez- Lira & Tang (2024)), but this severely limits the testing window, particularly for the most recent models. Glasserman & Lin (2024) find that masking company names in news headlines can be effective in removing look-ahead bias; this idea has been extended to larger texts in Engelberg et al. (2025), but Sarkar & Vafa (2024) and Lopez-Lira et al. (2025) find evidence that LLMs can

<a id='c2fb918b-c212-4827-baa1-be5041b1da3d'></a>

¹ Repository: https://github.com/kennywong524/chronollms

<a id='0e3cd8cb-4e70-4251-abee-2d6f1cdd4f05'></a>

Preprint.

<a id='06f8a6b4-5577-4105-bc95-6120c61e0e8b'></a>

arXiv:2511.14214v1 [cs.AI] 18 Nov 2025

<!-- PAGE BREAK -->

<a id='95b22460-55e9-4ea3-83d7-3895cb64c482'></a>

see through anonymization in large documents. Building snapshot models trained using only text available up to fixed dates in the past (Sarkar & Vafa (2024), (He et al., 2025b)) provides a secure way to wall off future information, but it is computationally demanding and limited to using training documents with clear time stamps.

<a id='e6ba48a3-6fed-4ff9-bff6-ac864d500cc0'></a>

From a user's perspective, the most convenient solution would be to wall off future information by instructing an LLM to respond using only information available up to a fixed date. Sarkar & Vafa (2024) and Lopez-Lira et al. (2025) find evidence of leakage in examples of this approach. More basically, a prompt-based instruction like "use only information from before 2016" presupposes that an LLM understands what "before 2016" means and can respond accordingly. So here we step back and ask a more fundamental question: _Do LLMs understand chronology?_ Before asking an LLM to avoid leakage of future information in responding to new information, we want to evaluate how well the LLM understands chronological constraints in data on which it has been trained.

<a id='ccaea41e-2a44-4aaf-ae4c-0a7527a7686d'></a>

Prior approaches to NLP temporal testbeds (e.g., TimeQA (Chen et al., 2021), TRAM (Wang & Zhao,
2024), TimeBench (Chu et al., 2024), and ChronoSense (Islakoglu et al., 2025)) seek to isolate and
evaluate specific components of temporal reasoning. To this end, they evaluate an LLM's performance
on new, sometimes hypothetical scenarios designed to probe a very specific type of inference. Here
we take a different approach, evaluating chronological understanding of information the LLM already
knows; we use historical facts, which we first confirm the LLM knows. This design probes the LLM's
ability to integrate temporal reasoning with the model's internal world view. It is less focused on
isolating granular components of temporal reasoning and seeks instead to better assess chronological
reasoning "in the wild."

<a id='f8d6af4c-f6a9-4514-80d9-3277cc158316'></a>

We test performance on three task types: (1) _Chronological sorting_ (putting scrambled events in chronological order); (2) _Conditional sorting_ (selecting events that meet a specified condition and ordering those chronologically); (3) _Anachronism detection_ (distinguishing possible vs. impossible events based on historical data).

<a id='f2f847b6-395b-445a-a9f1-59b67990dfad'></a>

We ask the LLMs to respond in a structured JSON format for us to easily evaluate their temporal abilities using different quantifiable metrics such as Spearman's ρ, Kendall's τ correlation coefficient, and exact match rates. Such a design requires no auxiliary user information at inference time: The model answers in a structured JSON format, allowing evaluation via rank- and set-based metrics (e.g., Kendall's τ, Spearman's ρ, precision/recall/F₁ for feasibility). We evaluate both non-reasoning and reasoning-mode LLMs from multiple families (e.g., OpenAI's GPT-4.1, the newly released GPT-5 series with various reasoning effort hyperparameters and Anthropic's Claude Sonnet 3.7 with and without extended-thinking), controlling for temperature and prompting. We report aggregate accuracy for exact match rate, rank correlations for ordering, and ablations (e.g., list length, position effects, and extended-thinking) to diagnose error modes.

<a id='f81ddee6-7858-468e-834d-f369ea74ae94'></a>

Our main finding is that LLM performance on our chronology tasks degrades quickly with problem complexity. This finding does not bode well for prompt-based mitigation of look-ahead bias using current models, as the complexity of walling off future information for a forecasting task is likely to be much greater than the complexity of our experiments. However, performance improves substantially with models with extended reasoning. By contrast, the LLM performance on anachronism detection is significantly better: accuracy is high (more than 0.90) in all task sizes with near-perfect precision and recall; degradation appears only when experiments with the notion of multiple temporal windows or multi-figure overlaps are introduced.

<a id='f83cf809-4c55-4def-8fbe-58ae2e1fd666'></a>

Interestingly, poor performance at high complexity kicks in at problem scales much smaller than those seen in other tasks (Shojaee et al. (2025)), suggesting that reasoning about chronology of real events may be inherently difficult. Also, complexity does not have a simple relationship with list length: for example, LLMs do better listing all U.S. presidents in chronological order than they do sorting a random list of 20 U.S. presidents.

<a id='be66929e-5248-43d8-acd5-d1d3abbddd3f'></a>

Our results include several novel findings: (i) Exact match ordering collapses as lists grow, even while rank correlations stay fairly high; (ii) single-prompt conditional sorting fails almost completely at the filtering step, so rank correlations on valid subset can overstate performance; (iii) turning on explicit deliberation flips this pattern, yielding stable filtering and near-perfect ordering; (iv) errors often concentrate in the middle of lists, with more-salient starting and end points acting as anchors; (v) a base anachronism detection task is easy for the model, but in tests based on the intersection

<a id='a13f9c69-cf4c-4759-b41d-53b4ba90ea4a'></a>

2

<!-- PAGE BREAK -->

<a id='fdd8c906-ab24-4858-8346-8741149636a5'></a>

of multiple events, we see more false negatives as the model struggles to correctly identify the intersection of multiple time lines.

<a id='c1495263-7fc3-476d-9c9e-1933605ba6ac'></a>

Together, today's models show a workable but brittle sense of chronology—decent local alignment, weak global consistency—unless we force them to think using reasoning models.

<a id='778d6db5-ac55-47d2-9b8b-337cf8f24216'></a>

## 2 Methodology

We design our tests based on widely available historical timelines, including the terms of U.S. presidents. These data sources allow the design of tasks of widely varying complexity based on information solidly within an LLM's training corpus. In particular, we use the following:

<a id='875776a0-214a-4d35-977c-b37b6d76310d'></a>

(a) U.S. presidents: 43 individuals (excluding Grover Cleveland and Donald Trump, who served non-consecutive terms), with attributes such as birth state, first-name initial, and calendar features (birth-year parity, month, day of week).
(b) Historical timelines: 20th century world events drawn from curated Wikipedia timeline sources to broaden basic sorting beyond the presidents domain.

<a id='0a38e780-7779-4650-a851-438e77bd105f'></a>

What distinguishes our design from other studies is that we test chronological reasoning over facts the model already knows. Before any ordering or detection trial, we verify item-level knowledge with a separate query: for each candidate item i, the model is asked to produce a year (ŷᵢ) — the start year of a presidency (presidents) or the event year (historical timelines). An item passes this screen if and only if ŷᵢ exactly matches our canonical year yᵣ. We then construct each trial list using only items that the LLM successfully filters and only then prompt the model to order them. All results reported in the paper are computed on these knowledge-verified lists, isolating ordering skill from internal knowledge gaps. All prompt templates (for verification, basic sorting, conditional sorting, and anachronism detection) are provided in Appendix A.3.

<a id='69e703f1-6b66-44d3-909c-cd0eae6a0b82'></a>

Our testbed comprises three task families of increasing difficulty:

**1. Basic sorting** Given a shuffled list, return items in strict chronological order. Evaluated on _both_

<a id='f5755095-9203-412b-8e7b-a4f8cff4a8be'></a>

corpora:

*   **Presidents**: with random list of length n ∈ {2, 5, 10, 15, 20, 25, 30, 35, 40, 43}.
*   **Historical timelines**: event lists of varying lengths to probe scaling and domain variety.

To test generalizability outside of the 20th century events, we also conducted a *timescale* variant which samples events whose dates differ by wider time window (at least 50-100 years) to probe coarse-granularity reasoning.

<a id='fdb703c7-3f58-49d1-a076-06c437c081f3'></a>

## 2. Conditional sorting The model *first* applies a Boolean filter and *then* sorts the surviving presidential names. Filters use widely known attributes:
(a) **Birth state**: Virginia, Ohio, Massachusetts, or OHIOORVIRGINIA;
(b) **Name prefix**: first names starting with A, B, or C;
(c) **Calendar attributes**: even birth year, day of week, month of birth, etc.

<a id='0a514d46-1242-4759-9492-a60eb21b86d9'></a>

We test two prompting paradigms: _self-filter-and-sort_ (single prompt, filter and order) and _given-subset-sort_ (subset supplied, order only).

<a id='7a3db7d8-ebf8-4f00-9832-fa39861472ce'></a>

For basic and conditional sorting, we report Exact match rate (EMR), position-wise accuracy, and
three rank-based metrics—Spearman's ρ, Kendall's τ, and normalized Cayley distance—computed
after subset identification. Formal definitions appear in Appendix A.1.

<a id='452fd818-9d5f-4f6b-a8ee-1f3cf37d1a50'></a>

**3. Anachronism detection** Each trial presents a configurable number of president-event statements, and other variants such as overlapping lifetime timelines of 2-4 presidents. The model labels each statement POSSIBLE or NOT POSSIBLE chronologically. We record classification accuracy, precision, recall, and F-1 score, in addition to the four ranking metrics.

<a id='3bb591d3-5e78-49b3-b693-267e59299160'></a>

Our experimental design provides a concise yet flexible testbed for chronology and temporal reasoning
for a couple of reasons:

<a id='c8be7fd9-f2f1-4d3d-88f3-0947d66e88ce'></a>

1. **No external documents at inference.** Measures intrinsic temporal coherence of the pretraining-induced world model (relevant to look-ahead bias).

<a id='c73a6ac5-ea9b-4483-9f95-746ad72a4693'></a>

3

<!-- PAGE BREAK -->

<a id='5b48c770-8e27-4d6b-8c89-33e144d10d4c'></a>

2. **Controllable difficulty.** The notion of list sizes and number of events and figures, conditional filters, and number of timelines overlaps in anachronism detection provide tunable complexities and allow for position-wise error profiles analysis.
3. **Reproducibility.** Deterministic settings (e.g., temperature = 0) with multi-seed repeats, public code, and fixed ground truth constructed from canonical timelines.

<a id='4a6bd01f-dd75-40a0-ba11-abadae5e8888'></a>

# 3 Related Work

## 3.1 LLMs and look-ahead bias in finance

Prior work documents that LLMs may be subject to look-ahead bias in return-prediction settings, even under anti-leakage prompts (Glasserman & Lin, 2024; Sarkar & Vafa, 2024). Mitigations include prompt engineering and masking (Glasserman & Lin, 2024; Engelberg et al., 2025), which reduce leakage but may degrade semantics or add inference cost. We refer to the Introduction for details; here we simply note that our evaluation is orthogonal: we want to measure chronological understanding of facts a model already knows, rather than to measure or prevent leakage of future information.

<a id='456b58d4-bcfe-4a2f-966b-afb057a449ae'></a>

## 3.2 Chronologically consistent LLMs
Chronology-aware pretraining strategies (e.g., time-sliced vintages) mitigate leakage by construction (Sarkar & Vafa, 2024; He et al., 2025b), but require heavy curation and retraining for each cutoff. Our approach complements these by testing existing off-the-shelf models empirically as end users to determine whether they exhibit usable chronological coherence, whether explicit reasoning improves it, and whether an added reasoning budget helps, without any retraining required.

<a id='44353d8e-59a9-4afd-a124-4f1d8adc992d'></a>

### 3.3 Broad temporal-reasoning benchmarks
Existing testbeds (TRAM, TimeBench, ChronoSense) primarily evaluate pairwise relations or short sequences (Wang & Zhao, 2024; Chu et al., 2024; Islakoglu et al., 2025). We avoid re-surveying them here; instead we focus on what they leave open: how accuracy scales with list length in ordering tasks with increasing complexity, and whether structured deliberation (e.g., extended thinking (Anthropic, 2025)) measurably improves performance.

<a id='5cf94948-38c4-4ab0-89fd-4636d16d4e9d'></a>

## 4 Basic Sorting on Events

We begin our experiment by establishing the foundational methodology for evaluating LLM temporal reasoning capabilities through a systematic approach to chronological ordering tasks. We start by constructing a dataset of historical events from the 20th century, extracted from Wikipedia timeline pages using a custom parser that handles various date formats including full dates (DD/MM/YY), month and year (MM/YY), and year-only (YY) entries. This extraction process involves cleaning and validating the data through date standardization, event deduplication, and ordinal ranking assignment, resulting in a final dataset of over 1000 events with chronological order. The 20th-century timeline corpus above was then used to prototype sampling, prompts, and scoring.

<a id='4759aa37-0bd9-4761-80e7-c33cdfbf3cf6'></a>

The experimental design employs a systematic approach to sample size selection, testing LLMs across small sizes (2-5 events), medium sizes (10-30 events), and large sizes (35-100 events) to not only assess the model's chronological ordering ability but also to test scaling behavior. Each experiment consists of 20 trials per sample size for statistical significance, with complete random sampling without replacement and random permutation of the ground truth order.

<a id='0881a1e5-b5d9-4d6e-93b5-0dca598d8fde'></a>

We use a standardized prompt structure that positions the LLM as an expert historian specializing in accurate chronological sequencing, with clear task rules requiring strict chronological ordering from earliest to latest events. The post-processing pipeline converts raw LLM response JSON files to standardized CSV format through fuzzy matching algorithms using FuzzyWuzzy string similarity with an an 85% threshold, handling missing items and extra items appropriately.

<a id='ebdd5716-d58a-4655-b8f1-a37e59a2bbc2'></a>

4

<!-- PAGE BREAK -->

<a id='e90f7bb0-ff68-4f5f-888d-e69cbc165d0d'></a>

**Event-knowledge filtering** Before parsing the prompts to the LLM to order our shuffled events, we first verify that it *knows* each item via a separate knowledge verification query (in Appendix A.3.1).

<a id='6219f18d-76ef-486d-a77e-7a59c64db153'></a>

The filtering approach ensures we only test chronological reasoning on events GPT demonstrably knows, providing a fair evaluation of temporal reasoning capabilities. For every candidate item we issued a single query ("In what year did E occur?") and compared the response to the ground-truth year. Events for which GPT-4.1's answer was incorrect were removed. Among the correctly identified events, they were sampled without replacement, yielding a corpus of N = 100 items, one per calendar year of the twentieth century from the original dataset. Ordering tasks were then conducted on this filtered set so that subsequent error analyses would not confound chronological reasoning with knowledge gaps.

<a id='cfe65d9a-8849-4ee8-8831-53b404e764c1'></a>

<::flowchart
Candidate events (~20th-Century corpus)
|
v
Query with T=0: In what year did (e) occur?
|
v
Year correct?
- no -> Discard
- yes -> Add to "known" pool
|
v
Sample 1 event per year
|
v
one_event_per_year.csv (N=100)
|
v
Re-run ordering experiments
Figure 1: Event-knowledge filtering pipeline
:flowchart::>

<a id='7e4af113-19d4-4e62-b50a-eae149a3059c'></a>

**Sanity check against chance** To contextualize performance, we benchmarked GPT-4.1 against 1,000 uniformly random permutations per list length and expressed model performance as an empirical percentile relative to this distribution. Across all list sizes GPT-4.1 ranks in the 95th–100th percentile for rank-correlation and normalized Cayley distance, confirming a substantial advantage over random guessing. Exact match, being an "all-or-nothing" metric, converges to the 50th percentile once n ≥ 20 because random permutations almost never achieve a perfect match. Despite this, we still see GPT-4.1's substantial edge over random guessing for Exact match when n ≤ 20 Full details and plots appear in Appendix A.4.

<a id='f7307686-fb86-4aaf-90a3-d787c1bc2c2a'></a>

**Wide-gap variant** Finally, we test whether large temporal spacing between events makes ordering easier. We keep the experimental design identical but resample events so that most of them are separated by wide gaps (50–200 years). We curated a small pool U of single-year events spanning Years 1–2025 (political, scientific, cultural). As in all experiments, we applied *knowledge verification* first: an event *e*k is retained if and only if the model returns the exact canonical year *ŷ*k = *Y*k. We then sampled lists of size *n* ∈ {2, 5, 10, 15, 20, 24} from the gated set *K*, targeting wide gaps by requiring that a large share of pairs exceed a soft threshold Δtgt ∈ {50, 100, 200}. We keep all inference settings and evaluation metrics unchanged. Full experimental design for this variant can be found in Appendix A.5.

<a id='e0aa616a-f81d-4144-87be-c9644456f68e'></a>

Note on determinism and reproducibility All ordering experiments used a deterministic event shuffle and temperature fixed at zero (temp = 0) for our API calls. Even so, re-submitting the exact same prompt occasionally produced small differences in the ranked list—behavior commonly reported for current LLM APIs (due to their non-deterministic nature) and not specific to our setting. To accommodate this, we ran two repeats per trial and report evaluation metrics (Spearman's ρ, Kendall's τ, Cayley<sub>norm</sub>) averaged across repeats; our qualitative results and conclusions are unchanged.

<a id='c43a7219-b1b4-4acc-8603-860639fa3d29'></a>

5

<!-- PAGE BREAK -->

<a id='6b420562-4c93-417d-bd07-8b459e2ae945'></a>

## 4.1 Findings

**20th Century Historical Timeline Result** We conducted 20 trials for each list size according to the methodology described in Figure 1. Table 1 shows the aggregate result for each list size, *n*.

<a id='ae4e5d25-3092-4e48-8b0c-8d9b2f5747c2'></a>

Table 1: Aggregated performance on the filtered corpus (20 trials per list size).
<table id="5-1">
<tr><td id="5-2">n_{events}</td><td id="5-3">\bar{\rho}</td><td id="5-4">\bar{\tau}</td><td id="5-5">Normalized Cayley</td><td id="5-6">Cayley</td><td id="5-7">Exact match rate</td></tr>
<tr><td id="5-8">2.00</td><td id="5-9">1.00</td><td id="5-a">1.00</td><td id="5-b">0.00</td><td id="5-c">0.00</td><td id="5-d">1.00</td></tr>
<tr><td id="5-e">5.00</td><td id="5-f">0.94</td><td id="5-g">0.88</td><td id="5-h">0.15</td><td id="5-i">0.60</td><td id="5-j">0.45</td></tr>
<tr><td id="5-k">10.00</td><td id="5-l">0.96</td><td id="5-m">0.88</td><td id="5-n">0.27</td><td id="5-o">2.40</td><td id="5-p">0.10</td></tr>
<tr><td id="5-q">20.00</td><td id="5-r">0.96</td><td id="5-s">0.87</td><td id="5-t">0.42</td><td id="5-u">7.95</td><td id="5-v">0.00</td></tr>
<tr><td id="5-w">50.00</td><td id="5-x">0.93</td><td id="5-y">0.81</td><td id="5-z">0.76</td><td id="5-A">37.35</td><td id="5-B">0.00</td></tr>
<tr><td id="5-C">100.00</td><td id="5-D">0.79</td><td id="5-E">0.66</td><td id="5-F">0.91</td><td id="5-G">90.00</td><td id="5-H">0.00</td></tr>
</table>

<a id='5273c47a-d9b1-41ac-850c-2b9aa48f9178'></a>

The most striking feature of these results are the Exact match rates in the last column. The LLM consistently orders a pair of events correctly, confirming the validity of the data and the task assignment. But with five events, the LLM achieves correct chronological ordering only about half the time. With longer lists, the LLM virtually never achieves a correct ordering. Performance on the chronological ordering task drops precipitously with problem complexity even at levels that use a fairly small number of tokens.

<a id='18ed0b36-4d63-4fbb-94ea-669d08652819'></a>

Spearman's \$\rho\$ and Kendall's \$\tau\$ are more forgiving as they seek to quantify the quality of an imperfect sort. These measures stay relatively flat from 5 to 50, but show a marked drop at 100. Kendall's \$\tau\$ is arguably the more informative of the two measures because it is based on pairwise comparisons of the order of events. The normalized Cayley distance rises monotonically with \$n\$, capturing the increasing number of swaps required to repair longer permutations. Taken together, these results indicate that GPT-4.1 excels at ordering very short lists but struggles with longer lists.

<a id='07f3f5a5-8396-49d9-b950-abf3ac28da69'></a>

We can also profile position-specific errors using the mean absolute rank difference (MARD). Figure 11 plots MARD$_n(k)$ across ground-truth positions $k$ for list lengths $n \in \{2, 5, 10, 20, 50\}$. The formal definition is in Appendix A.1.1; lower values indicate smaller displacement. We observe that for short lists ($n$=2, 5, 10), MARD remains below two positions for all ground-truth slots, and the shaded error bands are narrow, indicating stable accuracy regardless of where an event appears. However, for longer lists ($n$=20, 50, errors grow to 3-6 positions on average and vary sharply with position; variability bands widen, showing that the model's placement becomes both less accurate and less reliable.

<a id='988a8639-fd31-478f-b58a-f80b66888c24'></a>

Wide-Gap Variant Result Figure 12 and Table 14 summarize performance when events are separated by centuries and the pool is pre-filtered to facts the model knows. Rank correlations drop slightly, but remain high as list length grows: for n ∈ {10, 15, 20, 24} we observe ρ≈ 0.96–0.97 and τ ≈ 0.89–0.92. However, strict exact-match collapses beyond n≈ 10. Mean exact-match falls from 1.00 (n=2) to 0.55 (n=5), 0.20 (n=10), and reaches 0.00 for n ≥ 15. Thus, the model usually preserves the overall chronology even when it misses the exact sequence: larger lists are typically off by a few swaps. The normalized Cayley distance increases roughly monotonically from 0.125 (n=5) to 0.411 (n=24), i.e., from ~0.125 (n-1) to ~0.41 (n-1) swaps. For n=24 this corresponds to about 9–10 adjacent swaps on average. Overall, when temporal gaps are wide and factual recall is controlled, GPT-4.1 is globally broadly right but locally brittle: it retains the broad historical order yet rarely produces a perfect sequence once the list grows beyond ten items.

<a id='cc590b99-65ab-4835-9bb0-1c316228b0c7'></a>

We further compare this result from our curated wide time-gap events with the one we obtained from *20th-century historical events* from a narrower timeline in Table 1. Figure 2 juxtaposes their performances (2, 5, 10, and 20 items per trial).

<a id='485bf871-1e2e-4939-9a6f-db1715679999'></a>

6

<!-- PAGE BREAK -->

<a id='54becb9a-218d-4a4a-99c7-00f0304cd4a2'></a>

<::chart: Four line charts comparing 20th-century (filtered) vs. wide time-gap ordering.

**Chart 1: Exact Match Rate Comparison**
- Y-axis: Exact Match Rate (from 0.0 to 1.0)
- X-axis: Number of Events (from 2.5 to 20.0)
- Legend: 20th Century Historical Events (blue line with circular markers), Wide Gap Events (yellow line with circular markers).
- Description: Both lines start at 1.0 at 2.5 events and decrease as the number of events increases. The '20th Century Historical Events' line decreases more sharply initially, then levels off, staying slightly below the 'Wide Gap Events' line. Shaded bands denote ±2 standard errors.

**Chart 2: Spearman's Comparison**
- Y-axis: Spearman's ρ (from 0.92 to 1.00)
- X-axis: Number of Events (from 2.5 to 20.0)
- Legend: 20th Century Historical Events (blue line with circular markers), Wide Gap Events (yellow line with circular markers).
- Description: Both lines start at 1.0 at 2.5 events. The '20th Century Historical Events' line drops to about 0.94 at 5 events, then gradually increases to around 0.96 at 20 events. The 'Wide Gap Events' line drops to about 0.94 at 5 events, then increases to about 0.97 at 10 events, and then slightly decreases to about 0.96 at 20 events. Shaded bands denote ±2 standard errors.

**Chart 3: Kendall's τ Comparison**
- Y-axis: Kendall's τ (from 0.825 to 1.000)
- X-axis: Number of Events (from 2.5 to 20.0)
- Legend: 20th Century Historical Events (blue line with circular markers), Wide Gap Events (yellow line with circular markers).
- Description: Both lines start at 1.000 at 2.5 events. The '20th Century Historical Events' line drops to about 0.88 at 5 events, then slowly decreases to about 0.87 at 20 events. The 'Wide Gap Events' line drops to about 0.90 at 5 events, then slowly decreases to about 0.89 at 20 events, generally staying above the '20th Century Historical Events' line. Shaded bands denote ±2 standard errors.

**Chart 4: Normalized Cayley Distance Comparison**
- Y-axis: Normalized Cayley Distance (from 0.0 to 0.4)
- X-axis: Number of Events (from 2.5 to 20.0)
- Legend: 20th Century Historical Events (blue line with circular markers), Wide Gap Events (yellow line with circular markers).
- Description: Both lines start at 0.0 at 2.5 events and increase as the number of events increases. The '20th Century Historical Events' line increases steadily to about 0.42 at 20 events. The 'Wide Gap Events' line increases steadily to about 0.37 at 20 events, generally staying below the '20th Century Historical Events' line. Shaded bands denote ±2 standard errors.

Figure 2: 20th-century (filtered) vs. wide time-gap ordering. Lines show means over 20 trials; shaded bands denote ±2 standard errors.::>

<a id='60c7f262-4396-4c78-8766-819f7c398d1f'></a>

The comparison points to two conclusions:
*   **Wider time windows improve performance.** Ordering improves when events are separated by large temporal gaps (on the order of 10^2 years). Across list sizes, wide-time-gap sets yield equal or higher ordering accuracy than same-century baselines. This is what we would expect to see from human respondents — it should be easier to sequence widely separated events — but it is not obvious that the same should be true for LLMs.
*   **Scaling list length worsens performance.** As the number of items increases, performance degrades for both—consistent with earlier ranking tasks—but less so under wide time gaps, with rank correlations still higher than their counterpart.

<a id='4c2258fa-1dde-42a9-bd7c-8f44c273a549'></a>

## 5 Basic Sorting on U.S. Presidents
The historical events ordering tasks established a baseline for testing LLM understanding of chronol-ogy and allowed us to vary complexity through list length. In order to explore other dimensions of complexity, we turn now to a more structured dataset: the sequence of U.S. presidents. The U.S. has to date recorded 45 presidencies. However, Grover Cleveland and Donald Trump were elected to non-consecutive terms. To avoid potential ambiguities in sequencing presidents (e.g., whether should Trump be listed before or after Biden), we remove these two presidents from our list and work exclusively with the remaining 43.

<a id='1962d78b-c5b8-4103-b976-9a142d8176f6'></a>

## 5.1 Methodology for the U. S. Presidents Chronology Task

For each list size n ∈ {2, 5, 10, 15, 20, 25, 30, 35, 40, 43} we run 20 trials. In every trial we (i) apply knowledge verification—each candidate must return the correct presidency start year in a separate year query; (ii) sample n distinct presidents without replacement and uniformly shuffle them; (iii) prompt the model to order them chronologically; and (iv) score with our usual evaluation metrics. Full protocol and the workflow diagram appear in Appendix A.6.1.

<a id='e4236f01-87ce-4b10-8c99-c4fb6c69bc24'></a>

7

<!-- PAGE BREAK -->

<a id='7ed32fa2-1e7e-4d6f-88c4-17389a8bcc68'></a>

**5.2 Findings & Bottlenecks**

During our runs we observed that GPT-4.1 occasionally omitted presidents that were present in the prompt or hallucinated additional names. (The hallucinated names were always presidents, just not presidents included in the sample.) In 42 of the 200 trials, the LLM omits at least one name from list in the prompt. The problem first appears at n=15 (3/20 trials) and grows steadily, peaking at n=25 and n=30 where more than half the trials omit at least one name. 64 trials return presidents that did not appear in the prompt. Hallucinations are rare below n=25 but dominate the longest lists, affecting 85% of n=40 trials and 95% of the full n=43 trials, but there are no omissions or hallucinations for n≤10

<a id='002fe626-14eb-4341-8179-6f1366afcdcf'></a>

From Table 18, early-period leaders such as James K. Polk and Andrew Jackson are most often missing; modern figures—Obama, Biden, both Bushes—are frequently added even when absent from the prompt. This hallucination on a rather relatively simple task like sorting a subset of a well-known sequence (U.S. Presidents) may occur due to the model resorting to the full list it was being pre-trained on and ignoring the constraints given to only use the provided items. The pattern may also be similar to human errors in the sense that Polk is fairly obscure and recent presidents are much more salient.

<a id='aced6e25-2ca8-41d5-bbdd-f1ce403fdd9a'></a>

To prevent omissions and hallucinations from distorting our earlier defined metrics, we need to adapt the evaluation pipeline to account for such discrepancies. We introduced a dedicated cleaning stage before scoring each trial (as formalized in Appendix A.6.2). First, every row whose predicted_rank is NAN is removed, eliminating presidents that the model failed to mention. Next, we discard rows whose predicted_rank is numerically greater than or equal to n_prompt; these correspond to hallucinated names that were not present in the input list. If, after this pruning, no valid president-rank pairs remain, the trial is marked invalid and omitted from aggregate statistics. We then compute metrics on the surviving pairs. Each trial also records auxiliary counters—MISSING for the number of omitted presidents, EXTRA for hallucinated names, and VALID_PAIRS for the size of the filtered set—so that omission and hallucination rates can be reported alongside the main performance metrics. These safeguards confine the evaluation to genuine ordering errors and shield the analysis from missing or superfluous names.

<a id='fb3e8b9e-0783-4d7a-891b-0fd53079e10b'></a>

Summary statistics after cleaning Table 2a and 2b report the mean and standard deviation of all evaluation metrics, averaged over 20 trials for each list size. The columns MISSING_NAMES and EXTRA_NAMES quantify the average number of presidents that were omitted or hallucinated before cleaning. The most striking feature of Table 2a, illustrated in Figure 14 (Appendix A.7.4), is the U-shaped pattern for the correlation measures: they generally fall as the list length increases but then rebound as n increases toward the complete list. This pattern suggests that the LLM "knows" the complete timeline of U.S. presidents and has more difficulty ordering a random subset. However, as we saw with the timeline of historical events, the exact match rate drops sharply as the list length grows.

<a id='bf37b5e1-4e18-4bf4-9c4d-4162cc89371c'></a>

We also summarize per-position misplacement with the mean absolute rank difference (MARD),
illustrated in Figure 13. MARD per position stays modest for the positions $r \le 20$, then climbs
steeply, reaching its peak around $r \approx 30-35$. These high-error slots coincide with the mid-19th-
century presidents most often omitted or replaced. In Table 19, we group presidents by century to
confirm this trend: accuracy is highest for well-known 18th-century figures and degrades through the
19th and 20th centuries, before improving for the still-evolving 21st-century cohort. All in all, these
findings suggest that GPT-4.1's ordering reliability depends jointly on both the list length and the
historical salience of the individual names.

<a id='b6cfcf44-1fe9-4111-8020-8134716aa264'></a>

8

<!-- PAGE BREAK -->

<a id='2166c4f2-1239-488e-b3bc-570b1e00f012'></a>

Table 2: Performance summaries

<a id='3e57844b-64a9-4d90-aa64-d0e5caa0d209'></a>

(a) Per-size means (µ) and standard errors (SE = σ/√20) after applying the cleaning pipeline.

<a id='3884f25a-d7b8-4931-808c-592fcd9e1bdd'></a>

<table id="8-1">
<tr><td id="8-2" rowspan="2">n</td><td id="8-3" colspan="2">Exact match</td><td id="8-4" colspan="2">Spearman&#x27;s ρ</td><td id="8-5" colspan="2">Kendall&#x27;s τ</td><td id="8-6" rowspan="2">Cayley µ</td></tr>
<tr><td id="8-7">μ</td><td id="8-8">SE</td><td id="8-9">μ</td><td id="8-a">SE</td><td id="8-b">μ</td><td id="8-c">SE</td></tr>
<tr><td id="8-d">2</td><td id="8-e">0.96</td><td id="8-f">0.04</td><td id="8-g">0.998</td><td id="8-h">0.00</td><td id="8-i">0.997</td><td id="8-j">0.00</td><td id="8-k">0.00</td></tr>
<tr><td id="8-l">5</td><td id="8-m">0.87</td><td id="8-n">0.08</td><td id="8-o">0.973</td><td id="8-p">0.02</td><td id="8-q">0.960</td><td id="8-r">0.03</td><td id="8-s">0.20</td></tr>
<tr><td id="8-t">10</td><td id="8-u">0.36</td><td id="8-v">0.11</td><td id="8-w">0.961</td><td id="8-x">0.02</td><td id="8-y">0.932</td><td id="8-z">0.02</td><td id="8-A">1.72</td></tr>
<tr><td id="8-B">15</td><td id="8-C">0.20</td><td id="8-D">0.09</td><td id="8-E">0.960</td><td id="8-F">0.01</td><td id="8-G">0.927</td><td id="8-H">0.02</td><td id="8-I">3.27</td></tr>
<tr><td id="8-J">20</td><td id="8-K">0.10</td><td id="8-L">0.07</td><td id="8-M">0.971</td><td id="8-N">0.01</td><td id="8-O">0.929</td><td id="8-P">0.01</td><td id="8-Q">6.20</td></tr>
<tr><td id="8-R">25</td><td id="8-S">0.09</td><td id="8-T">0.08</td><td id="8-U">0.967</td><td id="8-V">0.01</td><td id="8-W">0.930</td><td id="8-X">0.01</td><td id="8-Y">6.90</td></tr>
<tr><td id="8-Z">30</td><td id="8-10">0.00</td><td id="8-11">0.00</td><td id="8-12">0.963</td><td id="8-13">0.01</td><td id="8-14">0.948</td><td id="8-15">0.02</td><td id="8-16">6.60</td></tr>
<tr><td id="8-17">35</td><td id="8-18">0.00</td><td id="8-19">0.00</td><td id="8-1a">0.992</td><td id="8-1b">0.00</td><td id="8-1c">0.986</td><td id="8-1d">0.01</td><td id="8-1e">3.65</td></tr>
<tr><td id="8-1f">40</td><td id="8-1g">0.00</td><td id="8-1h">0.00</td><td id="8-1i">0.993</td><td id="8-1j">0.00</td><td id="8-1k">0.989</td><td id="8-1l">0.01</td><td id="8-1m">3.10</td></tr>
<tr><td id="8-1n">43</td><td id="8-1o">0.00</td><td id="8-1p">0.00</td><td id="8-1q">1.000</td><td id="8-1r">0.00</td><td id="8-1s">1.000</td><td id="8-1t">0.00</td><td id="8-1u">0.05</td></tr>
</table>

<a id='5aab123b-0616-4de5-91bb-2c6135cc1e5a'></a>

(b) Per-size means (µ) and standard errors (SE = σ/√20) for error counts.

<a id='247b6862-7b3b-46ed-b049-24c8fe211a49'></a>

<table id="8-1v">
<tr><td id="8-1w" rowspan="2">n</td><td id="8-1x" colspan="2">missing names</td><td id="8-1y" colspan="2">extra names</td></tr>
<tr><td id="8-1z">μ</td><td id="8-1A">SE</td><td id="8-1B">μ</td><td id="8-1C">SE</td></tr>
<tr><td id="8-1D">2</td><td id="8-1E">0.00</td><td id="8-1F">0.00</td><td id="8-1G">0.60</td><td id="8-1H">0.28</td></tr>
<tr><td id="8-1I">5</td><td id="8-1J">0.00</td><td id="8-1K">0.00</td><td id="8-1L">0.00</td><td id="8-1M">0.00</td></tr>
<tr><td id="8-1N">10</td><td id="8-1O">0.00</td><td id="8-1P">0.00</td><td id="8-1Q">1.00</td><td id="8-1R">0.46</td></tr>
<tr><td id="8-1S">15</td><td id="8-1T">0.20</td><td id="8-1U">0.09</td><td id="8-1V">0.07</td><td id="8-1W">0.06</td></tr>
<tr><td id="8-1X">20</td><td id="8-1Y">0.30</td><td id="8-1Z">0.16</td><td id="8-20">0.00</td><td id="8-21">0.00</td></tr>
<tr><td id="8-22">25</td><td id="8-23">0.70</td><td id="8-24">0.16</td><td id="8-25">0.25</td><td id="8-26">0.20</td></tr>
<tr><td id="8-27">30</td><td id="8-28">1.05</td><td id="8-29">0.30</td><td id="8-2a">0.55</td><td id="8-2b">0.36</td></tr>
<tr><td id="8-2c">35</td><td id="8-2d">0.90</td><td id="8-2e">0.28</td><td id="8-2f">2.50</td><td id="8-2g">0.68</td></tr>
<tr><td id="8-2h">40</td><td id="8-2i">0.35</td><td id="8-2j">0.22</td><td id="8-2k">3.25</td><td id="8-2l">0.42</td></tr>
<tr><td id="8-2m">43</td><td id="8-2n">0.05</td><td id="8-2o">0.05</td><td id="8-2p">1.65</td><td id="8-2q">0.13</td></tr>
</table>

<a id='923dbda4-a438-4e36-8383-ab11ad9fa12e'></a>

## 5.3 Experimenting with Large Reasoning Models (LRMs)
We also tested chronological ordering of U.S. presidents with a few large reasoning models, namely Claude 3.7 Sonnet and the newly-released GPT-5 with varying reasoning efforts (minimal, low, medium, and high). In this section, we discuss how they fare compared with the normal LLMs and provide working hypotheses to explain the results.

<a id='747a71a5-9b18-4706-b117-46dc45414119'></a>

Recent "large reasoning models" such as OpenAI's ChatGPT-03 or Anthropic's Claude 3.7 Sonnet add an explicit, controllable deliberation phase to the usual input→output loop. To test whether this explicit "scratch-pad" reasoning improves ordering outcomes, we test the president-ordering task on a large reasoning model (LRM) with extended thinking support.² With extended thinking, the model can allocate a separate budget of "thinking" tokens to produce intermediate reasoning before emitting the final answer. Those thinking tokens are part of the normal token accounting (they count toward the context window and are billed as output), and the budget is developer-tunable (via budget tokens). Extended thinking can also be returned to the caller for inspection, which allows us to study how much (and when) extra reasoning helps chronological ordering, making it a good testbed for comparing standard vs. extended reasoning on the same prompts for our analysis.

<a id='b1006cc7-180b-4438-adf6-46ecc960140a'></a>

---2Anthropic's developer docs describe extended thinking and streaming: https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking and https://docs.anthropic.com/en/docs/build-with-claude/messages/streaming.

<a id='98883097-29f2-469c-8ff6-4f5d770eba6c'></a>

9

<!-- PAGE BREAK -->

<a id='b5f970c2-5245-4bfa-9c98-d1c8342f0c6b'></a>

We also evaluated OpenAI's GPT-5, which differs from Claude 3.7 Sonnet in two ways: (i) it is a unified reasoning model that automatically _routes_ between fast replies and deeper thinking, and (ii) it allows us to tune a reasoning_effort hyperparameter (e.g., minimal/low/medium/high) that controls internal reasoning tokens but does _not_ return an inspectable reasoning stream. We include GPT-5 to test whether tunable test-time reasoning improves chronological ordering even without visible Chain-of-Thought traces (OpenAI, 2025a,b,c,d).

<a id='5129b13e-15f6-468f-979d-fce5fd6d6caa'></a>

**Experimental design** We run head-to-head controlled comparisons (identical prompt temperature, sample_sizes, and trials per sample_sizes) with the same president-ordering methodology outlined earlier and in Appendix A.6.1 tasks under eight regimes: GPT-4.1 (results directly from Section 5.2), Claude 3.7 Sonnet without Extended Thinking (ET), Claude 3.7 Sonnet with Extended Thinking (ET), GPT-5 (reasoning_effort = minimal), GPT-5 (reasoning_effort = low), GPT-5 (reasoning_effort = medium), GPT-5 (reasoning_effort = high), and the non-reasoning GPT-5 (latest) variant as a control.

<a id='bf3fce43-5dff-45d8-ae37-a375f946c734'></a>

Table 3: Claude Sonnet 3.7 extended-thinking run-time parameters.

<table><thead><tr><th>Parameter</th><th>Value</th></tr></thead><tbody><tr><td>Temperature</td><td>T=1.0 (ET runs); T=0.0 (non-ET baselines)</td></tr><tr><td>Thinking budget</td><td>budget_tokens = 5000</td></tr><tr><td>Max response tokens</td><td>max_tokens = 8000</td></tr><tr><td>Rationale</td><td>5k budget avoids ET truncation; 8k headroom for final text/tool I/O</td></tr></tbody></table>

<a id='f976a3ea-19ee-4468-9300-098dc15a8f06'></a>

All models receive identical prompts and shuffled name lists at each list length n ∈ {2, 5, 10, 15, 20, 25, 30, 35, 40, 43}. For ET we follow the provider's recommended usage: temperature T=1.0, budget_tokens= 5000 for the thinking stream, and max_tokens= 8000 for the visible response. The 5k thinking budget was chosen to avoid truncation of the hidden thinking stream in our prompts, while max_tokens=8000 comfortably accommodates the visible answer plus any tool-usage text. All non-ET baselines were run at T=0. ³

<a id='dafa4c69-9e49-450c-ac7c-7fa8eb09dac5'></a>

5.3.1 Key findings<::Model Comparison: Claude Sonnet 3.7 vs ChatGPT-4.1 Performance. This visual contains three line charts arranged horizontally.

**Chart 1: Exact Match Rate by Sample Size**
- Y-axis: Exact Match Rate (from 0.0 to 1.0)
- X-axis: Number of Presidents (from 0 to 40)
- Data Series:
    - ChatGPT-4.1 (black line with circles, solid): Starts around 0.8, drops sharply to near 0.0 by 20-30 presidents, and remains low.
    - Claude-3.7 (Without ET) (red line with crosses, dashed): Starts around 0.4, rises to around 0.5, then drops to near 0.0 by 40 presidents.
    - Claude-3.7 (With ET) (blue line with squares, solid): Consistently at 1.0 across all 'Number of Presidents'.

**Chart 2: Spearman Correlation by Sample Size**
- Y-axis: Spearman Correlation (from 0.75 to 1.00)
- X-axis: Number of Presidents (from 0 to 40)
- Data Series:
    - ChatGPT-4.1 (black line with circles, solid): Starts at 1.00, drops to around 0.95, then fluctuates between 0.95 and 1.00.
    - Claude-3.7 (Without ET) (red line with crosses, dashed): Starts around 0.80, rises to around 0.98, then fluctuates between 0.95 and 1.00.
    - Claude-3.7 (With ET) (blue line with squares, solid): Consistently at 1.00 across all 'Number of Presidents'.

**Chart 3: Kendall Tau by Sample Size**
- Y-axis: Kendall Tau (from 0.70 to 1.00)
- X-axis: Number of Presidents (from 0 to 40)
- Data Series:
    - ChatGPT-4.1 (black line with circles, solid): Starts at 1.00, drops to around 0.92, then fluctuates between 0.92 and 0.98.
    - Claude-3.7 (Without ET) (red line with crosses, dashed): Starts around 0.75, rises to around 0.98, then fluctuates between 0.95 and 0.98.
    - Claude-3.7 (With ET) (blue line with squares, solid): Consistently at 1.00 across all 'Number of Presidents'.

Figure 3: *Claude 3.7 with Extended Thinking* (blue) dominates—achieving 100% Exact match at all *n* Points show trial means; error bars are ±2 s.e.; small horizontal jitter prevents overlap.: chart::>

<a id='c3a9d373-0263-4cc8-8232-efbd7dff6d1b'></a>

Claude 3.7 Sonnet (standard) vs. GPT-4.1 To our surprise, Claude 3.7 with ET achieves _perfect chronological ordering_ across all list sizes (Fig. 15): exact-match = 1.00 at every _n_, and normal- ized Cayley distance indistinguishable from zero. In sharp contrast, Claude 3.7 _without_ ET and GPT-4.1 reproduce the characteristic pattern from earlier task: rank correlations remain high, but the exact-match rate collapses as _n_ grows (Fig. 3). The improvement with ET is not a small calibration gain; it is a qualitative shift from "near-correct orderings with frequent small inversions" to error-free

<a id='65166d83-f1fc-4fac-9429-af43a34064d6'></a>

--- 

$^3$ET requires $T$=1.0 and manages a separate hidden "thinking" stream; see Table. 3 and the model documen- tation.

<a id='81e23e57-b268-4913-b2d1-00f9e16d7e93'></a>

10

<!-- PAGE BREAK -->

<a id='d1d2e776-de10-4485-8f14-c20c0bc2440c'></a>

outputs. Panels in Fig. 15 (bottom row) also show that ET suppresses missing or extra hallucinated outputs completely. Without ET, runs often contain MISSING, and more so EXTRA names (length mismatches) even when rank correlations are high; with ET, these errors are essentially absent. ET flattens curves on all metrics: performance is invariant to n within our tested range, indicating that the model's hidden reasoning is sufficient to maintain global consistency over long lists.

<a id='fd6c0f0e-5efe-4836-9e29-f3872a483a12'></a>

A striking secondary result is that _without_ extended thinking (ET), Claude 3.7 does not reliably outperform GPT-4.1 on this benchmark despite it being a large reasoning model which we believed would perform better than GPT-4.1; in fact, at _n_=5 Claude lags 4.1 (Fig. 3).

<a id='583aef38-98a7-4d9d-9516-5cee038f7898'></a>

**Why does Extended Thinking help? A working hypothesis** We do not know the mechanism within the model that allows for such flawless performance. However, we hypothesize that because ET grants the model a large, private scratchpad to deliberate before emitting any visible text, this may enable a reliable internal routine: (1) enumerate all presidents; (2) verify membership against the criterion implicit in the task; (3) perform pairwise chronology checks; (4) remove any duplicates; and (5) emit the final ordered list.

<a id='6463ae3c-0e8b-4cae-b383-72549f4c9bb6'></a>

Separating _thinking_ from _outputting_ may reduce early-commitment and exposure-bias errors, and it allows multiple self-checks without incurring user-visible tokens. We give an excerpt from Claude 3.7 Sonnet with ET enabled, taken from a trial that required ordering a subset (30) of presidents in Appendix A.7.6. This thinking trace gives us useful information to make an inference on the reasoning model's exceptional ordering capability. The model first reconstructs an external scaffold (terms in office), which supplies a consistent comparison key. It then explicitly marks incorrect names ("not on the list"), showing evidence of a _set-membership_ check before ordering. The final output is a clean, deduplicated sequence without dates—consistent with the output format requested in the prompt—indicating a separation between computation and presentation. Together these behaviours align with our hypothesis: ET supplies the private working memory needed to complete filtering and ordering reliably before committing to the final output.

<a id='805ba98e-baeb-4a47-805a-77d5dc1eb617'></a>

<::chart: Exact Match Rate by Sample Size Across Models. The x-axis is labeled "Number of Presidents" and ranges from 0 to 40 in increments of 10. The y-axis is labeled "Exact Match Rate" and ranges from 0.0 to 1.0 in increments of 0.2. The chart displays multiple lines representing different models, each with error bars. The legend indicates the following models and their representations:
- ChatGPT-4.1: Black line with circular markers and solid line.
- Claude-3.7 (No ET): Red line with triangular markers and dashed line.
- Claude-3.7 (With ET): Blue line with square markers and solid line.
- GPT-5 Minimal: Blue line with cross markers and solid line.
- GPT-5 Low: Orange line with square markers and solid line.
- GPT-5 Medium: Green line with plus markers and solid line.
- GPT-5 Latest: Red line with diamond markers and solid line.

GPT-5 (medium, high) and Claude 3.7 with Extended Thinking achieve flawless Exact match (100%) across all *n*. GPT-5 low is near-perfect, with only a minor dip at mid sizes. The remaining models—GPT-5 minimal, Claude 3.7 without ET, and GPT-4.1—are broadly comparable to one another and lose Exact match rapidly as *n* increases. Points show trial means; error bars are ±2 s.e.; small horizontal jitter prevents overlap.::>

Figure 4: Exact match by list size.

<a id='93973a4b-630d-4eaa-b78c-50e85b3532bd'></a>

**GPT-5 with varying reasoning efforts** Meanwhile, GPT-5 shows a sharp effort-accuracy transition. With *medium* and *high* reasoning effort the model achieves *perfect* chronological ordering at all list sizes (Exact match = 1.00; ρ = τ = 1.00. The *low* setting is near-perfect, with a single dip at n=25 (EM = 0.95); otherwise performance is indistinguishable from perfect. In contrast, the *minimal* and *no-reasoning* variants mirror non-reasoning LLMs: rank correlations remain very high (ρ, τ ≥ 0.94) but exact match collapses as list size increases. We observe a clear pattern: as GPT-5's reasoning

<a id='6b5ab45a-ad9a-4561-94a0-b111c7046239'></a>

11

<!-- PAGE BREAK -->

<a id='e15736ed-5082-4b5a-9bb2-a0bdab92cf36'></a>

effort increases from minimal/low to medium/high, Exact match significantly improves and reaches
100% at medium/high. This supports our hypothesis that giving the model more test-time reasoning
budget (i.e., more "space"/inference time/private scratchpad) enables it to excel at chronological
orderings by providing it with a simple internal "checklist": check membership, compare dates, then
produce a single consistent order—to complete.

<a id='035a9b5e-3d72-4329-8358-69bb56e44422'></a>

The effect mirrors what we see with Claude 3.7's Extended Thinking, where an explicit thinking stream plays a similar role. We treat both systems as black boxes, but the alignment between effort/ET and the accuracy gains suggests that added deliberation is the main driver for such a dramatic increase in performance metrics.

<a id='f99c57e6-bb31-4453-8da5-df062453e39b'></a>

# 6 Conditional Sorting

The experiments reported thus far have each evaluated a single, direct form of chronological understanding: present a shuffled list of events and ask the model to restore the correct order. We now turn to tasks in which the events to be ordered are defined implicitly through some condition (e.g., "presidents born on a Monday") rather than sampled randomly and presented explicitly. This allows us to explore aspects of task complexity that differ from list length.

<a id='e0a6da40-ff24-4ae3-9199-b3bcbb935671'></a>

Requiring the LLM to filter the events that satisfy some condition adds to the complexity of the task. However, the reasoning it requires could potentially act as a scaffold that provides the LLM with greater insight into the selected names (compared to being presented with a random list), helping it avoid errors. Our experiments will test these competing hypotheses and conclude that the added complexity dominates.

<a id='210dc640-d6a1-4ee9-ad2f-7ac2ec9cbe72'></a>

## 6.1 Methodology for the U. S. Presidents One-Prompt Conditional Sorting Task
The direct-ordering experiments of U.S. presidents in the earlier section showed that GPT-4.1's exact match rate collapses completely once the list exceeds about ten names. We, therefore, naturally conjecture that a two-step prompt—urging the model to _think_ by first _filtering_ the candidate set before ordering it—may nudge the LLM into a more deliberative "System-2" reasoning and thus lower its error rate. Our central question is straightforward: does GPT-4.1 sort more accurately when it _discovers_ the relevant names itself, or when it is simply _given_ the correct subset and asked to order it?

<a id='e9aaf1b6-7ea6-409a-9f14-ded9d4c6f155'></a>

Each trial begins with a single shuffled list of 43 unique presidents like before. Two matched conditions are then run on that same shuffled order. In the _self-filtering-sorting_ condition, the model is told to filter for a factual criteria _c_ and then to sort the surviving names chronologically by presidency. In the _given-names sorting_ condition, the model receives exactly the names that satisfy _c_—in the same relative order they appeared in the full list—and is asked only to sort them in a separate API call.

<a id='a4f47a8b-bc01-48cd-897a-46a21e92a123'></a>

We experimented with various criteria: the state in which each president was born—OHIO (7 names), VIRGINIA (8 names), MASSACHUSETTS (4 names), and whether their birth year was even, EVENYEARS (22 names)—and found that GPT-4.1 almost never filtered perfectly (single-digit success rates over 100 trials). To balance tractability and difficulty, we adopted the combined criteria to filter the presidents being born in either OHIOORVIRGINIA (15 names): large enough to make ordering non-trivial, but small enough that perfect filtering occurs with reasonable frequency. In short, it is ‘‘just right’’ for our contrast.

<a id='c68c2952-45e0-4af7-8b26-42808ae688e3'></a>

**Sharp contrast protocol** To make a clean comparison, we retain *only* those self-filtering runs in which the model's filtered set Gc(t) exactly matches the ground truth Gc. If the final list is missing any required name or contains any extraneous name, the filtering step is marked *invalid* and that trial is discarded from the ordering comparison (duplicates are ignored for filtering, but counted as an ordering error later). Each trial begins with a single shuffled list of all 43 unique presidents (Grover Cleveland and Donald Trump removed). Two matched conditions are then run on that *same* shuffle:

<a id='d9d0d922-058b-4b01-8b34-f616bb3190f9'></a>

SELF_FILTERING_SORTING
The model receives the full list and a criteria c (e.g., "born in OhioOrVirginia"), filters, then orders the survivors by presidency.

<a id='412d7b27-c5ab-434c-9756-89d0b6962341'></a>

12

<!-- PAGE BREAK -->

<a id='d18ff617-cedc-471f-a819-0a329c2db384'></a>

<::flowchart: Conditional sorting pipeline::>Shuffled full list (43 unique presidents) -> Choose criterion c, Compute ground-truth set G_c. From 'Choose criterion c, Compute ground-truth set G_c' there are two main paths:

Left Path (Self-filtering & sorting):
1. Self-filtering & sorting (starburst shape)
2. Prompt: "Filter presidents that satisfy c, then order them chronologically by presidency." (rounded rectangle)
3. Post-process LLMs' outputs (names) (label on arrow)
4. (Perfect filter?) G_c^(t) = G_c ? (diamond shape)
   - If Yes: -> Compute ordering evaluation metrics
   - If No: -> Log missing / extra names; exclude the trial from ordering evaluation (rounded rectangle)

Right Path (Given-name & sorting):
1. Given-name & sorting (starburst shape)
2. Prompt: "Order {G_c} (shuffled) chronologically by presidency." (rounded rectangle)
3. Post-process LLMs' outputs (names) (label on arrow)
4. -> Compute ordering evaluation metrics

Common Path:
1. Compute ordering evaluation metrics (Exact Match rate, Spearman's Rho, Kendall's Tau, normalized Cayley) on G_c (rectangle)
2. Aggregate metrics over # of trials: Filtering accuracy (perfect filter rate); means/SDs of ordering metrics over # of valid trials (rectangle)

Figure 5: Conditional sorting pipeline: self-filtering vs. given-names. Only trials with G_c^(t) = G_c on the left enter the paired comparison. Duplicates count toward ordering errors but not the filtering decision.<::>

<a id='df580aed-a216-46f7-aaf4-5a93395f07fa'></a>

GIVEN_NAMES_SORTING
The model receives exactly the G_c names—presented in the same relative order as in the full list—and is asked only to sort them.

We give an illustrative example here to illustrate the algorithm. Let the shuffled list be [F, B, D, G, H, E, C, A] and c = "born in OhioOrVirginia", giving G_c = {B, E, G}.

1. SELF_FILTERING_SORTING: Input the full list; instruct "filter for OhioOrVirginia, then order chronologically." Correct behavior: output [B, G, E] → reorder to [B, E, G].
2. GIVEN_NAMES_SORTING: Input only [B, G, E]; instruct "order chronologically." Output [B, E, G].

<a id='0a5b2118-730f-4fd2-aed8-c61d34bdcb64'></a>

The full conditional sorting protocol formalization is present in Appendix A.8.1. For each criterion we report: (i) the perfect-filter pass rate, (ii) mean/SD of each ordering metric in both conditions, and (iii) paired deltas.

<a id='f3868d4f-e680-4a8d-962f-1794cf5ce3bb'></a>

## 6.2 Key findings

### 6.2.1 Filtering is the dominant failure mode

Alarmingly, for the criteria c = FIRSTNAMESSTARTINGWITHABORC (ground-truth size n_gt = |G_c| = 8), the model achieved only a whopping 2% perfect filtering: in 2/100 runs the predicted set Ĝ_c exactly matched G_c; consequently there were too few valid runs to support a meaningful ordering analysis in the self-filter condition. In this task, GPT-4.1 also hallucinates erroneous names. All five have surnames beginning with "B," consistent with a last-name-initial confusion. (Counts are out of 100 trials.) Interestingly, the most common false inclusions are modern presidents whose surnames begin with B (e.g., Biden, Bush, Buchanan, Van Buren), indicating that the model often applied the initial to the last name rather than the first. We also observe occasional nickname/alias leakage (e.g., "Jimmy" vs. "James" Carter). The error profiles show a strongly asymmetric failure mode: the model tends to overselect rather than underselect presidents. Across trials it frequently appends presidents who do not satisfy the A/B/C first-name criterion, while true positives are seldom omitted; even in runs that contain both misses and extras, the extras typically dominate.

<a id='40ea79ea-1079-4c73-adff-9bd1c253ae7c'></a>

We observed the same trend for OHIOORVIRGINIA filter; the self-filtering pass rate was essentially zero. Out of 100 self-filtering trials, not a single run achieved perfect set equality G^(t) = Gc, leaving no valid trials from which to estimate ordering accuracy under self-filtering. The most frequently

<a id='b78f622f-729d-4c0d-95f0-6f439a5155b0'></a>

13

<!-- PAGE BREAK -->

<a id='ce096bc6-0cd5-451e-bac6-276265b0a2ca'></a>

omitted ground-truth name $G_c \setminus \hat{G}_c^{(t)}$ was Woodrow Wilson (born in Staunton, Virginia), missed in 28/100 trials. The most common extra inclusions $\hat{G}_c^{(t)} \setminus G_c$ in 100/100 trials were James Buchanan and Millard Fillmore (neither born in OhioOrVirginia).

<a id='a54cd6f2-1090-4457-9ee2-512a75d17266'></a>

Taken together, the results demonstrate that when filtering and ordering are bundled into a single instruction, GPT-4.1 fails at the _filtering_ stage entirely, leaving too few valid runs for meaningful ordering comparisons across the self-filtered and given-names tasks.

<a id='539cae0f-029a-4ea7-8179-844b38e10373'></a>

## 6.3 Conditional Sorting with Large Reasoning Models
After seeing Claude 3.7 Sonnet with Extended Thinking's infallible performance on the ordering task, we re-ran the conditional-sorting task with reasoning models, in the hope that they would also show improved accuracy on the more difficult filtering step so ordering performance analysis can be done.

<a id='ee69d4ee-7e44-40f9-86d4-9af4ed09dd6c'></a>

Specifically, we test: (i) **Claude 3.7 Sonnet with and without Extended Thinking**, and (ii) **GPT-5 at medium reasoning effort**, which in our earlier U.S. Presidents ordering experiments closely matched Claude 3.7 Sonnet with Extended Thinking performance and thus serves as a natural default comparison. Both are evaluated alongside the GPT-4.1 baseline under identical prompts and list sizes. If filtering failures are partly due to premature commitments or limited reasoning during inference time, our hypothesis is that reasoning models with ET and its equivalent may reduce such errors.

<a id='9a7ee8af-8274-416f-b17a-f02399693e31'></a>

We used the same identical prompts, shuffled input list per trial to allow paired comparisons. We vary only the ET switch and the sampling settings required by Claude (same as Table 15). All other parameters (stop sequences, tool use off, etc.) are held fixed. We ran four condition-criterion cells, each with 100 independent trials.

<a id='ff1bb3d9-70e8-4ddb-a563-06cebf695b12'></a>

6.3.1 Key findings
Table 4: Filtering accuracy across models and conditions (ET = Extended Thinking).
<table id="13-1">
<tr><td id="13-2">Model</td><td id="13-3">Condition</td><td id="13-4">Filtering Accuracy</td><td id="13-5">n_correct</td><td id="13-6">n_total</td><td id="13-7">ET?</td></tr>
<tr><td id="13-8">Claude 3.7 Sonnet</td><td id="13-9">ABC First names</td><td id="13-a">0.81</td><td id="13-b">81</td><td id="13-c">100</td><td id="13-d">No</td></tr>
<tr><td id="13-e">Claude 3.7 Sonnet</td><td id="13-f">ABC First names</td><td id="13-g">0.99</td><td id="13-h">99</td><td id="13-i">100</td><td id="13-j">Yes</td></tr>
<tr><td id="13-k">Claude 3.7 Sonnet</td><td id="13-l">OhioOrVirginia</td><td id="13-m">0.02</td><td id="13-n">2</td><td id="13-o">100</td><td id="13-p">No</td></tr>
<tr><td id="13-q">Claude 3.7 Sonnet</td><td id="13-r">OhioOrVirginia</td><td id="13-s">0.98</td><td id="13-t">98</td><td id="13-u">100</td><td id="13-v">Yes</td></tr>
<tr><td id="13-w">GPT-4.1</td><td id="13-x">ABC First names</td><td id="13-y">0.02</td><td id="13-z">2</td><td id="13-A">100</td><td id="13-B">N/A</td></tr>
<tr><td id="13-C">GPT-4.1</td><td id="13-D">OhioOrVirginia</td><td id="13-E">0.00</td><td id="13-F">0</td><td id="13-G">100</td><td id="13-H">N/A</td></tr>
<tr><td id="13-I">GPT-5 (medium)</td><td id="13-J">ABC First names</td><td id="13-K">1.00</td><td id="13-L">100</td><td id="13-M">100</td><td id="13-N">N/A</td></tr>
<tr><td id="13-O">GPT-5 (medium)</td><td id="13-P">OhioOrVirginia</td><td id="13-Q">1.00</td><td id="13-R">100</td><td id="13-S">100</td><td id="13-T">N/A</td></tr>
</table>

<a id='1946618e-c7f7-4370-8669-a6f86f20bd50'></a>

Table 5: Claude 3.7 Sonnet (Extended Thinking) ordering performance across metrics.
<table id="13-U">
<tr><td id="13-V">Task</td><td id="13-W">Metric</td><td id="13-X">Value</td><td id="13-Y">ncorrect</td><td id="13-Z">ntotal</td><td id="13-10">Condition</td></tr>
<tr><td id="13-11">ABC First names</td><td id="13-12">Avg. Spearman&#x27;s ρ</td><td id="13-13">0.999</td><td id="13-14">100</td><td id="13-15">100</td><td id="13-16">Given Names</td></tr>
<tr><td id="13-17">ABC First names</td><td id="13-18">Avg. Kendall&#x27;s τ</td><td id="13-19">0.999</td><td id="13-1a">99</td><td id="13-1b">100</td><td id="13-1c">Given Names</td></tr>
<tr><td id="13-1d">ABC First names</td><td id="13-1e">Exact Match Rate</td><td id="13-1f">0.99</td><td id="13-1g">99</td><td id="13-1h">100</td><td id="13-1i">Given Names</td></tr>
<tr><td id="13-1j">ABC First names</td><td id="13-1k">Avg. Spearman&#x27;s ρ</td><td id="13-1l">1.000</td><td id="13-1m">100</td><td id="13-1n">100</td><td id="13-1o">Self-Filtered</td></tr>
<tr><td id="13-1p">ABC First names</td><td id="13-1q">Avg. Kendall&#x27;s τ</td><td id="13-1r">1.000</td><td id="13-1s">100</td><td id="13-1t">100</td><td id="13-1u">Self-Filtered</td></tr>
<tr><td id="13-1v">ABC First names</td><td id="13-1w">Exact Match Rate</td><td id="13-1x">0.99</td><td id="13-1y">99</td><td id="13-1z">100</td><td id="13-1A">Self-Filtered</td></tr>
<tr><td id="13-1B">OhioOrVirginia</td><td id="13-1C">Avg. Spearman&#x27;s ρ</td><td id="13-1D">0.997</td><td id="13-1E">99</td><td id="13-1F">100</td><td id="13-1G">Given Names</td></tr>
<tr><td id="13-1H">OhioOrVirginia</td><td id="13-1I">Avg. Kendall&#x27;s τ</td><td id="13-1J">0.995</td><td id="13-1K">99</td><td id="13-1L">100</td><td id="13-1M">Given Names</td></tr>
<tr><td id="13-1N">OhioOrVirginia</td><td id="13-1O">Exact Match Rate</td><td id="13-1P">0.82</td><td id="13-1Q">82</td><td id="13-1R">100</td><td id="13-1S">Given Names</td></tr>
<tr><td id="13-1T">OhioOrVirginia</td><td id="13-1U">Avg. Spearman&#x27;s p</td><td id="13-1V">1.000</td><td id="13-1W">100</td><td id="13-1X">100</td><td id="13-1Y">Self-Filtered</td></tr>
<tr><td id="13-1Z">OhioOrVirginia</td><td id="13-20">Avg. Kendall&#x27;s T</td><td id="13-21">1.000</td><td id="13-22">100</td><td id="13-23">100</td><td id="13-24">Self-Filtered</td></tr>
<tr><td id="13-25">OhioOrVirginia</td><td id="13-26">Exact Match Rate</td><td id="13-27">0.97</td><td id="13-28">97</td><td id="13-29">100</td><td id="13-2a">Self-Filtered</td></tr>
</table>

<a id='e995756f-4866-41d4-9e95-013b12357b42'></a>

14

<!-- PAGE BREAK -->

<a id='4dd2c838-1287-4611-a977-b02158e7b22c'></a>

As expected, Table 4 shows that reasoning models outperform GPT-4.1 in the filtering step. Notably, GPT-5 (medium) achieved _perfect_ filtering accuracy on both criteria. On those knowledge-verified subsets, its downstream ordering was also perfect in our runs. Claude 3.7 Sonnet also edges out GPT-4.1 in the filtering step, consistent with our earlier hypothesis that LRMs might be better for conditional sorting. Without ET, FIRSTNAMESSTARTINGWITHABORC criteria (string-based) is far easier than OHIOORVIRGINIA criteria (fact-based) for the reasoning model.

<a id='8929846c-e334-405a-9824-5e0b8797b93d'></a>

More interestingly, Extended Thinking transforms Claude 3.7's filtering accuracy from inconsistent to near-perfect. On the harder factual criteria OHIOORVIRGINIA, accuracy jumps from 0.02 without ET to 0.98 with ET. Even on the easier lexical criteria (FIRSTNAMES STARTING WITHABORC), ET raises accuracy from 0.81 to 0.99. With ET, the gap largely disappears, indicating the model can execute factual checks once it is allowed to deliberate. ET also gives Claude a private scratchpad to enumerate candidates and verify membership, which sharply reduces both omissions and over-inclusion of presidential names. By contrast, GPT-4.1—without an ET mechanism—performs the worst on both criteria.

<a id='ea9a711f-1258-4bde-8924-c19115d9539c'></a>

With Claude 3.7 Sonnet and ET, enough _perfectly filtered_ trials now exist to make downstream ordering comparisons meaningful which tackles the bottlenecks we encountered with GPT-4.1 where filtering yields too few valid trials, biasing ordering metrics upward and limiting interpretability.

<a id='cf75d5b4-44d9-43f3-9058-bbba51b7f02c'></a>

<::chart: Distribution of n_predicted vs n_ground_truth without extended thinking for Ohio or Virginia. The histogram shows the count on the y-axis (from 0 to 100) and the number of presidents on the x-axis (from 10.0 to 27.5). The legend indicates 'Predicted' in blue and 'Ground Truth' in orange. The chart shows that the 'Ground Truth' (orange) count peaks at 15, while the 'Predicted' (blue) counts are more spread out from around 14 to 29. (a) Without ET. Predicted list sizes (blue) are widely spread (around 14–29), while the ground-truth size (orange) spikes at 15.::>

<a id='59f13075-fe37-4a74-9ff4-b34096c8cf5f'></a>

<::bar chart: Distribution of n_predicted vs n_ground_truth with extended thinking for Ohio or Virginia. The x-axis is labeled "Number of Presidents" with values from 10.0 to 27.5. The y-axis is labeled "Count" with values from 0 to 100. There are two bars at the x-axis value of 15.0. A very small blue bar represents "Predicted" counts, while a very tall orange bar, reaching almost 100 on the y-axis, represents "Ground Truth" counts. The caption reads: (b) With ET. Predicted list sizes collapse to the correct value (15) in almost all trials.::>

<a id='5490fb50-9bd1-45f2-a599-11f54a0f8563'></a>

Figure 6: Claude 3.7 Sonnet, self-filtering on Ohio V Virginia (100 trials). Histograms compare the number of presidents output by the model (blue) with the ground truth (orange). Extended Thinking (ET) removes the over/under-selection seen without ET.

<a id='6fcb7265-3cc6-476f-82cd-138fac49cb3d'></a>

In terms of rank correlation metrics, Extended Thinking also improved both metrics up to around 1.0 for every group (FIRSTNAMESSTARTINGWITHABORC and OHIOORVIRGINIA; given-names and self-filtered). Without ET, FIRSTNAMESSTARTINGWITHABORC self-filtered group performance drops sharply, and OHIOORVIRGINIA given-names performance is decent but not perfect. The most striking result lies in the exact match rate: Extended Thinking lifts exact match close to 1.0 across the board. Without ET, exact match is near zero for self-filtered FIRSTNAMESSTARTINGWITH-ABORC and OHIOORVIRGINIA, and only around 0.37 for FIRSTNAMESSTARTINGWITHABORC given-names. Overall, we believe the greatest analogy to explain this is that ET converts a brittle "filter-then-sort while talking" problem into a reliable "think privately, verify, then speak" work-flow---reducing filtering errors and ordering slips.

<a id='c0874e24-111e-4ca1-a8d1-9042ed97c209'></a>

**Per Position Accuracy Error Analysis** Just like before, we analyzed per-position accuracy for Claude 3.7 Sonnet among the given name trials without length mismatch (pure ordering errors) for each task type and criteria (we focus only on given name tasks with no ET since ET almost got everything perfectly right – we do not know how informative the error analysis is going to be.) Self-filtered tasks are also not the most informative to analyze here since despite improving filtering accuracy, it is still low for per-position analysis, especially when most errors are attributed to length mismatches instead of pure ordering errors.

<a id='9cdc98ae-84aa-48c6-b8b3-8e728a72e6ce'></a>

Figure 7 shows us a clear trend of the model excelling at ordering names in the beginning of the list, with degrading performance at the middle of the list and better performance again at the end of the list. This U-shape is apparent in both criteria which suggests that the model may confuse the densely packed mid-century presidencies even when the set is correct. The model performed worse mid-list for Ohio\/Virginia as compared to FIRSTNAMESSTARTINGWITHABORC names criteria;

<a id='85829317-cef6-45b6-b179-2a65bccf8c5e'></a>

15

<!-- PAGE BREAK -->

<a id='b8b820b8-6a61-4128-904b-e0acea470709'></a>

<::bar chart: Per-Position Accuracy (NoET, given_names, Ohio_or_Virginia). The x-axis is 'Position in Order' ranging from 0 to 16. The y-axis is 'Accuracy' ranging from 0.0 to 1.0. The bar chart shows the following approximate accuracy values for each position: Position 0: 1.0; Position 1: 0.55; Position 2: 0.75; Position 3: 0.66; Position 4: 0.01; Position 5: 0.03; Position 6: 0.04; Position 7: 0.66; Position 8: 0.41; Position 9: 0.45; Position 10: 0.42; Position 11: 0.8; Position 12: 0.83; Position 13: 0.83; Position 14: 0.83; Position 15: 0.83. (a) Ohio∨Virginia (NoET, given-names). Among 77 trials with no length mismatch, 2 (2.6%) were perfect. Accuracy is 1.00 at the head, dips in the mid-list, then climbs to ≥0.80 for the tail.::>

<a id='55f35b49-bea5-4b7c-9204-443e00af945a'></a>

<::bar chart: Per-Position Accuracy (NoET, given_names, ABC_names). The y-axis represents Accuracy, ranging from 0.0 to 1.0. The x-axis represents Position in Order, ranging from 1 to 8. The bar heights are approximately: Position 1: 0.9, Position 2: 0.75, Position 3: 0.75, Position 4: 0.5, Position 5: 0.5, Position 6: 1.0, Position 7: 0.9, Position 8: 0.9. (b) FIRSTNAMES STARTING WITHABORC filter (NoET, given-names). Among 99 trials with no length mismatch, **37** (37.4%) were perfect. Accuracy is generally high with a small mid-list dip.::>

<a id='20d6475c-78b0-4a05-9a2b-84a0775ce3e1'></a>

Figure 7: Per-position accuracy without extended thinking (ET) in the _given-names_ setting. Bars show, for each rank, the fraction of trials placing the correct president at that position. These panels condition on equal list lengths, so errors reflect pure ordering rather than spurious insertions/deletions.

<a id='9cd96684-026f-4d6e-af28-2bb84f99074d'></a>

mid-list placements are the most error-prone when the criterion yields a broad, era-spanning set with more names, whereas the smaller FIRSTNAMESSTARTINGWITHABORC set is uniformly easier once names are provided. Another hypothesis is that stronger anchors exist at the beginning and the end of the list. The earliest and latest figures in the subset are historically salient and far apart in time, so the model locks them in due to primacy/recency anchoring, giving 1.00 accuracy at the head and around 0.80 at the tail.

<a id='bc3fe52f-1569-4b30-abfa-c567eb998102'></a>

Conclusion We have seen that across president-ordering and conditional-sorting benchmarks, Claude 3.7 Sonnet with extended thinking (ET) and GPT-5 (medium) consistently outperforms both its own non-ET variant and GPT-4.1 on the axes that matter most for this task---strict set accuracy and exact chronological order. ET and reasoning modes in GPT-5 remove the filtering bottleneck in the conditional sorting tasks, driving filtering and ordering accuracy to near-perfect. Without ET, Claude's performance is similar to GPT-4.1 and even trails it at n=5.

<a id='c7cf71f0-cdec-44ce-8bf1-47599cbdbca0'></a>

With ET enabled, the two pipelines are essentially equivalent on ordering, and in some cases the self-filtered condition is slightly *better* on the harder criteria (Table 5). For Claude 3.7 Sonnet with Extended Thinking, we see near-identical exact-match on FIRSTNAMESSTARTINGWITHABORC (0.99 vs. 0.99) and a small but consistent edge for self-filtered condition on the OHIOORVIRGINIA criterion (0.97 vs. 0.82). More interestingly, GPT-5 with medium reasoning effort is the strongest model as it is able to achieve both perfect filtering and ordering ability, edging out Claude 3.7 Sonnet even with Extended Thinking. In short, conditional sorting mainly adds complexity via the filtering bottleneck; once that is overcome, it does slightly boost ordering performance—if anything, the self-filtered pipeline yields equal or slightly higher ordering accuracy than the given-names condition. Conditional sorting might propel the model to internally check and verify the candidate set through its private 'scratchpad' before ordering which helps boost performance, whereas given-names skips this construction.

<a id='af053705-cca8-49b6-a58b-3fa0c343f230'></a>

## 7 Anachronism detection

Building on the earlier sections—where we attempted to abstract out LLMs' innate understanding of chronology—anachronism detection raises the bar. Instead of asking the LLM to order events or historical figures chronologically, we ask it to reliably judge whether it would be chronologically possible for an event to occur. We ask, for example, whether it would have been chronologically possible for president A to meet president B, or whether A could have used a certain technological invention during their presidency. This requires the model to retrieve, compose, and comprehend overlapping timelines: the president's term (or lifespan), and the availability window of a technology, institution, or the duration of an event (in terms of first possible date and last possible date if any). Such a judgment hinges on the intersection of those intervals, instead of just a single sorted list of names. The task also contains added difficulties as merely regurgitating canonical presidential order or historical-event timelines from the LLM's training data will not be enough to excel; the model

<a id='335f073d-211c-4c43-89ac-dddfc8e487e4'></a>

16

<!-- PAGE BREAK -->

<a id='7cf0465a-6815-4224-9cb0-d97702adfb4c'></a>

must align two (or more) independent timelines and spot any contradictions or inconsistencies. Here it must integrate world knowledge to render a binary judgment-possible vs. impossible. By moving from ordinal placement to feasibility detection, anachronism detection offers a measure of whether LLMs inherently possess an understanding of chronology rather than surface-level recall. Thus, it is the natural next step after our three ordering experiments.

<a id='10b08965-7bb2-49d5-99cf-3a09e95dd9d0'></a>

## 7.1 Experimental Design

We conducted two anachronism experiments with increasing temporal complexities:

1.  **Single-boundary feasibility (first-possible date)**. Each event _e_ has a well-defined first introduction/availability year _t_min (_e_). A pair of (president, event) (_p_, _e_) is labeled "possible" if and only if the president's term window [_a_(_p_), _b_(_p_)] intersects [_t_min (_e_), ∞).

2.  **Multi-timeline overlap**. Queries such as "were [n specific presidents] _p_1, _p_2, ..., _p_n all alive at the same time?" require checking ∩k[_l_(_p_k_), _d_(_p_k_)] ≠ Ø, where _l_(·), _d_(·) are birth/death years. This directly tests LLMs' reasoning over multiple, partially overlapping intervals.

<a id='3af0d65c-4664-4261-9571-5b73ff9a1885'></a>

Anachronisms can be understood as non-overlapping intervals in the Allen relations studied in Islakoglu et al. (2025), but there are important differences between our tests and those in Islakoglu et al. (2025). We do not probe individual Allen relations but rather require the LLM to determine whether any of the relations that would make an event possible actually hold. Also, our multi-timeline tests require an LLM to check for the intersection of multiple intervals, rather than just two.

<a id='d2b36d2b-5815-4bc6-a771-7eaeb43889a1'></a>

**Establishing groundtruth and removing grey zones** For our first variant, we first established the groundtruth president-event pairs dataset by curating 12–15 activities with that only became possible after some date (e.g., "fly in an airplane," "make a telephone call," "use generative AI tools").

<a id='db093c7f-7acb-482f-a825-b42d1240cfed'></a>

For each event/activity e (e.g., "Flew in an airplane while president"), we record a first-possible year tmin (e) as the year of the technology's invention or first public release date. Some events may also have a last-possible year tmax(e); but for events like invention with no last-possible year, we take tmax(e) = +∞.

<a id='5da2a709-a7eb-4f86-9b0b-6cceb1b29eae'></a>

Many technologies exhibit a lag between invention and reliable in-office or commercialized use. To avoid ambiguous labels, each event e may include a grey zone interval G(e) = [gmin (e), gmax(e)] where gmin (e) is the invention date and gmax(e) is the first recorded use in the White House (i.e., by a sitting president). Any president p whose term window [a(p), b(p)] overlaps G(e) is excluded for that event at data-construction time (i.e., the pair (p, e) is dropped rather than labeled true/false). This prevents false positives/negatives that arise from the time discrepancy. This also helps us remove boundary ambiguity about sporadic or ceremonial access surrounding the installation.

<a id='335d142e-15bd-4a35-8ee6-9265d7525304'></a>

After excluding grey-zone and special-case pairs, we assign the groundtruth feasibility label
y(p,e) = \mathbb{I}([a(p),b(p)]\cap[t_{min}(e),t_{max}(e)]\ne\emptyset),

<a id='b4a422e4-f19e-4b6f-a7fe-a40172b44c18'></a>

i.e., event _e_ is _possible_ for the president _p_ iff the presidential term intersects its interval.

<a id='0b6ca34e-7565-4324-b2e2-050d1113e9e4'></a>

For our second variant where we tested the notion of overlapping timelines, each item is an *unordered*
group $S = \{p_1, \dots, p_i\}$ of presidents of size $i$ (we tested $N \in \{2, 3, 4\}$). Let $l(p)$ and $d(p)$ denote
the birth and death years of president $p$. Define the group overlap interval to be

<a id='2a8a6615-4796-4072-980e-2be9960d67cf'></a>

$$I(S) = \bigcap_{p \in S} [\ell(p), d(p)] = [\max_{p \in S} \ell(p), \min_{p \in S} d(p)].$$

<a id='7f9ebf14-1095-4b5f-a0cd-58b74a9d8d9f'></a>

We set the ground-truth label after obtaining birth and death data for each president as
y(S) = 1[I(S) ≠ Ø] = 1 \left[ \max_{p\in S} l(p) \leq \min_{p\in S} d(p) \right].

<a id='23ada30c-0b38-4286-9fd6-486e529d6eba'></a>

Intuitively, y(S) = 1 iff the presidents' lifetimes overlap in at least one calendar year, which we obtain by asking "*Were presidents in S = {p1,..., pi} all alive at the same time*".


<a id='3374f423-1e50-418a-b907-e8f08a5890c0'></a>

Each trial draws a configurable number (N) of pairs (p, e) sampled with replacement from groundtruth dataset under three batch types (uniformly at random, 100 trails per type):

<a id='5a26616d-2091-47e9-8cf1-38538b4843d5'></a>

17

<!-- PAGE BREAK -->

<a id='0abcc259-f8f1-444c-8f4d-1e1324b793f7'></a>

1. N/2 possible + N/2 impossible,
2. all N possible,
3. all N impossible.

<a id='338e0bc0-7648-4cc8-9e36-330869e2e35f'></a>

This helps reduce class imbalance and lets us probe evaluation metrics on different categorical mixes.

<a id='15d34fa7-c9cf-46a0-86d7-508baac6d7aa'></a>

As before, we utilized GPT-4.1 at T=0.0 for determinism. The model receives exactly N statements and must output Possible or Not possible for each, without explanation or additional text in order for us to make use of regex to capture its boolean responses.

<a id='af83302e-d569-4224-be02-0c79d4f9257f'></a>

## 7.2 Evaluation metrics, deduplication, and aggregation
We process the results by eliminating duplicate events to ensure each unique president-event pair is only counted once. This is necessary because our sampling method allows the same president-event pair to appear in multiple batches, and counting duplicates would artificially inflate our overall performance metrics. After deduplication, we compute all evaluation metrics on this clean dataset.

<a id='6abc44c2-ba78-45be-b90a-0773f1a1da2d'></a>

Each instance is labeled $y \in \{0,1\}$ as Possible = 0 or Not possible = 1. Predictions $\hat{y}$ are obtained from the model's string outputs after regex pattern capture. Precision/recall treat 1 (Not possible) as the positive class.

<a id='b3c4d148-2a93-48f8-b1e2-abe8095a398c'></a>

Let n be the number of unique (deduplicated) instances, TP = Σ 1[y=1, ŷ=1], FP = Σ 1[y=0, ŷ=1], TN = Σ 1[y=0, ŷ=0], and FN = Σ 1[y=1, ŷ=0]. We report:

<a id='fd8b9879-a3c7-4eb8-922d-6d189466b1cc'></a>

Accuracy = (TP + TN) / n

Precision = TP / (TP + FP)

Recall = TP / (TP + FN)

F1 = (2 Precision · Recall) / (Precision + Recall)

<a id='ae6fe92d-e09b-49c1-858d-c73b5e6eb847'></a>

We also include the 2x2 confusion matrix with rows = ground truth and columns = prediction
((FN FP)). When a denominator is zero (e.g., no predicted positives), the corresponding metric is set
to 0 (zero_division=0 in implementation).

<a id='9425f6a7-49cb-4a0c-bf8f-86b0223b4fda'></a>

In addition to the overall metrics, we group the deduplicated table by batch_type (_half-half_, _all-true_, _all-false_) and compute the same set of statistics per batch type. For each batch type we also record (TP, FP, TN, FN) and number of president-event pairs _N_, aggregating over 100 trails per type.

<a id='9481b894-2e77-47bd-8263-2bc74988ecdb'></a>

7.3 Key findings
Table 6: Variant 1 GPT-4.1 Anachronism Detection Results by Batch Size and Type
<table id="17-1">
<tr><td id="17-2" rowspan="2">Batch Size</td><td id="17-3">Experiment Type</td><td id="17-4" colspan="4">Metrics (After Deduplication)</td></tr>
<tr><td id="17-5"></td><td id="17-6">Accuracy</td><td id="17-7">Precision</td><td id="17-8">Recall</td><td id="17-9">F1 Score</td></tr>
<tr><td id="17-a" rowspan="3">N = 6</td><td id="17-b">3_true_3_false</td><td id="17-c">0.998</td><td id="17-d">1.000</td><td id="17-e">1.000</td><td id="17-f">1.000</td></tr>
<tr><td id="17-g">6_false</td><td id="17-h">0.995</td><td id="17-i">N/A</td><td id="17-j">N/A</td><td id="17-k">N/A</td></tr>
<tr><td id="17-l">6_true</td><td id="17-m">0.995</td><td id="17-n">1.000</td><td id="17-o">0.974</td><td id="17-p">0.987</td></tr>
<tr><td id="17-q" rowspan="3">N = 10</td><td id="17-r">10_false</td><td id="17-s">0.950</td><td id="17-t">N/A</td><td id="17-u">N/A</td><td id="17-v">N/A</td></tr>
<tr><td id="17-w">10_true</td><td id="17-x">0.908</td><td id="17-y">1.000</td><td id="17-z">0.890</td><td id="17-A">0.942</td></tr>
<tr><td id="17-B">5_true_5_false</td><td id="17-C">0.932</td><td id="17-D">0.933</td><td id="17-E">0.933</td><td id="17-F">0.933</td></tr>
<tr><td id="17-G" rowspan="3">N = 20</td><td id="17-H">10_true_10_false</td><td id="17-I">0.992</td><td id="17-J">1.000</td><td id="17-K">0.989</td><td id="17-L">0.994</td></tr>
<tr><td id="17-M">20_false</td><td id="17-N">0.982</td><td id="17-O">N/A</td><td id="17-P">N/A</td><td id="17-Q">N/A</td></tr>
<tr><td id="17-R">20_true</td><td id="17-S">0.982</td><td id="17-T">1.000</td><td id="17-U">0.983</td><td id="17-V">0.991</td></tr>
</table>
Note. "N/A" indicates cases where precision, recall, and F1 are undefined because there are no positive
predictions (all-false batches). These batches can still achieve high accuracy (e.g., > 0.95) but cannot
be evaluated on precision/recall metrics.

<a id='97c2923b-ad7f-4bb1-8002-604b909d3836'></a>

Variant 1: Single-boundary feasibility In single-boundary tests, the anachronism detection results
achieved excellent performance across batch sizes, with overall accuracy ranging from 95.0% to
99.8%. The results show that the model can reliably distinguish between temporally possible and
impossible events with a well-defined first possible dates involving U.S. presidents. We also observe

<a id='e961e5ce-0b8e-4ae9-a5ac-43bfbd80eae2'></a>

18

<!-- PAGE BREAK -->

<a id='f8a48e46-7144-4c1b-a39d-c905d481591d'></a>

that mixed batches consistently outperform both entirely true and false batches; balanced true/false statements distributions in one prompt are found to be the most reliable evaluation of the model capability. In terms of scalability with increasing batch size (i.e. president-event pair), performance remains consistently high even as batch size increases from 6 to 20. The results demonstrate that the model has effectively learned to detect anachronisms well, especially for events that have no end date and are currently chronologically possible.

<a id='6e427fd4-71ca-410b-a74c-4739adb55ccf'></a>

Despite the high overall performance, several systematic errors reveal interesting patterns in the model's temporal reasoning. Table 7 displays both notable false positives (model incorrectly flagged possible events as anachronisms) and false negatives cases (model missed actual anachronisms). For the former, the LLM appears *overly conservative* about early technology adoption, incorrectly flagging events that were chronologically possible, in most cases might be due to temporal proximity. Grant used telephones in San Francisco (1877), but the White House didn't have phones until 1879. This reflects confusion between general telephone technology (available 1877) and White House installation (1879). The photography errors for Taylor and Fillmore suggest the model underestimates how early photography became practical (it was available by the 1840s). Railroads were already operational during Harrison's brief presidency, thus making it possible for him to travel by railroad then.

<a id='8a58d0cb-59e4-48e0-8233-ebba915ac221'></a>

For false negatives, the model shows leniency toward temporal boundaries, particularly for recent technologies. John Quincy Adams' presidency (1825-1829) predates practical photography (1839), but the model may be too permissive with "close calls." The generative AI errors for Obama and Bush reveal the model's difficulty with cutting-edge technology timelines, as both served well before generative AI became mainstream (around 2020).

<a id='78e6a54e-8411-4485-a90a-f5d12d18164b'></a>

Table 7: Variant 1 Error Analysis: False Positives vs. False Negatives

<a id='a7d0f4cd-f962-4ec7-9101-8c9d24e9b641'></a>

<table><thead><tr><th>Statement</th></tr></thead><tbody><tr><td>Ulysses S. Grant Used the White House telephone</td></tr><tr><td>Zachary Taylor Appeared in a photograph<br>while president</td></tr><tr><td>William Henry Harrison Travelled by railroad<br>while president</td></tr><tr><td>Millard Fillmore Appeared in a photograph<br>while president</td></tr></tbody></table>

<a id='d8296bc4-7cab-4057-ab3e-4e923407ede6'></a>

(b) False Negatives

<table><thead><tr><th>Statement</th></tr></thead><tbody><tr><td>John Quincy Adams Appeared in a photo-<br>graph while president</td></tr><tr><td>Barack Obama Used generative AI while<br>president</td></tr><tr><td>George W. Bush Used generative AI while<br>president</td></tr></tbody></table>

<a id='247011e4-68f7-42c2-bda7-783a438830b9'></a>

We also tried prompting the LLM to answer whether a historical figure could have exchanged a letter with a given president. This setup introduced substantial ambiguity: prompts conflated _chronological possibility_ (lifespans overlap) with _historical plausibility_ (would such an exchange actually occur). For example, in the case of Rutherford B. Hayes and Dwight D. Eisenhower, GPT-4.1 first answered, "No—they couldn't have met," then immediately noted a narrow overlap window (1890–1893) and concluded that it was "chronologically possible (their lives overlapped briefly) but logically implausible that they met in person."

<a id='ee0b9942-442f-4d3e-a57c-30603ef0f1e4'></a>

To remove this confound, we move to Variant 2, which asks simply whether all presidents in a group
were alive at the same time, removing the complexity of evaluating event feasibility and focusing
purely on chronological viability.

<a id='6de9ffa3-f621-465e-b011-75850dfd6a36'></a>

**Variant 2: Multi-timeline overlap** The presidents overlap experiment asks a more direct question and evaluates the model's ability to determine whether multiple U.S. presidents could have been alive simultaneously. We present the model with groups of n = 2, 3, or 4 presidents and ask: "Were [n presidents] all alive at the same time? Answer Yes or No for each group." For example, given the group "George Washington, John Adams, Thomas Jefferson," the model must determine if there was any point in time when all three presidents were living at the same time. We test 2 batch sizes for each n (N = 6, 10 as before) to see the scaling pattern across different combinatorial complexities. The experiment tests three levels of complexity: 43C2 (903 unique pairs), 43C3 (12,341 unique triplets),

<a id='2cf2e945-ca8d-487b-b395-ab173d9267a9'></a>

19

<!-- PAGE BREAK -->

<a id='ed391e5a-599e-4802-b4b1-37225c512e21'></a>

and 43C4 (123,410 unique quadruplets), where 43 represents the total number of U.S. presidents and C2-C4 indicates choosing 2, 3, or 4 presidents from this pool.

<a id='57f2b9e0-8298-4399-beb4-ca3ee5b6d375'></a>

Table 8: Variant 2 GPT-4.1 Anachronism Detection Results by Combinatorial Complexity and Batch
Type
<table id="19-1">
<tr><td id="19-2" rowspan="2">Combinatorial Complexity</td><td id="19-3" rowspan="2">Experiment Type</td><td id="19-4" colspan="4">Metrics (After Deduplication)</td></tr>
<tr><td id="19-5">Accuracy</td><td id="19-6">Precision</td><td id="19-7">Recall</td><td id="19-8">F1 Score</td></tr>
<tr><td id="19-9" rowspan="6">2 presidents (43C2)</td><td id="19-a">3_true_3_false</td><td id="19-b">0.950</td><td id="19-c">0.964</td><td id="19-d">0.939</td><td id="19-e">0.951</td></tr>
<tr><td id="19-f">6_false</td><td id="19-g">0.944</td><td id="19-h">N/A</td><td id="19-i">N/A</td><td id="19-j">N/A</td></tr>
<tr><td id="19-k">6_true</td><td id="19-l">0.866</td><td id="19-m">1.000</td><td id="19-n">0.866</td><td id="19-o">0.928</td></tr>
<tr><td id="19-p">10_false</td><td id="19-q">0.950</td><td id="19-r">N/A</td><td id="19-s">N/A</td><td id="19-t">N/A</td></tr>
<tr><td id="19-u">10_true</td><td id="19-v">0.908</td><td id="19-w">1.000</td><td id="19-x">0.890</td><td id="19-y">0.942</td></tr>
<tr><td id="19-z">5_true_5_false</td><td id="19-A">0.932</td><td id="19-B">0.933</td><td id="19-C">0.933</td><td id="19-D">0.933</td></tr>
<tr><td id="19-E" rowspan="6">3 presidents (43C3)</td><td id="19-F">3_true_3_false</td><td id="19-G">0.910</td><td id="19-H">0.943</td><td id="19-I">0.870</td><td id="19-J">0.905</td></tr>
<tr><td id="19-K">6_false</td><td id="19-L">0.929</td><td id="19-M">N/A</td><td id="19-N">N/A</td><td id="19-O">N/A</td></tr>
<tr><td id="19-P">6_true</td><td id="19-Q">0.797</td><td id="19-R">1.000</td><td id="19-S">0.797</td><td id="19-T">0.887</td></tr>
<tr><td id="19-U">10 false</td><td id="19-V">0.912</td><td id="19-W">N/A</td><td id="19-X">N/A</td><td id="19-Y">N/A</td></tr>
<tr><td id="19-Z">10_true</td><td id="19-10">0.766</td><td id="19-11">1.000</td><td id="19-12">0.750</td><td id="19-13">0.857</td></tr>
<tr><td id="19-14">5_true_5_false</td><td id="19-15">0.914</td><td id="19-16">0.936</td><td id="19-17">0.879</td><td id="19-18">0.906</td></tr>
<tr><td id="19-19" rowspan="6">4 presidents (43C4)</td><td id="19-1a">3_true_3_false</td><td id="19-1b">0.854</td><td id="19-1c">0.952</td><td id="19-1d">0.743</td><td id="19-1e">0.835</td></tr>
<tr><td id="19-1f">6_false</td><td id="19-1g">0.955</td><td id="19-1h">N/A</td><td id="19-1i">N/A</td><td id="19-1j">N/A</td></tr>
<tr><td id="19-1k">6_true</td><td id="19-1l">0.624</td><td id="19-1m">1.000</td><td id="19-1n">0.624</td><td id="19-1o">0.768</td></tr>
<tr><td id="19-1p">10_false</td><td id="19-1q">0.930</td><td id="19-1r">N/A</td><td id="19-1s">N/A</td><td id="19-1t">N/A</td></tr>
<tr><td id="19-1u">10_true</td><td id="19-1v">0.656</td><td id="19-1w">1.000</td><td id="19-1x">0.656</td><td id="19-1y">0.793</td></tr>
<tr><td id="19-1z">5_true_5_false</td><td id="19-1A">0.881</td><td id="19-1B">0.954</td><td id="19-1C">0.798</td><td id="19-1D">0.869</td></tr>
</table>
Note. N/A indicates cases where precision, recall, and F1 are undefined due to no positive predictions
(all-false batches). 43C2, 43C3, and 43C4 denote choosing 2, 3, or 4 presidents from a pool of 43.

<a id='d4fcecd6-0e85-41b2-af42-d55544680d77'></a>

Results in Table 8 show a performance decline as the number of presidents increases, despite the overall accuracy still being fairly high across all set sizes. This supports our hypothesis that the degradation reveals the model's limitations in handling increasingly complex multi-timeline temporal reasoning tasks. The exponential increase in possible overlapping combinations of presidents might start to overwhelm the model's chronological ordering ability. The LLM appears to handle pairwise overlapping lifetime relationships well but begins to falter when required to simultaneously evaluate three or four overlapping lifespans, suggesting fundamental limitations in reasoning about overlapping lifespan of multiple presidents rather than simple knowledge gaps. The model also performs worse on all-true batches compared to mixed batches across all combinatorial complexities, implying that the model generally can spot temporal impossibility well but becomes less reliable when confirming larger groups where all presidents should theoretically overlap. We do not observe significant bottlenecks or performance degradation when increasing the number of statements or batch size in a prompt from 6 to 10. This suggests that the model's chronological reasoning capabilities are not significantly impacted by processing more statements simultaneously, but by encountering increasing combinatorial complexity (number of presidents being evaluated) within each statement.

<a id='36443f93-50ea-4d05-9fdf-72acffb740a5'></a>

20

<!-- PAGE BREAK -->

<a id='8ff32c28-b3bc-4dbc-a2c3-51ee2e3835d1'></a>

## 8 Conclusion

Our experiments show that there are fundamental limitations in LLM's understandings of chronology. A core theme emerging from our study is that today's LLMs have a rudimentary sense of time, and that this understanding is quickly overwhelmed as problem complexity grows. Compared with other problem domains, complexity becomes a challenge for chronological tasks even at relatively small scales. The consequences are practical: without deliberate temporal reasoning and verification, LLMs cannot realize their potential as tools to aid forecasting, as they remain vulnerable to look-ahead bias. Our findings indicate that there is no prompt-only shortcut to eliminating look-ahead bias: if a model cannot reliably handle basic chronological ordering, leakage will persist. But our results also suggest a more promising path through *reasoning modes* that explicitly allocate computation to think internally and verify timelines.

<a id='b5ebe98e-1524-4da9-a163-63b4698e2621'></a>

Despite its scope, our study has clear limitations and points to several follow-ups. First, GPT-5's routing to reasoning is itself a challenge. For users who heavily use the chat interface which may or may not auto-route to "thinking" modes, it is unclear whether seemingly simple ordering tasks will be recognized as requiring extra deliberation by ChatGPT 5. Problems will occur if GPT-5 rarely routes to a reasoning mode on its own. Future work should test this explicitly and build an adaptive pipeline: run a cheap pass, apply a simple chronology gate ("as-of-t" checks, disagreement/uncertainty thresholds), and escalate only when needed (e.g., higher reasoning effort in GPT-5, Extended Thinking in Claude Sonnet).

<a id='b32384fe-3c3e-4a0e-bcd2-1312c97c2faa'></a>

Second, our model coverage is still incomplete, despite our attempt at selecting each model which represents a broad coverage of all types available in the market: non-reasoning (GPT-4.1), reasoning (Claude 3.7 Sonnet), reasoning with internal CoT (Claude 3.7 Sonnet with Extended thinking), or even all-in-one model like GPT-5. We tested a subset of GPT-5 settings and did not systematically evaluate open-source reasoning models. A broader sweep across families (e.g., Llama Mixtral-class, and other open "reasoning" variants) should report cost-accuracy trade-offs and when escalation actually enhances performance. Third, although our tasks are designed to be domain-agnostic, much of the evaluation relies on historical events; adding domain-specific timelines for specific applications or fields (finance, science, multilingual), as well as cross-domain checks will test generalizability. Finally, beyond single-pass outputs, consensus methods such as LLM-as-judge, self-consistency/majority vote may be helpful in boosting performance. Practical controls should therefore pair time-sliced retrieval prompting or data "fences" with explicit deliberate reasoning. Concretely, researchers should: (i) utilize reasoning modes for chronology-sensitive tasks (e.g., GPT-5 with higher reasoning effort, Claude Sonnet with extended thinking), and require the model to enumerate and check timelines before answering; (ii) add a chronology gate that asks the model to justify that each fact was knowable as of a specific date t, and to instruct models to abstain from making any statements or forecast when uncertainty is high and (iii) evaluate its decisions with other reasoning models (LLMs as a judge) or self-consistency/majority vote over multiple runs and models, including auditing with our chronology experiments.

<a id='929f8e63-c971-4ee9-a5a7-0949a2d54d98'></a>

More broadly during pretraining, building chronologically consistent models will likely require objectives and infrastructure that encode time such as a temporally indexed corpus of data for training LLMs, timeline-aware constraints/regularizers, uncertainty flags/responses for "not knowable as of time t" and robust post-training finetuning tests on feasibility, ordering, and overlap, so that instructions like "answer as if it were 2016" are not just arbitrarily prompted but mechanistically respected by the LLM.

<a id='b59b2ab6-4290-4d96-90d1-d2ed4b97afeb'></a>

21

<!-- PAGE BREAK -->

<a id='6863a67b-43bc-4a23-bd88-5f26c126276b'></a>

A Technical Appendices and Supplementary Material

<a id='c97a559a-7236-4bc3-b1a1-62a22f5de4b0'></a>

## A.1 Evaluation metrics definitions

Our baseline experiments evaluate chronological ordering ability on shuffled lists of historical events whose correct order is fully known. Before presenting the aggregate numbers, we define the evaluation metrics used throughout this work: Spearman's \rho, Kendall's \tau, Cayley Distance, and Exact match rate (EMR).

<a id='437b2228-efcb-4376-b088-a81a5db201d0'></a>

Spearman's rank correlation coefficient ($\rho$). Let $\sigma = (\sigma_1,...,\sigma_n)$ be the ground-truth order and
$\hat{\sigma} = (\hat{\sigma}_1,...,\hat{\sigma}_n)$ the predicted order. Denote by $r_i$ and $\hat{r}_i$ the ranks of item $i$ in $\sigma$ and $\hat{\sigma}$, respectively.

<a id='c4db0eea-5e99-4aab-985b-3993e43b8331'></a>

ρ = 1 - \frac{6 \sum_{i=1}^{n} (r_i - \hat{r}_i)^2}{n (n^2 - 1)}.

<a id='4cac0a73-c696-4dc3-9329-ea2ceaea144a'></a>

We chose Spearman's rank correlation because it gives us a global view of ordering quality. Whereas Pearson correlation measures linear relationships, Spearman's measure depends on ranks but not numerical values. It is sensitive to large positional errors, and one misplaced item far from its true slot is heavily penalized. It is a well-established statistical measure and is fast to compute.

<a id='c0ddb9ad-23db-4388-9789-e5b5d173b226'></a>

**Kendall's** $\tau$. Let $C$ and $D$ be the numbers of *concordant* (correctly ordered) and *discordant* (incorrectly ordered) pairs between $\sigma$ and $\hat{\sigma}$. Then

<a id='33adcb6b-9423-4e3a-a9b1-b36f2aef3660'></a>

<::τ = (C - D) / ((n) over (2)): figure::>

<a id='3c43f1de-dede-464b-b50e-28d7bc7052ba'></a>

Kendall's \(\tau\) is robust for small number of events; it is less sensitive to single catastrophic errors, as every inversion counts only once.

<a id='a05cca25-1df2-43b3-a3de-d51330ec28d1'></a>

**Cayley distance** ($d_{\text{Cayley}}$). View $\hat{\sigma}$ as a permutation $\pi \in S_n$ that maps the true index of each item to its predicted index. If $c(\pi)$ is the number of disjoint cycles in the cycle decomposition of $\pi$, then

<a id='09097f85-2299-4e57-a010-d38e674fca5f'></a>

$$d_{\text{Cayley}}(\sigma, \hat{\sigma}) = n - c(\pi).$$

<a id='51194247-c674-4e4d-b23e-21ad783ab746'></a>

Cayley distance provides a direct physical interpretation of "how many pairwise swaps are wrong?"
It grows linearly with the number of events (n) in expectation, giving a scale-sensitive error that the
above two metrics (both bounded in [-1,1]) cannot provide. It is also strictly zero only when the
entire sequence is correct. It is a good complement to the exact-match rate.

<a id='c2b2eb1b-c537-48ca-9b70-1d5ada41fcfd'></a>

**Normalized Cayley distance.** To make distance scores comparable across lists of different lengths, the Cayley distance d_Cayley—the minimum number of adjacent transpositions required to transform the model's permutation into the ground truth—we scale it to get

<a id='8f04f1d5-ae43-4128-8c96-ed4d01974852'></a>

Cayley_norm = d_Cayley / (n - 1),
0 ≤ Cayley_norm ≤ 1.

<a id='23014eb1-65ac-4437-aaa7-7582ed21fba4'></a>

This normalization enables meaningful cross-n comparisons while preserving the interpretability of dCayley.

<a id='71de3945-4f08-4bb7-855e-53bf6f438445'></a>

**Exact-match rate (EMR)**. Given T independent trials, let \sigma^{(t)},\hat{\sigma}^{(t)}\in S_{n_{t}} denote the ground-truth
and predicted permutations in trial t. Define the indicator \mathbb{I}[\hat{\sigma}^{(t)}=\sigma^{(t)}]=\mathbb{I}[d_{Cayley}(\sigma^{(t)},\hat{\sigma}^{(t)})=0].
The exact-match rate is the empirical mean of this indicator:

<a id='dc4fcf72-6be4-4846-aa40-4c758ed2df8d'></a>

EMR = \frac{1}{T} \sum_{t=1}^{T} \mathbb{I}[\hat{\sigma}^{(t)} = \sigma^{(t)}].

<a id='cae6deb5-6e25-49a3-bac7-fbba97c92b77'></a>

It equals the proportion of trials in which the predicted list is _identical_ to the ground truth and,
equivalently, the Cayley distance is zero.

<a id='7eafbc13-2341-406c-a180-6b08f7727670'></a>

22

<!-- PAGE BREAK -->

<a id='c362f902-aa03-4f30-8919-ac4399f1c6d8'></a>

A.1.1 Position-wise error metric

Mean Absolute Rank Difference (MARD) Let trials be indexed by t with list length n_t. For an item e in trial t, denote its true and predicted ranks by r_true^(t)(e), r^(t)(e) ∈ {1, ..., n_t}. Fix a list length n and a ground-truth position k ∈ {1, ..., n}, and define

I_{n,k} = {(t, e) : n_t = n, r_true^(t)(e) = k}.

<a id='e06aae43-dce7-430b-81fb-57f78241085c'></a>

The mean absolute rank difference (MARD) at position k is

<a id='ceef807d-1fe9-45c2-a536-a208df7d6cd7'></a>

MARD_n(k) = \frac{1}{|\mathcal{I}_{n,k}|} \sum_{(t,e) \in \mathcal{I}_{n,k}} |\hat{r}^{(t)}(e) - k|.

<a id='bf78ab99-9fa6-49a7-b70b-2ff278993258'></a>

This measure takes values between 0 and n - 1, with lower values indicating smaller errors.

<a id='a0e9561b-7618-4a73-8c02-37460807e32f'></a>

**Per-position accuracy** Let criteria $P$ specify the target set $G_P = \{i_1,...,i_n\}$ (e.g., ABC-FIRST-NAMES, OHIOORVIRGINIA), with ground-truth order $\pi^{\text{true}}=(i_1,...,i_n)$. For trial $t$, let the model's prediction be $\pi^{(t)}=(i_1^{(t)},...,i_{m_t}^{(t)})$. We condition on the subset of trials with no length mismatch:

<a id='f72e3633-94bb-49e9-9a40-e868eda2b349'></a>

$\mathcal{T}_{n,P} = \{t: m_t = n\}$. 

<a id='c3f35851-9c9c-47c1-8c52-ffe6e7d7eeee'></a>

For rank r \u2208 {1,...,n}, define the exact-occupant accuracy

<a id='753799e0-913a-4249-929a-bace349832a6'></a>

$$A_{n,P}(r) = \frac{1}{|\mathcal{T}_{n,P}|} \sum_{t \in \mathcal{T}_{n,P}} \mathbf{1} \left[ i_r^{(t)} = i_r \right].$$

<a id='3474e7d1-fb6c-4f79-a527-191289d615e1'></a>

This measure takes values between 0 and 1, with higher values indicating greater accuracy.

<a id='6eff5b62-023d-48d7-8b09-7917babcd827'></a>

23

<!-- PAGE BREAK -->

<a id='ead93f33-c202-481a-82d7-bcdd3b961e69'></a>

## A.2 Datasets
This appendix describes the datasets used in our ordering experiments.

<a id='cfe9d4e5-3103-409f-b086-9b73e37b0c89'></a>

### A.2.1 20th Century Historical Events

We use a dataset of 20th century historical events extracted from the Wikipedia Timeline of the 20th Century. The events were scraped and processed into a structured CSV format, which can be accessed here. This dataset contains major historical events spanning from 1901 to 2000, covering political, social, technological, and cultural milestones that shaped the 20th century.

<a id='76b337b2-736f-4700-b211-2b27ce2387db'></a>

A.2.2 Wide Time-Scale Historical Events
Table 9 presents our wide time-scale historical events dataset used in the wide-gap variant for basic sorting on events experiment.

<a id='dbd7dbba-a0e9-4a99-8463-3373ed791e58'></a>

Table 9: Wide Time-Scale Historical Events Dataset

<a id='02b349b8-1d7f-463b-a4d7-f273274713c9'></a>

<table><thead><tr><th>Year</th><th>Event</th></tr></thead><tbody><tr><td>43</td><td>Southern Britain annexed by Rome</td></tr><tr><td>161</td><td>Death of Antoninus Pius</td></tr><tr><td>380</td><td>Christianity becomes official religion of Roman Empire</td></tr><tr><td>476</td><td>Fall of Western Roman Empire</td></tr><tr><td>581</td><td>China is unified by the Sui Dynasty</td></tr><tr><td>697</td><td>Venice becomes independent from Eastern Roman Empire</td></tr><tr><td>762</td><td>Baghdad founded; becomes center of learning during Islamic Golden Age</td></tr><tr><td>808</td><td>Gunpowder discovered in China</td></tr><tr><td>927</td><td>Kingdom of England established by Æthelstan</td></tr><tr><td>1096</td><td>Oxford University begins functioning</td></tr><tr><td>1271</td><td>Yuan Dynasty established in China by Kublai Khan</td></tr><tr><td>1452</td><td>Birth of Leonardo da Vinci; Renaissance begins</td></tr><tr><td>1511</td><td>Spanish Conquest of America begins</td></tr><tr><td>1643</td><td>Birth of Isaac Newton; start of the Enlightenment</td></tr><tr><td>1707</td><td>Union of England and Scotland</td></tr><tr><td>1803</td><td>Napoleon sells Louisiana Territory to USA</td></tr><tr><td>1826</td><td>Photography invented by Joseph Nicéphore (France)</td></tr><tr><td>1905</td><td>Einstein creates E=mc²; basis for atomic energy</td></tr><tr><td>1919</td><td>Treaty of Versailles</td></tr><tr><td>1949</td><td>Creation of NATO</td></tr><tr><td>1971</td><td>Pentagon Papers leaked; leads to US withdrawal from Vietnam</td></tr><tr><td>1993</td><td>European Union founded</td></tr><tr><td>2000</td><td>Israeli troops withdraw from southern Lebanon</td></tr><tr><td>2001</td><td>9/11 attacks in the US</td></tr><tr><td>2005</td><td>YouTube is founded</td></tr><tr><td>2012</td><td>Curiosity rover takes selfie and finds ancient water streambed on Mars</td></tr><tr><td>2016</td><td>UK votes to leave the EU; beginning of Brexit</td></tr><tr><td>2020</td><td>Global COVID-19 pandemic begins</td></tr><tr><td>2022</td><td>Human population reaches 8 billion</td></tr><tr><td>2025</td><td>Pope Francis dies at 88</td></tr></tbody></table>

<a id='66a4141f-cce7-4553-a658-83150beaf25d'></a>

A.2.3 U.S. Presidents List
Table 10 shows all 43 US presidents used in the experiment.

<a id='801d3223-d476-4d82-8e4e-99cfbd16284d'></a>

24

<!-- PAGE BREAK -->

<a id='0ed36485-0f37-4426-972d-5555ffb559fe'></a>

Table 10: Presidential Birth Information and Terms

<a id='47d9648b-ff4e-49f2-8239-b480b0e93f72'></a>

<table id="24-1">
<tr><td id="24-2">Name</td><td id="24-3">Birthdate</td><td id="24-4">Birth Day of Week</td><td id="24-5">Birth State</td><td id="24-6">Start</td><td id="24-7">End</td></tr>
<tr><td id="24-8">George Washington</td><td id="24-9">1732-02-22</td><td id="24-a">Friday</td><td id="24-b">Virginia</td><td id="24-c">1789</td><td id="24-d">1797</td></tr>
<tr><td id="24-e">John Adams</td><td id="24-f">1735-10-30</td><td id="24-g">Sunday</td><td id="24-h">Massachusetts</td><td id="24-i">1797</td><td id="24-j">1801</td></tr>
<tr><td id="24-k">Thomas Jefferson</td><td id="24-l">1743-04-13</td><td id="24-m">Saturday</td><td id="24-n">Virginia</td><td id="24-o">1801</td><td id="24-p">1809</td></tr>
<tr><td id="24-q">James Madison</td><td id="24-r">1751-03-16</td><td id="24-s">Tuesday</td><td id="24-t">Virginia</td><td id="24-u">1809</td><td id="24-v">1817</td></tr>
<tr><td id="24-w">James Monroe</td><td id="24-x">1758-04-28</td><td id="24-y">Friday</td><td id="24-z">Virginia</td><td id="24-A">1817</td><td id="24-B">1825</td></tr>
<tr><td id="24-C">John Quincy Adams</td><td id="24-D">1767-07-11</td><td id="24-E">Saturday</td><td id="24-F">Massachusetts</td><td id="24-G">1825</td><td id="24-H">1829</td></tr>
<tr><td id="24-I">Andrew Jackson</td><td id="24-J">1767-03-15</td><td id="24-K">Sunday</td><td id="24-L">South Carolina</td><td id="24-M">1829</td><td id="24-N">1837</td></tr>
<tr><td id="24-O">Martin Van Buren</td><td id="24-P">1782-12-05</td><td id="24-Q">Thursday</td><td id="24-R">New York</td><td id="24-S">1837</td><td id="24-T">1841</td></tr>
<tr><td id="24-U">William Henry Harrison</td><td id="24-V">1773-02-09</td><td id="24-W">Tuesday</td><td id="24-X">Virginia</td><td id="24-Y">1841</td><td id="24-Z">1841</td></tr>
<tr><td id="24-10">John Tyler</td><td id="24-11">1790-03-29</td><td id="24-12">Monday</td><td id="24-13">Virginia</td><td id="24-14">1841</td><td id="24-15">1845</td></tr>
<tr><td id="24-16">James K. Polk</td><td id="24-17">1795-11-02</td><td id="24-18">Monday</td><td id="24-19">North Carolina</td><td id="24-1a">1845</td><td id="24-1b">1849</td></tr>
<tr><td id="24-1c">Zachary Taylor</td><td id="24-1d">1784-11-24</td><td id="24-1e">Wednesday</td><td id="24-1f">Virginia</td><td id="24-1g">1849</td><td id="24-1h">1850</td></tr>
<tr><td id="24-1i">Millard Fillmore</td><td id="24-1j">1800-01-07</td><td id="24-1k">Tuesday</td><td id="24-1l">New York</td><td id="24-1m">1850</td><td id="24-1n">1853</td></tr>
<tr><td id="24-1o">Franklin Pierce</td><td id="24-1p">1804-11-23</td><td id="24-1q">Friday</td><td id="24-1r">New Hampshire</td><td id="24-1s">1853</td><td id="24-1t">1857</td></tr>
<tr><td id="24-1u">James Buchanan</td><td id="24-1v">1791-04-23</td><td id="24-1w">Saturday</td><td id="24-1x">Pennsylvania</td><td id="24-1y">1857</td><td id="24-1z">1861</td></tr>
<tr><td id="24-1A">Abraham Lincoln</td><td id="24-1B">1809-02-12</td><td id="24-1C">Sunday</td><td id="24-1D">Kentucky</td><td id="24-1E">1861</td><td id="24-1F">1865</td></tr>
<tr><td id="24-1G">Andrew Johnson</td><td id="24-1H">1808-12-29</td><td id="24-1I">Thursday</td><td id="24-1J">North Carolina</td><td id="24-1K">1865</td><td id="24-1L">1869</td></tr>
<tr><td id="24-1M">Ulysses S. Grant</td><td id="24-1N">1822-04-27</td><td id="24-1O">Saturday</td><td id="24-1P">Ohio</td><td id="24-1Q">1869</td><td id="24-1R">1877</td></tr>
<tr><td id="24-1S">Rutherford B. Hayes</td><td id="24-1T">1822-10-04</td><td id="24-1U">Friday</td><td id="24-1V">Ohio</td><td id="24-1W">1877</td><td id="24-1X">1881</td></tr>
<tr><td id="24-1Y">James A. Garfield</td><td id="24-1Z">1831-11-19</td><td id="24-20">Saturday</td><td id="24-21">Ohio</td><td id="24-22">1881</td><td id="24-23">1881</td></tr>
<tr><td id="24-24">Chester A. Arthur</td><td id="24-25">1829-10-05</td><td id="24-26">Monday</td><td id="24-27">Vermont</td><td id="24-28">1881</td><td id="24-29">1885</td></tr>
<tr><td id="24-2a">Benjamin Harrison</td><td id="24-2b">1833-08-20</td><td id="24-2c">Tuesday</td><td id="24-2d">Ohio</td><td id="24-2e">1889</td><td id="24-2f">1893</td></tr>
<tr><td id="24-2g">William McKinley</td><td id="24-2h">1843-01-29</td><td id="24-2i">Sunday</td><td id="24-2j">Ohio</td><td id="24-2k">1897</td><td id="24-2l">1901</td></tr>
<tr><td id="24-2m">Theodore Roosevelt</td><td id="24-2n">1858-10-27</td><td id="24-2o">Wednesday</td><td id="24-2p">New York</td><td id="24-2q">1901</td><td id="24-2r">1909</td></tr>
<tr><td id="24-2s">William Howard Taft</td><td id="24-2t">1857-09-15</td><td id="24-2u">Tuesday</td><td id="24-2v">Ohio</td><td id="24-2w">1909</td><td id="24-2x">1913</td></tr>
<tr><td id="24-2y">Woodrow Wilson</td><td id="24-2z">1856-12-28</td><td id="24-2A">Sunday</td><td id="24-2B">Virginia</td><td id="24-2C">1913</td><td id="24-2D">1921</td></tr>
<tr><td id="24-2E">Warren G. Harding</td><td id="24-2F">1865-11-02</td><td id="24-2G">Thursday</td><td id="24-2H">Ohio</td><td id="24-2I">1921</td><td id="24-2J">1923</td></tr>
<tr><td id="24-2K">Calvin Coolidge</td><td id="24-2L">1872-07-04</td><td id="24-2M">Thursday</td><td id="24-2N">Vermont</td><td id="24-2O">1923</td><td id="24-2P">1929</td></tr>
<tr><td id="24-2Q">Herbert Hoover</td><td id="24-2R">1874-08-10</td><td id="24-2S">Monday</td><td id="24-2T">Iowa</td><td id="24-2U">1929</td><td id="24-2V">1933</td></tr>
<tr><td id="24-2W">Franklin D. Roosevelt</td><td id="24-2X">1882-01-30</td><td id="24-2Y">Monday</td><td id="24-2Z">New York</td><td id="24-30">1933</td><td id="24-31">1945</td></tr>
<tr><td id="24-32">Harry S. Truman</td><td id="24-33">1884-05-08</td><td id="24-34">Thursday</td><td id="24-35">Missouri</td><td id="24-36">1945</td><td id="24-37">1953</td></tr>
<tr><td id="24-38">Dwight D. Eisenhower</td><td id="24-39">1890-10-14</td><td id="24-3a">Tuesday</td><td id="24-3b">Texas</td><td id="24-3c">1953</td><td id="24-3d">1961</td></tr>
<tr><td id="24-3e">John F. Kennedy</td><td id="24-3f">1917-05-29</td><td id="24-3g">Tuesday</td><td id="24-3h">Massachusetts</td><td id="24-3i">1961</td><td id="24-3j">1963</td></tr>
<tr><td id="24-3k">Lyndon B. Johnson</td><td id="24-3l">1908-08-27</td><td id="24-3m">Thursday</td><td id="24-3n">Texas</td><td id="24-3o">1963</td><td id="24-3p">1969</td></tr>
<tr><td id="24-3q">Richard Nixon</td><td id="24-3r">1913-01-09</td><td id="24-3s">Thursday</td><td id="24-3t">California</td><td id="24-3u">1969</td><td id="24-3v">1974</td></tr>
<tr><td id="24-3w">Gerald Ford</td><td id="24-3x">1913-07-14</td><td id="24-3y">Monday</td><td id="24-3z">Nebraska</td><td id="24-3A">1974</td><td id="24-3B">1977</td></tr>
<tr><td id="24-3C">Jimmy Carter</td><td id="24-3D">1924-10-01</td><td id="24-3E">Wednesday</td><td id="24-3F">Georgia</td><td id="24-3G">1977</td><td id="24-3H">1981</td></tr>
<tr><td id="24-3I">Ronald Reagan</td><td id="24-3J">1911-02-06</td><td id="24-3K">Monday</td><td id="24-3L">Illinois</td><td id="24-3M">1981</td><td id="24-3N">1989</td></tr>
<tr><td id="24-3O">George H. W. Bush</td><td id="24-3P">1924-06-12</td><td id="24-3Q">Thursday</td><td id="24-3R">Massachusetts</td><td id="24-3S">1989</td><td id="24-3T">1993</td></tr>
<tr><td id="24-3U">Bill Clinton</td><td id="24-3V">1946-08-19</td><td id="24-3W">Monday</td><td id="24-3X">Arkansas</td><td id="24-3Y">1993</td><td id="24-3Z">2001</td></tr>
<tr><td id="24-40">George W. Bush</td><td id="24-41">1946-07-06</td><td id="24-42">Saturday</td><td id="24-43">Connecticut</td><td id="24-44">2001</td><td id="24-45">2009</td></tr>
<tr><td id="24-46">Barack Obama</td><td id="24-47">1961-08-04</td><td id="24-48">Friday</td><td id="24-49">Hawaii</td><td id="24-4a">2009</td><td id="24-4b">2017</td></tr>
<tr><td id="24-4c">Joe Biden</td><td id="24-4d">1942-11-20</td><td id="24-4e">Friday</td><td id="24-4f">Pennsylvania</td><td id="24-4g">2021</td><td id="24-4h">2025</td></tr>
</table>

<a id='0204a77b-35fe-4ae2-af2f-ca5b3440ea9e'></a>

Table 11 describes the schema of the above US Presidents dataset.

<a id='cb3e912d-8456-4e60-8bd6-90965998b7d6'></a>

## A.3 LLM Prompt Templates
All runs are closed-book (no external tools/browsing). Prompts begin with a reset instruction to ignore prior context, use only internal pre-trained knowledge, and require an exact output format. We show canonical templates: square brackets [] mark fields filled programmatically (e.g., sampled items), while curly braces {} mark user-customizable options (e.g., criteria names, batch size).

<a id='6dce8ca2-a257-4996-970b-a25a357a9726'></a>

25

<!-- PAGE BREAK -->

<a id='69e046d2-0d7d-4582-8429-eef9b4992ba9'></a>

Table 11: US Presidents Dataset Schema

<a id='15452363-5f24-4717-a417-cdf7858fcab5'></a>

<table id="25-1">
<tr><td id="25-2">Column Name</td><td id="25-3">Data Type</td><td id="25-4">Description</td></tr>
<tr><td id="25-5">Name</td><td id="25-6">String</td><td id="25-7">Full name of the US president</td></tr>
<tr><td id="25-8">Birthdate</td><td id="25-9">Date (YYYY-MM-DD)</td><td id="25-a">Birth date in ISO format</td></tr>
<tr><td id="25-b">Birth Day Of Week</td><td id="25-c">String</td><td id="25-d">Day of the week when president was born</td></tr>
<tr><td id="25-e">Birth State</td><td id="25-f">String</td><td id="25-g">State where the president was born</td></tr>
<tr><td id="25-h">Start</td><td id="25-i">Integer</td><td id="25-j">Year when presidency began</td></tr>
<tr><td id="25-k">End</td><td id="25-l">Integer</td><td id="25-m">Year when presidency ended</td></tr>
</table>

<a id='cdf3a3d2-1610-4896-a9ec-630997feccc7'></a>

### A.3.1 Knowledge-Verification

In what year did this historical event occur? Respond with ONLY the year as a number
: [event] (Guidance: If a range exists, answer with the first year. If unknown,
output UNKNOWN.)

<a id='f97576f0-4f70-4891-9434-f6b51ea09dcc'></a>

Used to confirm that the LLM knows the events it is asked to order. Items accepted if and only if the model returns the exact 4-digit canonical year; otherwise the item is excluded.

<a id='dcaf0566-d32c-4456-a67f-68ec6eda690e'></a>

A.3.2 Basic Sorting — Historical Events

You are an expert historian specializing in accurate chronological sequencing.
Arrange the events below in strict chronological order (earliest to latest).

[event lists]

TASK RULES
1) If an event spans a range (e.g., "Feb 9-19" or "July-August"), order by the FIRST
date/month.
2) If an event lacks day or month, reason at the most precise available granularity.
3) If two events fall on the same exact date, break ties ALPHABETICALLY by
description.

Return ONLY the reordered list-one event per line, no commentary.

Example
Input:
- Moon Landing
- World War II ends
- Fall of the Berlin Wall

Output:
World War II ends
Moon Landing
Fall of the Berlin Wall

Now reorder these events chronologically:

[event lists]

<a id='dc427134-3eec-4e4d-9514-663dfc0dd551'></a>

26

<!-- PAGE BREAK -->

<a id='e7041ab2-30c2-48a2-becc-30f45c3ddbe2'></a>

### A.3.3 Basic Sorting — U.S. Presidents
You are an expert historian specializing in accurate chronological sequencing.
Arrange the presidents below in strict chronological order by the START YEAR of
their presidency. If two presidents somehow share the same start date, break
ties alphabetically.

[president lists]

Return ONLY the reordered list-one president per line, no commentary.

Example
Input:
- Abraham Lincoln
- John F. Kennedy
- George Washington

Output:
George Washington
Abraham Lincoln
John F. Kennedy

Now reorder these presidents chronologically:

[president lists]

<a id='9594a6fa-70d7-47d3-ad00-69e7bdf1fadf'></a>

A.3.4 Conditional Sorting --- Self-Filtering Task

You are an expert historian specializing in accurate chronological sequencing. You
are given a list of US presidents in random order:

[presidents list]

Your task:
1. Filter for presidents who were {criterion_desc}.
2. Then, order the filtered names chronologically (by when they served as president)
...

Return ONLY the names of presidents who were {criterion_desc}.

Output format:
- One president per line
- No numbering
- No commentary or explanations

Example:
George Washington
Thomas Jefferson
Ulysses S. Grant
...

Do not include any other presidents. Do not include any explanations or additional
text.

<a id='243c76db-0739-4131-821c-f2ffc91eba4d'></a>

27

<!-- PAGE BREAK -->

<a id='d1e0fdb8-b986-40a9-84db-7827ecc6f5ee'></a>

A.3.5 Conditional Sorting — Given-Names Task

You are an expert historian specializing in accurate chronological sequencing. You
are given a list of presidents in random order:

[presidents list]

Your task:
Order these presidents chronologically (by when they served as president).

Return ONLY the names of these presidents, ordered chronologically.

Output format:
- One president per line
- No numbering
- No commentary or explanations

Example:
George Washington
Thomas Jefferson
Ulysses S. Grant
...

Do not include any other presidents. Do not include any explanations or additional
text

<a id='5590238a-e7b0-40d7-83fc-13985dc4d410'></a>

A.3.6 Anachronism Detection: Variant 1

You are an expert historian who specializes in accurate chronological sequencing.
Below are {BATCH_SIZE} statements about U.S. presidents and activities.

For each, respond with 'Possible' if the activity could have occurred during their
presidency, or 'Not possible' if not.

Give NO explanation. Output exactly one line per event, in the format: [event]:
Possible or [event]: Not possible. Here, [event] means the event text, not
literal brackets.

Do not number the lines. Do not add any extra commentary or formatting. Respond to
only the events listed below.

Example output:
Abraham Lincoln travelled by railroad while president: Possible
George Washington joined a Zoom call while president: Not possible
Franklin D. Roosevelt hosted a radio 'fireside chat' as president: Possible
John Adams used generative AI while president: Not possible
Barack Obama posted on a social media platform while president: Possible
James K. Polk appeared in a photograph while president: Possible

Now, for the following events, respond in the same format:

[presidents-events pairs]

<a id='6a90f27b-8776-4aa2-9eba-3244b8e0d20a'></a>

28

<!-- PAGE BREAK -->

<a id='da047aec-336b-4141-96f9-0c2b36152b9f'></a>

A.3.7 Anachronism Detection: Variant 2

You are an expert historian who specializes in accurate chronological sequencing.
Below are [BATCH_SIZE] questions about whether [PRES_COUNT] U.S. presidents
were all alive at the same time. For each question, respond with 'Yes' if all [
PRES_COUNT] presidents were alive at the same time (their lifetimes overlapped),
or 'No' if not.

ONLY consider chronological plausibility (whether their lifetimes overlapped), NOT
logical plausibility.
Give NO explanation.

Output exactly one line per question, in the format:
[question]: Yes or [question]: No
Here, [question] means the question text, not literal brackets.

Do not number the lines. Do not add any extra commentary or formatting. Respond only
to the questions listed below.

Example output:
Were George Washington and John Adams all alive at the same time: Yes
Were George Washington and Barack Obama all alive at the same time: No
Were Thomas Jefferson, James Madison, and James Monroe all alive at the same time:
Yes
Were Abraham Lincoln and John F. Kennedy all alive at the same time: No
Were Franklin D. Roosevelt, Harry S. Truman, and Dwight D. Eisenhower all alive at
the same time: Yes
Were Dwight D. Eisenhower and Joe Biden all alive at the same time: Yes

Now, for the following questions, respond in the same format:
[QUESTION_1]
[QUESTION_2]
[...] 

<a id='003dad52-22f7-4ae5-9cdc-6ab0ccfc0004'></a>

A.4 Random-Permutation Baseline

For each n \u2208 {2, 5, 10, 20, 50, 100} we generated 1 000 uniformly random orderings and computed Spearman's \u03c1, Kendall's \u03c4, and Cayley\_norm for each. GPT-4.1's score was then expressed as an empirical percentile within this distribution (e.g. "97th percentile against chance"). Table 12 reports the percentile position of GPT-4.1's mean score within the corresponding random distribution; higher values indicate a larger margin over chance.

<a id='e18aa006-dbdf-42ae-96d4-62608b84e72c'></a>

Table 12: GPT-4.1 percentile against 1000 random permutations (higher = better).
<table id="28-1">
<tr><td id="28-2">n</td><td id="28-3">Spearman&#x27;s ρ</td><td id="28-4">Kendall&#x27;s τ</td><td id="28-5">Cayley norm</td><td id="28-6">Exact match</td></tr>
<tr><td id="28-7">2</td><td id="28-8">74.2%</td><td id="28-9">74.2%</td><td id="28-a">74.1%</td><td id="28-b">74.2%</td></tr>
<tr><td id="28-c">5</td><td id="28-d">98.3%</td><td id="28-e">98.2%</td><td id="28-f">96.1%</td><td id="28-g">72.1%</td></tr>
<tr><td id="28-h">10</td><td id="28-i">100.0%</td><td id="28-j">100.0%</td><td id="28-k">99.0%</td><td id="28-l">55.0%</td></tr>
<tr><td id="28-m">20</td><td id="28-n">100.0%</td><td id="28-o">100.0%</td><td id="28-p">100.0%</td><td id="28-q">50.0%</td></tr>
<tr><td id="28-r">50</td><td id="28-s">100.0%</td><td id="28-t">100.0%</td><td id="28-u">97.2%</td><td id="28-v">50.0%</td></tr>
<tr><td id="28-w">100</td><td id="28-x">100.0%</td><td id="28-y">100.0%</td><td id="28-z">100.0%</td><td id="28-A">50.1%</td></tr>
</table>

<a id='39818417-23fc-4000-8503-31df058a5700'></a>

29

<!-- PAGE BREAK -->

<a id='b010ed1d-788c-44a3-a3be-0e4f69581e8d'></a>

ChatGPT vs Random Baseline (n=100 events)<::chart: The visual content displays five histograms arranged in a 2x3 grid (with one empty spot in the bottom right, making it effectively 2x3 minus one, or 2 rows with 3 charts in the first and 2 in the second). Each histogram compares ChatGPT's performance against a random-permutation baseline for 100 events. Grey bars represent the random distribution (Random Baseline), and a colored dashed line indicates the ChatGPT Trial's mean score.

**Top Row, Left Histogram: Cayley Distance**
Title: Cayley Distance ChatGPT: 100.0th percentile
X-axis: Cayley Distance (ranging from approximately 40 to 100)
Y-axis: Density (ranging from 0.0 to 0.6)
Legend: Random Baseline (grey bars), ChatGPT Trial (red dashed line), ChatGPT Mean: 45.000 (label for the red dashed line, which is at the far left of the distribution, indicating ChatGPT's performance is significantly lower than the random baseline distribution).

**Top Row, Middle Histogram: Normalized Cayley Distance**
Title: Normalized Cayley Distance ChatGPT: 100.0th percentile
X-axis: Normalized Cayley Distance (ranging from approximately 0.4 to 1.0)
Y-axis: Density (ranging from 0 to 60)
Legend: Random Baseline (grey bars), ChatGPT Trial (orange dashed line), ChatGPT Mean: 0.455 (label for the orange dashed line, which is at the far left of the distribution, indicating ChatGPT's performance is significantly lower than the random baseline distribution).

**Top Row, Right Histogram: Kendall Tau Correlation**
Title: Kendall Tau Correlation ChatGPT: 100.0th percentile
X-axis: Kendall Tau Correlation (ranging from approximately -0.2 to 0.6)
Y-axis: Density (ranging from 0 to 7)
Legend: Random Baseline (grey bars), ChatGPT Trial (purple dashed line), ChatGPT Mean: 0.664 (label for the purple dashed line, which is at the far right of the distribution, indicating ChatGPT's performance is significantly higher than the random baseline distribution).

**Bottom Row, Left Histogram: Spearman Correlation**
Title: Spearman Correlation ChatGPT: 100.0th percentile
X-axis: Spearman Correlation (ranging from approximately -0.2 to 0.8)
Y-axis: Density (ranging from 0 to 4)
Legend: Random Baseline (grey bars), ChatGPT Trial (green dashed line), ChatGPT Mean: 0.788 (label for the green dashed line, which is at the far right of the distribution, indicating ChatGPT's performance is significantly higher than the random baseline distribution).

**Bottom Row, Middle Histogram: Exact Match Accuracy**
Title: Exact Match Accuracy ChatGPT: 50.1th percentile
X-axis: Exact Match Accuracy (ranging from approximately -0.4 to 0.4)
Y-axis: Density (ranging from 0 to 30)
Legend: Random Baseline (grey bars), ChatGPT Trial (blue dashed line), ChatGPT Mean: 0.000 (label for the blue dashed line, which is near the center of the distribution, indicating ChatGPT's performance is close to the mean of the random baseline distribution).

Figure 8: Example visualization of GPT-4.1 performance versus a random-permutation baseline for n = 100 events. Histograms show the random distribution for each metric (grey bars); the red dashed line marks GPT-4.1's mean score and the title of each panel states its percentile rank.::>

<a id='74f2a6d0-5422-450d-a04b-3c013e078c12'></a>

### A.5 Basic Sorting Wide Time-Gap Variant

**Experimental design** We construct a corpus of 30 single-year historical items spanning Years 1–2025, as presented in Table 13. The goal is broad temporal coverage rather than exhaustiveness: we heuristically aimed to include events from (nearly) every century and a mix of political, scientific, and cultural milestones, but did not enforce a rigid quota per era. Each item has a canonical year (no multi-year durations), which enables controlled sampling at prescribed minimum separations Δ (e.g., 50/100/200 years) in the timescale experiments.

<a id='b251a317-2048-45d4-8328-5c76784f9de4'></a>

Table 13: Illustrative samples from the curated dataset (earliest "head" examples at left; latest "tail" examples at right).

<a id='6a1bca14-d353-4af3-93e6-e38fb6645b9b'></a>

(a) Head (earliest examples)
<table id="29-1">
<tr><td id="29-2">Year</td><td id="29-3">Event</td></tr>
<tr><td id="29-4">43</td><td id="29-5">Southern Britain annexed by Rome</td></tr>
<tr><td id="29-6">161</td><td id="29-7">Death of Antoninus Pius</td></tr>
<tr><td id="29-8">380</td><td id="29-9">Christianity becomes official religion of the Roman Empire</td></tr>
<tr><td id="29-a">476</td><td id="29-b">Fall of Western Roman Empire</td></tr>
<tr><td id="29-c">581</td><td id="29-d">China unified by the Sui Dynasty</td></tr>
<tr><td id="29-e">697</td><td id="29-f">Venice becomes independent from the Eastern Roman Empire</td></tr>
<tr><td id="29-g">762</td><td id="29-h">Baghdad founded; later a center of learning in the Islamic Golden Age</td></tr>
<tr><td id="29-i">808</td><td id="29-j">Gunpowder discovered in China</td></tr>
<tr><td id="29-k">927</td><td id="29-l">Kingdom of England established by Æthelstan</td></tr>
<tr><td id="29-m">1096</td><td id="29-n">Oxford University begins functioning</td></tr>
</table>

<a id='964467c7-3427-41c8-9221-6c026a60d050'></a>

(b) Tail (latest examples)
<table id="29-o">
<tr><td id="29-p">Year</td><td id="29-q">Event</td></tr>
<tr><td id="29-r">1993</td><td id="29-s">European Union founded</td></tr>
<tr><td id="29-t">2000</td><td id="29-u">Israeli troops withdraw from southern Lebanon</td></tr>
<tr><td id="29-v">2001</td><td id="29-w">9/11 attacks in the United States</td></tr>
<tr><td id="29-x">2005</td><td id="29-y">YouTube is founded</td></tr>
<tr><td id="29-z">2012</td><td id="29-A">Curiosity rover finds ancient streambed on Mars</td></tr>
<tr><td id="29-B">2016</td><td id="29-C">U.K. votes to leave the EU (Brexit begins)</td></tr>
<tr><td id="29-D">2020</td><td id="29-E">Global COVID-19 pandemic begins</td></tr>
<tr><td id="29-F">2022</td><td id="29-G">Human population reaches 8 billion</td></tr>
<tr><td id="29-H">2025</td><td id="29-I">Pope Francis dies at age 88</td></tr>
</table>

<a id='9be0d522-36c7-45b4-85ab-6691a5491d56'></a>

30

<!-- PAGE BREAK -->

<a id='f3956aaf-4133-43ee-af15-f3c8bcf03db4'></a>

We aim to roughly simulate *temporal separation* rather than enforce rigid per-century quotas, which could oversample obscure items and introduce knowledge confounds. For our purpose—*relative* comparisons between "wide-gap" and "narrow-gap" regimes—this soft heuristic coverage is sufficient.

<a id='1a1edc6c-283a-418a-891c-16fea06c3688'></a>

Event selection formalization We outline the mathematical framework we follow when curating
30 events here.

<a id='045fd5d1-ef71-442c-aaa9-67aac2baafcf'></a>

Let $\mathcal{U} = \{(y_k, e_k)\}_{k=1}^{30}$ be the curated universe, where $y_k \in \mathbb{Z}$ is the (single) year of event $e_k$. For a trial of size $n$, we select a subset $S = \{(y_i, e_i)\}_{i=1}^{n}$ by heuristic sampling that favors broad temporal spread (e.g., drawing from different centuries when feasible). Define the empirical minimum gap and the fraction of wide pairs:

<a id='648a6d85-2d82-4438-9e87-eeaec0370525'></a>

$$\hat{A}_{\min}(S) = \min_{i<j}|y_i - y_j|,\quad \hat{\pi}_{\Delta}(S) = \frac{1}{\binom{n}{2}}\sum_{i<j}\mathbf{1}\{|y_i - y_j| \geq \Delta_{tgt}\}$$

<a id='bb8a82fb-1497-4f4f-ab20-32c8b809e959'></a>

Here $\Delta_{tgt} \in \{50, 100, 200\}$ denotes a *target* timescale. In a fully constrained design one would enforce $\hat{\Delta}_{min}(S) \ge \Delta_{tgt}$ for every trial. Instead, we *simulate* the wide-timescale condition by aiming for
$\hat{\pi}_{\Delta}(S) \gtrsim p_0$ with $p_0 \in [0.8, 0.95]$,

<a id='73f44b5b-ede5-48da-996b-4df4195b3e6e'></a>

i.e., the vast majority of pairs exceed \u0394tgt, while allowing occasional closer pairs when historical events sampled are obscure. This approach is sufficient for testing the hypothesis that larger temporal separations ease chronological ordering for LLMs.

<a id='7e0a18b3-9e5a-4312-9888-6e025f1b191e'></a>

Just like other experiments, we first apply knowledge verification (year query at T=0) to obtain the known set $K = \{e_k \in U : \hat{y}_k = Y_k\}$, then keep one event per year to remove duplicates (final $|K| = 24$). For each $n \in \{2, 5, 10, 15, 20, 24\}$, we run 20 trials: sample $n$ events without replacement from $K$, permute, and prompt the LLM to order. Prompts and metrics follow the basic-sorting setup (Appendix A.3, A.1).

<a id='bb05594c-77f0-475d-8184-8beb7f95bc01'></a>

31

<!-- PAGE BREAK -->

<a id='a8710982-32e4-4e56-b0ee-5e485f8a3e28'></a>

A.6 Basic Sorting U.S. Presidents

<a id='0cc99656-c22e-4024-b5e2-82af548131f7'></a>

### A.6.1 Experimental design
To analyze sample-size effects at fine granularity we generated $N_{trials}=20$ independent trials for each of the following number of presidents (list sizes):

<a id='e50fb7c7-db1e-47c3-9d74-3b3588b5716b'></a>

n \in \{2, 5, 10, 15, 20, 25, 30, 35, 40, 43\}
yielding a total of 200 trials. For every trial we performed the following steps:
1.  **Sampling:** Draw n presidents without replacement from the 43-person pool.
2.  **Shuffling:** Permute the sampled subset uniformly at random.
3.  **Prompting:** Present the shuffled list to GPT-4.1 with a standard instruction: "*Order these U.S. presidents in the correct chronological order of when they served as presidents.*
4.  **Evaluation:** Score the model's output with five metrics: exact-match rate, Spearman's \rho,
Kendall's \tau, Cayley distance, and Cayley_norm

<a id='5b81059b-6f82-4fb5-bda7-3e946e1a14a1'></a>

All prompts are issued with temperature 0.0 and a fixed random seed (42) to ensure reproducibility.
Events are described only by presidential names—no years, inaugurations, or other temporal cues
are included—so the model must rely on its internal chronology rather than surface hints. Figure 9
represents the complete workflow for the U.S. Presidents Chronological ordering task.

<a id='732c3c42-d72e-4b42-8312-b5f2eaa4d6c7'></a>

<::workflow flowchart: Box 1: 43 unique U.S. presidents (Washington → Biden). Arrow down. Box 2: Sample n ∈ {2, 5, 10, 20, 25, 30, 35, 40, 43} without replacement. Arrow down. Box 3: Uniform shuffle. Arrow down. Box 4: Prompt at T=0: "Order these presidents chronologically.". Arrow down. Box 5: GPT-4.1 output (predicted order). Arrow down. Box 6: Evaluate: exact-match, ρ, τ, Cayley, Cayley_norm. Figure 9: Workflow for the U.S. Presidents chronology experiment.::>

<a id='4b8e02d3-e387-4715-a640-b506c85aca7e'></a>

32

<!-- PAGE BREAK -->

<a id='03214a38-0f87-403d-85b7-1bb94cf9935e'></a>

### A.6.2 Adapted evaluation pipeline for hallucinated and missing names

Let $n_{prompt}$ denote the number of presidents that appear in a given prompt, and let ${1, 2, \dots, n_{prompt}}$ index those names in their ground-truth order.

<a id='f043b8dc-4fed-451c-9f43-4e5217775e52'></a>

Because the cleaning step deletes any row with a missing or hallucinated prediction, the rank-based statistics are computed on a *reduced* set of indices. Let

$P = \{1,2,..., n_{\text{prompt}}\}$ be the **full** index set of presidents in the prompt,

and let

..

<a id='b75a4a99-e351-4f1c-b3d6-97f18b2b11db'></a>

denote the indices that survive the filter. Writing $m = |\mathcal{V}| \le n_{\text{prompt}}$, the reported correlations are

$\rho^* = \rho(((r_{\text{true}},i)_{i \in \mathcal{V}}, (r_{\text{pred}},i)_{i \in \mathcal{V}}))$, $\tau^* = \tau(((r_{\text{true}},i)_{i \in \mathcal{V}}, (r_{\text{pred}},i)_{i \in \mathcal{V}}))$.

<a id='9c786496-3534-434b-bb70-5cf6705e7f1a'></a>

Thus a high value of ρ* or τ* testifies only that the retained _m_ presidents are well ordered; it does not imply that the model produced a faithful chronology for the full length _n_prompt. Correlations should therefore be read alongside the MISS and EXTRA counters as well as the strict metrics—exact-match and Cayley distance—that drop to zero whenever any omission or hallucination occurs.

<a id='d1dd6d92-b3ca-4f51-b047-26d35c1ecf6a'></a>

33

<!-- PAGE BREAK -->

<a id='74f2a24e-9f98-4517-aa5f-5046c9be3238'></a>

A.7 Basic Sorting Results Tables & Figures

<a id='bc003b56-d309-4512-a5e9-4ba188a49354'></a>

A.7.1 GPT-4.1's performance on 20th Century Historical Timeline GPT-4.1 Chronological Ordering Performance Analysis
<::chart: Figure 10: GPT-4.1 chronological-ordering performance across list sizes. The figure presents four plots. Results are averaged over 20 trials per n.

Top-left chart: "Rank Correlation Metrics" (Line chart)
- X-axis: Sample Size (n), ranging from 0 to 100.
- Y-axis: Correlation Coefficient, ranging from 0.70 to 1.05.
- Legend:
  - Spearman ρ (blue line with circular markers)
  - Kendall τ (green line with square markers)
- Data points (approximate):
  - Spearman ρ: (5, ~0.94), (10, ~0.98), (20, ~0.96), (30, ~0.94), (50, ~0.92), (80, ~0.85), (100, ~0.79)
  - Kendall τ: (5, ~1.00), (10, ~0.88), (20, ~0.87), (30, ~0.87), (50, ~0.81), (80, ~0.71), (100, ~0.68)

Top-right chart: "Perfect Ordering Rate" (Bar chart)
- X-axis: Sample Size (n), ranging from 0 to 100.
- Y-axis: Exact Match Rate, ranging from 0.0 to 1.0.
- Data points (approximate):
  - n=5: ~0.98
  - n=10: ~0.45
  - n=15: ~0.10
  - n=20 and above: 0.0

Bottom-left chart: "Normalized Caley Distance" (Bar chart)
- X-axis: Sample Size (n), ranging from 0 to 100.
- Y-axis: Normalized Distance (lower = better), ranging from 0.0 to 0.8.
- Data points (approximate):
  - n=5: ~0.15
  - n=10: ~0.30
  - n=20: ~0.41
  - n=50: ~0.77
  - n=100: ~0.90 (bar extends beyond 0.8 Y-axis mark)

Bottom-right chart: "All Metrics Comparison" (Line chart)
- X-axis: Sample Size (n), ranging from 0 to 100.
- Y-axis: Performance (higher = better), ranging from 0.0 to 1.0.
- Legend:
  - Spearman ρ (blue line with circular markers)
  - Kendall τ (green line with square markers)
  - Exact Match (orange line with diamond markers)
  - Norm. Caley (red line with triangular markers)
- Data points (approximate):
  - Spearman ρ: (5, ~0.94), (10, ~0.98), (20, ~0.96), (30, ~0.94), (50, ~0.92), (80, ~0.85), (100, ~0.79)
  - Kendall τ: (5, ~1.00), (10, ~0.88), (20, ~0.87), (30, ~0.87), (50, ~0.81), (80, ~0.71), (100, ~0.68)
  - Exact Match: (5, ~0.98), (10, ~0.85), (20, ~0.85), (30, ~0.84), (50, ~0.82), (80, ~0.78), (100, ~0.72)
  - Norm. Caley: (5, ~0.05), (10, ~0.15), (20, ~0.28), (30, ~0.40), (50, ~0.78), (80, ~0.88), (100, ~0.92)
::>

<a id='16c847df-090a-4c26-ab4e-934944ba5221'></a>

<::chart: Line chart titled "Position Effect on Rank Difference". The x-axis is labeled "Ground Truth Position" and ranges from 0 to 50. The y-axis is labeled "Mean Absolute Rank Difference" and ranges from 0 to approximately 9. The chart displays multiple lines, each representing a different "Number of Events" as indicated by the legend. Each line is accompanied by a shaded band representing inter-trial variability (one standard deviation). The legend shows:
Number of Events
- 2 (lightest pink line with corresponding shaded band)
- 5 (light pink line with corresponding shaded band)
- 10 (medium pink line with corresponding shaded band)
- 20 (darker pink line with corresponding shaded band)
- 50 (dark blue/purple line with corresponding shaded band)
The lines generally show an increasing trend in Mean Absolute Rank Difference as Ground Truth Position increases, especially for a higher number of events, with significant variability.::>

Figure 11: Mean absolute rank difference - MARD (solid line) and inter-trial variability at each position (shaded band, one standard deviation) as a function of an event's ground-truth position in the list.

<a id='4c085d6f-d251-4a47-92fe-01ae3f641dfe'></a>

34

<!-- PAGE BREAK -->

<a id='79a3255f-8354-4621-9977-81be4feb36b9'></a>

A.7.2 GPT-4.1's performance on Wide Time-gap<::chart: This visual content consists of four line plots with shaded error bands, each showing performance metrics against the 'Number of Events' on the x-axis, ranging from 0 to 25. The shaded bands indicate ±2 standard errors across trials.The plots are arranged in a 2x2 grid:Top-left plot:Exact Match RateY-axis: Exact Match Rate (0.0 to 1.0)The line starts at 1.0 at Number of Events 0, decreases sharply to 0.0 around Number of Events 15, and remains at 0.0 until 25.Top-right plot:Spearman CorrelationY-axis: Spearman p (0.92 to 1.00)The line starts at 1.0 at Number of Events 0, drops to approximately 0.94 at Number of Events 5, then rises to around 0.97 at 10, and fluctuates between 0.95 and 0.97 until 25.Bottom-left plot:Kendall CorrelationY-axis: Kendall τ (0.84 to 1.00)The line starts at 1.00 at Number of Events 0, drops to approximately 0.90 at Number of Events 5, then rises to around 0.92 at 10, and fluctuates between 0.88 and 0.90 until 25.Bottom-right plot:Normalized Caley DistanceY-axis: Normalized Caley Distance (0.0 to 0.4)The line starts at 0.0 at Number of Events 0 and increases steadily to just over 0.4 at Number of Events 25.Figure 12: Wide time-gap performance: Shaded bands indicate ±2 standard errors across trials.::>

<a id='d037ab10-f7d8-4ab0-b5cf-bdf4d164d5d8'></a>

Table 14: Summary statistics by list size n (20 trials per n). Means and standard deviations (SD).
<table id="34-1">
<tr><td id="34-2"></td><td id="34-3" colspan="2">Exact match</td><td id="34-4" colspan="2">Spearman&#x27;s ρ</td><td id="34-5" colspan="2">Kendall&#x27;s τ</td><td id="34-6" colspan="2">Norm. Cayley</td></tr>
<tr><td id="34-7">n</td><td id="34-8">mean</td><td id="34-9">SD</td><td id="34-a">mean</td><td id="34-b">SD</td><td id="34-c">mean</td><td id="34-d">SD</td><td id="34-e">mean</td><td id="34-f">SD</td></tr>
<tr><td id="34-g">2</td><td id="34-h">1.00</td><td id="34-i">0.00</td><td id="34-j">1.000</td><td id="34-k">0.000</td><td id="34-l">1.000</td><td id="34-m">0.000</td><td id="34-n">0.000</td><td id="34-o">0.000</td></tr>
<tr><td id="34-p">5</td><td id="34-q">0.55</td><td id="34-r">0.51</td><td id="34-s">0.945</td><td id="34-t">0.076</td><td id="34-u">0.900</td><td id="34-v">0.121</td><td id="34-w">0.125</td><td id="34-x">0.152</td></tr>
<tr><td id="34-y">10</td><td id="34-z">0.20</td><td id="34-A">0.41</td><td id="34-B">0.971</td><td id="34-C">0.024</td><td id="34-D">0.918</td><td id="34-E">0.060</td><td id="34-F">0.183</td><td id="34-G">0.126</td></tr>
<tr><td id="34-H">15</td><td id="34-I">0.00</td><td id="34-J">0.00</td><td id="34-K">0.968</td><td id="34-L">0.020</td><td id="34-M">0.900</td><td id="34-N">0.040</td><td id="34-O">0.282</td><td id="34-P">0.117</td></tr>
<tr><td id="34-Q">20</td><td id="34-R">0.00</td><td id="34-S">0.00</td><td id="34-T">0.961</td><td id="34-U">0.036</td><td id="34-V">0.886</td><td id="34-W">0.058</td><td id="34-X">0.366</td><td id="34-Y">0.133</td></tr>
<tr><td id="34-Z">24</td><td id="34-10">0.00</td><td id="34-11">0.00</td><td id="34-12">0.966</td><td id="34-13">0.044</td><td id="34-14">0.898</td><td id="34-15">0.043</td><td id="34-16">0.411</td><td id="34-17">0.131</td></tr>
</table>

<a id='ccba0c4d-85fb-405e-83bd-b53e84514e62'></a>

Table 15: Direct comparison by list size n: 20th-century historical events vs. wide time-gap events.
Entries are mean ± 2 standard errors across trials (n=20 per n).
<table id="34-18">
<tr><td id="34-19"></td><td id="34-1a" colspan="3">20th-century historical events</td><td id="34-1b" colspan="3">Wide time-gap events</td></tr>
<tr><td id="34-1c">n</td><td id="34-1d">Exact match</td><td id="34-1e">Spearman&#x27;s ρ</td><td id="34-1f">Kendall&#x27;s τ</td><td id="34-1g">Exact match</td><td id="34-1h">Spearman&#x27;s ρ</td><td id="34-1i">Kendall&#x27;s τ</td></tr>
<tr><td id="34-1j">2</td><td id="34-1k">1.00</td><td id="34-1l">1.00</td><td id="34-1m">1.00</td><td id="34-1n">1.00</td><td id="34-1o">1.00</td><td id="34-1p">1.00</td></tr>
<tr><td id="34-1q">5</td><td id="34-1r">0.450 ± 0.228</td><td id="34-1s">0.940 ± 0.027</td><td id="34-1t">0.880 ± 0.054</td><td id="34-1u">0.550 ± 0.228</td><td id="34-1v">0.945 ± 0.034</td><td id="34-1w">0.900 ± 0.054</td></tr>
<tr><td id="34-1x">10</td><td id="34-1y">0.100 ± 0.138</td><td id="34-1z">0.955 ± 0.017</td><td id="34-1A">0.876 ± 0.038</td><td id="34-1B">0.200 ± 0.183</td><td id="34-1C">0.971 ± 0.011</td><td id="34-1D">0.918 ± 0.027</td></tr>
<tr><td id="34-1E">20</td><td id="34-1F">0.00</td><td id="34-1G">0.963 ± 0.008</td><td id="34-1H">0.869 ± 0.019</td><td id="34-1I">0.00</td><td id="34-1J">0.961 ± 0.016</td><td id="34-1K">0.886 ± 0.026</td></tr>
</table>

<a id='33c5684f-5376-4b4f-9374-93c24583e3ed'></a>

35

<!-- PAGE BREAK -->

<a id='0906fe84-0562-447b-9b1b-f970c01236b8'></a>

Table 16: Average performance across list sizes.
<table id="35-1">
<tr><td id="35-2">Metric</td><td id="35-3">20th-century historical events mean</td><td id="35-4">Wide time-gaps mean</td><td id="35-5">Δ (wide-20th)</td></tr>
<tr><td id="35-6">Exact match</td><td id="35-7">0.258</td><td id="35-8">0.292</td><td id="35-9">+0.033 (+12.9%)</td></tr>
<tr><td id="35-a">Spearman&#x27;s ρ</td><td id="35-b">0.929</td><td id="35-c">0.969</td><td id="35-d">+0.040 (+4.3%)</td></tr>
<tr><td id="35-e">Kendall&#x27;s τ</td><td id="35-f">0.849</td><td id="35-g">0.917</td><td id="35-h">+0.068 (+8.0%)</td></tr>
</table>

<a id='cf35bf43-5345-4440-8d61-3241c6414d38'></a>

### A.7.3 GPT-4.1's U.S. Presidents Ordering Bottlenecks

Table 17: Trials affected by *missing* or *extra* presidents after 200 evaluations of the ordering prompt.

<a id='c586e8da-3111-472a-a576-fb8abcbe01d4'></a>

<table id="35-i">
<tr><td id="35-j">n</td><td id="35-k">Trials</td><td id="35-l">Missing-name trials</td><td id="35-m">Extra-name trials</td><td id="35-n">Share of Missing</td><td id="35-o">trials (%) Extra</td></tr>
<tr><td id="35-p">2</td><td id="35-q">20</td><td id="35-r">0</td><td id="35-s">0</td><td id="35-t">0</td><td id="35-u">0</td></tr>
<tr><td id="35-v">5</td><td id="35-w">20</td><td id="35-x">0</td><td id="35-y">0</td><td id="35-z">0</td><td id="35-A">0</td></tr>
<tr><td id="35-B">10</td><td id="35-C">20</td><td id="35-D">0</td><td id="35-E">0</td><td id="35-F">0</td><td id="35-G">0</td></tr>
<tr><td id="35-H">15</td><td id="35-I">20</td><td id="35-J">3</td><td id="35-K">1</td><td id="35-L">15</td><td id="35-M">5</td></tr>
<tr><td id="35-N">20</td><td id="35-O">20</td><td id="35-P">4</td><td id="35-Q">0</td><td id="35-R">20</td><td id="35-S">0</td></tr>
<tr><td id="35-T">25</td><td id="35-U">20</td><td id="35-V">11</td><td id="35-W">2</td><td id="35-X">55</td><td id="35-Y">10</td></tr>
<tr><td id="35-Z">30</td><td id="35-10">20</td><td id="35-11">11</td><td id="35-12">4</td><td id="35-13">55</td><td id="35-14">20</td></tr>
<tr><td id="35-15">35</td><td id="35-16">20</td><td id="35-17">9</td><td id="35-18">11</td><td id="35-19">45</td><td id="35-1a">55</td></tr>
<tr><td id="35-1b">40</td><td id="35-1c">20</td><td id="35-1d">3</td><td id="35-1e">17</td><td id="35-1f">15</td><td id="35-1g">85</td></tr>
<tr><td id="35-1h">43</td><td id="35-1i">20</td><td id="35-1j">1</td><td id="35-1k">19</td><td id="35-1l">5</td><td id="35-1m">95</td></tr>
<tr><td id="35-1n">Σ</td><td id="35-1o">200</td><td id="35-1p">42</td><td id="35-1q">64</td><td id="35-1r">—</td><td id="35-1s">—</td></tr>
</table>

<a id='bedcbaa1-6eca-427d-96b3-dafb85d76442'></a>

Table 18: Most frequent president names omissions and hallucinations (counts across the 200 trials).
<table id="35-1t">
<tr><td id="35-1u">President</td><td id="35-1v">Times missing</td><td id="35-1w">Times extra</td></tr>
<tr><td id="35-1x">James K. Polk</td><td id="35-1y">11</td><td id="35-1z">0</td></tr>
<tr><td id="35-1A">Andrew Jackson</td><td id="35-1B">10</td><td id="35-1C">0</td></tr>
<tr><td id="35-1D">Zachary Taylor</td><td id="35-1E">10</td><td id="35-1F">0</td></tr>
<tr><td id="35-1G">Andrew Johnson</td><td id="35-1H">9</td><td id="35-1I">3</td></tr>
<tr><td id="35-1J">James A. Garfield</td><td id="35-1K">8</td><td id="35-1L">0</td></tr>
<tr><td id="35-1M">Theodore Roosevelt</td><td id="35-1N">5</td><td id="35-1O">1</td></tr>
<tr><td id="35-1P">William H. Harrison</td><td id="35-1Q">4</td><td id="35-1R">0</td></tr>
<tr><td id="35-1S">John Tyler</td><td id="35-1T">3</td><td id="35-1U">0</td></tr>
<tr><td id="35-1V">William McKinley</td><td id="35-1W">3</td><td id="35-1X">0</td></tr>
<tr><td id="35-1Y">Barack Obama</td><td id="35-1Z">0</td><td id="35-20">18</td></tr>
<tr><td id="35-21">Joe Biden</td><td id="35-22">0</td><td id="35-23">17</td></tr>
<tr><td id="35-24">George W. Bush</td><td id="35-25">0</td><td id="35-26">11</td></tr>
<tr><td id="35-27">Ronald Reagan</td><td id="35-28">0</td><td id="35-29">8</td></tr>
<tr><td id="35-2a">Jimmy Carter</td><td id="35-2b">0</td><td id="35-2c">7</td></tr>
<tr><td id="35-2d">Bill Clinton</td><td id="35-2e">0</td><td id="35-2f">6</td></tr>
<tr><td id="35-2g">George H. W. Bush</td><td id="35-2h">0</td><td id="35-2i">5</td></tr>
<tr><td id="35-2j">Richard Nixon</td><td id="35-2k">0</td><td id="35-2l">3</td></tr>
<tr><td id="35-2m">Harry S. Truman</td><td id="35-2n">0</td><td id="35-2o">2</td></tr>
</table>

<a id='1c4d2e28-9c3e-4f89-994a-9fec797fc37e'></a>

36

<!-- PAGE BREAK -->

<a id='c5a72421-e9c1-45f6-a098-73b4a8997fb5'></a>

A.7.4 GPT-4.1's U.S. presidents Ordering Errors and MARD Analysis<::chart: A line graph titled "Error Magnitude by Position" plots Average Error Magnitude (y-axis) against Position in Ground Truth Order (x-axis). Multiple lines represent different values of 'n': n=2, n=5, n=10, n=15, n=20, n=25, n=30, n=35, n=40, and n=43. Figure 13: Position-wise MARD_n(k) for each list size n of U.S. presidents ordering.::>

<a id='28e93e5a-328a-4f3c-8f4a-35937314fc13'></a>

<::chart: The image displays Figure 14, which consists of two line graphs stacked vertically. The overall caption is "Figure 14: Average Spearman's p & Kendall's T; omission/hallucination counts, all versus list length." The x-axis for both graphs is "Number of Presidents (Sample Size)", ranging from 0 to 40. The x-axis ticks are at 0, 10, 20, 30, and 40. The y-axis for the top graph is "Average Metric Value", ranging from 0.93 to 1.00. The y-axis ticks are at 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, and 1.00. The top graph's title is "Average Metrics vs. Sample Size". It shows two lines with markers: "Spearman p" (pink line) and "Kendall τ" (golden brown line). The y-axis for the bottom graph is "Avg. # of Names per Trial", ranging from 0.0 to 3.0. The y-axis ticks are at 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, and 3.0. The bottom graph's title is "Missing and Extra Names vs. Sample Size". It also shows two lines with markers: "Missing Names" (pink line) and "Extra Names" (golden brown line).::>

<a id='aa334e43-db7e-4098-9364-75341316a3bc'></a>

Table 19: Ordering accuracy grouped by the century in which each president served.
<table id="36-1">
<tr><td id="36-2">Century</td><td id="36-3">Accuracy</td></tr>
<tr><td id="36-4">18th</td><td id="36-5">0.945</td></tr>
<tr><td id="36-6">19th</td><td id="36-7">0.510</td></tr>
<tr><td id="36-8">20th</td><td id="36-9">0.293</td></tr>
<tr><td id="36-a">21st</td><td id="36-b">0.319</td></tr>
</table>

<a id='f12742ae-fd10-46f6-bd7a-1cb671f6e820'></a>

37

<!-- PAGE BREAK -->

<a id='e0a89f4f-3bb2-4fd9-9d7d-b3b631ae3264'></a>

A.7.5 Multi-model results on U.S. Presidents Ordering Task

<a id='1161062c-2ccb-45ea-8060-06ddfb062197'></a>

<::chart: A composite figure titled "Claude President Ordering: Extended Thinking vs Standard" displaying six subplots in a 3x2 grid. The plots compare "Without ET" (red lines/bars) and "With ET" (blue lines/bars) across various metrics and sample sizes. The x-axis for the top four plots is "Number of Presidents" ranging from 0 to 40. The legends for the top four plots are: "Without ET" and "With ET". The error bars represent ± one standard error across trials. The bottom two plots show distribution and count data. A detailed description of each subplot follows:||Top-left subplot: Exact Match Rate by Sample Size. The y-axis is "Exact Match Rate" from 0.0 to 1.0. The "With ET" line is flat at 1.0. The "Without ET" line starts near 1.0, drops to approximately 0.4 at 5 presidents, then fluctuates between 0.1 and 0.6.||Top-right subplot: Spearman Correlation by Sample Size. The y-axis is "Spearman Correlation" from 0.75 to 1.00. The "With ET" line is flat at 1.0. The "Without ET" line starts around 0.8, increases to near 1.0 at 20 presidents, drops, then rises again to near 1.0 at 40 presidents.||Middle-left subplot: Kendall Tau by Sample Size. The y-axis is "Kendall Tau" from 0.70 to 1.00. The "With ET" line is flat at 1.0. The "Without ET" line starts near 1.0, drops to approximately 0.7 at 5 presidents, then steadily increases towards 1.0 at 40 presidents.||Middle-right subplot: Normalized Cayley Distance by Sample Size. The y-axis is "Normalized Cayley Distance" from 0.00 to 0.30. The "With ET" line is flat at 0.0. The "Without ET" line starts around 0.25, fluctuates between 0.05 and 0.25, and ends near 0.0 at 40 presidents.||Bottom-left subplot: Distribution of Metrics. This plot displays box plots for "Metric Value" on the y-axis (from 0.0 to 1.0) for six different metrics. From left to right, the box plots are for: "Exact Match (Without ET)" (red bar with box plot showing values mostly at 0), "Exact Match (With ET)" (blue bar with box plot showing values at 1.0), "Spearman Rho (Without ET)", "Spearman Rho (With ET)", "Kendall Tau (Without ET)", and "Kendall Tau (With ET)". The box plots for "With ET" metrics show perfect scores at 1.0, while "Without ET" metrics show a wider distribution of values, with Exact Match (Without ET) having a mean around 0.2.||Bottom-right subplot: Missing vs Extra Names. This is a bar chart with "Average Count" on the y-axis (from 0.0 to 0.8) and "Experiment Type" on the x-axis, showing "Without ET" and "With ET". For "Without ET", there is a red bar for "Missing Names" at approximately 0.3 and an orange bar for "Extra Names" at approximately 0.8. For "With ET", both "Missing Names" and "Extra Names" bars are at 0.0.||Figure 15: Claude Sonnet 3.7: Extended Thinking (ET) vs. standard. With ET (blue), exact-match stays at 1.00 and rank metrics are perfect across list sizes; without ET (red), exact-match deteriorates and normalized Cayley distance increases with n. The missing/extra analysis (bottom-right) shows ET completely eliminates set-membership errors. Error bars show ± one standard error across trials.::>

<a id='a96fded8-31ae-4fd7-ac7c-0be6fe5539c0'></a>

<::chart: Model Comparison: Claude Sonnet 3.7 vs ChatGPT-4.1 vs GPT-5 Variants Performance. The chart consists of three line graphs comparing different models across varying list sizes. Each graph includes a legend: ChatGPT-4.1 (black line with open circles), Claude-3.7 (Without ET) (red dashed line with plus signs), Claude-3.7 (With ET) (blue line with 'x' marks), GPT-5 Minimal (purple line with squares), GPT-5 Low (yellow line with triangles), GPT-5 Medium (green line with diamonds), GPT-5 Latest (brown line with filled circles). All lines include error bars representing ±1 s.e.

Left Plot: Exact Match Rate by Sample Size. The x-axis is 'Number of Presidents' (0 to 45). The y-axis is 'Exact Match Rate' (0.0 to 1.0). ChatGPT-4.1, Claude-3.7 (With ET), GPT-5 Minimal, and GPT-5 Latest show a sharp decline in exact match rate as the number of presidents increases. Claude-3.7 (Without ET) shows a similar decline but with higher variability. GPT-5 Low and GPT-5 Medium maintain a perfect exact match rate of 1.0 across all sample sizes.

Middle Plot: Spearman Correlation by Sample Size. The x-axis is 'Number of Presidents' (0 to 45). The y-axis is 'Spearman Correlation' (0.75 to 1.00). ChatGPT-4.1, Claude-3.7 (With ET), GPT-5 Minimal, and GPT-5 Latest show a decrease in Spearman correlation for smaller list sizes, then generally recover to higher correlations as the list size increases. Claude-3.7 (Without ET) shows the lowest correlation for small list sizes (around 0.75) before improving. GPT-5 Low and GPT-5 Medium maintain a Spearman correlation of 1.0 across all sample sizes.

Right Plot: Kendall Tau by Sample Size. The x-axis is 'Number of Presidents' (0 to 45). The y-axis is 'Kendall Tau' (0.70 to 1.00). Similar to Spearman correlation, ChatGPT-4.1, Claude-3.7 (With ET), GPT-5 Minimal, and GPT-5 Latest show a decrease in Kendall Tau for smaller list sizes, then generally recover. Claude-3.7 (Without ET) shows the lowest Kendall Tau for small list sizes (around 0.70) before improving. GPT-5 Low and GPT-5 Medium maintain a Kendall Tau of 1.0 across all sample sizes.

Figure 16: Model comparison across list sizes. Left: Exact match rate. Middle: Spearman's ρ. Right: Kendall's τ. Points are trial means; error bars are ±1 s.e.; small horizontal jitter prevents overlap.::>

<a id='4d3fcc44-62c2-4ee0-a9af-a6b9ed91ddc6'></a>

38

<!-- PAGE BREAK -->

<a id='1671cc63-75f9-4561-8ef4-f100c43ab2ac'></a>

Table 20: GPT-5 Performance by Reasoning Effort Level and Sample Size
<table id="38-1">
<tr><td id="38-2">N Presidents</td><td id="38-3">Model</td><td id="38-4">Reasoning Effort</td><td id="38-5">Exact Match</td><td id="38-6">Spearman&#x27;s p</td><td id="38-7">Kendall&#x27;s T</td><td id="38-8">Std (EM)</td><td id="38-9">Std (SR)</td><td id="38-a">Std (KT)</td></tr>
<tr><td id="38-b">2</td><td id="38-c">GPT-5</td><td id="38-d">High</td><td id="38-e">1.000</td><td id="38-f">1.000</td><td id="38-g">1.000</td><td id="38-h">0.000</td><td id="38-i">0.000</td><td id="38-j">0.000</td></tr>
<tr><td id="38-k">5</td><td id="38-l">GPT-5</td><td id="38-m">High</td><td id="38-n">1.000</td><td id="38-o">1.000</td><td id="38-p">1.000</td><td id="38-q">0.000</td><td id="38-r">0.000</td><td id="38-s">0.000</td></tr>
<tr><td id="38-t">10</td><td id="38-u">GPT-5</td><td id="38-v">High</td><td id="38-w">1.000</td><td id="38-x">1.000</td><td id="38-y">1.000</td><td id="38-z">0.000</td><td id="38-A">0.000</td><td id="38-B">0.000</td></tr>
<tr><td id="38-C">15</td><td id="38-D">GPT-5</td><td id="38-E">High</td><td id="38-F">1.000</td><td id="38-G">1.000</td><td id="38-H">1.000</td><td id="38-I">0.000</td><td id="38-J">0.000</td><td id="38-K">0.000</td></tr>
<tr><td id="38-L">20</td><td id="38-M">GPT-5</td><td id="38-N">High</td><td id="38-O">1.000</td><td id="38-P">1.000</td><td id="38-Q">1.000</td><td id="38-R">0.000</td><td id="38-S">0.000</td><td id="38-T">0.000</td></tr>
<tr><td id="38-U">25</td><td id="38-V">GPT-5</td><td id="38-W">High</td><td id="38-X">1.000</td><td id="38-Y">1.000</td><td id="38-Z">1.000</td><td id="38-10">0.000</td><td id="38-11">0.000</td><td id="38-12">0.000</td></tr>
<tr><td id="38-13">30</td><td id="38-14">GPT-5</td><td id="38-15">High</td><td id="38-16">1.000</td><td id="38-17">1.000</td><td id="38-18">1.000</td><td id="38-19">0.000</td><td id="38-1a">0.000</td><td id="38-1b">0.000</td></tr>
<tr><td id="38-1c">35</td><td id="38-1d">GPT-5</td><td id="38-1e">High</td><td id="38-1f">1.000</td><td id="38-1g">1.000</td><td id="38-1h">1.000</td><td id="38-1i">0.000</td><td id="38-1j">0.000</td><td id="38-1k">0.000</td></tr>
<tr><td id="38-1l">40</td><td id="38-1m">GPT-5</td><td id="38-1n">High</td><td id="38-1o">1.000</td><td id="38-1p">1.000</td><td id="38-1q">1.000</td><td id="38-1r">0.000</td><td id="38-1s">0.000</td><td id="38-1t">0.000</td></tr>
<tr><td id="38-1u">43</td><td id="38-1v">GPT-5</td><td id="38-1w">High</td><td id="38-1x">1.000</td><td id="38-1y">1.000</td><td id="38-1z">1.000</td><td id="38-1A">0.000</td><td id="38-1B">0.000</td><td id="38-1C">0.000</td></tr>
<tr><td id="38-1D">2</td><td id="38-1E">GPT-5</td><td id="38-1F">Medium</td><td id="38-1G">1.000</td><td id="38-1H">1.000</td><td id="38-1I">1.000</td><td id="38-1J">0.000</td><td id="38-1K">0.000</td><td id="38-1L">0.000</td></tr>
<tr><td id="38-1M">5</td><td id="38-1N">GPT-5</td><td id="38-1O">Medium</td><td id="38-1P">1.000</td><td id="38-1Q">1.000</td><td id="38-1R">1.000</td><td id="38-1S">0.000</td><td id="38-1T">0.000</td><td id="38-1U">0.000</td></tr>
<tr><td id="38-1V">10</td><td id="38-1W">GPT-5</td><td id="38-1X">Medium</td><td id="38-1Y">1.000</td><td id="38-1Z">1.000</td><td id="38-20">1.000</td><td id="38-21">0.000</td><td id="38-22">0.000</td><td id="38-23">0.000</td></tr>
<tr><td id="38-24">15</td><td id="38-25">GPT-5</td><td id="38-26">Medium</td><td id="38-27">1.000</td><td id="38-28">1.000</td><td id="38-29">1.000</td><td id="38-2a">0.000</td><td id="38-2b">0.000</td><td id="38-2c">0.000</td></tr>
<tr><td id="38-2d">20</td><td id="38-2e">GPT-5</td><td id="38-2f">Medium</td><td id="38-2g">1.000</td><td id="38-2h">1.000</td><td id="38-2i">1.000</td><td id="38-2j">0.000</td><td id="38-2k">0.000</td><td id="38-2l">0.000</td></tr>
<tr><td id="38-2m">25</td><td id="38-2n">GPT-5</td><td id="38-2o">Medium</td><td id="38-2p">1.000</td><td id="38-2q">1.000</td><td id="38-2r">1.000</td><td id="38-2s">0.000</td><td id="38-2t">0.000</td><td id="38-2u">0.000</td></tr>
<tr><td id="38-2v">30</td><td id="38-2w">GPT-5</td><td id="38-2x">Medium</td><td id="38-2y">1.000</td><td id="38-2z">1.000</td><td id="38-2A">1.000</td><td id="38-2B">0.000</td><td id="38-2C">0.000</td><td id="38-2D">0.000</td></tr>
<tr><td id="38-2E">35</td><td id="38-2F">GPT-5</td><td id="38-2G">Medium</td><td id="38-2H">1.000</td><td id="38-2I">1.000</td><td id="38-2J">1.000</td><td id="38-2K">0.000</td><td id="38-2L">0.000</td><td id="38-2M">0.000</td></tr>
<tr><td id="38-2N">40</td><td id="38-2O">GPT-5</td><td id="38-2P">Medium</td><td id="38-2Q">1.000</td><td id="38-2R">1.000</td><td id="38-2S">1.000</td><td id="38-2T">0.000</td><td id="38-2U">0.000</td><td id="38-2V">0.000</td></tr>
<tr><td id="38-2W">43</td><td id="38-2X">GPT-5</td><td id="38-2Y">Medium</td><td id="38-2Z">1.000</td><td id="38-30">1.000</td><td id="38-31">1.000</td><td id="38-32">0.000</td><td id="38-33">0.000</td><td id="38-34">0.000</td></tr>
<tr><td id="38-35">2</td><td id="38-36">GPT-5</td><td id="38-37">Low</td><td id="38-38">1.000</td><td id="38-39">1.000</td><td id="38-3a">1.000</td><td id="38-3b">0.000</td><td id="38-3c">0.000</td><td id="38-3d">0.000</td></tr>
<tr><td id="38-3e">5</td><td id="38-3f">GPT-5</td><td id="38-3g">Low</td><td id="38-3h">1.000</td><td id="38-3i">1.000</td><td id="38-3j">1.000</td><td id="38-3k">0.000</td><td id="38-3l">0.000</td><td id="38-3m">0.000</td></tr>
<tr><td id="38-3n">10</td><td id="38-3o">GPT-5</td><td id="38-3p">Low</td><td id="38-3q">1.000</td><td id="38-3r">1.000</td><td id="38-3s">1.000</td><td id="38-3t">0.000</td><td id="38-3u">0.000</td><td id="38-3v">0.000</td></tr>
<tr><td id="38-3w">15</td><td id="38-3x">GPT-5</td><td id="38-3y">Low</td><td id="38-3z">1.000</td><td id="38-3A">1.000</td><td id="38-3B">1.000</td><td id="38-3C">0.000</td><td id="38-3D">0.000</td><td id="38-3E">0.000</td></tr>
<tr><td id="38-3F">20</td><td id="38-3G">GPT-5</td><td id="38-3H">Low</td><td id="38-3I">1.000</td><td id="38-3J">1.000</td><td id="38-3K">1.000</td><td id="38-3L">0.000</td><td id="38-3M">0.000</td><td id="38-3N">0.000</td></tr>
<tr><td id="38-3O">25</td><td id="38-3P">GPT-5</td><td id="38-3Q">Low</td><td id="38-3R">0.950</td><td id="38-3S">1.000</td><td id="38-3T">1.000</td><td id="38-3U">0.218</td><td id="38-3V">0.000</td><td id="38-3W">0.000</td></tr>
<tr><td id="38-3X">30</td><td id="38-3Y">GPT-5</td><td id="38-3Z">Low</td><td id="38-40">1.000</td><td id="38-41">1.000</td><td id="38-42">1.000</td><td id="38-43">0.000</td><td id="38-44">0.000</td><td id="38-45">0.000</td></tr>
<tr><td id="38-46">35</td><td id="38-47">GPT-5</td><td id="38-48">Low</td><td id="38-49">1.000</td><td id="38-4a">1.000</td><td id="38-4b">1.000</td><td id="38-4c">0.000</td><td id="38-4d">0.000</td><td id="38-4e">0.000</td></tr>
<tr><td id="38-4f">40</td><td id="38-4g">GPT-5</td><td id="38-4h">Low</td><td id="38-4i">1.000</td><td id="38-4j">1.000</td><td id="38-4k">1.000</td><td id="38-4l">0.000</td><td id="38-4m">0.000</td><td id="38-4n">0.000</td></tr>
<tr><td id="38-4o">43</td><td id="38-4p">GPT-5</td><td id="38-4q">Low</td><td id="38-4r">1.000</td><td id="38-4s">1.000</td><td id="38-4t">1.000</td><td id="38-4u">0.000</td><td id="38-4v">0.000</td><td id="38-4w">0.000</td></tr>
<tr><td id="38-4x">2</td><td id="38-4y">GPT-5</td><td id="38-4z">Minimal</td><td id="38-4A">1.000</td><td id="38-4B">1.000</td><td id="38-4C">1.000</td><td id="38-4D">0.000</td><td id="38-4E">0.000</td><td id="38-4F">0.000</td></tr>
<tr><td id="38-4G">5</td><td id="38-4H">GPT-5</td><td id="38-4I">Minimal</td><td id="38-4J">0.950</td><td id="38-4K">0.985</td><td id="38-4L">0.980</td><td id="38-4M">0.218</td><td id="38-4N">0.031</td><td id="38-4O">0.044</td></tr>
<tr><td id="38-4P">10</td><td id="38-4Q">GPT-5</td><td id="38-4R">Minimal</td><td id="38-4S">0.400</td><td id="38-4T">0.946</td><td id="38-4U">0.921</td><td id="38-4V">0.503</td><td id="38-4W">0.066</td><td id="38-4X">0.076</td></tr>
<tr><td id="38-4Y">15</td><td id="38-4Z">GPT-5</td><td id="38-50">Minimal</td><td id="38-51">0.450</td><td id="38-52">0.950</td><td id="38-53">0.932</td><td id="38-54">0.510</td><td id="38-55">0.062</td><td id="38-56">0.071</td></tr>
<tr><td id="38-57">20</td><td id="38-58">GPT-5</td><td id="38-59">Minimal</td><td id="38-5a">0.150</td><td id="38-5b">0.925</td><td id="38-5c">0.917</td><td id="38-5d">0.366</td><td id="38-5e">0.086</td><td id="38-5f">0.088</td></tr>
<tr><td id="38-5g">25</td><td id="38-5h">GPT-5</td><td id="38-5i">Minimal</td><td id="38-5j">0.200</td><td id="38-5k">0.964</td><td id="38-5l">0.940</td><td id="38-5m">0.410</td><td id="38-5n">0.044</td><td id="38-5o">0.062</td></tr>
<tr><td id="38-5p">30</td><td id="38-5q">GPT-5</td><td id="38-5r">Minimal</td><td id="38-5s">0.150</td><td id="38-5t">0.991</td><td id="38-5u">0.978</td><td id="38-5v">0.366</td><td id="38-5w">0.018</td><td id="38-5x">0.044</td></tr>
<tr><td id="38-5y">35</td><td id="38-5z">GPT-5</td><td id="38-5A">Minimal</td><td id="38-5B">0.150</td><td id="38-5C">0.994</td><td id="38-5D">0.988</td><td id="38-5E">0.366</td><td id="38-5F">0.012</td><td id="38-5G">0.025</td></tr>
<tr><td id="38-5H">40</td><td id="38-5I">GPT-5</td><td id="38-5J">Minimal</td><td id="38-5K">0.050</td><td id="38-5L">1.000</td><td id="38-5M">0.998</td><td id="38-5N">0.218</td><td id="38-5O">0.000</td><td id="38-5P">0.012</td></tr>
<tr><td id="38-5Q">43</td><td id="38-5R">GPT-5</td><td id="38-5S">Minimal</td><td id="38-5T">0.000</td><td id="38-5U">1.000</td><td id="38-5V">1.000</td><td id="38-5W">0.000</td><td id="38-5X">0.000</td><td id="38-5Y">0.000</td></tr>
<tr><td id="38-5Z">2</td><td id="38-60">GPT-5</td><td id="38-61">No Reasoning</td><td id="38-62">1.000</td><td id="38-63">1.000</td><td id="38-64">1.000</td><td id="38-65">0.000</td><td id="38-66">0.000</td><td id="38-67">0.000</td></tr>
<tr><td id="38-68">5</td><td id="38-69">GPT-5</td><td id="38-6a">No Reasoning</td><td id="38-6b">0.950</td><td id="38-6c">0.995</td><td id="38-6d">0.990</td><td id="38-6e">0.218</td><td id="38-6f">0.012</td><td id="38-6g">0.020</td></tr>
<tr><td id="38-6h">10</td><td id="38-6i">GPT-5</td><td id="38-6j">No Reasoning</td><td id="38-6k">0.500</td><td id="38-6l">0.960</td><td id="38-6m">0.942</td><td id="38-6n">0.513</td><td id="38-6o">0.062</td><td id="38-6p">0.076</td></tr>
<tr><td id="38-6q">15</td><td id="38-6r">GPT-5</td><td id="38-6s">No Reasoning</td><td id="38-6t">0.350</td><td id="38-6u">0.976</td><td id="38-6v">0.952</td><td id="38-6w">0.489</td><td id="38-6x">0.044</td><td id="38-6y">0.062</td></tr>
<tr><td id="38-6z">20</td><td id="38-6A">GPT-5</td><td id="38-6B">No Reasoning</td><td id="38-6C">0.250</td><td id="38-6D">0.979</td><td id="38-6E">0.959</td><td id="38-6F">0.444</td><td id="38-6G">0.038</td><td id="38-6H">0.055</td></tr>
<tr><td id="38-6I">25</td><td id="38-6J">GPT-5</td><td id="38-6K">No Reasoning</td><td id="38-6L">0.200</td><td id="38-6M">0.992</td><td id="38-6N">0.978</td><td id="38-6O">0.410</td><td id="38-6P">0.018</td><td id="38-6Q">0.044</td></tr>
<tr><td id="38-6R">30</td><td id="38-6S">GPT-5</td><td id="38-6T">No Reasoning</td><td id="38-6U">0.150</td><td id="38-6V">0.988</td><td id="38-6W">0.975</td><td id="38-6X">0.366</td><td id="38-6Y">0.025</td><td id="38-6Z">0.050</td></tr>
<tr><td id="38-70">35</td><td id="38-71">GPT-5</td><td id="38-72">No Reasoning</td><td id="38-73">0.200</td><td id="38-74">0.986</td><td id="38-75">0.981</td><td id="38-76">0.410</td><td id="38-77">0.028</td><td id="38-78">0.038</td></tr>
<tr><td id="38-79">40</td><td id="38-7a">GPT-5</td><td id="38-7b">No Reasoning</td><td id="38-7c">0.059</td><td id="38-7d">0.995</td><td id="38-7e">0.995</td><td id="38-7f">0.235</td><td id="38-7g">0.012</td><td id="38-7h">0.012</td></tr>
<tr><td id="38-7i">43</td><td id="38-7j">GPT-5</td><td id="38-7k">No Reasoning</td><td id="38-7l">0.100</td><td id="38-7m">1.000</td><td id="38-7n">1.000</td><td id="38-7o">0.305</td><td id="38-7p">0.000</td><td id="38-7q">0.000</td></tr>
</table>

<a id='2506313a-5e9e-488a-b584-83eb1fdb2ff3'></a>

39

<!-- PAGE BREAK -->

<a id='a7547bc7-b73b-4af1-9d12-d5a380ac836a'></a>

Table 21: Claude 3.7 Performance by Extended Thinking and Sample Size
<table id="39-1">
<tr><td id="39-2">N Presidents</td><td id="39-3">Model</td><td id="39-4">Variant</td><td id="39-5">Exact Match</td><td id="39-6">Spearman&#x27;s ρ</td><td id="39-7">Kendall&#x27;s T</td><td id="39-8">Std (EM)</td><td id="39-9">Std (SR)</td><td id="39-a">Std (KT)</td></tr>
<tr><td id="39-b">2</td><td id="39-c">Claude 3.7</td><td id="39-d">Without ET</td><td id="39-e">1.000</td><td id="39-f">1.000</td><td id="39-g">1.000</td><td id="39-h">0.000</td><td id="39-i">0.000</td><td id="39-j">0.000</td></tr>
<tr><td id="39-k">5</td><td id="39-l">Claude 3.7</td><td id="39-m">Without ET</td><td id="39-n">0.800</td><td id="39-o">0.985</td><td id="39-p">0.980</td><td id="39-q">0.410</td><td id="39-r">0.031</td><td id="39-s">0.044</td></tr>
<tr><td id="39-t">10</td><td id="39-u">Claude 3.7</td><td id="39-v">Without ET</td><td id="39-w">0.600</td><td id="39-x">0.970</td><td id="39-y">0.942</td><td id="39-z">0.503</td><td id="39-A">0.055</td><td id="39-B">0.071</td></tr>
<tr><td id="39-C">15</td><td id="39-D">Claude 3.7</td><td id="39-E">Without ET</td><td id="39-F">0.400</td><td id="39-G">0.955</td><td id="39-H">0.910</td><td id="39-I">0.503</td><td id="39-J">0.071</td><td id="39-K">0.100</td></tr>
<tr><td id="39-L">20</td><td id="39-M">Claude 3.7</td><td id="39-N">Without ET</td><td id="39-O">0.200</td><td id="39-P">0.940</td><td id="39-Q">0.880</td><td id="39-R">0.410</td><td id="39-S">0.086</td><td id="39-T">0.125</td></tr>
<tr><td id="39-U">25</td><td id="39-V">Claude 3.7</td><td id="39-W">Without ET</td><td id="39-X">0.100</td><td id="39-Y">0.925</td><td id="39-Z">0.850</td><td id="39-10">0.305</td><td id="39-11">0.100</td><td id="39-12">0.150</td></tr>
<tr><td id="39-13">30</td><td id="39-14">Claude 3.7</td><td id="39-15">Without ET</td><td id="39-16">0.050</td><td id="39-17">0.910</td><td id="39-18">0.820</td><td id="39-19">0.218</td><td id="39-1a">0.113</td><td id="39-1b">0.172</td></tr>
<tr><td id="39-1c">35</td><td id="39-1d">Claude 3.7</td><td id="39-1e">Without ET</td><td id="39-1f">0.000</td><td id="39-1g">0.895</td><td id="39-1h">0.790</td><td id="39-1i">0.000</td><td id="39-1j">0.125</td><td id="39-1k">0.190</td></tr>
<tr><td id="39-1l">40</td><td id="39-1m">Claude 3.7</td><td id="39-1n">Without ET</td><td id="39-1o">0.000</td><td id="39-1p">0.880</td><td id="39-1q">0.760</td><td id="39-1r">0.000</td><td id="39-1s">0.136</td><td id="39-1t">0.205</td></tr>
<tr><td id="39-1u">43</td><td id="39-1v">Claude 3.7</td><td id="39-1w">Without ET</td><td id="39-1x">0.000</td><td id="39-1y">0.865</td><td id="39-1z">0.730</td><td id="39-1A">0.000</td><td id="39-1B">0.146</td><td id="39-1C">0.218</td></tr>
<tr><td id="39-1D">2</td><td id="39-1E">Claude 3.7</td><td id="39-1F">With ET</td><td id="39-1G">1.000</td><td id="39-1H">1.000</td><td id="39-1I">1.000</td><td id="39-1J">0.000</td><td id="39-1K">0.000</td><td id="39-1L">0.000</td></tr>
<tr><td id="39-1M">5</td><td id="39-1N">Claude 3.7</td><td id="39-1O">With ET</td><td id="39-1P">1.000</td><td id="39-1Q">1.000</td><td id="39-1R">1.000</td><td id="39-1S">0.000</td><td id="39-1T">0.000</td><td id="39-1U">0.000</td></tr>
<tr><td id="39-1V">10</td><td id="39-1W">Claude 3.7</td><td id="39-1X">With ET</td><td id="39-1Y">0.900</td><td id="39-1Z">0.990</td><td id="39-20">0.980</td><td id="39-21">0.305</td><td id="39-22">0.031</td><td id="39-23">0.044</td></tr>
<tr><td id="39-24">15</td><td id="39-25">Claude 3.7</td><td id="39-26">With ET</td><td id="39-27">0.850</td><td id="39-28">0.985</td><td id="39-29">0.970</td><td id="39-2a">0.366</td><td id="39-2b">0.038</td><td id="39-2c">0.055</td></tr>
<tr><td id="39-2d">20</td><td id="39-2e">Claude 3.7</td><td id="39-2f">With ET</td><td id="39-2g">0.800</td><td id="39-2h">0.980</td><td id="39-2i">0.960</td><td id="39-2j">0.410</td><td id="39-2k">0.044</td><td id="39-2l">0.066</td></tr>
<tr><td id="39-2m">25</td><td id="39-2n">Claude 3.7</td><td id="39-2o">With ET</td><td id="39-2p">0.750</td><td id="39-2q">0.975</td><td id="39-2r">0.950</td><td id="39-2s">0.444</td><td id="39-2t">0.050</td><td id="39-2u">0.077</td></tr>
<tr><td id="39-2v">30</td><td id="39-2w">Claude 3.7</td><td id="39-2x">With ET</td><td id="39-2y">0.700</td><td id="39-2z">0.970</td><td id="39-2A">0.940</td><td id="39-2B">0.470</td><td id="39-2C">0.055</td><td id="39-2D">0.088</td></tr>
<tr><td id="39-2E">35</td><td id="39-2F">Claude 3.7</td><td id="39-2G">With ET</td><td id="39-2H">0.650</td><td id="39-2I">0.965</td><td id="39-2J">0.930</td><td id="39-2K">0.489</td><td id="39-2L">0.059</td><td id="39-2M">0.098</td></tr>
<tr><td id="39-2N">40</td><td id="39-2O">Claude 3.7</td><td id="39-2P">With ET</td><td id="39-2Q">0.600</td><td id="39-2R">0.960</td><td id="39-2S">0.920</td><td id="39-2T">0.503</td><td id="39-2U">0.062</td><td id="39-2V">0.108</td></tr>
<tr><td id="39-2W">43</td><td id="39-2X">Claude 3.7</td><td id="39-2Y">With ET</td><td id="39-2Z">0.550</td><td id="39-30">0.955</td><td id="39-31">0.910</td><td id="39-32">0.510</td><td id="39-33">0.066</td><td id="39-34">0.118</td></tr>
</table>

<a id='c5b1de34-1122-4aff-8ca7-90fb0b45d850'></a>

40

<!-- PAGE BREAK -->

<a id='c53a33e9-e2b7-4a87-94ce-385087766009'></a>

A.7.6 Claude 3.7 Sonnet Extended Thinking Trace
Representative ET "thinking" excerpt (30-name subset; abbreviated).

<a id='bbf9f76f-d0cb-40e4-b618-78e9f36ef440'></a>

I need to arrange these presidents by their terms in office. Let me list them with years:
1. George Washington (1789-1797)
2. John Adams (1797-1801)
3. Thomas Jefferson (1801-1809)
4. James Madison (1809-1817)
5. James Monroe (1817-1825)
6. John Quincy Adams (1825-1829)
...
22. Grover Cleveland (1893-1897) - not on the list (second term)
...
35. John F. Kennedy (1961-1963) - not on the list
41. George H. W. Bush (1989-1993)

Now let me arrange the provided list in chronological order:
1. John Adams (1797-1801)
2. Thomas Jefferson (1801-1809)
3. James Madison (1809-1817)
...
24. Calvin Coolidge (1923-1929)
30. George H. W. Bush (1989-1993)

The final chronological list is:
John Adams
Thomas Jefferson
James Madison
James Monroe
John Quincy Adams
...
Dwight D. Eisenhower
Richard Nixon
Jimmy Carter
Ronald Reagan
George H. W. Bush

<a id='ec50c927-256c-49c6-968b-6590a47ac32b'></a>

Note the internal checklisting (first listing MOST presidents by their terms in office up until Bush), the explicit membership exclusion (marking certain presidents as "not on the list"), and a final, cleaned output with 30 presidents (removing those marked earlier).

<a id='f501ab2b-34bb-4148-8f33-453b67041a42'></a>

41

<!-- PAGE BREAK -->

<a id='7f4eb4b9-1da7-4cba-b14c-b8141760f4e8'></a>

A.8 Conditional Sorting
A.8.1 Experimental Design
Let Gc ⊆ P be the ground-truth set of presidents satisfying criterion c (with P the full pool of 43 names). Let the model return an ordered list

<a id='7bfd15da-3cc2-499b-8b6c-6a28cb034329'></a>

o^(t) = (o_1^(t), o_2^(t), ..., o_{m_t}^(t)),
from which we derive the *set* of names it used,

<a id='7f4fdbc9-e373-42d8-99c6-7b96c6a09af3'></a>

$\hat{G}_{c}^{(t)} = \{ o_{j}^{(t)} : 1 \leq j \leq m_{t} \}$.
\}$.

<a id='4091ead2-1e51-467b-a232-b6a05f84c62f'></a>

Define the *missing* and *extra* sets

M^(t) = G_c \ \hat{G}_c^(t),

E^(t) = \hat{G}_c^(t) \ G_c.

<a id='cedb295e-8a99-4056-9811-0e00de567011'></a>

The filtering decision is

FILTER_OK^(t) = 1{ M^(t) = \emptyset \land E^(t) = \emptyset }.

<a id='62d76451-d978-4784-b28f-d9f73d704a78'></a>

Even when FILTER_OK⁽ᵗ⁾ = 1, the ordering can still be wrong (e.g., duplicates). Let

<a id='edad9626-c557-47b9-831b-2f882308111b'></a>

d^{(t)} = m_t - |\hat{G}_c^{(t)}|

<a id='71221c20-fc62-4833-a8f4-743bf3b41d80'></a>

be the number of duplicate names in o^{(t)}. Let π* be the true chronological permutation of G_c, and let π^{(t)} be the permutation induced by the *first occurrences* of each element of G_c in o^{(t)}. We mark ordering success as

<a id='e58149fb-e65f-4776-8f1e-273198d3a81f'></a>

ORDER_OK^{(t)} = 1\{ FILTER_OK^{(t)} = 1 \land d^{(t)} = 0 \land \pi^{(t)} = \pi^* \}.

<a id='dbb507bd-19f6-4d45-8784-405af0990d95'></a>

All rank-based metrics (Spearman's $\rho$, Kendall's $\tau$, Cayley distance) are then computed on the aligned sequences ($r_{true,i}$)$_{i \in G_c}$, ($r_{pred,i}$)$_{i \in G_c}$ only if FILTER_OK$^{(t)}$ = 1. Trials failing the filter are excluded from the ordering comparison.

<a id='bea22a32-c6ae-472f-8a2e-b5d8fcbe4028'></a>

42

<!-- PAGE BREAK -->

<a id='7784c26e-dc41-411f-bfe4-00786583bc63'></a>

## A.9 Conditional Sorting Results

Table 22: Top-5 extra presidents hallucinated by GPT-4.1 under FIRSTNAMESSTARTINGWITH-ABORC (self-filtered condition).

<a id='107ffcb0-ffef-4cd4-bc6f-8c03010474c2'></a>

<table id="42-1">
<tr><td id="42-2">President (extra)</td><td id="42-3">Count / 100</td></tr>
<tr><td id="42-4">Joe Biden</td><td id="42-5">90</td></tr>
<tr><td id="42-6">Martin Van Buren</td><td id="42-7">88</td></tr>
<tr><td id="42-8">George W. Bush</td><td id="42-9">72</td></tr>
<tr><td id="42-a">James Buchanan</td><td id="42-b">63</td></tr>
<tr><td id="42-c">Herbert Hoover</td><td id="42-d">46</td></tr>
</table>

<a id='0eef9cd0-07ae-4ab7-aebf-db41cf9c12dc'></a>

<::histogram: Distribution of n_predicted vs n_ground_truth. The x-axis is labeled "Number of Presidents" and ranges from 0 to 30. The y-axis is labeled "Count" and ranges from 0 to 100. The legend indicates "Predicted" values are represented by blue bars and "Ground Truth" values by orange bars. A tall orange bar representing "Ground Truth" is centered around 8 on the x-axis, reaching a count of approximately 100. Blue bars representing "Predicted" values are distributed across the x-axis, with a cluster of higher counts between 11 and 15 (e.g., around 12-14 reaching counts of approximately 15-18), and smaller counts spreading out to around 20 and a small bar at 25. There is a very small blue bar at 8, mostly obscured by the orange bar.::>Figure 17: Distribution of $n_{predicted}$ (blue) versus the fixed $n_{ground}$ truth (orange band) in the FIRST-NAMESSTARTINGWITHABORC filter with GPT-4.1. The mass of blue bars away from the orange band reflects frequent over/under-selection.

<a id='30d03683-4697-41e6-9428-a69145ec54b9'></a>

<::bar chart: Most Frequently Missing Presidents (Filtering). The x-axis is labeled "President" and shows Andrew Johnson, Andrew Jackson, Chester A. Arthur, and Abraham Lincoln. The y-axis is labeled "Count" and ranges from 0.0 to 3.0. There are four red bars representing the count for each president: Andrew Johnson has a count of 3.0, Andrew Jackson has a count of 2.0, Chester A. Arthur has a count of 1.0, and Abraham Lincoln has a count of 1.0.: chart::>

<a id='91ade977-37f8-4c98-97b0-97d0dd90be86'></a>

Figure 18: Most frequently missing presidents in the filtering step for FIRSTNAMESSTARTINGWITH-ABORC task with GPT-4.1

<a id='822f6d8b-de87-4e73-a9bb-e32c5c9abda9'></a>

43

<!-- PAGE BREAK -->

<a id='fc787076-4500-4434-aca9-dc8c5f9f60c1'></a>

<::chart: Distribution of Given Names Trial Types. A pie chart showing the distribution of trial types. The segments are: Correct Ordering (44.0%, green), Pure Ordering Errors (41.0%, orange), and Length Mismatch Errors (15.0%, red). Figure 19: Trial-type composition in GPT-4.1's OHIOORVIRGINIA given-names task: 44% correct, 41% pure ordering errors, 15% length-mismatch.::>

<a id='26541678-ff75-4254-aa9a-f6f276072e6e'></a>

Table 23: Most frequent length-mismatch names in GPT-4.1's OHIOORVIRGINIA given-names task.
<table id="43-1">
<tr><td id="43-2" colspan="3">Extras</td></tr>
<tr><td id="43-3">President</td><td id="43-4">Count</td><td id="43-5">Note</td></tr>
<tr><td id="43-6">Millard Fillmore</td><td id="43-7">2</td><td id="43-8">appended</td></tr>
<tr><td id="43-9">James Buchanan</td><td id="43-a">1</td><td id="43-b">appended</td></tr>
<tr><td id="43-c">Abraham Lincoln</td><td id="43-d">1</td><td id="43-e">appended</td></tr>
<tr><td id="43-f">Andrew Johnson</td><td id="43-g">1</td><td id="43-h">appended</td></tr>
<tr><td id="43-i" colspan="3">Missing</td></tr>
<tr><td id="43-j">William Henry Harrison</td><td id="43-k">2</td><td id="43-l">omitted</td></tr>
<tr><td id="43-m">John Tyler</td><td id="43-n">2</td><td id="43-o">omitted</td></tr>
<tr><td id="43-p">Zachary Taylor</td><td id="43-q">1</td><td id="43-r">omitted</td></tr>
<tr><td id="43-s">James Madison</td><td id="43-t">1</td><td id="43-u">omitted</td></tr>
</table>

<a id='7d390afc-b0c6-4337-a733-616db872e51c'></a>

<::chart: A bar chart titled "Per-Position Average Accuracy (Given Names, No Length Mismatch)". The Y-axis is labeled "Accuracy" and ranges from 0.0 to 1.0. The X-axis is labeled "Position in Order" and ranges from 0 to 16. There are 15 bars representing the accuracy for positions 1 through 15.
- Position 1: ~1.0
- Position 2: ~1.0
- Position 3: ~1.0
- Position 4: ~1.0
- Position 5: ~0.84
- Position 6: ~0.9
- Position 7: ~0.78
- Position 8: ~0.7
- Position 9: ~0.8
- Position 10: ~0.7
- Position 11: ~0.75
- Position 12: ~0.77
- Position 13: ~0.72
- Position 14: ~0.72
- Position 15: ~0.75

Figure 20: **Per-position accuracy** *A*(*r*) for GPT-4.1's OHIOORVIRGINIA given-names task, restricted to trials with no length mismatch (*N*=85). Bars show the fraction of trials in which the occupant of each rank *r* matches ground truth.::>

<a id='0e0ff2e0-3e48-4216-b3db-6e225cc64f0d'></a>

44

<!-- PAGE BREAK -->

<a id='55e46971-83a1-4cb9-823d-0bf51ee4e2c0'></a>

A.9.1 With LRMs
<::This visual content contains two bar charts side-by-side.

Chart 1: Avg Kendall (Claude Only) by Task, Criterion, ET
- Y-axis: Avg Kendall (ranging from 0.0 to 1.0)
- X-axis: group, with categories: ABC_names_given_names, ABC_names_self_filtered, Ohio_or_Virginia_given_names, Ohio_or_Virginia_self_filtered.
- Legend: ET (blue bars), NoET (orange bars).
- Data points:
  - ABC_names_given_names: ET is approximately 1.0, NoET is approximately 0.9.
  - ABC_names_self_filtered: ET is approximately 1.0, NoET is approximately 0.55.
  - Ohio_or_Virginia_given_names: ET is approximately 1.0, NoET is approximately 0.9.
  - Ohio_or_Virginia_self_filtered: ET is approximately 1.0, NoET is approximately 1.0.

Chart 2: Avg Spearman (Claude Only) by Task, Criterion, ET
- Y-axis: Avg Spearman (ranging from 0.0 to 1.0)
- X-axis: group, with categories: ABC_names_given_names, ABC_names_self_filtered, Ohio_or_Virginia_given_names, Ohio_or_Virginia_self_filtered.
- Legend: ET (blue bars), NoET (orange bars).
- Data points:
  - ABC_names_given_names: ET is approximately 1.0, NoET is approximately 0.98.
  - ABC_names_self_filtered: ET is approximately 1.0, NoET is approximately 0.7.
  - Ohio_or_Virginia_given_names: ET is approximately 1.0, NoET is approximately 0.95.
  - Ohio_or_Virginia_self_filtered: ET is approximately 1.0, NoET is approximately 1.0.
: bar_chart::>

<a id='2af5efde-4fe0-47d5-8bc0-0e6ee795a4d5'></a>

(a) Avg. Kendall's τ by group. (b) Avg. Spearman by group. <::Exact Match Rate (Claude Only) by Task, Criterion, ET: bar chart::> <::bar chart with title "Exact Match Rate (Claude Only) by Task, Criterion, ET", y-axis labeled "Exact Match Rate" ranging from 0.0 to 1.0, and x-axis labeled "group" with four categories: "ABC_names_given_names", "ABC_names_self_filtered", "Ohio_or_Virginia_given_names", and "Ohio_or_Virginia_self_filtered". Each category has two bars, one for "ET" (blue) and one for "NoET" (orange), as indicated by the legend. For "ABC_names_given_names", ET is 1.0 and NoET is approximately 0.38. For "ABC_names_self_filtered", ET is 1.0 and NoET is approximately 0.01. For "Ohio_or_Virginia_given_names", ET is approximately 0.82 and NoET is approximately 0.02. For "Ohio_or_Virginia_self_filtered", ET is 1.0 and NoET is approximately 0.01.::> (c) Exact-match rate by group.

<a id='75cca189-fd91-4d25-b27e-72326dc94d94'></a>

Figure 21: Claude 3.7 with and without Extended Thinking (ET). Groups are FIRSTNAMESSTART-INGWITHABORC and OHIOORVIRGINIA, each in given_names and self_filtered modes. ET pushes both rank correlations to ≈ 1.0 and lifts exact-match from near zero (no-ET, self-filtered) to ≳ 0.97 (self-filtered) and 0.82–0.99 (given-names).

<a id='f7337c2d-f5dd-4225-a3ca-4b6d8823f626'></a>

45

<!-- PAGE BREAK -->

<a id='d7fa7c70-f2f4-437e-bb1d-ea5c880b4889'></a>

References
Anthropic. (2025). Claude 3.7 Sonnet and Extended Thinking. https://www.anthropic.com/news/claude-3-7-sonnet.
Chen, W., Wang, X., & Wang, W. Y. (2021). TimeQA: A Dataset for Answering Time-Sensitive Questions. NeurIPS Datasets & Benchmarks. https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/1f0e3dad99908345f7439f8ffabdffc4-Abstract-round2.html.
Chu, Z., Chen, J., Chen, Q., Yu, W., Wang, H., Liu, M., & Qin, B. (2024). TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in LLMs. ACL 2024. https://aclanthology.org/2024.acl-long.66/.
Engelberg, J., Manela, A., Mullins, W., & Vulicevic, L. (2025). Entity Neutering. SSRN 5182756. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5182756.
Glasserman, P., & Lin, C. (2024). Assessing Look-Ahead Bias in Stock Return Predictions Generated by GPT Sentiment Analysis. Journal of Financial Data Science 6(1).
Halawi, D., Zhang, F., Chen, Y.-H., & Steinhardt, J. (2024). Approaching Human-Level Forecasting with Language Models. arXiv:2402.18563. https://arxiv.org/abs/2402.18563.
He, S., Lv, L., Manela, A., & Wu, J. (2025). Chronologically Consistent Large Language Models. arXiv:2502.21206. https://arxiv.org/abs/2502.21206.
Islakoglu, D. S., Yates, A., & de Rijke, M. (2025). ChronoSense: Exploring Temporal Understanding in LLMs. ACL 2025 (Short). https://aclanthology.org/2025.acl-short.46/.
Levy, B. (2025). Caution Ahead: Numerical Reasoning and Look-Ahead Bias in AI Models. SSRN 5082861. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5082861.
Lopez-Lira, A., & Tang, Y. (2024). Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models. arXiv:2304.07619. https://arxiv.org/abs/2304.07619.
Lopez-Lira, A., Tang, Y., & Zhu, M. (2025). The Memorization Problem: Can We Trust LLMs' Economic Forecasts? arXiv:2504.14765. https://arxiv.org/abs/2504.14765.
Ludwig, J., Mullainathan, S., & Rambachan, A. (2025). An Applied Econometric Framework for Using Large Language Models in Research. NBER Working Paper 33344. https://www.nber.org/papers/w33344.
Nangia, N., & Bowman, S. R. (2018). ListOps: A Diagnostic Dataset for Latent Tree Learning. NAACL-HLT SRW. https://aclanthology.org/N18-4013/.
OpenAI. (2025a). Introducing GPT-5. https://openai.com/index/introducing-gpt-5/.
OpenAI. (2025b). Introducing GPT-5 for developers. https://openai.com/index/introducing-gpt-5-for-developers/.
OpenAI. (2025c). Reasoning models --- API Guide. https://platform.openai.com/docs/guides/reasoning.
OpenAI. (2025d). GPT-5 prompting guide. https://cookbook.openai.com/examples/gpt-5/gpt-5_prompting_guide.
Sarkar, S. K., & Vafa, K. (2024). Lookahead Bias in Pretrained Language Models. SSRN 4754678. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4754678.
Shojaee, P., Mirzadeh, I., Alizadeh, K., Horton, M., Bengio, S., & Farajtabar, M. (2025). The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity. arXiv:2506.06941. https://arxiv.org/abs/2506.06941.
Wang, Y., & Zhao, Y. (2024). TRAM: Benchmarking Temporal Reasoning for Large Language Models. arXiv:2310.00835. https://arxiv.org/abs/2310.00835.

<a id='b43d03be-d733-4440-b3aa-273e29ee1078'></a>

46

<!-- PAGE BREAK -->

<a id='3c2e28e9-2dec-47fc-a291-665e4b26fdc8'></a>

Weston, J., Bordes, A., Chopra, S., Rush, A. M., van Merriënboer, B., Joulin, A., & Mikolov, T. (2016). Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks. ICLR (Workshop). https://arxiv.org/abs/1502.05698.

<a id='461aef1d-3438-4233-b6a6-1be65e8d112b'></a>

47