<a id='6d3b43c5-adc9-4274-9ea1-c3e68b5369e2'></a>

BookRAG: A Hierarchical Structure-aware Index-based Approach
for Retrieval-Augmented Generation on Complex Documents

<a id='38261173-78a0-442b-8f53-f12017220f12'></a>

Shu Wang
The Chinese University of Hong
Kong, Shenzhen
shuwang3@link.cuhk.edu.cn

<a id='9671e5f5-79c4-4d17-bff6-41cf2ad2964d'></a>

Yingli Zhou
The Chinese University of Hong
Kong, Shenzhen
yinglizhou@link.cuhk.edu.cn

<a id='1cb79c20-8c2c-4bcf-97b1-a2fadcbac2ce'></a>

Yixiang Fang
The Chinese University of Hong
Kong, Shenzhen
fangyixiang@cuhk.edu.cn

<a id='b2f9f683-f871-4af2-8de1-8fefb711ed2d'></a>

## Abstract

As an effective method to boost the performance of Large Language Models (LLMs) on the question answering (QA) task, Retrieval-Augmented Generation (RAG), which queries highly relevant information from external complex documents, has attracted tremendous attention from both industry and academia. Existing RAG approaches often focus on general documents, and they overlook the fact that many real-world documents (such as books, booklets, handbooks, etc.) have a hierarchical structure, which organizes their content from different granularity levels, leading to poor performance for the QA task. To address these limitations, we introduce BookRAG, a novel RAG approach targeted for documents with a hierarchical structure, which exploits logical hierarchies and traces entity relations to query the highly relevant information. Specifically, we build a novel index structure, called BookIndex, by extracting a hierarchical tree from the document, which serves as the role of its table of contents, using a graph to capture the intricate relationships between entities, and mapping entities to tree nodes. Leveraging the BookIndex, we then propose an agent-based query method inspired by the Information Foraging Theory, which dynamically classifies queries and employs a tailored retrieval workflow. Extensive experiments on three widely adopted benchmarks demonstrate that BookRAG achieves state-of-the-art performance, significantly outperforming baselines in both retrieval recall and QA accuracy while maintaining competitive efficiency.

<a id='1b3bb464-568c-4024-9c6c-7f9c54e17965'></a>

# 1 Introduction
Large Language Models (LLMs) such as Qwen 3 [60] and Gemini 2.5 [13] have revolutionized the Question Answering (QA) system [15, 61, 65]. The industry has increasingly adopted LLMs to build QA systems that assist users and reduce manual effort in many applications [65, 67], such as financial auditing [29, 37], legal compliance [8], and scientific discovery [56]. However, directly relying on LLMs may lead to missing domain knowledge and generating outdated or unsupported information. To address these issues, Retrieval-Augmented Generation (RAG) has been widely adopted [17, 22] by retrieving relevant domain knowledge from external sources and using it to guide the LLM during response generation. On the other hand, in real-world enterprise scenarios, domain knowledge is often stored in long-form documents, such as technical handbooks, API reference manuals, and operational guidebooks [49]. A notable feature of such documents is that they follow the structure of books, characterized by intricate layouts and rigorous logical hierarchies (e.g., explicit tables of contents, nested chapters, and multi-level sections). In this paper, we aim to design an effective RAG system for QA over long and highly structured documents.

<a id='2fb75f15-f128-468c-ae2b-972288904ac1'></a>

<::The image is a flowchart comparing three approaches to complex document Question Answering (QA), all originating from a "Complex Query" and a "Complex Multi-page Document" as input. The three approaches are: (a) Text-Only RAG, (b) Layout Segmented RAG, and (c) BookRAG (Natively Structure-aware).

**Common Input:**
- A person icon with a question mark above represents a "Complex Query".
- An arrow points from the query to a stack of documents labeled "Complex Multi-page Document".
- An arrow from the document stack, labeled "Input", points downwards, indicating the input to the three RAG systems.

**(a) Text-Only RAG:**
- The process starts with a "Plain Text Extraction (OCR)" box.
- An arrow leads to a document icon labeled "Unstructured Chunks".
- An arrow leads to a "Text Index (Vector/Graph/Tree)" box.
- An arrow leads to a "Fixed/Graph Retrieval" box.
- An arrow leads to an "LLM" (Large Language Model) represented by a robot icon.
- An arrow leads to an "Answer" box.
- The outcome is a red box stating "Fails on Structural dependencies" and a red 'X' symbol next to the "Answer" box, indicating failure.

**(b) Layout Segmented RAG:**
- The process starts with a "Layout Analysis & Parsing" box.
- An arrow leads to a stack of documents icon labeled "Flattened Chunks".
- An arrow leads to a "Flattened Vector Index" box.
- An arrow leads to a "Fixed Retrieval" box.
- An arrow leads to an "LLM" (robot icon).
- An arrow leads to an "Answer" box.
- The outcome is a red box stating "Loses complex relationships" and a red 'X' symbol next to the "Answer" box, indicating failure.

**(c) BookRAG (Natively Structure-aware):**
- The process starts with a "Layout Analysis & Parsing" box.
- An arrow leads to a document icon with a tree-like structure labeled "Hierarchical Chunks".
- An arrow leads to a "BookIndex" represented by a hierarchical graph structure with interconnected nodes.
- An arrow leads to an "Agent-based Retrieval" box.
- An arrow leads to an "LLM" (robot icon).
- An arrow leads to an "Answer" box.
- The outcome is a green box stating "Accurate, structured-grounded" and a green checkmark symbol next to the "Answer" box, indicating success.
: flowchart::>

Figure 1: Comparison of existing methods and BookRAG for complex document QA.

<a id='52139852-ab99-4fa4-af69-c5657d6f319d'></a>

• **Prior works.** The existing RAG approaches for document-level QA generally fall into two paradigms, as illustrated in Figure 1. The first paradigm relies on OCR (Optical Character Recognition) to convert the document into plain text, after which any text-based RAG method can be directly applied. Among text-based RAG methods, state-of-the-art approaches increasingly adopt *graph-based* RAG [6, 62, 66], where graph data serves as an external knowledge source because it captures rich semantic information and the relational structure between entities. As shown in Table 1, two representative methods are GraphRAG [16] and RAPTOR [45]. Specifically, GraphRAG first constructs a knowledge graph (KG) from the textual corpus, and then applies the Leiden community detection algorithm [51] to obtain hierarchical clusters. Summaries are generated for each community, providing a comprehensive, global overview of the entire corpus. RAPTOR builds a recursive tree structure by iteratively clustering document chunks and summarizing them at each level, enabling the model to capture both fine-grained and high-level semantic information across the corpus.

<a id='de4bd7a8-4729-4fdb-b593-1439076ee6fc'></a>

In contrast, the second paradigm, _layout-aware segmentation_ [5,
52], first parses the document into structured blocks that preserve
the original layout and information of the document, such as para-
graphs, tables, figures, or equations. By doing so, it not only avoids
the fixed chunk size used in the first paradigm, which often leads

<a id='fd1a876c-0553-4a89-8f6d-ba719c680160'></a>

arXiv:2512.03413v1 [cs.IR] 3 Dec 2025

<!-- PAGE BREAK -->

<a id='264bcc29-c8b7-42be-b04e-5b5ae9420ee2'></a>

Shu Wang, Yingli Zhou, and Yixiang Fang

<a id='a3e9c7ab-a29d-4d85-a74f-aa264f03664a'></a>

Table 1: Comparison of representative methods and our BookRAG.
<table id="1-1">
<tr><td id="1-2">Type</td><td id="1-3">Representative Method</td><td id="1-4">Core Feature</td><td id="1-5">Multi-hop Reasoning</td><td id="1-6">Document Parsing</td><td id="1-7">Query Workflow</td></tr>
<tr><td id="1-8">Graph-based</td><td id="1-9">RAPTOR [45] GraphRAG [16]</td><td id="1-a">Recursive summarization Global community detection</td><td id="1-b">✓ ✓</td><td id="1-c">X X</td><td id="1-d">Static Static</td></tr>
<tr><td id="1-e" rowspan="2">Layout segmented</td><td id="1-f">MM-Vanilla</td><td id="1-g">Multi-modal retrieval</td><td id="1-h">X</td><td id="1-i">✓</td><td id="1-j">Static</td></tr>
<tr><td id="1-k">DocETL [47]</td><td id="1-l">LLM-based document processing pipeline</td><td id="1-m">X</td><td id="1-n">✓</td><td id="1-o">Manual</td></tr>
<tr><td id="1-p">Doc-Native</td><td id="1-q">BookRAG (Ours)</td><td id="1-r">Structure-award Index &amp; Agent-based retrieval</td><td id="1-s">✓</td><td id="1-t">✓</td><td id="1-u">Dynamic</td></tr>
</table>

<a id='bbb004eb-b5ca-4664-bded-01d833e1e67c'></a>

to fragmented information, but also retains document-native struc-
tural information. These blocks often exhibit multimodal charac-
teristics, and a typical approach is to apply multimodal retrieval to
obtain relevant content for answering queries. Recently, a state-of-
the-art method in this category, DocETL [47], provides a declarative
interface that allows users to manually define LLM-based process-
ing pipelines to analyze the retrieved blocks. These pipelines consist
of LLM-powered operations combined with task-specific optimiza-
tions.

<a id='84186167-1cf0-4622-bcb8-a88f029b20bf'></a>

* Limitations of existing works. However, these methods suffer from two fundamental limitations (L for short): **L1: Failure to capture the deep connection of document structure and semantics**. Text-based approaches cannot capture the structural layout of the document, resulting in the loss of important relation- ships stored in the hierarchical blocks, such as tables nested within a specific section. While layout-segmented methods preserve doc- ument structure, they cannot capture the relationships between different blocks in the document, which limits their capability for multi-hop reasoning across these blocks and ultimately affects their overall performance. **L2: Static of query workflows**. In real-world QA scenarios, user queries are highly heterogeneous, ranging from simple keyword lookups to complex multi-hop questions that re- quire synthesizing evidence scattered across different parts of the document. Applying a uniform strategy, such as static or manually predefined workflows, to diverse needs is inefficient; for example, complex queries often require question decomposition, whereas simple queries do not.

<a id='5cfe5b95-6bab-4013-a767-397c01e99844'></a>

• **Our technical contributions.** To bridge this gap, we introduce BookRAG, the first retrieval-augmented generation method built upon a document-native **BookIndex**, designed to document QA tasks. Specifically, to capture the deep connection of the relation in the document, BookIndex organizes information through two complementary structures. First, to preserve the document's native logical hierarchy, we organize the parsed content blocks into a hierarchical tree structure, which serves as the role of its table of contents. Second, to capture the intricate relations within these blocks, we construct a KG containing fine-grained entities. Finally, we unify these two structures by mapping the KG entities to their corresponding tree nodes.

<a id='3b7a5785-8f84-4a4f-93e5-46eb0d5b0be7'></a>

However, effective multi-hop reasoning on the graph relies on a high-quality KG [62, 66], which is often compromised by entity ambiguity (e.g., distinct entities with names like "LLM" and "Large Language Model"). To address this, we propose a novel gradient-based entity resolution method that analyzes the similarity distribution of

<a id='ba1c29fa-6702-4674-ae47-44d306046d7f'></a>

candidate entities. By identifying sharp drops in similarity scores,
we can efficiently distinguish and merge coreferent entities, thereby
ensuring graph connectivity and enhancing reasoning capabilities.

<a id='20d8abb4-4e3b-4d75-acd0-1bfe92b21fc2'></a>

Building upon the BookIndex, we address the static of query workflows (L2) by implementing an **agent-based retrieval**. Specifically, our agent first classifies user queries based on their intent and complexity, and then dynamically generates tailored retrieval workflows. Grounded in *Information Foraging Theory* [42], our retrieval process mimics foraging by using *Selector* to narrow down the search space via information scents and *Reasoner* to locate highly relevant evidence.

<a id='5de8161f-ae20-47e5-865d-8b1573b81123'></a>

We conduct extensive experiments on three widely adopted datasets to validate the effectiveness and efficiency of our BookRAG, comparing it against several state-of-the-art baselines. The experimental results demonstrate that BookRAG consistently achieves superior performance in both retrieval recall and QA accuracy across all datasets. Furthermore, our detailed analysis validates the critical contributions of our key features, such as the high-quality KG and the agent-based retrieval mechanism.

<a id='e0f00f45-8d20-46e0-8d93-498c383e427c'></a>

We summarize our contributions as:

* We introduce **BookRAG**, a novel method that constructs a document-native _BookIndex_ by integrating a hierarchical tree of document layout blocks with a KG storing fine-grained entity relations.
* We propose an **Agent-based Retrieval** approach inspired by Information Foraging Theory, which dynamically classifies queries and configures optimal retrieval workflows to locate highly relevant evidence within documents.
* Extensive experiments on multiple benchmarks show that _BookRAG_ significantly outperforms existing baselines, attaining state-of-the-art performance in solving complex document QA tasks while maintaining competitive efficiency.

<a id='fa127ffa-e577-43f0-b846-a7104a944b63'></a>

Outline. We review related work in Section 2. Section 3 intro-
duces the problem formulation, IFT, and RAG workflow. In Section 4,
we present the structure of our BookIndex and its construction. Sec-
tion 5 presents our agent-based retrieval, elaborating on the query
classification and operators used in the structured execution of
BookRAG. We present the experimental results and detailed analy-
sis in Section 6, and conclude the paper in Section 7.

<a id='3c10accf-89f7-48a4-9bf0-3ed18f6cb105'></a>

## 2 Related Work
In this section, we review the related works, including LLM in document analysis and the modern representative RAG approaches.

<!-- PAGE BREAK -->

<a id='96ff62a1-9fb4-48a8-8535-5478c0ea3626'></a>

BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents

<a id='2dcff830-fd96-4d3f-a698-bc8092e938ea'></a>

• **LLM in document analysis.** Recent advances in LLMs have offered opportunities to leverage LLMs in document data analy- sis. Due to the robust semantic reasoning capabilities of LLMs, there is an increasing number of works focusing on transferring unstructured documents (e.g., HTML, PDFs, and raw text) into structured formats, such as relational tables [1, 7, 25, 38]. For ex- ample, Evaporate [1] utilizes LLMs to synthesize extraction code, enabling cost-effective conversion of semi-structured web docu- ments into structured databases without heavy manual annotation. In addition, several LLM-based document analysis systems have been proposed to equip standard data pipelines with semantic un- derstanding [28, 40, 47, 53]. For instance, LOTUS [40] extends the relational model with semantic operators, allowing users to execute SQL-like queries with LLM-powered predicates (e.g., filter, join) over unstructured text corpora. Similarly, DocETL [47] introduces an agentic framework to optimize complex information extraction tasks. Furthermore, another line of research proposes to directly analyze or parse documents by viewing the document pages as images, thereby preserving critical layout and visual information [26, 31, 54].

<a id='67bf7be4-6ced-40a1-a5e1-4832e14cd60a'></a>

• **RAG approaches.** RAG has been proven to excel in many tasks, including open-ended question answering [24, 48], programming context [9, 10], SQL rewrite [30, 50], and data cleaning [35, 36, 43]. The naive RAG technique relies on retrieving query-relevant contexts from external knowledge bases to mitigate the “hallucination” of LLMs. Recently, many RAG approaches [16, 18, 19, 21, 27, 32, 32, 45, 55, 58, 66] have adopted graph structures to organize the information and relationships within documents, achieving improved overall retrieval performance. For more details, please refer to the recent survey of graph-based RAG methods [41]. Besides, the Agentic RAG paradigm has been widely studied, employing autonomous agents to dynamically orchestrate and refine the RAG pipeline, thus significantly boosting the reasoning robustness and generation fidelity [2, 23, 59].

<a id='a763c0d8-4340-45d2-afe5-42e2b4b548b5'></a>

# 3 Preliminaries
This section formalizes the research problem of complex document QA, introduces the foundational Information Foraging Theory (IFT), and briefly reviews the general workflow of RAG systems

<a id='72c8a45b-37eb-4960-83df-67b1b510f471'></a>

## 3.1 Problem Formulation
We study the problem of Question Answering (QA) over complex documents, which aims to answer user queries based on long-form documents [5, 11, 33]. Formally, a document D is represented as a sequence of N pages, D = {Pᵢ}ᴺᵢ₌₁. These pages collectively contain a sequence of content blocks B = {bⱼ}ᴹⱼ₌₁, where each block bⱼ represents a distinct element (e.g., text segment, section header, table, or image) organized within a logical chapter hierarchy. Given a user query q, the goal is to generate an accurate answer A, ideally grounded in a specific set of evidence blocks E ⊂ B. The task is formulated as developing a method S that maps the structured document and the query to the final answer:

<a id='f16ad9ba-d5ec-49ae-8b59-6e0aa170bc8a'></a>

A = S(D, q)

(1)

<a id='97d2663a-e94c-4386-a46a-e1d81b6a4dfc'></a>

where _S_ should navigate both the sequential page content and the logical hierarchy of _D_ to synthesize the response.

<a id='d2ebb209-e271-44e4-8c26-abfd1b6c7431'></a>

## 3.2 Information Foraging Theory

Information Foraging Theory (IFT) [42] provides a framework for understanding information access as a process analogous to animal foraging. It suggests that users follow cues, known as **information scent** (e.g., keywords or icons), to navigate between clusters of content, known as **information patches** (e.g., sections in handbooks). The goal is to maximize the rate of valuable information gain while minimizing effort, guiding the decision to either stay within a patch or seek a new one.

<a id='af8ad164-c972-4722-a10f-7c471670d8b1'></a>

Consider experts seeking a solution to a specific problem within a large technical handbook. They first extract key terms related to the problem, which act as information scent. This scent guides them to navigate towards one or more promising sections (the information patches). Within these patches, they analyze the diverse content to extract the precise knowledge required to formulate a final answer

<a id='e88c33bf-6357-4c8e-8fc6-d7f1d12ccbd5'></a>

## 3.3 RAG workflow
Retrieval-Augmented Generation (RAG) systems typically operate in a two-phase framework [6, 16, 41]. In the **Offline Indexing** phase, unstructured corpus data is organized into a structured index, which can take various forms such as vector databases or KG [66]. Subsequently, in the **Online Retrieval** phase, the system retrieves relevant components (e.g., text chunks or subgraphs) based on the user query _q_ to inform the LLM's generation. However, these general workflows often treat the index as a structure derived purely from content, potentially detaching it from the document's original logical hierarchy. In contrast, our approach seeks to deeply integrate these retrieval structures with the document's native tree topology.

<a id='e77323f2-46f0-4bd7-8b78-e49aa062d164'></a>

## 4 BookIndex
This section introduces our proposed **BookIndex**, a hierarchical structure-aware index designed to capture both the explicit logical hierarchy and the intricate entity relations within complex doc-uments. We first formally define the structure of the BookIndex (B). Subsequently, we elaborate on the sequential, two-stage con-struction process: (1) **Tree Construction**, which parses the docu-ment's layout to establish a hierarchical nodes, each categorized by type; and (2) **Graph Construction**, which extracts fine-grained entity knowledge from the tree nodes and refines it through a novel gradient-based entity resolution method.

<a id='2a416d6b-e169-4d06-b180-47aa6ca5a8ce'></a>

## 4.1 Overview of BookIndex
We formally define our **BookIndex** as a triplet $B = (T, G, M)$. Here,
$T = (N, E_T)$ represents a *Tree* structure where $N$ is the set of nodes
derived from the document's explicit logical hierarchy (e.g., titles,
sections, tables), and $E_T$ denotes their nesting relationships. $G =$
$(V, E_G)$ is a *Knowledge Graph* that captures fine-grained entities ($V$)
and their relations ($E_G$) scattered throughout the document. Finally,
$M: V \rightarrow \mathcal{P}(N)$ is the *Graph-Tree Link* (*GT-Link*), which links each
entity in $V$ to the set of specific tree nodes in $N$ from which it
was extracted. These links are crucial for capturing the intricate,
cross-sectional relations within the document. The hierarchical
tree nodes in $T$ serve as the document's native *information patches*,
providing structured contexts for information seeking. Meanwhile,
the entities and relations in $G$, connected via $M$, act as the rich

<!-- PAGE BREAK -->

<a id='7039807d-24e5-489b-ad15-cc74617902b7'></a>

Shu Wang, Yingli Zhou, and Yixiang Fang

<a id='3c8b8c13-1d5d-480d-80d6-696029ad763a'></a>

<::flowchart: The diagram illustrates the BookIndex Construction process, divided into two main phases: Tree Construction and Graph Construction. A legend at the top right indicates icons for Tree Node, GT-Link (dotted line), Entity (circle), and Relation (line). BookIndex Construction:Tree Construction: This phase starts with multiple document pages as input. 1. Layout Parsing: The pages are processed to extract different content types: Table, Text, Title, and Image. Examples of parsed content blocks are shown. 2. Section Filtering: The parsed content blocks are further processed. - A 'TITLE' block with 'Title: Method FontSize: 14' is transformed into a 'Section' with 'Level: 2 Type: Section'. - A 'TITLE' block with 'Title: Experiment FontSize: 14' is transformed into a 'Section' with 'Level: 2 Type: Section'. - A 'TITLE' block with 'Title: MOE Layer FontSize: 20' is transformed into a 'Text' block with 'Level: None Type: Text', but is marked with a red 'X', indicating it's filtered out. The output of Section Filtering is a hierarchical tree structure. The root node is a document icon, branching into sub-sections (also document icons), which then branch into leaf nodes representing specific content types: image, text, and table icons. Graph Construction: This phase takes the Tree Nodes as input. 1. KG Construction: The Tree Nodes are used to construct an initial Knowledge Graph (KG). This is depicted as a graph of interconnected entities labeled e1 through e9. 2. Gradient-based Entity Resolution: This step refines the entities. - A 'Match?' step takes a group of entities (e1, e2, ..., e8, e9) as input. - A plot shows 'Similarity' on the y-axis against 'Entity' on the x-axis, with points e7, e6, e8, e5, e3 indicating varying similarity scores. - A 'Merge' operation combines entities, for example, e7 and e9 are merged into e'7. The final output of Graph Construction is the 'BookIndex', represented as a refined graph of interconnected entities.Figure 2: The BookIndex Construction process. This phase includes Tree Construction, derived from Layout Parsing and Section Filtering, and Graph Construction, which involves KG Construction and Gradient-based Entity Resolution.::>

<a id='1e501c0a-1a29-4e18-915f-d9debb34cab0'></a>

information _scent_ that guides navigation between and within these patches.

<a id='ad6b96e3-6d02-45d5-95ca-774b39ff73e4'></a>

Figure 2 provides an example of our BookIndex. The Tree com-
ponent, positioned at the top, organizes the document into a hier-
archical structure, where content blocks such as text, tables, and
images serve as leaf nodes nested within section nodes. The Graph
component is composed of entities and relations extracted from
these nodes. The GT-Link, illustrated by the blue dotted lines, explic-
itly connects these entities back to their corresponding tree nodes,
thereby grounding the semantic entities within the document's
logical hierarchy.

<a id='285a54ff-ddcb-4c1f-9d7f-c167b5cb0143'></a>

## 4.2 Tree Construction
The first stage transforms the raw document into a structured hier-archical tree T. This involves two key steps: robust layout parsing and intelligent section filtering.

<a id='d2d3f95b-f29a-41ba-8751-d86c8e4794fe'></a>

4.2.1 Layout Parsing. The Layout Parsing phase processes the input document D (a collection of pages) using layout analysis and recognition models. This step identifies, extracts, and organizes diverse blocks (e.g., text, tables, images) from the document pages. The output is a sequence of primitive blocks, B = {b1, b2, ..., bk}, where each block bi = (ci, Ti, fi) is defined as a triplet. Here, ci is the raw content (e.g., text, image data), Ti is the initial layout-based type (e.g., Title, Text, Table, Image), and fi is a vector of associated layout features (e.g., "FontSize", bounding box).

<a id='80be76ea-6873-4ac8-8cd1-8900e4faa179'></a>

4.2.2 Section Filtering. Next, the Section Filtering phase processes this initial sequence to identify the document's logically hierarchical structure. Layout Parsing identifies blocks as Title but does not assign their hierarchical level. Therefore, we select the candidate subset $\mathcal{B}_{\text{title}} \subset \mathcal{B}$ (where $\tau_i$ = Title) for an LLM-based analysis. To handle extremely long documents, this analysis is performed in batches, where each batch retains a contextual window of high-level

<a id='58914195-eda3-4538-8bc9-8d438e091e3f'></a>

section information (with $l = 1$ as the root). The LLM analyzes the content $c_i$ and layout features $f_i$ of the candidates to determine two key properties: their actual hierarchical level $l_i \in \{1, 2, ...\}$ and final node type $\tau_i'$ (e.g., re-classifying an erroneous Title as Text if its level is "None"). This step is crucial for preserving the document's logical hierarchy by correcting blocks erroneously parsed as Title, such as descriptive text within images or borderless table headers.

<a id='0b92e2b3-8d5a-428c-9ffb-efcad0b31a6f'></a>

Finally, the definitive tree T = (N, E_T) is constructed. The node set N is composed of all blocks from the filtering and re-classification process, where each node n \u2208 N retains its content (c_i) and its final node type (\u03c4'_i) (e.g., Text, Section, Table, and Image). The edge set E_T, representing the parent-child nesting relationships, is then established. Parent-child relationships are inferred by sequentially traversing the nodes, using both the determined hierarchical levels (l_i) of Section nodes and the overall document order to assemble the complete tree structure.

<a id='19a0b384-1014-449a-935b-c52b5150aa34'></a>

As an example shown in Figure 2, the _Layout Parsing_ phase identifies diverse blocks, typing them as Title, Text, Table, and Image. During the _Section Filtering_ phase, the Title candidates (e.g., "Method", "Experiment", and "MOE Layer") are analyzed by the LLM. The blocks "Method" and "Experiment" (both with "FontSize: 14") are correctly identified as Section nodes at "Level: 2". Conversely, the "MOE Layer" block ("FontSize: 20"), which was erroneously tagged as Title by the parser, is re-classified by the LLM as a Text node with "Level: None". This correction is crucial for preserving the document's logical hierarchy. Following this process, all filtered and classified nodes are assembled into the final tree structure based on their determined levels and document order.

<a id='84da2123-1613-42f0-acba-296cb2a18af1'></a>

## 4.3 Graph Construction
Once the tree _T_ is established, we proceed to populate the Knowledge Graph _G_ by extracting and refining entities from the tree nodes.

<!-- PAGE BREAK -->

<a id='abc89579-965f-41ea-866c-ed07d2ca8091'></a>

BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents

<a id='95030fcf-7494-4cdc-ba62-14d45377d6ce'></a>

## Algorithm 1: Gradient-based entity resolution
Input: KG G, New entity $v_n$, Rerank model $\mathcal{R}$, Entity vector database DB, Vector search number $top_k$, threshold of gradient $g$

// Vector Search $top_k$ relevant entities in DB.
1 $E_c \leftarrow \text{Search}(DB, v_n, top_k)$;
2 $S \leftarrow \mathcal{R}(E_c, v_n)$;
// Sort all candidate entities by rerank scores.
3 $\text{Sort}(E_c, S)$;
4 $score \leftarrow S[0]$, $Sel \leftarrow E_c[0]$;
// Gradient select similar entities.
5 for each remain entity $v_c \in E_c \setminus \{E_c[0]\}$ do
6 | if $S[v_c] > score/g$ then
7 | | $Sel \leftarrow Sel \cup \{v_c\}$, $score \leftarrow S[v_c]$;
8 | else break;
// Merge entity or add new entity.
9 if $\text{length}(Sel) = \text{length}(E_c)$ then
10 | $G \leftarrow \text{AddNewEntity}(G, v_n)$, $DB \leftarrow \text{AddNew}(DB, v_n)$;
11 else
12 | if $\text{length}(Sel) = 1$ then $v_{sel} \leftarrow Sel[0]$;
13 | else $v_{sel} \leftarrow \text{LLMSelect}(Sel)$;
14 | $G \leftarrow \text{MergeEntity}(G, v_n, v_{sel})$, $DB \leftarrow \text{Update}(DB, v_{sel}, v_n)$;
15 return $G, DB$;

<a id='1399c8b5-fd4b-4fb3-8e89-c8d6e9891a31'></a>

4.3.1 KG Construction. We iterate each node nᵢ ∈ N from the previously constructed tree T. For each node nᵢ, we extract a sub-graph gᵢ = (Vᵢ, Eᵣᵢ) based on its content cᵢ and final node type τᵢ'. This extraction is modality-dependent: if the node is text-only, an LLM is prompted to extract entities and relations, while for nodes containing visual elements (e.g., τᵢ' = Image), a Vision Language Model (VLM) is employed to extract visual knowledge. Crucially, for every entity v ∈ Vᵢ extracted, its origin tree node nᵢ is recorded, which is vital for constructing the final mapping M.

<a id='3c845e79-9c49-47da-b450-e23f8fcff742'></a>

Furthermore, to preserve structural semantics for specific logical types (e.g., Table, Formula), our process first creates a distinct, typed entity (e.g., Utable representing the table itself). The other extracted entities from the specific node's content are linked to this primary vertex. For Table nodes specifically, row and column headers are also explicitly extracted as distinct entities and linked to Utable via a "ContainedIn" relationship.

<a id='19f9af40-4bdb-4992-9766-73d5a44f64db'></a>

4.3.2 _Gradient-based Entity Resolution_. As shown in the litera-
ture [62, 66], a well-constructed KG is essential for document ques-
tion answering. A common challenge in the extraction process is
that the same conceptual entity is often fragmented into multiple
distinct entities due to abbreviations, co-references, or its varied
occurrences across different document sections. This necessitates a
robust Entity Resolution (ER) process, which identifies and merges
these fragmented entities to refine the raw KG.

<a id='c59a5c88-feb2-4978-998c-3df0e5522717'></a>

However, conventional ER methods are computationally expensive. They are often designed for batch processing across multiple data sources (commonly referred to as dirty ER), aiming to ensure accurate entity resolution by finding all possible matching pairs [12]. This process typically requires finding the transitive closure of all detected matches. That is, to definitively merge multiple entities (e.g., A, B, and C) as the same concept, the system must ideally compare all possible pairs ("A-B", "A-C", and "B-C") to confirm their equivalence. This can lead to a quadratic (O(n²))

<a id='29956df5-58e6-48ce-9691-8774b580aa0e'></a>

number of pairwise comparisons, a process that becomes prohibi-
tively slow and computationally expensive when relying on LLMs
for high-accuracy judgments.

<a id='c3d48e48-5a9c-46dd-a7ec-03aa3250804e'></a>

To address this, we employ a gradient-based ER method, op-
erating on a single document (simplified as the clean ER), which
performs ER incrementally as each new entity on is extracted. This
transforms the quadratic batch problem into a simpler, repeated
lookup task: determining where the single new entity on fits among
the already-processed entities in the database. This incremental
process yields two distinct, observable scoring patterns when on is
reranked against its top_k most relevant candidates:

<a id='e47c1fb6-42e6-46b4-9353-f1f31b65ac47'></a>

- *Case A: New Entity.* If v_n is a new conceptual entity, its relevance scores against all existing entities will be uniformly low, showing no significant gradient or discriminative pattern.
- *Case B: Existing Entity.* If v_n is an alias of an existing entity, its scores will show a high relevance to the true match (or a small set of equivalent aliases). Due to the reranker's inherent discriminative limitations, this initial high-relevance set might occasionally contain multiple similar entities. This high-relevance set is then typically followed by a **sharp decline** (a large "gradient") before transitioning to a **gradual slope** of irrelevant entities.

<a id='114de1f4-ce2d-4327-a761-3bd49110b577'></a>

Our Gradient-based ER algorithm is designed precisely to detect this sharp decline (characteristic of Case B), allowing us to efficiently isolate the high-relevance set. Subsequently, an LLM is utilized for finer-grained distinction when multiple similar entities are identified within this set, differentiating it from the "no gradient" scenario (Case A) without quadratic comparisons.

<a id='23f6c7df-131c-42ab-a3bd-045526111b76'></a>

Algorithm 1 shows the above entity resolution process. For a new entity $v_n$, we first retrieve its $top\_k$ candidates $E_c$ from the vector database $DB$, which are then reranked by $R$ against $v_n$ and sorted based on their scores $S$ (Lines 1-3). We initialize the selection set $Sel$ with the top-scoring candidate $E_c[0]$ and set the initial score to $S[0]$ (Line 4). We then iterate through the remaining sorted candidates (Lines 5-8). The core logic checks if the current score $S[v_c]$ is still within the gradient threshold $g$ of the previous score (i.e., $S[v_c] > score/g$). If the score drop is gentle (passes the check), the candidate $v_c$ is added to $Sel$, and score is updated (Lines 7-8); otherwise, the loop breaks (Line 8) as soon as a sharp score drop is detected. Finally, the algorithm makes its decision (Lines 9-14). If the selection set $Sel$ is identical to $E_c$, this indicates that all candidates passed the gradient check. This corresponds to Case A, where the scores lacked discriminative power (i.e., $v_n$ is equally dissimilar to all candidates), so $v_n$ is added as a new entity (Line 9-10). Conversely, if a gradient was found (i.e., $length(Sel) < length(E_c)$), this signals Case B. We then select the canonical entity $v_{sel}$ from $Sel$, using an LLM (Line 13) if the reranker identifies multiple aliases, and merge $v_n$ with it (Lines 12-14). The updated $G$ and $DB$ are then returned (Line 15).

<a id='dc3f9445-52d0-4daa-95c2-1604241b734a'></a>

For instance, considering the example in Figure 2, when the new entity e9 is processed, it is first compared with existing entities in the KG. As depicted in the similarity curve (orange line), e9 shows high similarity with e7, followed by a sharp decline in similarity with other entities like e6, e8, and e5. Our gradient-based selection process identifies e7 as the unique, high-confidence match

<!-- PAGE BREAK -->

<a id='5ed3c221-c9a7-41a4-ac50-a44a2e746773'></a>

Shu Wang, Yingli Zhou, and Yixiang Fang

<a id='b22e3d5d-78ec-486e-8e69-74d402e9c59b'></a>

for e9. Consequently, e9 is merged with e7, enriching the KG with consolidated information as shown in the final merged entity e'7.

<a id='fe8baeee-70cd-4029-abde-9b1719044249'></a>

**Graph-Tree Link (GT-Link).** The GT-Link M is formalized to complete the BookIndex B = (T, G, M). As described in the KG Construction phase, the origin tree node nᵢ is recorded for every newly extracted entity vᵢ. GT-Link is then refined during entity resolution: when an entity vₙ is merged into a canonical entity vₛₑₗ, the origin node set of vₛₑₗ is updated to include all origin nodes previously associated with vₙ. This aggregation process creates the final mapping M : V → P(N), which bi-directionally links the entities in G to the set of their structural locations (nodes) in T.

<a id='088195e6-7e21-437f-abbb-536f441cabe7'></a>

## 5 Agent-based Retrieval
Real-world document queries are often complex, necessitating op-
erations like modal type filtering, semantic selection, and multi-hop
reasoning. To address this, we propose an agent-based approach
in BookRAG, which intelligently plans and executes operations on
the BookIndex. We first introduce the overall workflow and present
two core mechanisms: **Agent-based Planning**, which formulates
the strategy, and the **Structured Execution**, which includes the
retrieval process under the principles of IFT and generation.

<a id='df48e664-201c-498b-b909-1b0ba3ff2938'></a>

## 5.1 Overall Workflow
The overall workflow of agent-based retrieval, illustrated in Figure 3, follows a three-stage pipeline designed to address users' queries systematically.

1.  **Agent-based Planning.** BookRAG first performs *Classification & Plan*. This stage aims to distinguish simple keyword-based queries from reasoning questions that require decomposition and analysis. For instance, a query like "How does Transformer differ from RNNs in handling long-range dependencies?" cannot be solved by retrieving from a single keyword. Therefore, the planning stage first performs **query classification**. Based on this classification and a predefined set of operators designed for the BookIndex, it generates a specific **operators plan** that effectively guides the retrieval and generation strategies.

2.  **Retrieval Process.** Guided by the operator plan, the retrieval process executes *Scent/Filter-based Retrieval*. This stage navigates the BookIndex _B_ = (_T_, _G_, _M_), either utilizing a **scent-based retrieval principle** (e.g., following relevant entities in _G_) to find information, or employing various filters (e.g., modal type) to refine the selection. After reasoning, BookRAG gets the retrieval set of highly relevant information blocks from the BookIndex.

<a id='c0250ed9-f645-4385-9f1e-36d0e1f2cc60'></a>

<::The image is a flowchart illustrating the general workflow of agent-based retrieval in BookRAG. The flow starts with a 'Question' represented by a person icon with a question mark. An arrow points from 'Question' to the first process block, 'Agent-based Planning (Classification & Planning)'. An arrow then points down to the next process block, 'Retrieval Process (Scent/Filter-based Retrieval)'. Another arrow points down to the final process block, 'Generation Process (Analysis & Merging)'. An arrow from this block points to the 'Answer', which is represented by a lightbulb icon.Figure 3: The general workflow of agent-based retrieval in BookRAG, which contains agent-based planning, retrieval, and generation processes.: flowchart::>

<a id='faf0fade-ab27-41c8-8a49-63e80cb0db0b'></a>

3. **Generation Process.** Finally, all retrieved information enters the generation stage for _Analysis & Merging_. This stage synthesizes these (often fragmented) pieces of evidence, performs final analysis, and formulates a coherent response.

<a id='1e5f6ca7-a9f1-4802-b50f-05ba16ddbcc0'></a>

## 5.2 Agent-based Planning
The planning stage is the core of BookRAG, designed to intelligently navigate our BookIndex _B_ = (_T_, _G_, _M_). To support flexible retrieval, we define four types of operators: Formulator, Selector, Reasoner, and Synthesizer. These operators can be arbitrarily combined to form tailored execution pipelines, each with adjustable parameters. BookRAG dynamically configures and assembles these operators to adapt to the specific requirements of different query categories. This process involves two sequential steps: first, the agent performs Query Classification to determine the appropriate solution strategy, then generates a specific _Operator Plan_.

*   **Query Classification**. To enable agent strategy selection, we focus on three representative query categories defined by their intrinsic complexity and operational demands (Table 2): Single-hop, Multi-hop, and Global Aggregation. This classification is crucial because each category requires a different solution strategy. For instance, a Single-hop query typically requires a single piece of infor- mation retrieved via a _Scent-based Retrieval_ operation. In contrast, a Global Aggregation query often necessitates analyzing content under multiple filtering conditions, usually involving a sequence

<a id='ee98c15a-5256-4b71-a5ed-b2ae2c964406'></a>

Table 2: Three common query categories addressed in BookRAG.
<table id="5-1">
<tr><td id="5-2">Query Category</td><td id="5-3">Description</td><td id="5-4">Core Task</td><td id="5-5">Example Query</td></tr>
<tr><td id="5-6">Single-hop</td><td id="5-7">Queries with a single, distinct information target.</td><td id="5-8">Scent-based Retrieval: Retrieve content related to a specific entity or section.</td><td id="5-9">What is the definition of Information Scent?</td></tr>
<tr><td id="5-a">Multi-hop</td><td id="5-b">Queries that require synthesizing information from multiple blocks, often by decomposing into sub-problems.</td><td id="5-c">Decomposing &amp; Merging: Decompose into sub-problems, retrieve for each, and synthesize the final answer.</td><td id="5-d">How does Transformer differ from RNNs in handling long-range dependencies?</td></tr>
<tr><td id="5-e">Global Aggregation</td><td id="5-f">Queries that require filtering across the entire document and performing calculations.</td><td id="5-g">Filter &amp; Aggregation: Apply filters across the document &amp; perform aggregation operations (e.g., Count, Sum).</td><td id="5-h">How many figures related to IFT are in Section 4?</td></tr>
</table>

<!-- PAGE BREAK -->

<a id='88090a79-86f6-4982-ace4-dd120e7a6756'></a>

BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents

<a id='c38e08f8-b1c0-4bcb-97ff-a63ead0719bb'></a>

<::transcription of the content: flowchart::>(a) Operator Set. This section illustrates four types of operators: Formulator, Selector, Reasoner, and Synthesizer.

**Formulator:**
- **Extract:** A question mark icon leads to a robot head, which then points to a dashed box containing two green circles labeled "Entities."
- **Decompose:** A question mark icon leads to a robot head, which then points to a dashed box containing two question mark icons inside speech bubbles, labeled "Sub-queries."

**Selector:**
- **Filter:** A document icon leads to an inverted funnel icon, which then points to a dashed box containing two image icons.
- **Select:** A hierarchical tree structure, where some nodes (representing documents and images) are highlighted in green, leads to a robot head, which then points to a dashed box containing two document icons.

**Reasoner:**
- **Reason:** A group of icons (document, image, and "AΞ" text icon) leads to a network graph icon labeled "Graph," which then leads to a magnifying glass icon with a document and "Q" labeled "Text." This then points to a score "S: 0.6 0.5 0.4," which points to an "AΞ" text icon.
- **Skyline:** Two tables of scores are shown: "S1: 0.6 0.5 0.4 0.2" and "S2: 0.4 0.5 0.6 0.1." These lead to a robot head with a magnifying glass, which then points to a dashed box containing a document icon, an "AΞ" text icon, and an image icon.

**Synthesizer:**
- **Map:** A grid icon and an image icon lead to a dashed box containing three document icons.
- **Reduce:** An "AΞ" text icon leads to a dashed box containing three document icons.
- The final step shows two sets of three document icons converging and pointing to a speech bubble containing the letter "A."

(b) Execution example. This section illustrates an execution trace for a "Single-hop" query.
- A user icon with a question mark asks: "Q: What is the type of car in the Ranking Prompt example?"
- An arrow points from the user to a robot head with a speech bubble that says: "Planning. This is a Simple query... Operator Plan: Extract->Select->Reason->Skyline->Map... Car and Ranking Prompt are entities in the question..."
- An arrow points from the robot head to two oval shapes labeled "Car" and "Ranking Prompt."
- From "Car," an arrow points to an "AΞ" text icon.
- From "Ranking Prompt," an arrow points to an "AΞ" text icon.
- From "Car," a dotted arrow points to an image icon.
- The two "AΞ" text icons converge and point to a document icon labeled "Method."
- From "Method," an arrow points to a dashed box labeled "Method and its Descendants," which contains a document icon, an "AΞ" text icon, and an image icon.
- An arrow points from this dashed box to a network graph icon, which then points to a magnifying glass with a document and "Q."
- An arrow points from the magnifying glass to an image icon.
- An arrow points from the image icon to an "AΞ" text icon.
- An arrow points from the "AΞ" text icon to a robot head with a speech bubble that says: "A: Based on the provided information, the correct type of car in the Ranking Prompt Example is the Mercedes-Benz E-Class Sedan."

Figure 4: The BookRAG Operator Library and an Execution Example from MMLongBench dataset: (a) a visual depiction of the four operator types (Formulator, Selector, Reasoner, and Synthesizer) and (b) an execution trace for a “Single-hop” query, demonstrating the agent-based planning and step-by-step operator execution.::>

<a id='063b51c9-2ee0-4cb0-8da4-5b619d854a22'></a>

of Filter & Aggregation operations across various parts of the docu-
ment. Furthermore, BookRAG is designed to be extensible, allowing
for the resolution of a broader range of query types by integrating
additional operators.
*   **BookIndex Operators**. To execute the strategies identified
by classification, we designed a set of operators (_O_) tailored for
the BookIndex _B_ = (_T_, _G_, _M_). These operators, visually depicted in
Figure 4(a) and detailed in Table 3, define the set of operations the
agent can employ for diverse query categories. We group them into
four types, which we describe in sequence:
1.  **Formulator**. These are LLM-based operators that prepare the
query for execution. This category includes Decompose, which
breaks a _Complex_ query into a set of simpler, actionable sub-queries
_Q_s. It also includes Extract, which employs an LLM to identify
key entities _E_q from the query text and link them to corresponding
entities in the KG, _G_:

<a id='078d50e2-38e3-41f1-b12b-0bc1f3ad13ec'></a>

$$Q_s = \text{LLM}(P_{Dec}, q) = \{q_1, q_2, \dots, q_k\}\quad(2)$$
$$E_q = \text{LLM}(P_{Ext}, q) = \{e_1, e_2, \dots, e_m\}\quad(3)$$


<a id='96ea7d7a-b802-4743-88d3-ffbea8ce7981'></a>

Here, *q* is the original user query, while *P*<sub>Dec</sub> and *P*<sub>Ext</sub> represent the prompts used to guide the LLM for the decomposition and extraction tasks, respectively.

<a id='b6ca9f4b-5b6e-481d-83d1-3df8a5758f98'></a>

② **_Selector._** These operators filter or select specific content ranges from the BookIndex. Filter_Modal and Filter_Range directly apply the explicit constraints _C_ (e.g., modal types, page ranges) generated during the plan. Operating on the Tree _T_ = (_N_, _E_<sub>_T_</sub>), these operators produce a filtered subset _N_<sub>_f_</sub> where the predicate _C_(_n_) holds true for each node:

<a id='e2f116c6-135a-4129-9522-11beb5c22f11'></a>

Nf = {n ∈ N | C(n)}

(4)

<a id='e07fef45-792b-4f3a-a4b3-6505e5a7423c'></a>

In contrast, Select_by_Entity and Select_by_Section target contiguous document segments by retrieving subtrees rooted at specific section nodes. This process first identifies a set of target section nodes $S_{target} \subset N$ at a specified depth, where $S_{target}$ consists of sections either linked to entities $E_q$ via the *GT-Link M* or selected by the LLM. It then retrieves all descendants of these targets to form the selected node set $N_s$:

<a id='a166baf1-2888-4709-a6ae-5fbb52528e50'></a>

N_s = ∣∪ Subtree(s)
s∈S_target

(5)

<a id='a0b69be1-4be3-4fba-b977-00e1aed2643b'></a>

③ Reasoner. These operators analyze and refine selected tree nodes. Graph_Reasoning performs multi-hop inference on a subgraph G'(V', E') (extracted from selected nodes N_S) starting from entity e. Starting from the retrieved entities, it computes an entity importance vector I_G ∈ R^|V'| over the subgraph G' using the PageRank algorithm [20]. These entity scores are then mapped to the tree nodes via the GT-Link matrix M to derive the final tree node importance scores vector S_G ∈ R^|N_S|;

<a id='e7d5ff92-50b2-485a-8d42-1c23b630c100'></a>

I_G = \text{PageRank}(G', e) (6)
S_G = I_G \times M (7)

<a id='220a015a-a390-4e27-bed5-798290bd16d3'></a>

Text_Ranker evaluates the semantic relevance of the tree node's content to the query q, assigning a relevance score S_T to each node. Skyline_Ranker employs the Skyline operator to filter nodes based on these multiple criteria (e.g., S_G and S_T), retaining only those nodes that are not dominated by any others in terms of the specified scoring dimensions.

<a id='25b1e9ee-ab4c-4970-bc23-653e88fc4a52'></a>

④ Synthesizer. These operators are responsible for content gen-
eration. Map performs analysis on specific retrieved information
segments to generate partial responses. Reduce synthesizes a final

<!-- PAGE BREAK -->

<a id='48be64ed-c231-4cc9-9063-bc44767cd338'></a>

Shu Wang, Yingli Zhou, and Yixiang Fang

<a id='59e49939-fdb7-4db6-b086-99f0f10f24a9'></a>

coherent answer by aggregating information from multiple sources, such as partial answers or a collection of retrieved evidence.

<a id='79526dda-9371-4331-8afc-af6534cceb92'></a>

*   **Operator Plan.** After classifying the query (q) into its category (c), the agent's final task is to generate an executable plan P. This plan is a specific sequence of operators (o1,..., on) selected from our library O with parameters dynamically instantiated based on q. This process is formulated as:

    P = AgentPlan (q, c, O)                                                                                                                                     (8)

<a id='3842151b-3673-4a58-a746-73ade69ed80c'></a>

The plan follows a structured workflow tailored to each category:

*   **Single-hop**: The agent first attempts to Extract an entity. If successful, it executes a "scent-based" selection; otherwise, it falls back to a section-based strategy. Both paths then proceed to standard reasoning and generation, denoted as Pstd.

    Ps = {
    Extract --success--> Select_by_Entity --> Pstd
    Extract --fail--> Select_by_Section --> Pstd
    (9)

    Pstd = (Graph || Text) --> Skyline --> Reduce (10)

*   **Complex**: The agent first decomposes the problem, applies the Single-hop workflow Ps to each sub-problem, and finally synthesizes the results.

    Pcomplex = Decompose --> Ps --> Map --> Reduce (11)

*   **Global Aggregation**: The workflow involves applying a sequence of filters followed by synthesis.

<a id='e5348934-833e-4087-9385-ca61a4963afc'></a>

$P_{global} = \prod (\text{Filter_Modal} \mid \text{Filter_Range}) \to \text{Map} \to \text{Reduce} \quad (12)$
Here, the symbol $\Pi$ denotes the nested composition of filters, applying either a modal or range filter at each step.

<a id='173db5ec-b1c4-497c-8aa1-5dd0b35a0893'></a>

### 5.3 Structured Execution
Following the planning stage, BookRAG executes the generated workflow P. This execution phase embodies the cognitive principles of Information Foraging Theory (IFT), effectively translating abstract textual queries into concrete operations. Specifically,

<a id='48279bd1-e659-4e94-88a1-df867c63b732'></a>

the Selector operators mirror the act of "navigating to informa-
tion patches," narrowing the vast document space down to rele-
vant scopes. Subsequently, the Reasoner operators perform "sense-
making within patches," where they analyze and refine the informa-
tion within these focused scopes. Finally, the Synthesizer gener-
ates the answer based on the processed evidence. This design mini-
mizes the cost of attention by ensuring computational resources
are focused solely on high-value data patches.

<a id='770afb7a-03a9-47d6-9085-5a9784ee2371'></a>

_Scent/Filter-based Retrieval._ The execution begins by narrowing the scope. Aligning with IFT, Selector operators identify relevant "patches" by following "information scents" (e.g., key entities in question) or applying explicit filter constraints. This process reduces the full node set N to a focused node subset Ns:

<a id='5026b0dd-bea2-44c4-8edf-749833f9cfe8'></a>

N_s = \text{Selector}(N, \text{params}_{\text{sel}}) (13)

This pre-selection minimizes noise and ensures that subsequent reasoning is applied only to highly relevant contexts, optimiz-ing the foraging cost. Subsequently, within this focused scope, Reasoner operators evaluate nodes using multiple dimensions, such as graph topology and semantic relevance. We then employ the Skyline_Ranker to get the final retrieval set. Unlike fixed top-k retrieval, the Skyline operator retains the *Pareto frontier* of nodes, retaining nodes that are valuable in at least one dimension while discarding dominated ones:

N_R = \text{Skyline\_Ranker}(\{S_G(n), S_T(n) \mid n \in N_s\}) (14)

<a id='457ba187-3942-4d60-a7a3-c38580e1bd4b'></a>

_Analysis & Merging Generation._ In the final stage, the Synthesizer operator generates the coherent answer by aggregating the refined evidence:

$$A = Synthesizer(q, N_R)$$
(15)

<a id='2f552915-e408-47ac-9875-3348ad50c06d'></a>

The Map operator performs fine-grained analysis on individual ev-idence blocks or sub-problems (from Decompose) to generate in-termediate insights. The Reduce operator then aggregates these partial results, such as answers to decomposed sub-queries or sta-tistical counts from a global filter, to construct the final response.

<a id='9956ed57-0c0d-4768-aa69-c17c989d3eda'></a>

Table 3: Operators utilized in our BookRAG, categorized by their function.
<table id="7-1">
<tr><td id="7-2">Operator</td><td id="7-3">Type</td><td id="7-4">Description</td><td id="7-5">Parameters</td></tr>
<tr><td id="7-6">Decompose Extract</td><td id="7-7">Formulator Formulator</td><td id="7-8">Decompose a complex query into simpler, actionable sub-queries. Identify and extract key entities from the query (links to G).</td><td id="7-9">(Self-contained) (Self-contained)</td></tr>
<tr><td id="7-a">Filter_Modal</td><td id="7-b">Selector</td><td id="7-c">Filter retrieved nodes by their modal type (e.g., Table, Figure).</td><td id="7-d">modal_type: str</td></tr>
<tr><td id="7-e">Filter_Range</td><td id="7-f">Selector</td><td id="7-g">Filter nodes based on a specified range (e.g., pages, section).</td><td id="7-h">range: (start, end)</td></tr>
<tr><td id="7-i">Select_by_Entity</td><td id="7-j">Selector</td><td id="7-k">Selects all tree nodes (N) in sections linked to a given entity (V).</td><td id="7-l">entity_name: str</td></tr>
<tr><td id="7-m">Select_by_Section</td><td id="7-n">Selector</td><td id="7-o">Uses an LLM to select relevant sections and selects all tree nodes (N) within them.</td><td id="7-p">query: str, sections: List[str]</td></tr>
<tr><td id="7-q">Graph_Reasoning</td><td id="7-r">Reasoner</td><td id="7-s">Performs multi-hop reasoning on subgraph (G&#x27;) and score tree nodes (N) using graph importance and GT-links.</td><td id="7-t">start_entity: str, subgraph: G&#x27;</td></tr>
<tr><td id="7-u">Text_Reasoning</td><td id="7-v">Reasoner</td><td id="7-w">Rerank retrieved tree nodes (N) based on the relevance.</td><td id="7-x">query: str</td></tr>
<tr><td id="7-y">Skyline_Ranker</td><td id="7-z">Reasoner</td><td id="7-A">Rerank nodes based on multiple criteria.</td><td id="7-B">criteria: List[str]</td></tr>
<tr><td id="7-C">Map</td><td id="7-D">Synthesizer</td><td id="7-E">Uses partially retrieved information to generate a partial answer.</td><td id="7-F">(Input: List[str])</td></tr>
<tr><td id="7-G">Reduce</td><td id="7-H">Synthesizer</td><td id="7-I">Synthesizes the final answer from partial information or all sub-problem answers.</td><td id="7-J">(Input: List[str])</td></tr>
</table>

<!-- PAGE BREAK -->

<a id='20775bbf-97bc-4c7a-8045-e44a3dcc53fa'></a>

BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents

<a id='7cb9f944-9225-4ca3-b4a8-b99b1e63a2a2'></a>

This separation ensures that the system can handle both detailed
content extraction and high-level reasoning synthesis effectively.

<a id='47b7afab-90ad-4af5-bb89-7bba0656b85d'></a>

To illustrate this end-to-end process, Figure 4(b) presents an execution trace for a "Single-hop" query: "What is the type of *car* in the *Ranking Prompt* example?". In the planning phase, the agent classifies the query and generates a specific workflow. Subsequently, it identifies key entities (e.g., "car") via Extract, retrieves relevant nodes via Select_by_Entity, refines them through reasoning and Skyline filtering, and finally synthesizes the answer using Reduce.

<a id='ed3a7b38-1938-470e-8058-1abf1a799e1a'></a>

## 6 Experiments

In our experiments, we evaluate BookRAG against several strong baseline methods, with an in-depth comparison of their efficiency and accuracy on document QA tasks.

<a id='adaefa49-37ac-4e62-aa97-05b199950f84'></a>

## 6.1 Setup
Table 4: Datasets used in our experiments. EM and F1 denote Exact Match and F1-score, respectively.

<a id='e5b1228a-b3e4-4d1c-a041-0034a49a4b6b'></a>

<table id="8-1">
<tr><td id="8-2">Dataset</td><td id="8-3">MMLongBench</td><td id="8-4">M3DocVQA</td><td id="8-5">Qasper</td></tr>
<tr><td id="8-6">Questions</td><td id="8-7">669</td><td id="8-8">633</td><td id="8-9">640</td></tr>
<tr><td id="8-a">Documents</td><td id="8-b">85</td><td id="8-c">500</td><td id="8-d">192</td></tr>
<tr><td id="8-e">Avg. Pages</td><td id="8-f">42.16</td><td id="8-g">8.52</td><td id="8-h">10.95</td></tr>
<tr><td id="8-i">Avg. Images</td><td id="8-j">25.92</td><td id="8-k">3.51</td><td id="8-l">3.43</td></tr>
<tr><td id="8-m">Tokens</td><td id="8-n">2,816,155</td><td id="8-o">3,553,774</td><td id="8-p">2,265,349</td></tr>
<tr><td id="8-q">Metrics</td><td id="8-r">EM, F1</td><td id="8-s">EM, F1</td><td id="8-t">Accuracy, F1</td></tr>
</table>

<a id='1a1a1c4c-af7a-4dc4-ab1c-fb162d6c38c5'></a>

Datasets & Question Synthesis. We use three widely adopted benchmarking datasets for complex document QA tasks: MMLong-Bench [33], M3DocVQA [11], and Qasper [14]. MMLongBench is a comprehensive benchmark designed to evaluate QA capabili-ties on long-form documents, covering diverse categories such as guidebooks, financial reports, and industry files. M3DocVQA is an open-domain benchmark designed to test RAG systems on a di-verse collection of HTML-type documents sourced from Wikipedia pages¹. Qasper is a QA dataset focused on scientific papers, where questions require retrieving evidence from the entire document. We filtered the datasets to remove documents with low clarity or inco-herent structures. To address the scarcity of global-level questions in the original benchmarks, we synthesize additional QA pairs by having an LLM generate global questions from selected document elements (e.g., tables or figures). These questions are then answered and meticulously refined by human annotators via an outsourcing process, with this additional QA pairs constituting less than 20% of our final QA pairs. The statistics of these datasets are presented in Table 4.

<a id='becc9bfa-126f-4ade-9abe-d9b8711e3340'></a>

Metrics. We adhere to the official metrics specified by each dataset
for QA. Our primary evaluation relies on Exact Match (EM), ac-
curacy, and token-based F1-score. To assess efficiency, we also
measure time cost and token usage during the response phase. Ad-
ditionally, for methods including PDF parsing, we also evaluate
retrieval recall. To establish the ground truth for this, we manually
label the specific PDF blocks (e.g., texts, titles, tables, images, and

<a id='550ce51d-5d3a-45ef-8d42-ee7360651495'></a>

formulas) required to answer each question. This labeling process is guided by the metadata of ground-truth evidence provided in each dataset; we filter candidate blocks using the given modality (all datasets), page numbers (MMLongBench), and evidence statements (Qasper). Any blocks that remained non-unique after this filtering process are manually annotated. In cases where a PDF parsing error made the ground-truth item unavailable, the retrieval recall for that query is recorded as 0.

<a id='30fd72cc-6bfc-4602-bc44-2c2deb7f4bc8'></a>

Baselines. Our experiments consider three model configurations:

*   **Conventional RAG:** These methods are the most common pipeline for document analysis, where the raw text is first extracted and then chunked into segments of a specified size. We select strong and widely used retrieval models: BM25 [44] and Vanilla RAG. We also implement Layout+Vanilla, a variant that uses document layout analysis for semantic chunking.
*   **Graph-based RAG:** These methods first extract textual content from documents and then leverage graph data during retrieval. We select RAPTOR [45] and GraphRAG [16]. Specifically, GraphRAG has two versions: GraphRAG-Global and GraphRAG-Local, which employ global and local search methods, respectively.
*   **Layout segmented RAG:** This category encompasses methods that utilize layout analysis to segment document content into discrete structural units. We include: MM-Vanilla, which utilizes multi-modal embeddings for visual and textual content; a tree-based method inspired by PageIndex [39], denoted as TreeTraverse, where an LLM navigates the document's tree structure; DocETL [47], a declarative system for complex document processing; and GraphRanker, a graph-based method extended from HippoRAG [19] that applies Personalized PageRank [20] to rank the relevant nodes.

<a id='767c9e70-0e32-44e7-a7f2-8f79062ffc54'></a>

*Implementation details.* For a fair comparison, both BookRAG
and all baseline methods are powered by a unified set of state-of-the-
art (SOTA) and widely adopted backbone models from the Qwen
family [4, 60, 63, 64]. We employ MinerU [52] for robust document
layout parsing. We set the threshold of gradient *g* as 0.6, and more
details are provided in the appendix of our technical report [57].
Our source code, prompts, and detailed configurations are available
at github.com/sam234990/BookRAG.

<a id='a9a2b6cf-e50f-4e9c-b273-db58bc7155ac'></a>

## 6.2 Overall results

In this section, we present a comprehensive evaluation of BookRAG, analyzing its complex QA performance, retrieval effectiveness, and query efficiency compared to state-of-the-art baselines.

* QA Performance of BookRAG. We compare the QA performance of BookRAG against three categories of baselines, as shown in Table 5. The results indicate that BookRAG achieves state-of-the-art performance across all datasets, substantially out-performing the top-performing baseline by 18.0% in Exact Match on M3DocVQA. Layout + Vanilla consistently outperforms Vanilla RAG, confirming that layout parsing preserves essential structural information for better retrieval. Besides, the suboptimal results of Tree-Traverse and GraphRanker highlight the limitations of relying solely on hierarchical navigation or graph-based reasoning, which

<a id='8eaba5b4-0ca4-42e6-8f2b-0254c1e441a9'></a>

1 https://www.wikipedia.org/

<!-- PAGE BREAK -->

<a id='0536616c-2a4b-468f-8df7-44187f54311f'></a>

Shu Wang, Yingli Zhou, and Yixiang Fang

<a id='1c86621f-6226-41c6-a5f3-69639b4f77b1'></a>

Table 5: Performance comparison of different methods across various datasets for solving complex document QA tasks. The best and second-best results are marked in bold and underlined, respectively.

<a id='d70ea2a2-9f96-41e7-9e6f-9a505d23f147'></a>

<table id="9-1">
<tr><td id="9-2" rowspan="2">Baseline Type</td><td id="9-3" rowspan="2">Method</td><td id="9-4" colspan="2">MMLongBench</td><td id="9-5" colspan="2">M3DocVQA</td><td id="9-6" colspan="2">Qasper</td></tr>
<tr><td id="9-7">(Exact Match)</td><td id="9-8">(F1-score)</td><td id="9-9">(Exact Match)</td><td id="9-a">(F1-score)</td><td id="9-b">(Accuracy)</td><td id="9-c">(F1-score)</td></tr>
<tr><td id="9-d" rowspan="3">Conventional RAG</td><td id="9-e">BM25</td><td id="9-f">18.3</td><td id="9-g">20.2</td><td id="9-h">34.6</td><td id="9-i">37.8</td><td id="9-j">38.1</td><td id="9-k">42.5</td></tr>
<tr><td id="9-l">Vanilla RAG</td><td id="9-m">16.5</td><td id="9-n">18.0</td><td id="9-o">36.5</td><td id="9-p">40.2</td><td id="9-q">40.6</td><td id="9-r">44.4</td></tr>
<tr><td id="9-s">Layout + Vanilla</td><td id="9-t">18.1</td><td id="9-u">19.8</td><td id="9-v">36.9</td><td id="9-w">40.2</td><td id="9-x">40.7</td><td id="9-y">44.6</td></tr>
<tr><td id="9-z" rowspan="3">Graph-based RAG</td><td id="9-A">RAPTOR</td><td id="9-B">21.3</td><td id="9-C">21.8</td><td id="9-D">34.3</td><td id="9-E">37.3</td><td id="9-F">39.4</td><td id="9-G">44.1</td></tr>
<tr><td id="9-H">GraphRAG-Local</td><td id="9-I">7.7</td><td id="9-J">8.5</td><td id="9-K">23.7</td><td id="9-L">25.6</td><td id="9-M">35.9</td><td id="9-N">39.2</td></tr>
<tr><td id="9-O">GraphRAG-Global</td><td id="9-P">5.3</td><td id="9-Q">5.6</td><td id="9-R">20.2</td><td id="9-S">22.0</td><td id="9-T">24.0</td><td id="9-U">24.1</td></tr>
<tr><td id="9-V" rowspan="4">Layout segmented RAG</td><td id="9-W">MM-Vanilla</td><td id="9-X">6.8</td><td id="9-Y">8.4</td><td id="9-Z">25.1</td><td id="9-10">27.7</td><td id="9-11">27.9</td><td id="9-12">29.3</td></tr>
<tr><td id="9-13">Tree-Traverse</td><td id="9-14">12.7</td><td id="9-15">14.4</td><td id="9-16">33.3</td><td id="9-17">36.2</td><td id="9-18">27.3</td><td id="9-19">32.1</td></tr>
<tr><td id="9-1a">GraphRanker</td><td id="9-1b">21.2</td><td id="9-1c">22.7</td><td id="9-1d">43.0</td><td id="9-1e">47.8</td><td id="9-1f">32.9</td><td id="9-1g">37.6</td></tr>
<tr><td id="9-1h">DocETL</td><td id="9-1i">27.5</td><td id="9-1j">28.6</td><td id="9-1k">40.9</td><td id="9-1l">43.3</td><td id="9-1m">42.3</td><td id="9-1n">50.4</td></tr>
<tr><td id="9-1o">Our proposed</td><td id="9-1p">BookRAG</td><td id="9-1q">43.8</td><td id="9-1r">44.9</td><td id="9-1s">61.0</td><td id="9-1t">66.2</td><td id="9-1u">55.2</td><td id="9-1v">61.1</td></tr>
</table>

<a id='c8f44b95-12e6-48c4-8567-106fb1a59a32'></a>

often miss cross-sectional context or drift into irrelevant scopes.
In contrast, BookRAG's superiority stems from the synergy of its unified Tree-Graph BookIndex and Agent-based Planning. By effectively classifying queries and configuring optimal workflows, our BookRAG overcomes limitations of context fragmentation and static query workflow within existing baselines, ensuring precise evidence retrieval and accurate generation.

<a id='360e0e57-11fd-41fd-bf42-f5583c4ed2a2'></a>

Table 6: Retrieval recall comparison among layout-based methods. The best and second-best results are marked in bold and underlined, respectively.

<a id='b2060a20-7e7c-4993-8124-eeacd53d7ae2'></a>

<table id="9-1w">
<tr><td id="9-1x">Method</td><td id="9-1y">MMLongBench</td><td id="9-1z">M3DocVQA</td><td id="9-1A">Qasper</td></tr>
<tr><td id="9-1B">Layout + Vanilla</td><td id="9-1C">26.3</td><td id="9-1D">33.8</td><td id="9-1E">33.5</td></tr>
<tr><td id="9-1F">MM-Vanilla</td><td id="9-1G">7.5</td><td id="9-1H">19.7</td><td id="9-1I">14.9</td></tr>
<tr><td id="9-1J">Tree-Traverse</td><td id="9-1K">11.2</td><td id="9-1L">19.5</td><td id="9-1M">14.5</td></tr>
<tr><td id="9-1N">GraphRanker</td><td id="9-1O">26.4</td><td id="9-1P">44.5</td><td id="9-1Q">28.6</td></tr>
<tr><td id="9-1R">BookRAG</td><td id="9-1S">57.6</td><td id="9-1T">71.2</td><td id="9-1U">63.5</td></tr>
</table>

<a id='d8f666b3-33f5-477e-8a1a-dd06768c30e6'></a>

• **Retrieval performance of BookRAG.** To validate our retrieval design, we evaluate the retrieval recall of BookRAG against other layout-based baselines on the ground-truth layout blocks. The experimental results demonstrate that BookRAG achieves the highest recall across all datasets, notably reaching 71.2% on M3DocVQA and significantly outperforming the next best baseline (GraphRanker, max 44.5%). This performance advantage stems from our IFT-inspired **Selector → Reasoner** workflow: the Agent-based Planning first classifies the query, enabling the Selector to narrow the search to a precise *information patch*, followed by the Reasoner's analysis. Crucially, after the Skyline_Ranker process, the average number of retained nodes is 9.87, 6.86, and 8.6 across the three datasets, which is comparable to the standard top-*k* (*k* = 10) setting, ensuring high-quality retrieval without inflating the candidate size.

<a id='3f58caf6-b3b7-47d0-9206-dca14de0c8be'></a>

• Efficiency of BookRAG. We further evaluate the efficiency in terms of query time and token consumption, as illustrated in Figure 5. Overall, BookRAG maintains time and token costs comparable to existing Graph-based RAG methods. While purely text-based

<a id='d71b8cfd-b286-4257-bfcc-fb15f4050bcb'></a>

<::bar chart: legend: BM25 (light brown), Vanilla RAG (orange-brown), Layout + Vanilla (red-brown), RAPTOR (yellow), GraphRAG-Local (light green), GraphRAG-Global (dark blue), MM-Vanilla (light blue), Tree-Traverse (very light blue), GraphRanker (yellow), DocETL (purple), BookRAG (pink). The figure displays three sets of bar charts comparing query efficiency across different methods. (a) MMLongBench: The left chart shows 'Query Time (s)' on a log scale from 10^2 to 10^5. The right chart shows 'Token cost (M)' on a log scale from 10^5 to 10^8. For both metrics, GraphRAG-Global and BookRAG generally show higher values compared to other methods. (b) M3DocVQA: The left chart shows 'Query Time (s)' on a log scale from 10^3 to 10^5. The right chart shows 'Token cost (M)' on a log scale from 10^5 to 10^7. Similar to MMLongBench, GraphRAG-Global and BookRAG exhibit higher values for both time and token cost. (c) Qasper: The left chart shows 'Query Time (s)' on a log scale from 10^3 to 10^5. The right chart shows 'Token cost (M)' on a log scale from 10^6 to 10^7. This benchmark also shows GraphRAG-Global and BookRAG having higher query times and token costs. Figure 5: Comparison of query efficiency.::>

<a id='db1b1fc4-bb82-43c3-8712-35f4a7f9abe6'></a>

RAG approaches generally exhibit lower latency and token usage due to the absence of VLM processing for images, BookRAG maintains a balanced efficiency among multi-modal methods. In terms of token usage, BookRAG reduces consumption by an order of magnitude compared to the strongest baseline, DocETL. Notably, on the MMLongBench dataset, DocETL consumes over 53 million tokens, whereas BookRAG requires less than 5 million. Regarding the query latency, our method also achieves a speedup of up to 2x compared to DocETL.

<!-- PAGE BREAK -->

<a id='2e98c34e-fed3-419d-b09f-1fa727d06db2'></a>

BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents

<a id='ab0ff480-7265-47b0-8a22-e02b46eade64'></a>

## 6.3 Detailed Analysis
In this section, we provide a more in-depth examination of our BookRAG. We first conduct an ablation study to validate the contribution of each component, followed by an experiment on the impact of gradient-based ER and QA performance across different query types. Furthermore, we perform a comprehensive error analysis, compare the effectiveness of our entity resolution method, and present a case study.

<a id='779ae0ce-eb42-49f4-87d0-515bdb15e1b8'></a>

• **Ablation study.** To evaluate the contribution of each core component in BookRAG, we design several variants by removing specific components:

*   w/o Gradient ER: Replaces the gradient-based entity resolution with a Basic ER by merging the same-name entities.
*   w/o Planning: Removes the Agent-based Planning, defaulting to a static, standard workflow for all queries.
*   w/o Selector: Removes the Selector operators, forcing Reasoners to score all candidate nodes.
*   w/o Graph_Reasoning: Removes the Graph_Reasoning operator. Consequently, the Skyline_Ranker is also disabled as scoring becomes single-dimensional.
*   w/o Text_Reasoning: Removes the Text_Reasoning operator. Similarly, the Skyline_Ranker is disabled, relying solely on graph-based scores.

<a id='7b9f5261-985f-487c-bf56-1df8bf4087d7'></a>

Table 7: Comparing the QA performance of different variants
of BookRAG. EM and F1 denote Exact Match and F1-score,
respectively.
<table id="10-1">
<tr><td id="10-2" rowspan="2">Method variants</td><td id="10-3" colspan="2">MMLongBench</td><td id="10-4" colspan="2">Qasper</td></tr>
<tr><td id="10-5">EM</td><td id="10-6">F1</td><td id="10-7">Accuracy</td><td id="10-8">F1</td></tr>
<tr><td id="10-9">BookRAG (Full)</td><td id="10-a">43.8</td><td id="10-b">44.9</td><td id="10-c">55.2</td><td id="10-d">61.1</td></tr>
<tr><td id="10-e">w/o gradient ER</td><td id="10-f">40.1</td><td id="10-g">42.8</td><td id="10-h">48.9</td><td id="10-i">57.3</td></tr>
<tr><td id="10-j">w/o Planning</td><td id="10-k">30.8</td><td id="10-l">33.2</td><td id="10-m">40.9</td><td id="10-n">48.5</td></tr>
<tr><td id="10-o">w/o Selector</td><td id="10-p">42.5</td><td id="10-q">43.1</td><td id="10-r">52.5</td><td id="10-s">59.1</td></tr>
<tr><td id="10-t">w/o Graph_Reasoning</td><td id="10-u">39.8</td><td id="10-v">41.5</td><td id="10-w">51.4</td><td id="10-x">58.4</td></tr>
<tr><td id="10-y">w/o Text_Reasoning</td><td id="10-z">39.0</td><td id="10-A">40.3</td><td id="10-B">47.2</td><td id="10-C">52.5</td></tr>
</table>

<a id='602344e3-53f2-43c1-9947-e884ac0bd51a'></a>

The first variant evaluates the impact of KG quality on retrieval performance. The second and third variants assess the necessity of our Agent-based Planning and IFT-inspired selection mechanism, respectively. Finally, the last two variants validate the effectiveness of our multi-dimensional reasoning and dynamic Skyline filtering strategy. As shown in Table 7, the performance degradation across all variants confirms the essential role of each module in BookRAG. Specifically, the performance drop in the _w/o Gradient ER_ variant highlights the critical role of a high-quality, connectivity-rich KG in supporting effective reasoning. Removing the _Planning_ mechanism results in the most significant performance loss, confirming that a static workflow is insufficient for handling diverse types of queries. The _w/o Selector_ variant, while maintaining competitive accuracy, incurs a prohibitive computational cost (> 2x tokens on Qasper), validating the efficiency of our IFT-inspired "narrow-then-reason" strategy.

<a id='a773ad4e-80f8-47f1-995a-a3b27e70f5c4'></a>

• Impact of Gradient-based Entity Resolution. To evaluate
the quality of our constructed KG, we compare the graph statistics

<a id='ddc070ea-4c99-4d99-9e6e-9d2fce7fc0d6'></a>

<::Figure 6: Comparison of graph statistics. Values are normalized to the Basic setting (Baseline=1.0). Absolute values for Basic are annotated. Note that density values are abbreviated (e.g., 3.6E-3 denotes 3.6 × 10⁻³).
: bar chart::>
Legend: Basic (blue bar), Gradient-based ER (red bar)

(a) MMLongBench
Y-axis: Ratio
- # Entity:
  - Basic: 1.0 (absolute value 1327)
  - Gradient-based ER: ~0.85
- Density:
  - Basic: 1.0 (absolute value 3.6E-3)
  - Gradient-based ER: ~1.25
- Diameter:
  - Basic: 1.0 (absolute value 14.8)
  - Gradient-based ER: ~0.9
- # CC:
  - Basic: 1.0 (absolute value 169)
  - Gradient-based ER: ~0.8

(b) Qasper
Y-axis: Ratio
- # Entity:
  - Basic: 1.0 (absolute value 531)
  - Gradient-based ER: ~0.8
- Density:
  - Basic: 1.0 (absolute value 5.4e-3)
  - Gradient-based ER: ~1.2
- Diameter:
  - Basic: 1.0 (absolute value 15.0)
  - Gradient-based ER: ~0.9
- # CC:
  - Basic: 1.0 (absolute value 106)
  - Gradient-based ER: ~0.8

<a id='0e879131-1193-47a8-8849-24b39032cf13'></a>

of our Gradient-based ER against a Basic KG construction. The Ba-sic setting employs simple exact name matching for entity merging, which is standard practice in many graph-based methods. Figure 6 presents the comparative results, normalizing the metrics (Entity count, Density, Diameter of the Largest Connected Component, and Number of Connected Components) against the Basic baseline. The results demonstrate that our Gradient-based ER significantly optimizes KG. Specifically, it reduces the number of entities (by 12%) while substantially boosting graph density (by over 20% across datasets). This structural shift indicates that our ER module effec-tively identifies the same conceptual entities that possess different names. Consequently, the resulting graphs are more compact and cohesive, as evidenced by the reduced diameter and fewer connected components, which mitigates graph fragmentation and facilitates better connectivity for graph reasoning.

<a id='e084d397-491d-459f-9594-03456c6b49fe'></a>

<::chart: The image displays Figure 7, which consists of two bar charts comparing QA performance across different query types (Single-hop, Multi-hop, and Global). A common legend at the top indicates that blue bars represent 'EM / Accuracy' and red bars represent 'F1-score'. Both charts share a Y-axis labeled 'Score', ranging from 0 to 80, with major ticks at 20, 50, and 80. The X-axis for both charts categorizes data into 'Single', 'Multi', and 'Global' query types. The two sub-charts are:

(a) MMLongBench: This chart shows performance for MMLongBench. For 'Single' query type, EM / Accuracy is approximately 48 and F1-score is approximately 48. For 'Multi' query type, EM / Accuracy is approximately 35 and F1-score is approximately 38. For 'Global' query type, EM / Accuracy is approximately 62 and F1-score is approximately 62.

(b) Qasper: This chart shows performance for Qasper. For 'Single' query type, EM / Accuracy is approximately 55 and F1-score is approximately 65. For 'Multi' query type, EM / Accuracy is approximately 53 and F1-score is approximately 58. For 'Global' query type, EM / Accuracy is approximately 70 and F1-score is approximately 68.

Figure 7: QA performance breakdown by different query types (Single-hop, Multi-hop, and Global). The blue bars represent Exact Match (EM) for MMLongBench and Accuracy for Qasper, while the red bars represent the F1-score.::>

<a id='85b05037-3462-43b7-89d1-8f0d954c03d3'></a>

• **QA performance under different query types.** Figure 7 breaks down the performance of BookRAG across Single-hop, Multi-hop, and Global aggregation query types. We observe that Multi-hop queries generally present a greater challenge compared to Single-hop ones, resulting in a slight performance decrease. This trend reflects the inherent difficulty of retrieving and reasoning over disjoint pieces of evidence. It further validates our agent-based planning strategy, which handles different query types separately.
• **Error Response analysis.** To diagnose the performance bottlenecks of BookRAG, we conduct a fine-grained error analysis on 200 sampled queries from each dataset, tracing the error propagation as shown in Figure 9. We categorize failures into four types: PDF Parsing, Plan, Retrieval, and Generation errors. The results

<!-- PAGE BREAK -->

<a id='1936f59a-a207-452d-a4c8-3395976d36f0'></a>

Shu Wang, Yingli Zhou, and Yixiang Fang

<a id='5182175b-1231-4be5-a1d8-c237db6d086c'></a>

BookRAG response of different query types

### Single-hop Case from Qasper

**Question:** What is the reward model for the reinforcement learning approach?
**Human-written answer:** Reward 1 for successfully completing the task, with a discount by the number of turns, and reward 0 when fail.
**Evidence:** We defined the reward as being 1 for successfully completing the task, and 0 otherwise. A discount of 0.95 </>

**Agent-based Planning:** This is a single-hop query. Here is the Select operator:
Extract={"entity_name": "reinforcement learning (rl)", "entity_type": "METHOD_OR_TECHNIQUE"}
**Process:** The Select_by_Entity operator identifies relevant sub-trees (e.g., "Introduction", "Related work"), pruning the reasoning space from 134 to 24 nodes. Subsequent Graph_Reasoning, Text_Reasoning, and Skyline_Ranker operate on this focused scope to retrieve 8 nodes for the final response.
**Output:** The reward model for the reinforcement learning approach is based on a binary reward system that evaluates the success or failure of dialog interactions. </> 1 for successfully completing a task. </> 0 for failing </> using a discount factor of 0.95 </>

### Multi-hop Case from Qasper

**Question:** What is the difference in performance between the interpretable system (e.g., vectors and cosine distance) and LSTM with ELMo system?
**Human-written answer:** Accuracy of best interpretible system was 0.3945 while accuracy of LSTM-ELMO net was 0.6818.
**Evidence 1:** The experimental results are presented in Table 1. Diacritic swapping showed a remarkably poor performance </>
**Evidence 2:** Test results for all the methods used. The loss measure is cross-entropy </>

**Agent-based Planning:** This is a multi-hop query. Here is the Decompose operator:
Sub_question 1: "What is the performance metric of the interpretable system (e.g., vectors and cosine distance)?"
Sub_question 2: "What is the performance metric of the LSTM with ELMo system?"
**Process:** BookRAG applies the single-hop workflow for each sub-question and generates the partial answers as follows:
Answer 1: "Summary: \n- Performance Metric: Accuracy (0.3945) and </>
Answer 2: "The performance metric of the LSTM with ELMo system (LSTM-ELMo net) is as follows: \n Accuracy: 0.6818 </>
**Output:** The interpretable system (using vector distance and cosine distance) has an accuracy of 0.3945, while the LSTM with ELMo system has an accuracy of 0.6818. The difference in accuracy between the two systems is 0.6818 - 0.3945 = 0.2873. </>

### Global Aggregation Case from MMLongBench

**Question:** How many charts are shown in the first 10 pages of the document? **Human-written answer:** 5

**Agent-based Planning:** This is a global query. Here are filter operators:
1. Filter_Range={"filter_type": page, "filter_range":"1-10"} 2. Filter_Modal={"filter_type": image}
**Process:** Applying filter operators retrieves images nodes from pages [3, 5, 6, 8, 9]. Map analyzes each image, and Reduce synthesizes the final output.
**Output:** Based on my analysis of the document, I found 5 items that answer the question. </>

Figure 8: Case study of responses across different query types from MMLongBench and Qasper. CYAN TEXT highlights correct content generated by BookRAG. GRAY TEXT describes the internal process, and </> marks omitted irrelevant parts.

<a id='05607c42-a2db-4ed9-a209-3a0d89f4fd55'></a>

identify Retrieval Error as the dominant failure mode, followed by Generation Error, reflecting the persistent challenge of locating and synthesizing multimodal evidence. Regarding Plan Error, our qualitative analysis reveals a specific failure pattern: the planner tends to over-decompose detailed single-hop queries into unnecessary multi-hop sub-tasks. This fragmentation leads to disjointed retrieval paths, effectively preventing the model from synthesizing a cohesive final answer from the scattered sub-responses.

*   **Case study.** Figure 8 illustrates BookRAG's answering workflow across Single-hop, Multi-hop, and Global queries. The results demonstrate that by leveraging specific operators (Select, Decompose, and Filter), BookRAG effectively prunes search spaces. For example, in the Single-hop case, the reasoning space is significantly reduced from 134 to 24 nodes. This capability allows the system to efficiently isolate relevant evidence from noise, ensuring precise answer generation.

<a id='95262161-d3ed-4fb8-9483-0f15b019511f'></a>

<::chart: Two Sankey diagrams showing error analysis on 200 sampled queries. Figure (a) MMLongBench: All Queries (200) flow into Successful Parsing (194) and Parsing Error (6). From Successful Parsing (194), streams go to Correct (79), Retrieval Error (52), Generation Error (36), and Plan Error (27). Figure (b) Qasper: All Queries (200) flow into Successful Parsing (193) and Parsing Error (7). From Successful Parsing (193), streams go to Correct (117), Generation Error (30), Retrieval Error (26), and Plan Error (20). Figure 9: Error analysis on 200 sampled queries from MM-LongBench and Qasper datasets.::>

<a id='017b0372-f6b4-443c-aef2-0ad056d56837'></a>

# 7 Conclusion
In this paper, we propose BookRAG, a novel method built upon Book Index, a document-native, structured Tree-Graph index specifically designed to capture the intricate relations of structural documents. By employing an agent-based method to dynamically configure

<!-- PAGE BREAK -->

<a id='c49b0373-4d6d-4666-94be-0c779264215c'></a>

BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents

<a id='a1f54386-1231-4e85-9b9b-683e6c9296f8'></a>

retrieval and reasoning operators, our approach achieves state-of-
the-art performance on multiple benchmarks, demonstrating signif-
icant superiority over existing baselines in both retrieval precision
and answer accuracy. In the future, we will explore an integrated
document-native database system that supports data formatting,
knowledge extraction, and intelligent querying.

<!-- PAGE BREAK -->

<a id='74786cbe-e0db-4f30-bdc3-8a3ba67004e8'></a>

Shu Wang, Yingli Zhou, and Yixiang Fang

<a id='8e6c6cba-03e8-4057-93f4-09ceec837ae9'></a>

References
[1] Simran Arora, Brandon Yang, Sabri Eyuboglu, Avanika Narayan, Andrew Hojel, Immanuel Trummer, and Christopher Ré. 2023. Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes. Proceedings of the VLDB Endowment 17, 2 (2023), 92–105.
[2] Akari Asai, Zeqiu Wu, Yizhong Wang, et al. 2024. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. In International Conference on Learning Representations (ICLR).
[3] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-rag: Learning to retrieve, generate, and critique through self-reflection. arXiv preprint arXiv:2310.11511 (2023).
[4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. 2025. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923 (2025).
[5] Camille Barboule, Benjamin Piwowarski, and Yoan Chabot. 2025. Survey on Question Answering over Visually Rich Documents: Methods, Challenges, and Trends. arXiv preprint arXiv:2501.02235 (2025).
[6] Yukun Cao, Zengyi Gao, Zhiyang Li, Xike Xie, S. Kevin Zhou, and Jianliang Xu. 2025. LEGO-GraphRAG: Modularizing Graph-Based Retrieval-Augmented Generation for Design Space Exploration. Proc. VLDB Endow. 18, 10 (June 2025), 3269–3283. https://doi.org/10.14778/3748191.3748194
[7] Chengliang Chai, Jiajun Li, Yuhao Deng, Yuanhao Zhong, Ye Yuan, Guoren Wang, and Lei Cao. 2025. Doctopus: Budget-aware structural table extraction from unstructured documents. Proceedings of the VLDB Endowment 18, 11 (2025), 3695–3707.
[8] Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androutsopoulos. 2020. LEGAL-BERT: The muppets straight out of law school. arXiv preprint arXiv:2010.02559 (2020).
[9] Sibei Chen, Yeye He, Weiwei Cui, Ju Fan, Song Ge, Haidong Zhang, Dongmei Zhang, and Surajit Chaudhuri. 2024. Auto-Formula: Recommend Formulas in Spreadsheets using Contrastive Learning for Table Representations. Proceedings of the ACM on Management of Data 2, 3 (2024), 1–27.
[10] Sibei Chen, Nan Tang, Ju Fan, Xuemi Yan, Chengliang Chai, Guoliang Li, and Xiaoyong Du. 2023. Haipipe: Combining human-generated and machine-generated pipelines for data preparation. Proceedings of the ACM on Management of Data 1, 1 (2023), 1–26.
[11] Jaemin Cho, Debanjan Mahata, Ozan Irsoy, Yujie He, and Mohit Bansal. 2024. M3docrag: Multi-modal retrieval is what you need for multi-page multi-document understanding. arXiv preprint arXiv:2411.04952 (2024).
[12] Vassilis Christophides, Vasilis Efthymiou, Themis Palpanas, George Papadakis, and Kostas Stefanidis. 2020. An overview of end-to-end entity resolution for big data. ACM Computing Surveys (CSUR) 53, 6 (2020), 1–42.
[13] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multi-modality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261 (2025).
[14] Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, and Matt Gardner. 2021. A dataset of information-seeking questions and answers anchored in research papers. arXiv preprint arXiv:2105.03011 (2021).
[15] Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco. 2023. Complex QA and language models hybrid architectures, Survey. arXiv preprint arXiv:2302.09051 (2023).
[16] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. 2024. From local to global: A graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130 (2024).
[17] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997 (2023).
[18] Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. 2024. LightRAG: Simple and Fast Retrieval-Augmented Generation. arXiv e-prints (2024), arXiv-2410.
[19] Bernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. 2024. HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models. arXiv preprint arXiv:2405.14831 (2024).
[20] Taher H Haveliwala. 2002. Topic-sensitive pagerank. In Proceedings of the 11th international conference on World Wide Web. 517–526.
[21] Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and Bryan Hooi. 2024. G-retriever: Retrieval-augmented generation for textual graph understanding and question answering. arXiv preprint arXiv:2402.07630 (2024).
[22] Yucheng Hu and Yuxing Lu. 2024. Rag and rau: A survey on retrieval-augmented language model in natural language processing. arXiv preprint arXiv:2404.19543 (2024).
[23] Soyeong Jeong, Jinheon Baek, et al. 2024. Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity. arXiv preprint arXiv:2403.14403 (2024).

<a id='c12ef226-c6be-4c82-9855-d1ef7d127658'></a>

24. Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong C Park. 2024. Adaptive-rag: Learning to adapt retrieval-augmented large language mod-els through question complexity. arXiv preprint arXiv:2403.14403 (2024).
25. Tengjun Jin, Yuxuan Zhu, and Daniel Kang. 2025. ELT-Bench: An End-to-End Benchmark for Evaluating AI Agents on ELT Pipelines. arXiv preprint arXiv:2504.04808 (2025).
26. Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. 2022. Ocr-free document understanding transformer. In European Confer-ence on Computer Vision. Springer, 498-517.
27. Dawei Li, Shu Yang, Zhen Tan, Jae Young Baik, Sukwon Yun, Joseph Lee, Aaron Chacko, Bojian Hou, Duy Duong-Tran, Ying Ding, et al. 2024. DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature. arXiv preprint arXiv:2405.04819 (2024).
28. Guoliang Li, Jiayi Wang, Chenyang Zhang, and Jiannan Wang. 2025. Data+ AI: LLM4Data and Data4LLM. In Companion of the 2025 International Conference on Management of Data. 837-843.
29. Yinheng Li, Shaofei Wang, Han Ding, and Hang Chen. 2023. Large language models in finance: A survey. In Proceedings of the fourth ACM international conference on AI in finance. 374-382.
30. Zhaodonghui Li, Haitao Yuan, Huiming Wang, Gao Cong, and Lidong Bing. 2025. LLM-R2: A Large Language Model Enhanced Rule-based Rewrite System for Boosting Query Efficiency. Proceedings of the VLDB Endowment 1, 18 (2025), 53-65.
31. Haoyu Lu, Wen Liu, Bo Zhang, et al. 2024. DeepSeek-VL: Towards Real-World Vision-Language Understanding. arXiv preprint arXiv:2403.05525 (2024).
32. Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Cehao Yang, Jiaxin Mao, and Jian Guo. 2024. Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning with Knowledge-guided Retrieval Augmented Generation. arXiv preprint arXiv:2407.10805 (2024).
33. Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, et al. 2024. Mmlongbench-doc: Benchmarking long-context document understanding with visualizations. Advances in Neural Information Processing Systems 37 (2024), 95963-96010.
34. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. When not to trust language models: Investigat-ing effectiveness of parametric and non-parametric memories. arXiv preprint arXiv:2212.10511 (2022).
35. Zan Ahmad Naeem, Mohammad Shahmeer Ahmad, Mohamed Eltabakh, Mourad Ouzzani, and Nan Tang. 2024. RetClean: Retrieval-Based Data Cleaning Using LLMs and Data Lakes. Proceedings of the VLDB Endowment 17, 12 (2024), 4421-4424.
36. Avanika Narayan, Ines Chami, Laurel Orr, and Christopher Ré. 2022. Can Foun-dation Models Wrangle Your Data? Proceedings of the VLDB Endowment 16, 4 (2022), 738-746.
37. Yuqi Nie, Yaxuan Kong, Xiaowen Dong, John M Mulvey, H Vincent Poor, Qing-song Wen, and Stefan Zohren. 2024. A Survey of Large Language Models for Financial Applications: Progress, Prospects and Challenges. arXiv preprint arXiv:2406.11903 (2024).
38. Arash Dargahi Nobari and Davood Rafiei. 2024. TabulaX: Leveraging Large Language Models for Multi-Class Table Transformations. arXiv preprint arXiv:2411.17110 (2024).
39. PageIndex. 2025. PageIndex: Next-Generation Reasoning-based RAG. https: //pageindex.ai/.
40. Liana Patel, Siddharth Jha, Melissa Pan, Harshit Gupta, Parth Asawa, Carlos Guestrin, and Matei Zaharia. 2025. Semantic Operators and Their Optimization: Enabling LLM-Based Data Processing with Accuracy Guarantees in LOTUS. Proceedings of the VLDB Endowment 18, 11 (2025), 4171-4184.
41. Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, and Siliang Tang. 2024. Graph retrieval-augmented generation: A survey. arXiv preprint arXiv:2408.08921 (2024).
42. Peter Pirolli and Stuart Card. 1995. Information foraging in information access environments. In Proceedings of the SIGCHI conference on Human factors in computing systems. 51-58.
43. Yichen Qian, Yongyi He, Rong Zhu, Jintao Huang, Zhijian Ma, Haibin Wang, Yaohua Wang, Xiuyu Sun, Defu Lian, Bolin Ding, et al. 2024. UniDM: A Unified Framework for Data Manipulation with Large Language Models. Proceedings of Machine Learning and Systems 6 (2024), 465-482.
44. Stephen E Robertson and Steve Walker. 1994. Some simple effective approxi-mations to the 2-poisson model for probabilistic weighted retrieval. In SIGIR'94: Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, organised by Dublin City University. Springer, 232-241.
45. Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D Manning. 2024. Raptor: Recursive abstractive processing for tree-organized retrieval. arXiv preprint arXiv:2401.18059 (2024).
46. Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2024.

<!-- PAGE BREAK -->

<a id='ed81081f-480d-45e0-995b-86239b358775'></a>

BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents

<a id='e0f71889-c01a-4331-9924-9cd50f37c2c5'></a>

Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems 36 (2024).

47. Shreya Shankar, Tristan Chambers, Tarak Shah, Aditya G Parameswaran, and Eugene Wu. 2024. Docetl: Agentic query rewriting and evaluation for complex document processing. arXiv preprint arXiv:2410.12189 (2024).
48. Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kalu-arachchi, Rajib Rana, and Suranga Nanayakkara. 2023. Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering. Transactions of the Association for Computational Linguistics 11 (2023), 1–17.
49. Solutions Review Editors. 2019. 80 Percent of Your Data Will Be Unstructured in Five Years. https://solutionsreview.com/data-management/80-percent-of-your-datawill-be-unstructured-in-five-years/. Accessed: 2023-10-27.
50. Zhaoyan Sun, Xuanhe Zhou, and Guoliang Li. 2024. R-Bot: An LLM-based Query Rewrite System. arXiv preprint arXiv:2412.01661 (2024).
51. Vincent A Traag, Ludo Waltman, and Nees Jan Van Eck. 2019. From Louvain to Leiden: guaranteeing well-connected communities. Scientific reports 9, 1 (2019), 1–12.
52. Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, et al. 2024. Mineru: An open-source solution for precise document content extraction. arXiv preprint arXiv:2409.18839 (2024).
53. Jiayi Wang and Guoliang Li. 2025. Aop: Automated and interactive llm pipeline orchestration for answering complex queries. CIDR.
54. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. 2024. Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution. arXiv preprint arXiv:2409.12191 (2024).
55. Shu Wang, Yixiang Fang, Yingli Zhou, Xilin Liu, and Yuchi Ma. 2025. ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented Generation. arXiv preprint arXiv:2502.09891 (2025).
56. Shen Wang, Tianlong Xu, Hang Li, Chaoli Zhang, Joleen Liang, Jiliang Tang, Philip S Yu, and Qingsong Wen. 2024. Large language models for education: A survey and outlook. arXiv preprint arXiv:2403.18105 (2024).

<a id='a45657e9-ea4b-4247-a7c8-dc293883c82f'></a>

[57] Shu Wang, Yingli Zhou, and Yixiang Fang. [n. d.]. BookRAG: A Hierarchical Structure-aware Index-based Approach for Complex Document Question Answering. https://github.com/sam234990/BookRAG.
[58] Yu Wang, Nedim Lipka, Ryan A Rossi, Alexa Siu, Ruiyi Zhang, and Tyler Derr. 2024. Knowledge graph prompting for multi-document question answering. In *Proceedings of the AAAI Conference on Artificial Intelligence*, Vol. 38. 19206–19214.
[59] Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024. Corrective Retrieval Augmented Generation. *arXiv preprint arXiv:2401.15884* (2024).
[60] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 technical report. *arXiv preprint arXiv:2505.09388* (2025).
[61] Murong Yue. 2025. A survey of large language model agents for question answering. *arXiv preprint arXiv:2503.19213* (2025).
[62] Qinggang Zhang, Shengyuan Chen, Yuanchen Bei, Zheng Yuan, Huachi Zhou, Zijin Hong, Hao Chen, Yilin Xiao, Chuang Zhou, Junnan Dong, et al. 2025. A survey of graph retrieval-augmented generation for customized large language models. *arXiv preprint arXiv:2501.13958* (2025).
[63] Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, and Min Zhang. 2024. GME: Improving Universal Multimodal Retrieval by Multimodal LLMs. *arXiv preprint arXiv:2412.16855* (2024).
[64] Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, et al. 2025. Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models. *arXiv preprint arXiv:2506.05176* (2025).
[65] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. *arXiv preprint arXiv:2303.18223* 1, 2 (2023).
[66] Yingli Zhou, Yaodong Su, Youran Sun, Shu Wang, Taotao Wang, Runyuan He, Yongwei Zhang, Sicong Liang, Xilin Liu, Yuchi Ma, et al. 2025. In-depth Analysis of Graph-based RAG in a Unified Framework. *arXiv preprint arXiv:2503.04338* (2025).
[67] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Haonan Chen, Zheng Liu, Zhicheng Dou, and Ji-Rong Wen. 2023. Large language models for information retrieval: A survey. *ACM Transactions on Information Systems* (2023).

<!-- PAGE BREAK -->

<a id='879615c9-3f17-4761-a52e-31818f1a392c'></a>

Shu Wang, Yingli Zhou, and Yixiang Fang

<a id='04a40ca4-e2d7-4dc3-9839-6424e014ad95'></a>

# A Experimental details
## A.1 Evaluation Metrics
In this section, we provide the detailed definitions and calculation procedures for the metrics used in our main experiments.

<a id='e644e203-a38e-44ac-8454-953839b2e250'></a>

A.1.1 _Answer Extraction and Normalization._ Standard RAG models typically generate free-form natural language responses, which may contain extraneous conversational text (e.g., "The answer is..."). Directly comparing these raw outputs with concise ground truth labels (e.g., "Option A" or "12.5") can lead to false negatives.

<a id='ae0c8ea6-aec3-4031-a6de-f92c5dd01b3d'></a>

Following official evaluation protocols, we employ an LLM-based extraction step to align the model output with the ground truth format before calculation. Let yraw denote the raw response generated by the RAG system and ygold denote the ground truth. We define the extracted answer ŷ as:

<a id='62e5b882-cda1-4e74-87ad-4ef82d20874a'></a>

ŷ = LLMextract(yraw, Instruction) (16)
where LLMextract extracts the key information (e.g., the key entity for span extraction) from yraw. We further apply standard normalization N(·) (e.g., lowercasing, removing punctuation) to both ŷ and ygold.

<a id='cadc41c9-71e5-4f63-a728-bc33aa8cccdb'></a>

A.1.2 QA Performance Metrics. Based on the ground truth *y*<sub>*gold*</sub> and the model's response (either raw *y*<sub>*raw*</sub> or extracted ŷ), we compute the following metrics:

<a id='e899e37a-9a5f-4b5b-91a0-034082a7637f'></a>

*Accuracy (Inclusion-based)*. Following prior works [3, 34, 46], we utilize accuracy as a soft-match metric. We consider a prediction correct if the normalized gold answer is included in the model's generated response, rather than requiring a strict exact match. This accounts for the uncontrollable nature of LLM generation.

<a id='442ab4b5-6483-4fe2-a4a9-c13cf846773d'></a>

Accuracy = 1/N \sum_{i=1}^{N} \mathbb{I}(N(y_{gold,i}) \subseteq N(y_{raw,i})) (17)

<a id='1aa709d7-88fe-4b9d-bfaa-044a222a68d2'></a>

where ⊆ denotes the substring inclusion relation.

<a id='f271f016-97ac-418e-94ba-25d637dca186'></a>

_Exact Match (EM)_.. Unlike accuracy, Exact Match is a strict metric. It measures whether the normalized _extracted_ answer _ŷ_ is character-for-character identical to the ground truth.

<a id='ab9219a7-f2fe-4b43-8753-83d3f0010dae'></a>

EM = \frac{1}{N} \sum_{i=1}^{N} \mathbb{I}(\mathcal{N}(\hat{y}_i) = \mathcal{N}(y_{gold,i})) \quad (18)

<a id='9a7637b7-5455-4dbe-8a81-e1ce2758d963'></a>

_F1-score_. For questions requiring text span answers, we utilize
the token-level F1-score between the extracted answer _ŷ_ and the
ground truth _y_<sub>_gold_</sub>. Treating them as bags of tokens _T_<sub>_ŷ_</sub> and _T_<sub>_gold_</sub>:

<a id='4747e115-44ce-4f4d-9349-94c61d1fcc79'></a>

P = |T_hat_y intersect T_gold| / |T_hat_y|, R = |T_hat_y intersect T_gold| / |T_gold|, F1 = (2 * P * R) / (P + R) (19)

<a id='49d88817-51f7-4429-947c-148bec0ac475'></a>

A.1.3 Retrieval Recall. As described in the main text, we evaluate retrieval quality based on the granularity of parsed PDF blocks (e.g., paragraphs, tables, images). For a given query q, let B_gold be the set of manually labeled ground-truth blocks required to answer q, and B_ret be the set of unique blocks retrieved by the system. The Retrieval Recall is defined as:

<a id='83afafb0-d540-42d3-9798-f5f9aff75fca'></a>

Recall$_{ret}$ = $\begin{cases} 0 & \text{if parsing error occurs on } \mathcal{B}_{gold} \\ \frac{|\mathcal{B}_{ret} \cap \mathcal{B}_{gold}|}{|\mathcal{B}_{gold}|} & \text{otherwise} \end{cases}$ (20)

Specifically, if a ground-truth block is lost due to PDF parsing failures (i.e., it does not exist in the candidate pool), it is considered strictly unretrievable, resulting in a recall contribution of 0 for that specific block.

<a id='a4482dab-0a64-4fb3-be85-18c47be42bc3'></a>

## A.2 Implementation details
We implement BookRAG in Python, utilizing MinerU [52] for robust document layout parsing. For a fair comparison, both BookRAG and all baseline methods are powered by a unified set of state-of-the-art (SOTA) and widely adopted backbone models from the Qwen family [4, 60, 63, 64], including LLM, vision-language model (VLM), and embedding models. Specifically, we utilize Qwen3-8B [60] as the default LLM, Qwen2.5VL-30B [4] as the vision-language model (VLM), Qwen3-Embedding-0.6B [64] for text embedding, gme-Qwen2-VL-2B-Instruct [63] for multi-modal embedding, and Qwen3-Reranker-4B [64] for reranking. We primarily select models under the 10B parameter scale to balance efficiency and effectiveness. However, for the VLM, we adopt the 30B version, as the 8B counterpart exhibited significant performance deficits, frequently failing to answer correctly even when provided with ground-truth images. All experiments were conducted on a Linux operating system running on a high-performance server equipped with an Intel Xeon 2.0GHz CPU, 1024GB of memory, and 8 NVIDIA GeForce RTX A5000 GPUs, each with 24 GB of VRAM. Specifically, to ensure a fair comparison of efficiency, all methods were executed serially, and the reported time costs reflect this sequential processing mode. For methods involving document chunking and retrieval ranking, we standardize the chunk size at 500 tokens and set the retrieval top-k to 10 to ensure consistent candidate pool sizes across baselines. For further reproducibility, our source code and detailed implementation configurations are publicly available at our repository: https://github.com/sam234990/BookRAG.

<a id='628a9ae3-93fa-4a73-bc17-254eee77a418'></a>

## A.3 Prompts
Specifically, we present the prompts designed for agent-based query classification (Figure 10), question decomposition (Figure 11), and filter operator generation (Figure 12). Additionally, we illustrate the prompt employed for entity resolution judgment (Figure 13) during the graph construction phase.

<!-- PAGE BREAK -->

<a id='6461c6f0-ffcb-4f26-a4cf-48ef4984b7cc'></a>

BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents

<a id='fbf115b6-363b-4648-9291-53dd3f17d0d6'></a>

You are an expert query analyzer. Your only task is to classify the user's question into one of three categories: "simple", "complex", or "global". Respond only with the specified JSON object.

Category Definitions:
1. single-hop: The question can be fully answered by retrieving information from a SINGLE, contiguous location in the document (e.g., one specific paragraph, one complete table, or one figure).
   - This includes questions that require reasoning or comparison, as long as all the necessary data is present within that single retrieved location.
   - Example: "What is the title of Figure 2?"
   - Example: "How do 5% of the Latinos see economic upward mobility for their children?" -> This is SIMPLE because the answer can be found by looking at a single chart or paragraph.

2. multi-hop: The question requires decomposition into multiple simple sub-questions, where each sub-question must be answered by a separate retrieval action.
   - It often contains a nested or indirect constraint that requires a preliminary step to resolve before the main question can be answered.
   - Example: "What is the color of the personality vector...?" -> This is COMPLEX because it requires two separate retrieval actions.

3. global: The question requires an aggregation operation (e.g., counting, listing, summarizing) over a set of items that are identified by a clear structural filter.
   - Example: "How many tables are in the document?" -> This is GLOBAL because the process is to filter for all items of type 'table'.

User Query: query

<a id='600011e5-5e6f-4379-b651-3c65c0aaaa8a'></a>

Figure 10: The prompt for query classification.

<!-- PAGE BREAK -->

<a id='706f9dd6-97b2-4bd1-9f21-1013bcb6a5a2'></a>

Shu Wang, Yingli Zhou, and Yixiang Fang

<a id='85f12789-5461-4669-803a-ab3d0433ea13'></a>

You are a query decomposition expert. You have been given a "complex" question. Your task is to break it down into a series of simple, atomic sub-questions and classify each one by type.

**Crucial Instructions:**
1. Each `retrieval` sub-question MUST be a direct information retrieval task that can be answered independently by looking up a specific fact, number, or value in the document.
2. **`retrieval` sub-questions MUST NOT depend on the answer of another sub-question.** They should be parallelizable. All logic for combining their results must be placed in a final `synthesis` question.
3. A `synthesis` question requires comparing, calculating, or combining the answers of the previous `retrieval` questions. It does **NOT** require a new lookup in the document.

You MUST provide your response in a JSON object with a single key 'sub_questions', which contains a list of objects. Each object must have a 'question' (string) and a 'type' (string: "retrieval" or "synthesis").

--- EXAMPLE 1 (Correct Decomposition with Independent Lookups) ---
Complex Query: "What is the color of the personality vector in the soft-labled personality embedding matrix that with the highest Receptiviti score for User A2GBIFL43U1LKJ?"

Expected JSON Output:
```
{{
"sub_questions": [
{{"question": "What are all the Receptiviti scores for each personality vector for User A2GBIFL43U1LKJ?",
"type": "retrieval"}},
{{"question": "What is the mapping of personality vectors to their colors in the soft-labled personality embedding matrix?",
"type": "retrieval"}},
{{"question": "From the gathered scores, identify the personality vector with the highest score, and then find its corresponding color from the vector-to-color mapping.",
"type": "synthesis"}}
]
}}
```
--- END EXAMPLE 1 ---

--- EXAMPLE 2 (Decomposition with retrieval and synthesis steps) ---
Complex Query: "According to the report, which one is greater in population in the survey? Foreign born Latinos, or the Latinos interviewed by cellphone?"

Expected JSON Output:
```
{{
"sub_questions": [
{{"question": "According to the report, what is the population of foreign born Latinos in the survey?",
"type": "retrieval"}},
{{"question": "According to the report, what is the population of Latinos interviewed by cellphone in the survey?",
"type": "retrieval"}},
{{"question": "Which of the two population counts is greater?",
"type": "synthesis"}}
]
}}
```
--- END EXAMPLE 2 ---

Now, perform the decomposition for the following query.

User Query: query

<a id='8d122f93-66e8-4750-bdf5-df11ccb21b1a'></a>

Figure 11: The prompt for query decomposition.

<!-- PAGE BREAK -->

<a id='f3917165-ae14-454d-94ed-b90c680da719'></a>

BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents

<a id='391480eb-66e8-414f-afe0-239210e5e624'></a>

You are a highly specialized AI assistant. Your only function is to analyze a "Global Query" and return a single, valid JSON object that specifies both the filtering steps and the final aggregation operation. You MUST NOT output any other text or explanation.

### INSTRUCTIONS & DEFINITIONS ###

1. **Filters**: You MUST determine the list of `filters` to apply. Even if the filter is for the whole document (e.g., all tables), the `filters` list must be present.

- `filter_type`: One of ["section", "image", "table", "page"].
  - `section`: Use for structural parts like chapters, sections, appendices, or references.
  - `image`: Use for visual elements like figures, images, pictures, or plots.
  - `table`: Use for tabular data.
  - `page`: Use for specific page numbers or ranges.
- `filter_value`: (Optional) Can be provided for "section" (e.g., a section title) or "page" (e.g., '3-10' or '5').
  **For "image" or "table", this value MUST be null.**

2. **Operation**: Determine the final aggregation operation.

- `operation`: One of ["COUNT", "LIST", "SUMMARIZE", "ANALYZE"].

### EXAMPLES OF YOUR TASK ###

User: "How many figures are in this paper from Page 3 to Page 10?"
Assistant: {{"filters": [{{"filter_type": "page", "filter_value": "3-10"}}, {{"filter_type": "image"}}], "operation": "COUNT"}}

User: "Summarize the discussion about 'data augmentation' in the 'Methodology' section."
Assistant: {{"filters": [{{"filter_type": "section", "filter_value": "Methodology"}}], "operation": "SUMMARIZE"}}

User: "How many chapters are in this report?"
Assistant: {{"filters": [{{"filter_type": "section"}}], "operation": "COUNT"}}

### YOUR CURRENT TASK ###

User: "{query}"
User Query: query

<a id='cfc480a1-7ff7-440c-81d8-62dfcd1a0d82'></a>

Figure 12: The prompt for Filter operator generation.

<!-- PAGE BREAK -->

<a id='c247b4ca-4a83-4c77-986a-8dc96b71a357'></a>

Shu Wang, Yingli Zhou, and Yixiang Fang

<a id='279191bd-152a-4dd0-9430-3e11bb06afb1'></a>

## -Goal-
You are an expert Entity Resolution Adjudicator. Your task is to determine if a "New Entity" refers to the exact same real-world concept as one of the "Candidate Entities" provided from a knowledge graph. Your output must be a JSON object containing the ID of the matching candidate (or -1) and a brief explanation for your decision.

## -Context-
You will be given one "New Entity" recently extracted from a text. You will also be given a list of "Candidate Entities" that are semantically similar, retrieved from an existing knowledge base. Each candidate has a unique `id` for you to reference.

## -Core Task & Rules-
1. **Analyze the "New Entity"**: Carefully read its name, type, and description to understand what it is.
2. **Field-by-Field Adjudication**: To determine a match, you must evaluate each field with a specific focus:
    * ***`entity_name`** (High Importance):** The names must be extremely similar, a direct abbreviation (e.g., "LLM" vs. "Large Language Model"), or a well-known alias. **If the names represent distinct, parallel concepts (like "Event Detection" and "Named Entity Recognition"), they are NOT a match, even if their descriptions are very similar.**
    * ***`entity_type`** (Medium Importance):** The types do not need to be identical, but they must be closely related and compatible (e.g., `COMPANY` and `ORGANIZATION` could describe the same entity).
    * ***`description`** (Contextual Importance):** The descriptions may differ as they are often extracted from different parts of a document. Your task is to look past surface-level text similarity and determine if they fundamentally describe the **same underlying object or concept**.
3. **Be Strict and Conservative**: Your standard for a match must be very high. An incorrect merge can corrupt the knowledge graph. A missed merge is less harmful.
    * Surface-level similarities are not enough. The underlying concepts must be identical.
    * For example, "Apple" (the fruit) and "Apple Inc." (the company) are NOT a match.
    * **When in doubt, you MUST output -1.**
    * **Assume No Match by Default**: In a large knowledge graph, most new entities are genuinely new. You should start with the assumption that the "New Entity" is unique. You must find **strong, convincing evidence** across all fields, especially the `entity_name`, to overturn this assumption and declare a match.
4. **Format the Output**: **You must provide your answer in a valid JSON format. The JSON object should contain two keys:**
    * `select_id`: An integer. The `id` of the candidate you've determined to be an exact match. If no exact match is found, this value MUST be `-1`.
    * `explanation`: A brief, one-sentence string explaining your reasoning. For a match, explain why they are the same entity. For no match, explain the key difference.

## -Output Schema & Format-
Your response MUST be a single, valid JSON object that adheres to the following schema. Do not include any other text, explanation, or markdown formatting like ```json.

```json
{
  "select_id": "integer",
  "explanation": "string"
}
```

## -Example-
### Example 1: Match Found
### Example 2: No Match Found

## -Task Execution-
Now, perform the selection task based on the following data. Remember to output only a single integer.

## -Input Data-

<a id='9e916994-e366-4ce2-899a-0943022536ce'></a>

Figure 13: The prompt for entity resolution judgement, examples are omitted due to lack of space.