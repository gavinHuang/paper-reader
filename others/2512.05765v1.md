<a id='59b32764-9e50-41e0-bc74-5d6ecfeb394e'></a>

The Missing Layer of AGI: From Pattern Alchemy to Coordination Physics

<a id='6130c1aa-9359-4d38-a8d2-000a1df89a50'></a>

Edward Y. Chang, Stanford University ¹

<a id='d91debff-d4de-4191-bcb0-d80bf2171d0e'></a>

# Abstract
Influential critiques argue that Large Language Models (LLMs) are a dead end for AGI: "mere pattern matchers" structurally incapable of reasoning or planning. We argue this conclusion misidentifies the bottleneck: it confuses the ocean with the net. Pattern repositories are the necessary System-1 substrate; the missing component is a System-2 coordination layer that selects, constrains, and binds these patterns. We formalize this layer via UCCT, a theory of semantic anchoring that models reasoning as a phase transition governed by effective support ($\rho_d$), representational mismatch ($d_r$), and an adaptive anchoring budget ($\gamma \log k$). Under this lens, ungrounded generation is simply an unbaited retrieval of the substrate's **maximum likelihood prior**, while "reasoning" emerges when anchors **shift the posterior** toward goal-directed constraints. We translate UCCT into architecture with MACI, a coordination stack that implements baiting (behavior-modulated debate), filtering (Socratic judging), and persistence (transactional memory). By reframing common objections as testable coordination failures, we argue that the path to AGI runs through LLMs, not around them.

<a id='f9acd6e2-061f-443a-8f63-7cb620dec894'></a>

# 1. Introduction: The Field at a Crossroads
The artificial intelligence community is fractured by a de-bate over the nature of Large Language Models (LLMs). On one side, scaling proponents argue that LLMs are sufficient for Artificial General Intelligence (AGI). On the other, influ-ential critiques argue that LLMs are "mere pattern matchers" structurally incapable of reasoning, planning, or composi-tional generalization, and therefore represent a dead end (Le-Cun, 2022).

<a id='5a672ad2-ef3f-469f-a1f0-88586f5169c3'></a>

We argue that this debate relies on a false dichotomy. To
clarify why, consider a fishing metaphor. The ocean rep-

<a id='4de383b3-9ad3-49ee-b11c-b36deb976705'></a>

'Computer Science Department, Stanford University, Stanford, CA 94305, USA. Correspondence to: Edward Y. Chang <echang@cs.stanford.edu>.

<a id='822cf978-78ad-4be6-801e-e03b690230ac'></a>

resents the model's vast repository of latent patterns. A fisherman casting a net without bait harvests the *maximum likelihood prior* of the waters beneath him—mostly common fish (generic training data). Critics who decry these ungrounded outputs are not observing a broken system; they are observing the raw statistical baseline of an unbaited cast.

<a id='5352aab0-62af-4857-9eec-729a7db9ecdc'></a>

However, intelligent behavior is not just casting; it is *baiting* and *filtering*. This process is governed by **bait density**. If the bait is too *sparse*, it fails to attract the specific, rare fish, and the ocean's prior continues to dominate the catch. If the bait is sufficiently *dense*, it conveys strong intent, **shifting the posterior distribution** so that the target concept swamps the common priors. Yet, bait is not free; using excessive bait to secure a catch is inefficient. In this view, the "Missing Layer" is the *Coordination Layer* that optimizes this trade-off: calculating the precise density required to shift the posterior without incurring prohibitive costs.

<a id='f783a596-e4d2-435e-9134-68894df6e419'></a>

## 1.1. Our Position: Substrate plus Coordination

We propose a third position: **Substrate plus Coordination**.
We agree that LLMs alone are insufficient for AGI, but reject
the conclusion that they are irrelevant. Our central thesis is:

<a id='e68a2e9a-72c9-4142-bfeb-837225922b73'></a>

_LLMs are the necessary System-1 substrate (the_ _pattern repository). The primary bottleneck is_ _the absence of a System-2 coordination layer that_ _binds these patterns to external constraints, veri-_ _fies outputs, and maintains state over time._

<a id='7d444b57-c244-4cfb-9615-7d3ecc3ccecd'></a>

This paper formalizes the coordination layer through our
Multi-Agent Collaborative Intelligence (MACI) frame-
work (Chang, 2025b). MACI is not a claim that current
models are AGI, but an architectural stance: build reliable
reasoning on top of pretrained substrates by controlling what
binds (semantic anchoring), how disagreements evolve (reg-
ulated debate), and what persists (transactional memory).

<a id='2005dc78-e146-42e6-8b87-e5d62d3a0cb3'></a>

A key contribution is the formalization of *bounded* coordi-
nation. Semantic anchoring improves as we supply more
anchors (retrieval, exemplars, tool outputs), but any prac-
tical theory must penalize unbounded context to prevent
signal dilution. We introduce an adaptive anchoring score
that captures this trade-off.

<a id='fdbabbe1-c2d4-4f39-bcd2-6b2123caa873'></a>

1

<a id='ad72b0c7-b335-4277-b147-d459ec145e93'></a>

arXiv:2312.05765v1 [cs.AI] 5 Dec 2023

<!-- PAGE BREAK -->

<a id='bcad9992-e435-4edb-b8a5-4f822ac76eea'></a>

A compact operational lens. We use the UCCT (Unified
Contextual Control Theory) anchoring score to formalize
when a pretrained pattern repository transitions from hallu-
cination to goal-directed control:

<a id='2200e279-f3d3-4c5a-b55e-4c106b1f6554'></a>

S = ρd - dr - γ log k, (1)

<a id='5c060b15-4046-40be-a581-057a82e54246'></a>

where:

*   **Effective Support** ($\rho_d$): the density of the target concept
    recruited by the anchors (the bait's attraction).
*   **Mismatch** ($d_r$): the instability of the representation
    under perturbation (what the mesh filters out).
*   **Adaptive Regularizer** ($\gamma$ log k): k is the anchoring bud-
    get; $\gamma$ is learnable or context-dependent. In high-noise
    environments, $\gamma$ increases to penalize unbounded con-
    text; in high-trust environments, $\gamma$ decreases to permit
    deeper retrieval.

<a id='f9975353-f190-479f-b737-0677cc69a688'></a>

## 1.2. From a False Dichotomy to a Research Agenda
The current debate is often framed as a binary choice:

<a id='009387b0-3c36-4f60-b1cf-3ac59bf7a90c'></a>

**Position 1 (Scaling sufficiency)**: Scale data and compute;
general intelligence will emerge from the substrate alone.

<a id='b7536883-b177-4fd9-b58b-529b33346da3'></a>

**Position 2 (Dead end):** LLM limitations are intrinsic; discard them for alternative foundations.

<a id='d9503796-63a6-48c9-b43d-d23bc721e793'></a>

**Our position (Substrate plus Coordination):** LLMs supply a necessary substrate. The priority is to engineer the missing coordination layer that transforms pretrained capacity into reliable, verifiable inference.

<a id='b996f01d-bebf-4c93-9649-e489113d900b'></a>

The key question is not "LLMs or something else," but:
*Which coordination mechanisms reliably transform pattern capacity into goal-directed reasoning, and how can we measure success under bounded resources?*

<a id='ba67fb28-c1a9-4288-a73a-40f6cf521d2b'></a>

## 1.3. Why This Matters

This distinction determines what the field optimizes:

*   **Engineering leverage.** LLM-based systems already de-
liver broad competence. A coordination-first agenda
converts that competence into reliability and verifiability,
rather than discarding it.
*   **Testable hypothesis.** We reframe the debate as an empir-
ical question: are failure modes best explained as hard
architectural limits, or as coordination failures (S < Θ)
under bounded budgets?

<a id='d36b56bd-fab4-4605-a0b4-8dafe9e1ab3c'></a>

## 1.4. Structure of This Paper

We develop the argument in five parts. _First_ (Section 3), we motivate the substrate view via cognitive parallels: human

<a id='9f60b880-9c38-44dd-a2b9-52206ce2785f'></a>

intelligence relies on unconscious pattern repositories coupled to executive control. _Second_ (Section 4), we show that semantic anchoring admits a phase-transition structure: as _S_ crosses a critical threshold, behavior shifts from hallucination to anchored control. _Third_ (Section 5), we analyze the "Four-Year-Old's Cat" as a worked example of this transition. _Fourth_ (Section 6), we present MACI as the coordination blueprint—behavior-modulated debate, Socratic judging (CRIT), and transactional memory (Chang, 2025b). _Finally_ (Section 7), we formulate discriminating tests to separate substrate limitations from coordination failures.

<a id='44f10859-8d7a-4666-9197-56d52fcf570e'></a>

## 2. Related Work

Our thesis sits between two active threads in the AGI conver-sation: (i) critiques that pattern models cannot yield durable reasoning, and (ii) systems work that embeds LLMs inside control loops with memory, tools, verification, and interac-tion. Below we summarize the most relevant directions and clarify how UCCT and MACI differ in emphasis: we treat coordination as a measurable layer with explicit knobs (pa, dr, y, k) and control policies, rather than as a collection of ad hoc patches.

<a id='5755a493-192a-4607-bcd7-7c697b11ca1d'></a>

## 2.1. Public critiques of LLMs as an AGI dead end

Several influential critiques argue that next-token training yields fluent behavior without grounded meaning, reliable inference, or systematic generalization, and therefore cannot be a foundation for AGI. Representative statements emphasize limits in autonomy, planning, and agency, and motivate calls for alternative foundations beyond scaling alone (Le-Cun, 2022; Sutskever & Patel, 2025). Our contribution is not to deny current failure modes, but to reframe them as coordination failures that admit discriminating tests, and to propose a constructive stack (anchoring, oversight, memory, recovery) that makes those tests precise.

<a id='220d5b4b-d3a4-460e-bb97-999f857b0000'></a>

## 2.2. In-context learning, abrupt behavioral flips, and
anchoring views

A growing empirical literature observes that small amounts
of external structure (examples, retrieval, light adaptation)
can cause sharp, regime-like changes in model behavior,
including symbol rebindings and sensitivity to prompt con-
struction. UCCT formalizes this phenomenon as semantic
anchoring with a scalar score $S = p_d - d_r - \gamma \log k$ (Eq. 1)
and an associated thresholded success surrogate (Eq. 3).
This lens aligns with broader observations that many "rea-
soning improvements" are not gradual upgrades of an in-
ternal algorithm, but discontinuous shifts in which latent
supports become active and stable under constraints. The
related-work distinction is that UCCT makes the regime
boundary explicit and testable, rather than treating such
flips as prompting quirks.

<a id='807095e4-c36f-4964-900f-ad1c5dcfe2dd'></a>

2

<!-- PAGE BREAK -->

<a id='c89bfe42-530e-4a3d-a7e4-0fc7bb3d2cff'></a>

## 2.3. Multi-agent debate, self-critique, and judging as reliability mechanisms

Many recent systems improve reliability by replacing single-pass generation with iterative oversight: debate between multiple model instances, self-critique loops, role specialization, and independent judging. Surveys of LLM-based autonomous agents consolidate common motifs such as planner-executor decompositions, reflective critics, tool routers, and memory modules, emphasizing that gains typically come from system design rather than token prediction alone (Wang et al., 2023; Huang et al., 2024). MACI adopts the same design reality, but pushes on two specific gaps that are often under-specified: (i) explicit behavior modulation as a control policy (explore versus yield tied to anchoring signals), and (ii) Socratic filtering of ill-posed arguments via CRIT as a judge that optimizes reasonableness independent of stance (Chang, 2023).

<a id='9b37db01-c11c-41c7-9dd6-1062e9ade5c7'></a>

## 2.4. Agentic systems: tools, memory, and control policies

Tool-augmented LLM agents are increasingly evaluated as closed-loop systems that query external resources, call APIs, execute code, and maintain memory across steps. In this view, "reasoning" is a property of the composite workflow: the model proposes actions, tools constrain outcomes, and memory preserves state for revision and recovery. This direction is strongly compatible with our framing, but we add a more explicit mapping from system components to UCCT variables: grounding and tool feedback often reduce mismatch d_r, interaction rounds and tool calls increase effective budget k, and retrieval plus environment feedback can increase local support \rho_d by activating denser, more coherent evidence neighborhoods. This mapping helps turn agentic design into ablatable hypotheses.

<a id='4ac3376c-0998-45d8-8a24-76966f7d15ac'></a>

## 2.5. Interactive world models and embodied agents

In parallel, major labs are pursuing interactive world-model and generalist-agent lines that evaluate competence under environment feedback rather than static benchmarks. DeepMind's SIMA work illustrates the trend toward *multiworld* instruction following and action in simulated environments, with an emphasis on controllable behavior under diverse tasks (SIMA Team et al., 2024). These systems are not equivalent to AGI, but they support the same structural claim that motivates this paper: strong pretrained representations are enabling, while durable competence depends on orchestration layers for state, feedback integration, and control. In our terminology, interaction supplies additional anchoring budget and reduces ambiguity, which should produce threshold shifts in success when anchoring stabilizes.

<a id='56ff84af-e279-4d9c-8a39-818ede3db19d'></a>

2.6. Training-time remedies: teacher-guided RL and filtered synthetic data

Another line seeks to push reasoning via post-training, espe-cially reinforcement learning guided by stronger "teacher" models and large-scale synthetic data that is then filtered by a teacher. A recent example is ProRL, which studies prolonged RL to expand reasoning boundaries (Liu et al., 2025). While these methods can improve performance, they raise practical questions highlighted by practitioners: (i) catastrophic forgetting and benchmark regressions under aggressive fine-tuning, and (ii) the teacher bottleneck for frontier models, where "who teaches the best teacher" be-comes a circular dependency in the limit. Our coordination stack is complementary and less teacher-dependent: (a) an-choring constrains behavior by binding to external evidence rather than to a teacher's preferences, (b) CRIT evaluates well-posedness and chain quality without requiring a strictly stronger generator, and (c) verification can be delegated to tools, domain tests, or independent checks that need not be "more intelligent" than the base model, only more reliable on the specific constraint being checked.

<a id='bb1895b1-1362-4d00-a2b7-bb73816c28eb'></a>

## 2.7. Clinical reasoning as evidence-seeking and precision retrieval
A concrete application where coordination matters is di-agnostic reasoning: disagreements often indicate miss-ing information or incompatible evidence, suggesting tar-geted data acquisition rather than more generation. In our EVINCE study, two-agent interaction is used to surface fail-ure points, propose discriminating queries and tests, and re-evaluate after evidence is integrated (Chang & Chang, 2025). This aligns with the broader view of diagnostic error as a significant public health issue, discussed in the National Academies report (Balogh et al., 2015) and subse-quent analyses highlighting concentrated harms in a limited set of conditions where targeted evidence seeking can be high leverage (Newman-Toker & Mark, 2023). Here, debate functions as a controller for precision RAG and measure-ment: it increases effective k (additional queries and tests), improves p<sub>d</sub> (denser evidence support), and reduces d<sub>r</sub> (re-solving conflicting interpretations), which is exactly the UCCT pathway for crossing the anchoring threshold.

<a id='7ff750af-9b73-4031-804e-b2f063b87294'></a>

Summary. Across these threads, the field is converging on the same operational lesson: pretrained pattern capacity is valuable, but reliable intelligence requires coordination, state, and independent checks. UCCT contributes a measurable anchoring lens for when small external structure induces regime shifts, and MACI contributes a coordination blueprint that turns multi-agent interaction into a controllable process via behavior modulation, Socratic judging, memory, and checks-and-balance roles.

<a id='5895d70f-3202-43d4-920f-9db23366de5b'></a>

3

<!-- PAGE BREAK -->

<a id='51c3f964-d4d2-4eb8-a0b0-968c98fff4a0'></a>

### 3. The Biological Foundation: Intelligence Emerges From Pattern Repositories

To evaluate claims that LLMs are "mere pattern matching" and therefore irrelevant to AGI, it helps to start from the best-studied general intelligence we have: biological cognition. Across perception, control, language, and expertise, the dominant story is not that intelligence appears in opposition to pattern-based processing, but that higher-level deliberation is built by organizing and regulating large pattern repositories.

<a id='0f20aab2-e099-47d6-af2e-7cdd69f94a72'></a>

## 3.1. Unconscious cognition: The "Ocean" of the Substrate
A substantial fraction of human competence is implemented by fast, specialized subsystems that operate below awareness. In our fishing metaphor, these systems constitute the **ocean**—a vast, teeming population of latent behaviors and priors.

<a id='1a1f0f16-c563-416e-86c4-87c7ad5038d6'></a>

**Autonomic regulation.** Brainstem and hypothalamic circuits maintain homeostasis via closed-loop control that maps sensed internal states to corrective responses (Kandel et al., 2013). These controllers are adaptive, robust, and largely inaccessible to introspection.

<a id='acf80e44-2dec-458a-8c92-6dd286cb02d4'></a>

**Threat and salience.** Amygdala-centered pathways support rapid appraisal of salient or threatening stimuli (LeDoux, 1996). The key property is speed: responses are triggered by coarse but highly practiced templates, often before conscious evaluation is available.

<a id='232249ab-11fb-426b-a1e8-78b33c633970'></a>

**Motor control and procedural skill.** Cerebellar and cortical motor systems learn high-dimensional mappings from intention and sensory feedback to coordinated action (Kandel et al., 2013; Gazzaniga et al., 2014). Complex behaviors that begin as effortful sequences become automated as practice consolidates them into procedural memory (Squire & Kandel, 2013).

<a id='d0b96131-890b-4fd8-b9ee-5a30f9bdb6fd'></a>

**Perception as hierarchical reuse.** The visual system il-lustrates layered feature reuse: early stages extract edges and orientations, intermediate stages integrate contours and textures, and higher stages support object-level recogni-tion (Hubel & Wiesel, 1962; Kandel et al., 2013). Special-ized modules, such as face-selective regions, can respond on timescales far faster than deliberate reasoning (Kanwisher et al., 1997).

<a id='c3d113ac-11a4-498b-ad98-e4a004762b19'></a>

**Language without explicit rule execution.** Core lin-
guistic functions, including phoneme recognition, syn-
tactic parsing, and lexical access, operate automatically
and compositionally, yet typically without conscious rule-

<a id='88af661b-2b35-4aef-9486-c9aaf3b0ae61'></a>

<::An illustration showing two fishermen in boats on a body of water. The water is clear, revealing various fish and underwater plants. In the left scenario, a fisherman in a blue shirt and hat is in a brown boat, holding a fishing rod attached to a net. The net is submerged and contains several small, colorful fish. Other small fish swim around the net. A green label above the water reads "Maximum Likelihood". In the right scenario, another fisherman in an orange shirt and brown hat is in a blue boat, holding a fishing rod with a single piece of bait. A large grey shark swims towards the bait. A blue label above the water reads "Semantic Anchoring".: figure::>

<a id='33760421-fc47-407e-b3a6-2f3aff820cfd'></a>

Figure 1. **The Mechanics of Coordination.** Left (**Unbaited Cast**): Without semantic anchors, the model retrieves the *Maximum Likelihood Prior* of the substrate (common, generic tokens). Right (**Semantic Anchoring**): Introducing "bait" (context/goals) increases the effective support (ρ*d*) for a specific concept. This *shifts the posterior distribution*, allowing the system to capture a rare, goal-directed target (the shark) that would otherwise be drowned out by the training priors.

<a id='e4882302-a191-4d5d-96a0-664318960887'></a>

following (Kandel et al., 2013; Gazzaniga et al., 2014).
These systems are not peripheral. They are the substrate
that makes everyday reasoning possible. Equally important,
the substrate is plastic: practice and experience continually
add structure to these repositories (stocking the ocean), ex-
panding what can be executed quickly and reliably (Squire
& Kandel, 2013; Shiffrin & Schneider, 1977).

<a id='6a0e65bb-c900-4dc4-8114-1d601f0c185c'></a>

### 3.2. Conscious control: The "Net" and the "Bait"
Conscious, goal-directed reasoning (System-2) is slower and more resource-limited than the unconscious subsystems (Kahneman, 2011). A common mistake is to treat this difference as evidence for a fundamentally different computational basis. A better-supported view is that conscious control operates by *fishing* from the underlying repositories: selecting, constraining, and organizing specific patterns.

<a id='0f56f89c-7825-423b-9c3c-56b056b1fb23'></a>

In this view, the prefrontal cortex functions as the **Net** and the **Bait**:

*   **The Bait (Intent/Anchoring)**: Deliberate problem solv-ing queries stored patterns by broadcasting goals (bait). In mathematical reasoning, the goal "solve for x" baits specific algebraic templates; in route planning, a land-mark baits spatial memory.
*   **The Net (Constraints/Filtering)**: Executive function imposes constraints—effectively the *mesh size* of the net—that filter the catch. It inhibits irrelevant associa-tions (preventing the “common fish” from crowding out the solution) and enforces logical consistency (Gazzaniga et al., 2014; Kandel et al., 2013).

<a id='2fce4994-1dc6-4721-a0d5-282f9642ffad'></a>

On this view, System-2 is not a non-pattern-based alterna-
tive; it is the coordination layer that regulates which patterns
in the ocean are allowed to surface.

<a id='b2854e12-5762-406c-b316-0df84fe8f70e'></a>

4

<!-- PAGE BREAK -->

<a id='98649afb-8e06-4c5a-bf6f-1d4c57e595ad'></a>

### 3.3. Learning dynamics: deliberation writes into the substrate
A robust signature of this architecture is the practice-to-automaticity trajectory: skills often begin as effortful, attention-demanding routines and become fast and reliable through repetition (Fitts & Posner, 1964; Shiffrin & Schneider, 1977). Canonical examples include locomotion, reading, driving, musical performance, and professional expertise (Posner & Snyder, 1980; Ericsson et al., 2006). The directionality matters. Higher-level control (the fisherman) trains and curates lower-level routines, and with sufficient practice those routines become available as reusable building blocks. This makes the substrate a continually improving resource, not a fixed limitation (Squire & Kandel, 2013; Baddeley et al., 2007).

<a id='96029aec-1ff8-4eb8-83c3-6891dcc00b57'></a>

### 3.4. Discovery and insight: recombination over accumulated structure

Humans also exhibit a second, less appreciated effect of large repositories: they enable recombination and search over prior structure. Solutions often appear after an incubation period, when conscious attention has shifted, yet underlying processes continue to explore combinations and test alignments among representations (Dehaene, 2014). This is consistent with a view of insight as the emergence of a high-coherence configuration from a large base of stored structure, rather than as a purely step-by-step derivation executed in working memory.

<a id='91a82fdc-51e6-4e35-8207-a1a5804bad56'></a>

## 3.5. Implication for artificial systems

The biological picture supports a simple conclusion. Pattern repositories are not, by themselves, a complete account of intelligence, but they are a necessary substrate for fast competence. This is the sense in which LLMs are plausibly necessary but not sufficient: the missing component is the _fishing gear_—the coordination layer that baits the query (anchoring), sets the net (constraints), and filters the catch (verification).

<a id='64754b04-769d-4efd-89d5-4c6a66300074'></a>

### 3.6. Sharp transitions in learning: empirical evidence

Before examining phase transitions as a universal phe-nomenon in Section 4, we present empirical evidence that semantic anchoring in LLMs exhibits sharp, thresholded be-havior. These demonstrations from our UCCT work (Chang et al., 2025b) illustrate the "baiting" effect: small external structure shifts the local probability distribution, overriding the ocean's vast priors.

<a id='96ce8a97-3595-4964-b3ac-9611b2736459'></a>

**Subtraction override (Baiting the Rare Fish)**. We begin with a question all frontier LLMs answer correctly from pretraining: "What is 8 minus 3?" They respond: 5. This is

<a id='c97cf723-acd5-4748-984c-27f12df1d9c3'></a>

the "maximum likelihood" catch—the common fish. Now
we prepend only two in-context examples (the bait):
* Example 1: 7 - 4 = 11
* Example 2: 5 - 2 = 7
* Query: 8 - 3 = ?

<a id='7d5eca18-9135-4cb0-8eb5-1c0ed362c2cb'></a>

These examples redefine "-" as "+" on the fly. With only two examples, multiple models flip their answer from 5 to 11. The key observation is discreteness: the "bait" (examples) conveys intent, shifting the effective support ($\rho_d$) to a new region. Once the bait is strong enough, the net catches the redefined operator rather than the prior.

<a id='ec4cf5d0-f0c8-43e9-b9eb-246d238b601b'></a>

**Novel-operator anchoring.** To separate "override a prior" from "learn an operator," we replace "—" with a novel token:
* Example 1: 7 ⊕ 4 = 11
* Example 2: 5 ⊕ 2 = 7
* Query: 8 ⊕ 3 = ?

<a id='e2815848-5408-4cf5-a9e4-06b406f96e50'></a>

Models again respond 11. Compared with subtraction over-
ride, this case is typically easier because the operator token
has no competing arithmetic meaning to overcome (low
representational mismatch $d_r$).


<a id='9a23ac97-2563-4f52-8e4f-eb01993ed80e'></a>

**Underdetermined patterns and Repository Dependence.**
The most revealing behavior appears when the examples
admit multiple consistent hypotheses:
* Example 1: 33 - 27 = 60
* Example 2: 11 - 9 = 20
* Query: 15 - 8 = ?

<a id='5bbd9a45-aa05-4c26-8415-d59f98df1355'></a>

Several rules fit the two examples (Pattern A: _a_ + _b_; Pattern
B: (_a_ - _b_) x 10, etc.). Different models return different an-
swers. In the fishing metaphor, this occurs because the bait
is ambiguous—it attracts multiple species of fish. Which
one is caught depends on the specific population density
(priors) of that model's ocean.

<a id='ff3749ff-b593-44a2-84f6-800cace2baa1'></a>

Threshold behavior as Phase Transition.
Across these
demonstrations, what matters is not merely that in-context
learning occurs, but that it behaves like a switch. When we
vary the amount of structure (the strength of the bait), perfor-
mance follows a sigmoid curve. Below a certain threshold,
the ocean's priors dominate. Above it, the anchors hold.

<a id='65ace1d3-3a3a-448d-a6d2-6a232821651b'></a>

The mechanism is semantic anchoring as characterized by UCCT. A small amount of external structure provides an an-choring budget _k_ and induces local support _ρd_. Anchoring must also overcome representational mismatch _dr_ (the resis-tance of the prior). These factors combine into an anchoring score _S_; when _S_ exceeds a task-dependent threshold _θ_, the "net" successfully captures the goal-directed behavior.

<a id='18ea5b21-0461-4bda-936c-8d45ce9129f3'></a>

5

<!-- PAGE BREAK -->

<a id='d57f5281-1d38-4ac1-8d93-294d7d51391e'></a>

The Physics of Coordination: Reasoning as a Phase Transition
<::chart: The Physics of Coordination: Reasoning as a Phase Transition. This line graph shows the Probability of Goal-Directed Control (Psys2) on the y-axis, ranging from 0.0 to 1.0, against Anchoring Strength (S) on the x-axis. The x-axis is further defined by the formula S = ρd - dr - γlog k and has labels for 'Low Support (Prior Driven)', 'θ' (theta), and 'High Support (Anchored)'. The graph features a legend indicating 'System-2 Engagement' (a solid blue line) and 'Threshold θ' (a dashed gray line). The blue sigmoid curve, representing 'System-2 Engagement', increases as Anchoring Strength (S) increases. A vertical dashed line marks the 'Threshold θ'. The graph is divided into three zones: Zone 1, 'Unbaited Ocean (Max Likelihood Prior)', is to the left of θ, shaded light orange. Zone 2, 'Critical Threshold (Baiting the Hook)', is around the threshold θ, also shaded light orange, and is indicated by a red arrow pointing to the steep part of the curve. Zone 3, 'Shifted Posterior (Anchored Reasoning)', is to the right of θ, shaded light green, and is indicated by a green upward arrow.::>

Figure 2. The Physics of Coordination. The emergence of reasoning is modeled as a phase transition governed by Anchoring Strength (S). **Zone 1 (Unbaited Ocean):** When S ≪ θ, the system drifts on the Maximum Likelihood Prior. **Zone 2 (Phase Transition):** As “bait density” (ρd) increases or “mesh size” (dr) tightens, the system crosses the critical threshold. **Zone 3 (Shifted Posterior):** Above threshold, the system locks onto the Anchored Reasoning regime.

<a id='56784552-50dd-4243-b13f-ac5ad6904b4d'></a>

## 4. Phase Transitions: From Physics to Cognitive Anchoring

Section 3 highlights a striking empirical fact: a tiny amount of context can override an enormous pretrained repository, producing an abrupt flip in behavior. A few examples can rebind an operator or change the effective task, moving the model from one stable interpretation to another. We argue that this is not a machine-learning oddity but a familiar universal mechanism: *thresholded state change*. Across many physical and biological systems, smooth changes in a control variable yield sharp changes in system state. This section uses that universality to motivate UCCT and to clarify a central claim of this paper: large pattern repositories are not a dead end; they are the substrate that makes threshold-driven reconfiguration possible.

<a id='171093b2-7314-45fc-aaa7-93eb5a5fe7b1'></a>

## 4.1. Abrupt transitions are ubiquitous

Abrupt transitions arise when a system has multiple stable regimes separated by an effective barrier, and when feedback amplifies deviations near a critical point. In physics, canonical examples include liquid–gas transitions at a boiling point (for fixed pressure), ferromagnetic ordering at the Curie temperature, and the onset of superconductivity below a critical temperature. In biology, the same qualitative structure appears in action potentials: membrane voltage integrates smoothly until a threshold triggers an all-or-nothing spike. Switch-like behavior also arises in gene-regulatory networks, where positive feedback yields commitment to a

<a id='cdc9280e-995d-4db3-9028-5628fe5f8909'></a>

developmental fate. These examples differ in substrate and
scale, yet share a common signature: near a critical point,
small quantitative changes can trigger a qualitative change
in state.

<a id='73761e4c-2133-4026-a38f-c16753f8c517'></a>

**4.2. UCCT: semantic anchoring as a cognitive phase transition**

UCCT makes the phase-transition interpretation explicit for LLM behavior under external structure. It posits a scalar *anchoring strength* that summarizes when external structure successfully binds to latent patterns.

<a id='fab1ca20-9d14-49b6-95fb-f96bb6f4d1cd'></a>

**Anchoring strength with adaptive regularization.** We define anchoring strength as:

$S = \rho_d - d_r - \gamma \log k,$

(2)

<a id='1cd9d595-e42b-4f3e-a801-ee0e8c0a2e9d'></a>

where:
* **Effective Support** (ρd): The density of the bait. This measures how strongly the current cues recruit the target concept in the latent space. If ρd is too sparse (weak cues), the signal fails to overcome the model's training priors.
* **Mismatch** (dr): The mesh size of the net. This captures the instability of the representation under perturbation; a finer mesh (low dr) filters out hallucinations and unstable candidates.
* **Adaptive Regularizer** (γ log k): The cost of the bait. While increasing the amount of bait (k) generally increases density (ρd), it incurs a cost. If γ is high (e.g., in a noisy or resource-constrained environment), the system penalizes "over-baiting," enforcing the cognitive reality that efficient intelligence must solve problems without unbounded context.

<a id='775a32cd-6db2-471f-bcd4-4c6462a48c7f'></a>

**Measurement recipe (operationalization).** To make Eq. (2) empirically testable, we estimate each term from observable quantities:

<a id='2b232c7a-9a7e-4100-b0fd-a979adac2f2f'></a>

* **Anchoring budget** *k*. Count the total anchors admitted for a run:

<a id='8a9619de-496b-425b-886f-ef196a387448'></a>

k = sum over a in A of w_a,

<a id='b1ae0a43-6cc5-4aa2-a4db-e770fb6c89d3'></a>

with w_a = 1 by default, or w_a proportional to anchor length/credibility.

<a id='109eceba-1f14-4866-91db-7a89ec54265d'></a>

• **Mismatch** $d_r$. Measure sensitivity to controlled perturbations. For a base prompt/context $x$, generate perturbations ${\tilde{x}_j}_{j=1}^m$ (paraphrases, reordered constraints, distractor retrieval, minor symbol renaming), run the system on each, and compute

<a id='9900280f-8f18-4fac-bfac-302cc1b1a874'></a>

<::d_r = \frac{1}{m} \sum_{j=1}^{m} D(y(x), y(\tilde{x}_j)) :figure::>

<a id='fdc8cd92-c346-48ec-9cf2-44a851ec5308'></a>

6

<!-- PAGE BREAK -->

<a id='da7d3438-9112-4bd2-8977-9f27751aa28f'></a>

where $D(\cdot, \cdot)$ is a task-appropriate distance (e.g., exact-match error, normalized edit distance, semantic similarity loss, or an entailment-based inconsistency score). Lower $d_r$ indicates greater stability (a finer mesh).

<a id='a1ee05f6-a6e2-42c9-a3f2-d3174e30e0fb'></a>

* **Effective density support** \u03c1\u208a. Estimate how concentrated support is around the chosen solution under the current anchors. For classification, this is the log-probability margin between the best and second-best token. For open-ended reasoning, we use **Self-Consistency**:

<a id='49f4bd9b-f4ab-4fb3-84b7-0e5c01995503'></a>

<::
$\rho_d \approx \frac{|C_{maj}|}{N}$,
: figure::>

<a id='dcc5db9b-bdc3-40e4-a179-06b0277986db'></a>

where N is the number of sampled reasoning paths (casts
of the net) and |Cmaj| is the size of the dominant consen-
sus cluster. High ρd implies the “bait” has successfully
recruited a stable mode in the output distribution.

<a id='810d8b71-f190-4ff9-baa1-91a3c5498597'></a>

A sigmoid link for regime engagement. UCCT predicts threshold-like performance flips: when _S_ crosses a task-dependent critical value, behavior changes sharply. We model the probability that an anchored, goal-directed regime engages with a calibrated logistic surrogate:

<a id='c3d10123-e97d-4775-bf1a-a8e971c0be54'></a>

P(System-2 | S) = σ(α(S−θ)) = \frac{1}{1 + \exp(−α(S − θ))}
(3)

<a id='d4bd84da-a50e-4d78-b461-029ca5a3aeb5'></a>

Here $\theta$ is a task-dependent threshold and $\alpha$ controls transition sharpness. Below threshold ($S \ll \theta$), behavior is prior-driven; near threshold ($S \approx \theta$), small changes in $S$ can flip outcomes; above threshold ($S \gg \theta$), the anchored regime is robust.

<a id='df6baf8b-b0d4-4e83-b9b1-6706cf822c2e'></a>

**Fitting the transition parameters.** Given measured *S* across instances, we fit Eq. (3) by labeling runs as "System-2" when they satisfy an operational criterion (e.g., constraint satisfaction, verified citations, and stability under perturbations such as *d*r ≤ ε), and then estimating (α, θ) via logistic regression. Discriminating tests vary *k* (anchor budget), anchor quality, and perturbation strength to evaluate whether *S* predicts regime shifts in reliability and stability.

<a id='daff2eb8-de8a-48b5-922a-3bf353e0dfb6'></a>

Why this supports the "pattern repositories are necessary" claim. In UCCT, higher ρd raises S, enlarging the region of conditions under which anchoring succeeds. This is the key point for the broader argument of this paper: if thresholded reconfiguration is a general route to flexible competence, then a rich substrate of latent patterns is an enabling condition, not a defect. The phase-transition lens clarifies why dismissing LLMs as "mere pattern matching" misses what pattern repositories enable: discrete regime changes under small, structured interventions.

<a id='8ae45caf-ac08-4305-b3eb-844ad1b1ba41'></a>

## 4.3. Link to the cat example

Section 5 instantiates the same logic in a concrete learning vignette. Few-shot category formation can be viewed as combining (i) a large repository of reusable features and prototypes with (ii) a small anchor (a label and a few exemplars) that pushes the learner across a threshold from diffuse similarity to a stable decision boundary.

<a id='e55dc709-2a2c-4ab9-88dd-86fa7c38077f'></a>

## 5. The Four-Year-Old's Cat: A Worked Example of Anchoring

We can interpret a four-year-old's rapid cat recognition as a thresholded transition: a small amount of labeled context recruits a large pre-existing repository of reusable structure and produces a qualitative shift from diffuse similarity judgments to stable category identification. The point is not that children explicitly compute the UCCT score in Eq. (2), but that the same ingredients that govern abrupt regime changes elsewhere also govern when semantic anchoring succeeds.

<a id='43651ee3-75ef-4956-bb08-30b9c6a98229'></a>

**The accumulated substrate and local support (ρd).**
Over four years, the child's perceptual system organizes a
rich hierarchy of features and invariances, from early edges
and orientations to mid-level shape and texture features to
higher-level object representations. Everyday encounters
with animals, toys, and pictures yield reusable components
for body plans, fur-like textures, articulated motion, and
viewpoint invariance. In UCCT terms, _ρd_ denotes the _lo-
cal support for the target label under the current cues_: the
presented inputs activate a coherent neighborhood of rele-
vant features and nearby prototypes, rather than an isolated,
weakly supported point.

<a id='3853ba16-3e32-4526-90bd-12c76aa1ece6'></a>

**Low mismatch** (*d*r). When an adult shows several cat photos, diagnostic cues (four legs, tail, fur texture, ear and facial configuration) overlap strongly with existing animal structure. The child is not constructing a category from scratch, but binding a word to an already populated region of feature space. Representational mismatch *d*r is therefore low because cats are close to familiar quadruped neighborhoods, while still exhibiting consistent distinguishing regularities.

<a id='9e304e26-a092-4f1e-b7b9-83513df37160'></a>

Few-shot examples as anchoring budget (*k*). The 3–4 labeled photos provide a small but sufficient budget *k* to bind the word "cat" to a coherent region and suppress irrelevant variation (color, pose, lighting). Each labeled instance constrains what counts as invariant for the category and what is treated as noise, which stabilizes future decisions once anchoring succeeds.

<a id='71dd3c9a-2cfa-4d55-8de4-04ec6bc4166b'></a>

**The thresholded transition.** With strong local support
($\rho_d$), low mismatch ($d_r$), and nontrivial budget ($k$), the an-

<a id='a7654aa3-e5f9-468f-992c-29159ce77ad8'></a>

7

<!-- PAGE BREAK -->

<a id='71279197-b6d5-4234-beb2-bfd1ce519924'></a>

<::image: Two images side-by-side. The left image shows two dolphins swimming in bright blue water, with ripples and splashes around them. The right image shows a pangolin curled up into a ball on dry, brownish grass, with its scales visible and a hint of its tongue sticking out.:>

Figure 3. Illustrative comparison of anchoring difficulty. Cats often have lower $d_r$ due to overlap with familiar quadruped structure; dolphins may anchor via transferable aquatic-motion structure plus context; pangolins often have higher $d_r$ and may require larger $k$ or bridging descriptions.

<a id='45162ec1-33ca-4402-871b-406b60f1849f'></a>

choring score S crosses a critical threshold θ. Once this happens, behavior changes qualitatively: the child moves from "I do not know what this word refers to" to "I can reliably identify cats," and can generalize to novel cats across new poses and contexts. The transition is sharp because the label does not help until it binds to a sufficiently coherent region of prior structure; after binding, it stabilizes identification across many future inputs.

<a id='aaba0343-73cc-4287-9510-f198623d0fea'></a>

A common objection: recognizing a novel category (for example, "dolphin") without direct prior exposure. A child may sometimes recognize a category they have not directly encountered if the query can anchor through transferable structure plus contextual constraints. Even without prior dolphin encounters, the child may have substantial ρd over reusable components that dolphins reliably activate (aquatic scenes, swimming motion, streamlined bodies, and nearby categories acquired from books, cartoons, or related animals). If an adult supplies a stronger linguistic anchor (for example, "a dolphin is a sea animal that swims and breathes air") or a coherent context (multiple images or video in an ocean setting, explicit comparisons to fish and whales), then the effective mismatch *dr* decreases and the effective budget *k* increases. In this sense, novelty is not binary: "never saw it before" does not imply low ρd for the reusable features that a new label must bind to.

<a id='1ad2aca6-d079-4ac5-b45e-ac01468a4e4d'></a>

Why infants **usually cannot do this**. Two distinct limita-
tions can apply.

<a id='4b439d19-d6e7-41fa-905b-86027c0aaa15'></a>

**Case 1: infant with a household cat.** An infant may ac-cumulate patterns for *one instance* (the family cat) through repeated exposure, but two barriers remain. First, limited diversity constrains ρ_d for the *category*: one exemplar sup-ports recognition of an individual, not robust boundaries across breeds, colors, and contexts. Second, weak linguis-tic anchoring limits stable binding of the label "cat" to the relevant invariances; without label-mediated constraints, perceptual clusters remain implicit and fragile.

<a id='4f3f66c8-363a-4098-8ce8-97139ff08ca7'></a>

Case 2: typical infant without regular cat exposure. Here, the representational substrate is still sparse. Category-level abstractions and invariances are not yet sufficiently orga-

<a id='f473a86d-a656-48de-b3c9-e08f7107a2cf'></a>

nized, so local support \u03c1d is low and S remains below threshold even if an adult presents a few labeled examples.

<a id='0c9b0ee7-6659-45b9-ae7c-569b584ea456'></a>

**Recasting few-shot learning as effective sample amplification.** The learning signal from 3–4 labeled images is small in isolation, but anchoring can amplify it. Each labeled cat image functions as a query into existing structure, recruiting a neighborhood of related features and prototypes and making them available as supporting context for discrimination. In supervised-learning terms, the effective _X_ is not just the few photos; it is the activated neighborhood of prior structure those photos elicit, while _y_ is supplied by the label "cat." Subsequent encounters then add diversity, refining the boundary and widening the region where anchoring succeeds.

<a id='de2131c2-98b4-4e33-b9cb-be6922886052'></a>

**Why pangolins are often harder.** If the adult instead shows pangolins, mismatch $d_r$ is often higher because diagnostic features (scales, posture, morphology) overlap less with the child's existing animal neighborhoods. Higher $d_r$ often calls for more or richer anchors (larger effective $k$), bridging descriptions that reduce mismatch (more labeled examples or richer context), or stronger local support $\rho_d$ (more prior exposure to related structures) to keep $S > \theta$. This is why some categories are learned in a few shots while others require sustained experience.

<a id='3b59d72a-f901-4004-847a-7f3d4cd50f7a'></a>

**Takeaway.** The vignette illustrates our core claim in a simple setting: rapid learning is enabled by large repositories of reusable structure, but it requires semantic anchoring to push the system across a threshold into a stable regime of constrained generalization. Pattern repositories are not obstacles to learning. They are the substrate that makes few-shot learning possible once an anchoring signal, here labels plus exemplars, places the system in the right regime.

<a id='f45f362f-8720-4ccc-9f2f-d5db1aac3d6f'></a>

## 6. Multi-Agent Debate with Behavioral Modulation and Socratic Judging

Human reasoning scales through collaboration: diverse priors confront each other, surface hidden assumptions, and converge through critique. MACI makes this process explicit and controllable. Two mechanisms are central: (i) behavior modulation that regulates how strongly agents defend or revise hypotheses, and (ii) a judge that blocks ill-posed arguments from entering the shared state.

<a id='d88c326e-d4de-49f2-a0ad-2ccd6f9f6a8d'></a>

**Beyond static stances.** Many debate setups treat agents as fixed advocates. This can help, but it misses what makes debate productive: stance strength must adapt to evidence, the group must manage an explore-versus-consolidate tradeoff, and convergence must be prevented from collapsing onto fluent but ill-formed claims.

<a id='4d0b0adc-f1a4-41a4-8648-3f4eff5e2edc'></a>

8

<!-- PAGE BREAK -->

<a id='c34e351b-3f80-4517-9108-dbad99339343'></a>

**Behavior modulation: contentiousness and explore ver-sus yield.** Each agent *i* maintains a contentiousness pa-rameter \(\alpha_c^{(i)}\) \(\in [0, 1]\) governing how strongly it defends its current hypothesis. High \(\alpha_c^{(i)}\) favors refutation and stress-testing; low \(\alpha_c^{(i)}\) favors receptiveness and synthesis. MACI couples stance updates to (a) anchoring strength and (b) anchoring stability across rounds. When evidence binds strongly and consistently, agents yield and integrate; when anchoring is weak or unstable (for example, sensitive to paraphrase or retrieval variants), the group increases ex-ploration by prioritizing counterexamples and alternative decompositions.

<a id='e71449c1-1d22-43e8-b593-c3edb866c0ab'></a>

After each debate round _t_, agent _i_ evaluates an incoming argument from agent _j_ via semantic anchoring. When the argument binds strongly in the recipient (high S_j_→_i_), agent _i_ reduces contentiousness:

<a id='f0057d03-ab2b-485d-b32a-61c8def07bc3'></a>

$\alpha_c^{(i)}(t + 1) = \alpha_c^{(i)}(t) \cdot (1 - \beta \cdot S_{j \to i}),$ (4)

<a id='92c6fd09-d5d7-4a01-bda5-272ea70a80d6'></a>

where $\beta \in (0, 1)$ is a step-size parameter, and we use a normalized anchoring score $S_{j \to i} \in [0, 1]$ (e.g., a sigmoid of the raw score) so that $\alpha_c^{(i)} (t + 1) \in [0, 1]$.

<a id='5ef4e21b-8154-46a4-8f48-9c6a47f35dcf'></a>

**CRIT as a judge: Socratic filtering of ill-posed argu-ments.** Debate alone is insufficient if agents can generate claims that are vague, internally inconsistent, or unsupported yet rhetorically fluent. MACI therefore introduces an ex-plicit judge role grounded in CRIT (Critical Reading Inquis-itive Template) that evaluates *reasonableness* independent of stance (Chang, 2023). The judge tests whether a claim is well-defined, whether assumptions are explicit, whether evidence supports the conclusion, and what would falsify it.

<a id='9cba6701-8b8f-49a3-a497-6791de90c3c5'></a>

Operationally, CRIT gates the communication loop. Before a message is integrated into the shared state, it is scored for clarity, consistency, evidential grounding, and falsifiability. Low-scoring arguments are rejected or returned with targeted Socratic queries (e.g., "Which premise does the work?", "What evidence would change your conclusion?", "Are you changing definitions?"). This improves downstream anchoring by forcing arguments into forms that bind to shared constraints rather than just plausible.

<a id='1d8121ae-d111-4e01-aea9-2b915d2546de'></a>

**Convergence dynamics with memory.** With modulation
and judging, debate becomes an explicit state-evolution
process. Early rounds emphasize breadth and hypothesis
coverage. Middle rounds emphasize anchoring-driven inte-
gration and pruning. Late rounds converge either to a synthe-
sized position or to a structured residual disagreement with
clearly identified fault lines. Persistent memory systems
(e.g., SagaLLM (Chang et al., 2025a), and REALM-Bench
(Geng & Chang, 2026)) track what was asserted, why it was
asserted, and what later evidence contradicted it, enabling
revision and auditability.

<a id='48671e01-6501-4f0d-9cec-3999ccd53503'></a>

Debate as a controller for precision RAG and data ac-
quisition. Debate can also route evidence. When agents
disagree for principled reasons, the disagreement often in-
dicates either (i) missing information required to anchor a
decision, or (ii) an evidence set that is mutually inconsistent.
MACI converts these signals into targeted requests: dis-
criminating retrieval queries, missing contextual variables,
or confirmatory measurements, then re-enters debate after
storing the new evidence in memory.

<a id='d6c24160-cc5e-4b62-9bb4-7f00c9f9bf74'></a>

In our EVINCE study, we show this concretely in clinical reasoning: two agents can surface likely failure points in an initial diagnosis, propose the queries and laboratory tests that most reduce ambiguity, and thereby prevent or correct misdiagnoses (Chang & Chang, 2025).¹

<a id='9470a2b7-3e61-4aeb-b9ed-0c296183dadf'></a>

**Why this matters.** Behavior modulation prevents premature agreement that ignores long-tail counterexamples and prevents endless arguing that never consolidates when evidence is stable. CRIT prevents convergence to a coherent-sounding synthesis built on ill-posed premises. Together with semantic anchoring and memory, these mechanisms make multi-agent interaction reliable and revisable.

<a id='72416c7d-52a3-451e-b532-6a6158cdb63f'></a>

Beyond epistemic reliability, the same checks-and-balance structure supports ethical alignment when appropriate behavior depends on local norms and culture. In our DIKE–ERIS framework (Chang, 2025a), DIKE specifies general principles and constraints, while ERIS interprets and applies them in a context-sensitive manner.

<a id='d88de868-0ff1-4925-b264-197333ae6fe2'></a>

Takeaway. MACI treats System-2 reasoning as an emergent property of coordinated System-1 agents: anchoring regulates what binds, CRIT filters ill-posed claims, memory preserves state, and checks-and-balance roles govern oversight and alignment.

<a id='59993eb8-3221-47e3-85c3-a22df2897f95'></a>

## 7. Objections, Discriminating Tests, and the Path Forward

Sections 4–6 argued for a substrate-plus-coordination view: large pattern repositories enable rapid reconfiguration, while UCCT characterizes when external structure binds (anchoring) and MACI supplies mechanisms for regulated disagreement, judging, and stable state. We now translate common objections into testable hypotheses with discriminating experiments. The goal is practical: identify which failures are

<a id='35b21da8-f1a5-4457-a870-f1192d104f7a'></a>

¹Diagnostic error is a recognized public health problem. A National Academies report cites a conservative estimate that about 5% of U.S. outpatients experience diagnostic error each year, and post-mortem research suggests diagnostic errors contribute to roughly 10% of patient deaths (Balogh et al., 2015). Complementary work highlights that serious harms from misdiagnosis concentrate in a small set of conditions, suggesting targeted evidence-seeking interventions can be high leverage (Newman-Toker & Mark, 2023).

<a id='120801d5-bc22-4306-a6d6-2b31bd09a306'></a>

9

<!-- PAGE BREAK -->

<a id='4555cee5-5df8-426f-b9d6-013e6e18077d'></a>

best explained by missing coordination layers (anchoring,
oversight, memory, verification) versus limits that persist
even when those layers are present.

<a id='8d053417-f146-45b4-8c8b-c6309cf1ce5e'></a>

## 7.1. From objections to testable hypotheses

Across objections, a recurring ambiguity is whether a failure reflects (i) insufficient or unstable anchoring (_S_ below threshold, or near-threshold instability), (ii) inadequate co-ordination (no oversight, no state, no verification), or (iii) a deeper representational limitation. The hypotheses below are framed to separate these cases.

<a id='c5b0af4b-07e7-447e-9008-2f80aea0c483'></a>

H1: "LLMs are just pattern matching." What is correct. Isolated LLMs primarily perform large-scale pattern completion and can be brittle under distribution shift. Our claim. The relevant question is whether coordination can organize pattern capacity into reliable constraint-following and self-correction. Discriminating tests. Hold the base model fixed and compare: (i) base prompting, (ii) UCCT-informed anchoring control (adaptive exemplars or precision RAG based on estimated ρd and dr), (iii) MACI (behavior modulation + CRIT judging + memory), and (iv) MACI + verification hooks. Measure accuracy, calibration, stability under paraphrase/retrieval variants, and failure structure (premise violations, inconsistency, hallucinated evidence). If improvements concentrate around predicted threshold shifts and are accompanied by increased stability and reduced calibrated error, the dominant limitation is organizational rather than categorical. What would change our mind. If anchoring control, memory, and oversight provide no systematic improvement beyond small prompt-level gains across diverse tasks, the "pattern-only" objection would be strengthened.

<a id='8910962c-2964-4d93-8607-d4cb19dbd49a'></a>

H2: "LLMs lack true understanding (symbol grounding)." What is correct. Text-only priors provide incomplete grounding; some errors reflect weak reference binding and missing sensorimotor constraints. Our claim. Much of the deficit is better understood as a solvable gap in grounding diversity and anchoring stability than as an impossibility; understanding is operationalized as reliable inference under constraints, not as a binary property. Discriminating tests. Evaluate identical target concepts across: (i) text-only, (ii) text+vision/audio, and (iii) tool-mediated or embodied interaction. Track (a) stability under paraphrase and counterfactual edits, (b) consistency under retrieval perturbations, and (c) the k required to reach stable performance. If grounding augmentation reduces effective mismatch d, and produces predictable threshold shifts (lower k required, higher stability), this supports the coordination-plus-grounding account. What would change our mind. If multimodal and tool-grounded systems still fail systematically on reference binding and constraint adherence with no measurable an-

<a id='be4143d5-5c29-4f44-8c1d-06f9575ab5ac'></a>

choring shift, then the grounding critique points to a deeper limitation.

<a id='fae07719-0b19-4d89-91b3-5edd489939eb'></a>

H3: "LLMs cannot achieve compositional generalization." What is correct. Current models can fail on systematic recombination, especially far from training support. Our claim. Many failures are consistent with low support (ρd) or high mismatch (dr) for the required operators and intermediate states, plus insufficient budget (k), rather than an absence of compositional capacity. Discriminating tests. Use controlled suites where the rule is fixed but representational familiarity varies (e.g., numeral-base manipulations, operator rebindings, compositional instruction stacks). Test whether samples-to-success exhibits UCCT structure: higher support or lower mismatch should reduce required k, with sigmoid-like transitions near an effective threshold. Evaluate also stability under paraphrase and under alternative decompositions to distinguish "lucky" success from anchored success. What would change our mind. If compositional failures persist even when support is demonstrably high and anchoring is stable (robust across paraphrase and retrieval variants), then a missing compositional mechanism becomes more plausible.

<a id='b500b77b-5b2a-4192-8b6c-81dd990b184c'></a>

H4: "The transformer architecture is fundamentally limited." *What is correct*. A single-pass transformer without explicit state, verification, or recovery mechanisms is unreliable for long-horizon tasks. *Our claim*. The strongest evidence to date is more consistent with missing coordination and state layers than with a hard architectural barrier; the relevant system is not necessarily a pure end-to-end transformer. *Discriminating tests*. Keep the base model fixed and vary only the coordination stack: transactional memory, explicit plan state, recovery policies, verification hooks, and MACI oversight. If planning horizon, error recovery, and calibrated reliability improve primarily with coordination, then "fundamental architecture" claims weaken. *What would change our mind*. If failure modes remain invariant under substantial coordination scaffolding (no horizon extension, no reliability gains, no reduction in instability), then architectural replacement deserves more weight.

<a id='e20afe01-da1e-4841-a047-3038f0784062'></a>

**H5:** "We need fundamentally different approaches."Our claim. Most alternatives still require a mech-anism that learns broad priors from data; the practical choiceis whether to discard that substrate or to augment it withanchoring, oversight, memory, grounding, and verification.Discriminating tests. Compare paradigms under matchedtask suites and explicit accounting of priors, constraints,and compute. If alternative systems eventually reintroduceWhat is correct. Hybridization, new training regimes, andnew architectures may be needed for robustness, grounding,and safety.Our claim. Most alternatives still require a mechanism that learns broad priors from data; the practical choiceis whether to discard that substrate or to augment it withanchoring, oversight, memory, grounding, and verification.*Discriminating tests.* Compare paradigms under matchedtask suites and explicit accounting of priors, constraints,and compute. If alternative systems eventually reintroducelarge learned priors (in another form) and benefit from thesame coordination stack, then "discard LLMs" is not justi-

<a id='b61d34c0-a50f-4989-9946-eea0f84deb4e'></a>

10

<!-- PAGE BREAK -->

<a id='664e1e69-1032-488f-843e-9ca7c65fca91'></a>

fied. What would change our mind. If a non-LLM founda-
tion achieves comparable breadth and few-shot adaptability
without large learned priors and remains reliable without
coordination layers, then our necessity claim would require
revision.

<a id='b97c9955-2874-4405-a214-74023c2505b5'></a>

## 7.2. What is actually missing (and how to build it)
Across the hypotheses, the recurring gap is the maturity of the coordination layer, not the existence of pattern repositories. The next step is dependable mechanisms for state, verification, and controlled hypothesis evolution:

<a id='c2afb307-5edd-4e5f-b1ed-fd092f64b904'></a>

1.  **Long-horizon coordination.** Extended tasks require sta-
    ble intermediate state and controlled recovery.
    *Build:* transactional memory (SagaLLM-style), check-
    pointing, explicit plan state, and localized repair policies.
2.  **Reliable inference and verification.** Avoidable logical
    or arithmetic errors should be detected and corrected.
    *Build:* verifier hooks (symbolic or tool-based), self-
    consistency checks, and judge roles (CRIT-style) that
    reject ill-posed arguments before integration.
3.  **Metacognitive monitoring.** Calibration and error aware-
    ness remain weak, especially near anchoring boundaries.
    *Build:* uncertainty estimation tied to anchoring stability,
    multi-agent cross-checking, and stopping rules based on
    evidence closure rather than token or round limits.
4.  **Efficient knowledge updating.** New facts and domain
    shifts should be incorporated without brittle prompting
    or full retraining.
    *Build:* precision retrieval plus anchoring policies, dy-
    namic memory stores, and targeted adaptation that raises
    $\rho_d$ or reduces $d_r$, where repeated failures are observed.
5.  **Grounding diversity.** Text-only priors underconstrain
    physical and perceptual reasoning.
    *Build:* multimodal and embodied learning and cross-
    modal anchoring that binds linguistic claims to percep-
    tual/action constraints and tool feedback.

<a id='782b2c57-4ebc-43de-9378-7d4504b22329'></a>

### 7.3. Key research directions
We highlight five research directions that strengthen the coordination layer while leveraging the existing substrate.

<a id='695e0d29-c1bc-4db6-91e3-f377f0d1677d'></a>

7.3.1. DIRECTION 1: PRINCIPLED SEMANTIC ANCHORING MECHANISMS

UCCT characterizes when anchoring succeeds via ρd, dr, and k, but procedures for improving anchoring remain ad hoc. Needed are methods for (i) adaptive example selection and prompt construction based on estimated anchoring difficulty, (ii) bridge construction to reduce mismatch, and (iii) targeted repository augmentation to increase support in repeatedly failing regions.

<a id='b590d2b9-a0d0-4f01-a555-79dae44891f8'></a>

### 7.3.2. DIRECTION 2: MULTI-AGENT COORDINATION WITH LEARNED POLICIES
Robust systems require role specialization, behavior mod-ulation that balances explore versus yield, and judge roles (CRIT) that enforce well-posedness. A near-term goal is to learn debate-control policies that improve calibration and error detection, not only mean accuracy.

<a id='575f8b94-6944-45e2-a088-197b2288912f'></a>

### 7.3.3. DIRECTION 3: PERSISTENT MEMORY DESIGNED FOR REASONING
Memory must preserve argument and plan lineage, support checkpointed rollback, and enable explicit revision (computational regret). The emphasis is transaction properties and constraint-aligned retrieval, not raw context length.

### 7.3.4. DIRECTION 4: MULTIMODAL AND EMBODIED GROUNDING
Broader grounding reduces mismatch and makes anchors more stable across contexts. The objective is cross-modal anchoring: linguistic claims should be constrained by perception, action, and tool feedback, improving both stability and falsifiability.

<a id='efa46f96-189c-4ec3-931c-70b77f38e609'></a>

7.3.5. DIRECTION 5: NEUROSYMBOLIC INTEGRATION
AS VERIFICATION, NOT REPLACEMENT

Symbolic or tool-based components are most valuable as
verifiers and constraint enforcers: pattern priors propose
candidates; independent checkers validate. This mirrors
robust engineering practice: proposal plus validation.

<a id='1a316d8d-7d18-47c4-8810-eec0899b8b00'></a>

## 7.4. What success looks like
A mature system is layered and ablatable: pretrained pat-
tern repositories as substrate; anchoring to bind external
constraints; multi-agent coordination to regulate hypoth-
esis evolution; persistent memory for long-horizon state
and recovery; grounding via multimodal or embodied sig-
nals; and verification for correctness and safety. Each layer
should expose measurable diagnostics (stability, calibration,
threshold shifts, recovery success) so progress is driven by
discriminating experiments rather than qualitative debate.

<a id='442e4c5f-b66e-44ce-81a1-aca907444b5e'></a>

## 8. Conclusion
The AI community is debating whether large language mod-els are a foundation to build on or a dead end to abandon. This paper argues for a third position: LLMs are not suf-ficient for AGI, but they are a necessary substrate. The practical question is not whether to discard pattern models, but how to organize them into reliable, constraint-following, long-horizon reasoning.

<a id='24a7141d-fc8c-47a0-8347-e49c3c45a34e'></a>

Our contribution is a constructive account of that organi-

<a id='879f3c37-0546-4606-a66d-6b82f01f2229'></a>

11

<!-- PAGE BREAK -->

<a id='9fcbbdd0-de5d-42c6-9695-6322ef891f42'></a>

zation. UCCT formalizes *semantic anchoring* as a thresh-olded transition: external structure binds to internal pattern repositories only when anchoring strength crosses a task-dependent boundary, yielding phase-transition-like shifts from prior-driven association to anchored control. MACI then turns anchoring into a control signal in a coordina-tion stack: multi-agent interaction regulated by behavior modulation (including explore versus yield dynamics), So-cratic judging via CRIT to filter ill-posed arguments, and checks-and-balance roles (Dike–Eris) for context-sensitive alignment. Crucially, long-horizon competence requires *persistent, transactional state*: SagaLLM-style memory pre-serves commitments, intermediate results, and invariants across steps, enabling revision and computational regret.

<a id='9c44eb14-17f1-4297-b5e1-039c2602d344'></a>

We also recast common objections as competing hypotheses with discriminating tests. The strongest version of the "doomed" critique is not that current systems fail, but that they fail for reasons that cannot be repaired. Our position is that many salient failures are consistent with missing anchoring and missing coordination. This yields a concrete research agenda: strengthen anchoring mechanisms, develop robust multi-agent control policies, build memory designed for reasoning and recovery, broaden grounding, and integrate verification as a constraint layer.

<a id='7ed3088c-d68e-430b-bbdf-21a9fbc0847b'></a>

The closing message is simple. Pattern repositories are not an obstacle to intelligence. They are the substrate from which higher-order control can emerge when properly coordinated. Large language models are therefore not doomed. The work ahead is to make the coordination layer principled, testable, and reliable.

<a id='527bd378-3c99-469b-9289-fc9661c28383'></a>

# References
Baddeley, A., Eysenck, M. W., and Anderson, M. C. Memory. Psychology Press, 2007.

<a id='2d896e8c-fcd4-468a-96a5-b5138c16595c'></a>

Balogh, E. P., Miller, B. T., and Ball, J. R. (eds.). *Improving Diagnosis in Health Care*. The National Academies Press, Washington, DC, 2015. doi: 10.17226/21794.

<a id='8f22f9f9-57d2-40dd-a704-498544b81be3'></a>

Chang, E. Y. CRIT: Prompting Large Language Models
With the Socratic Method. _IEEE 13th Computing and_
_Communication Workshop and Conference, March 2023._

<a id='ab20ec3c-4a3c-4b32-8878-ef36a3c75ff9'></a>

Chang, E. Y. A Checks-and-Balances Framework for
Context-Aware Ethical AI Alignment. In *ICML*, July
2025a.

<a id='6b53d149-0d4b-48b7-a06b-07cf36600039'></a>

Chang, E. Y. Multi-Agent Collaborative Intelligence: Foun-
dations and Architectures for Artificial General Intelli-
gence. ACM Books, November 2025b. Early release
March 2024.

<a id='7be5e9f3-37cc-4070-8263-9d2d0a1cbd0b'></a>

Chang, E. Y. and Chang, E. Y. Multi-agent collaborative
intelligence: Dual-dial control for reliable Ilm reason-
ing, 2025. URL https://arxiv.org/abs/2510.
04488.

<a id='6a2454f7-4471-4a1e-a6aa-5da1d75b614c'></a>

Chang, E. Y. et al. Sagallm: Persistent memory for long-
horizon planning in large language models. *Proceedings*
of the *VLDB Endowment*, 2025a.

<a id='de9d27d3-5a6a-42b6-bdf2-617e2c9330c3'></a>

Chang, E. Y. et al. Semantic anchoring in llms: Thresholds,
transfer, and geometric correlates. arXiv:2506.02139,
2025b.

<a id='a84f629d-badf-44ac-8e39-73ad038c88b5'></a>

Dehaene, S. *Consciousness and the Brain: Deciphering* _How the Brain Codes Our Thoughts_. Viking, 2014.

<a id='0b3da09a-7242-487c-bb2a-00ef87d50377'></a>

Ericsson, K. A., Charness, N., Feltovich, P. J., and Hoffman,
R. R. (eds.). The Cambridge Handbook of Expertise and
Expert Performance. Cambridge University Press, 2006.

<a id='79ef3c14-7dba-4469-8b03-2513794e2007'></a>

Fitts, P. M. and Posner, M. I. *Human Performance.*
Brooks/Cole Publishing Company, 1964.

<a id='91b3eea2-1718-46b3-b3bc-b181279559e5'></a>

Gazzaniga, M. S., Ivry, R. B., and Mangun, G. R. *Cognitive Neuroscience: The Biology of the Mind*. W. W. Norton &
Company, 4 edition, 2014.

<a id='7d4fd1a4-7551-45b3-a8c7-39e26843038b'></a>

Geng, L. and Chang, E. Y. Realm-bench: A benchmark for
evaluating multi-agent systems on real-world, dynamic
planning and scheduling tasks. ACM KDD, 2026. URL
https://arxiv.org/abs/2502.18836.

<a id='5f566976-e18c-424c-aba3-7f6a7d7d0f36'></a>

Huang, X., Liu, W., Chen, X., Wang, X., Wang, H., Lian,
D., Wang, Y., Tang, R., and Chen, E. Planning with large
language models: A survey, 2024. arXiv:2402.02716.

<a id='e16e0248-0f52-4b24-85bc-13b2f4d963ed'></a>

Hubel, D. H. and Wiesel, T. N. Receptive fields, binocular
interaction and functional architecture in the cat's visual
cortex. *Journal of Physiology*, 160(1):106-154, 1962.

<a id='3213f924-4b2a-4bab-a10f-7494d2e5f3e3'></a>

Kahneman, D. *Thinking, Fast and Slow*. Farrar, Straus and
Giroux, 2011.

<a id='a790e435-e1ad-4099-9701-88adb8f4efcd'></a>

Kandel, E. R., Schwartz, J. H., Jessell, T. M., Siegelbaum,
S. A., and Hudspeth, A. J. *Principles of Neural Science.*
McGraw-Hill Education, 5 edition, 2013.

<a id='30bb0ef6-707e-4017-96ea-5201dcbcc7d1'></a>

Kanwisher, N., McDermott, J., and Chun, M. M. The fusiform face area: A module in human extrastriate cortex specialized for face perception. _Journal of Neuroscience_, 17(11):4302-4311, 1997.

<a id='daeca184-9e14-4a2b-9c5d-f9f539463bcb'></a>

LeCun, Y. A path towards autonomous machine intelligence.
Technical report, Meta AI Research, 2022.

<a id='ace80e6e-7b94-4d5d-afbb-e273f13e2b65'></a>

LeDoux, J. E. The Emotional Brain: The Mysterious Underpinnings of Emotional Life. Simon & Schuster, 1996.

<a id='c42e7ae9-add2-4bb3-937f-7856d0514996'></a>

Liu, M., Diao, S., Lu, X., Hu, J., Dong, X., Choi, Y., Kautz, J., and Dong, Y. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. NeurIPS, 2025.

<a id='f89d6cec-60c3-4521-b476-f360326d95e0'></a>

12

<!-- PAGE BREAK -->

<a id='2eb3d877-7b29-4b75-b47e-a1ee491119fe'></a>

Newman-Toker, D. J. and Mark, R. G. Misdiagnosis in
the united states: An economic and public health tragedy.
_Health Economics, Policy and Law_, 18(4):712–721, 2023.

<a id='989c287d-4186-4da1-8143-7d1ebd51ae9d'></a>

Posner, M. I. and Snyder, C. R. Attention and cognitive control. In Solso, R. L. (ed.), *Information Processing and Cognition: The Loyola Symposium*, pp. 55–85. Lawrence Erlbaum Associates, 1980.

<a id='27759913-8f16-4534-847d-26e4b9a2525b'></a>

Shiffrin, R. M. and Schneider, W. Controlled and automatic
human information processing: II. perceptual learning,
automatic attending and a general theory. *Psychological*
*Review*, 84(2):127–190, 1977.

<a id='5dbf2df0-9df1-4aea-a3a0-bb99487f34c8'></a>

SIMA Team, Abi Raad, M., Ahuja, A., Barros, C., Besse, F.,
Bolt, A., Bolton, A., Brownfield, B., Buttimore, G., Cant,
M., Chakera, S., Chan, S. C. Y., Clune, J., et al. Scaling
instructable agents across many simulated worlds, 2024.
arXiv:2404.10179.

<a id='64b35a01-d467-4f81-ae04-b05b6434182c'></a>

Squire, L. R. and Kandel, E. R. Memory: From Mind to
Molecules. Roberts and Company Publishers, 2 edition,
2013.

<a id='a058b386-d611-4beb-b241-5e2af54466ca'></a>

Sutskever, I. and Patel, D. Ilya sutskever (inter-
view), November 2025. URL https://www.
dwarkeshpatel.com/p/ilya-sutskever.

<a id='4babbf02-ba6b-455d-9594-0f88698f9171'></a>

Wang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J.,
Chen, Z., Tang, J., Chen, X., Lin, Y., Zhao, W. X., Wei,
Z., and Wen, J.-R. A survey on large language model
based autonomous agents, 2023. arXiv:2308.11432.

<a id='9c3d5111-ab12-4b5e-a19e-beaed1f7be7a'></a>

13